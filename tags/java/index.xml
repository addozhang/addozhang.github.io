<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Java on 乱世浮生</title><link>https://atbug.com/tags/java/</link><description>Recent content in Java on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 27 Jan 2022 11:07:34 +0800</lastBuildDate><atom:link href="https://atbug.com/tags/java/index.xml" rel="self" type="application/rss+xml"/><item><title>使用 sdkman 在 M1 Mac 上 安装 graalvm jdk</title><link>https://atbug.com/install-graalvm-on-m1-mac-with-sdkman/</link><pubDate>Thu, 27 Jan 2022 11:07:34 +0800</pubDate><guid>https://atbug.com/install-graalvm-on-m1-mac-with-sdkman/</guid><description>
&lt;p>SDKMAN 是一款管理多版本 SDK 的工具，可以实现在多个版本间的快速切换。安装和使用非常简单：&lt;/p></description></item><item><title>应“云”而生的 Java 框架：构建本机可执行文件</title><link>https://atbug.com/quarkus-build-native-executable-file/</link><pubDate>Sat, 17 Apr 2021 09:08:40 +0800</pubDate><guid>https://atbug.com/quarkus-build-native-executable-file/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/16186233244243.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/16186233244243.jpg 使用 Page Bundles: false 电影《功夫》中，火云邪神有句话：“天下武功无坚不摧，唯快不破。” 在 上一篇文章 中，我们写了第一个 Quarkus 应用，并尝试着构建了 legacy-jar 和 fast-jar。 今天来看一下 Quarkus 构建出来的本机可执行文件到底比 Spring 应用能快多少，生态的成熟度不在这里讨论。 TLDR 先上结论， 与只有一个 Controller 的Spring Web 应用做下对比。 应用启动时间：0.012s vs 2.294s Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0900292x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0900292x.png 使用 Page Bundles: false Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0915282x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0915282x.png 使用 Page Bundles: false 镜像大小：49MB vs 237 MB Spring 应用镜像使用 openjdk:11.0-jre-slim 作为 base 镜像，大小为 220MB。 docker images REPOSITORY</description></item><item><title>应“云”而生的 Java 框架：Hello, Quarkus</title><link>https://atbug.com/hello-quarkus/</link><pubDate>Mon, 05 Apr 2021 21:08:40 +0800</pubDate><guid>https://atbug.com/hello-quarkus/</guid><description>
Wikipedia上有关 Quarkus 的信息还很少，只有一句简单的介绍： Quarkus 是专为 OpenJDK HotSpot 和 GraalVM 定制的全栈 Kubernetes 原生 Java 应用程序框架。与如 Spring 之类的其他框架相比，它提供了较小的内存占用并缩短了启动时间。它允许结合命令式和非阻塞响应式编程。 从 Quarkus 的官网，可以看到其有几个特性： 容器优先 统一了命令式和响应式编程 开发者友好 最佳品种的库及标准 更多 Quarkus 可以参考官网的介绍及文档。今天主要就是跑一下 Quarkus 的 Hello world。 放一张官网的图： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/05/16176279246527.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/05/16176279246527.jpg 使用 Page Bundles: false 环境准备 基于 Java 11 的 GraalVM Maven 3.6.2+ 笔者使用的是 macos 10.15.4，GraalVM 和 Maven 建议通过 sdkman 进行安装。 $ sdk install java 21.0.0.2.r11-grl #如果已使用其他 java 版本，可以使用命令 sdk</description></item><item><title>Java 中的 Mysql 时区问题</title><link>https://atbug.com/mysql-timezone-in-java/</link><pubDate>Thu, 14 May 2020 11:34:24 +0800</pubDate><guid>https://atbug.com/mysql-timezone-in-java/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/14/anonymouspersonwithminiatureairplaneonchalkboard37.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/14/anonymouspersonwithminiatureairplaneonchalkboard37.jpg 使用 Page Bundles: false (Photo by Andrea Piacquadio from Pexels) 话说工作十多年，mysql 还真没用几年。起初是外企银行，无法直接接触到 DB；后来一直从事架构方面，也多是解决问题为主。 这次搭建海外机房，围绕时区大家做了一番讨论。不说最终的结果是什么，期间有同事认为 DB 返回的是 UTC 时间。 这里简单做个验证，顺便看下时区的问题到底是如何处理。 环境 openjdk version &amp;ldquo;1.8.0_242&amp;rdquo; mysql-connector-java &amp;ldquo;8.0.20&amp;rdquo; mysql &amp;ldquo;5.7&amp;rdquo; 时区 TZ=Europe/London 本地时区 GMT+8 创建个简单的库test及表user， 表结构如下： CREATE TABLE `user` ( `name` varchar(50) NOT NULL, `birth_date` timestamp NULL DEFAULT CURRENT_TIMESTAMP ) ENGINE=InnoDB DEFAULT CHARSET=latin1 插入一条测试数据： mysql&amp;gt; insert into `user` -&amp;gt; values (&amp;#39;Tom&amp;#39;, time(&amp;#39;2020-05-15 08:00:00&amp;#39;)); Query OK, 1 row affected (0.01 sec) mysql&amp;gt; select * from user; +------+---------------------+ | name | birth_date | +------+---------------------+ | Tom | 2020-05-14 08:00:00 | +------+---------------------+ 1 row in set (0.00 sec) 测试代码： Connection conn = DriverManager.getConnection(&amp;#34;jdbc:mysql://localhost:3306/test?useSSL=false&amp;#34;, &amp;#34;root&amp;#34;, &amp;#34;root&amp;#34;); Statement stmt = conn.createStatement(); stmt.execute(&amp;#34;select</description></item><item><title>加速云原生的 Java 开发</title><link>https://atbug.com/speed-up-java-development-on-kubernetes/</link><pubDate>Sat, 21 Dec 2019 20:45:22 +0800</pubDate><guid>https://atbug.com/speed-up-java-development-on-kubernetes/</guid><description>
今天来说说日常在Kubernetes开发Java项目遇到的问题. 当我们新建一个项目的时候, 总是面临需要新建manifest, 平时都是copy+paste+modify. 能否以变成的方式来生成? 开发时的步骤也比较繁琐: docker build, docker push, kubectl apple, kubectl delete pod. 对于一个Java应用来说还多了一步编译. 操作一次还ok, 但是一天十几次总会有想吐的感觉. 这些步骤能否简化成一个命令, 甚至修改了代码自动就完成上面一系列的操作? 实现这些我们需要几个工具: dekorate, Jib, Skaffold. 其中Jib也在上一篇文章使用Jib为Java应用构建镜像中介绍过. dekorate Dekorate is a collection of Java compile-time generators and decorators for Kubernetes/OpenShift manifests. Dekorate是Java编译时生成和装饰Kubernetes/OpenShift</description></item><item><title>SpringBoot源码 - 启动</title><link>https://atbug.com/glance-over-spring-boot-source/</link><pubDate>Fri, 08 Dec 2017 17:48:43 +0000</pubDate><guid>https://atbug.com/glance-over-spring-boot-source/</guid><description>
SpringBoot Application启动部分的源码阅读. SpringApplication 常用的SpringApplication.run(Class, Args)启动Spring应用, 创建或者更新ApplicationContext 静态方法run 使用source类实例化一个SpringApplication实例, 并调用实例方法run. public static ConfigurableApplicationContext run(Object[] sources, String[] args) { return new SpringApplication(sources).run(args); } 初始化initialize 实例化的时候首先通过尝试加载javax.servlet.Servlet和org.springframework.web.context.ConfigurableWebApplicationContext推断当前是否是web环境. 然后从spring.facto</description></item><item><title>Kafka 恰好一次发送和事务消费示例</title><link>https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging-example/</link><pubDate>Fri, 22 Sep 2017 18:03:43 +0000</pubDate><guid>https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging-example/</guid><description>
核心思想 生产端一致性: 开启幂等和事务, 包含重试, 发送确认, 同一个连接的最大未确认请求数. 消费端一致性: 通过设置读已提交的数据和同时处理完成每一条消息之后手动提交offset. 生产端 public class ProducerTest { public static void main(String[] args) throws InterruptedException, ExecutionException { Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;#34;192.168.31.186:9092&amp;#34;); props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, &amp;#34;my-transactional-id&amp;#34;); props.put(ProducerConfig.ACKS_CONFIG, &amp;#34;all&amp;#34;); props.put(ProducerConfig.RETRIES_CONFIG, &amp;#34;3&amp;#34;); props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, &amp;#34;1&amp;#34;); Producer&amp;lt;String, String&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(props, new StringSerializer(), new StringSerializer()); producer.initTransactions(); try { producer.beginTransaction(); for (int i = 0; i &amp;lt; 5; i++) { Future&amp;lt;RecordMetadata&amp;gt; send = producer .send(new ProducerRecord&amp;lt;&amp;gt;(&amp;#34;my-topic&amp;#34;, Integer.toString(i), Integer.toString(i))); System.out.println(send.get().offset()); TimeUnit.MILLISECONDS.sleep(1000L); } producer.commitTransaction(); } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) { // We can&amp;#39;t recover from these exceptions, so our only option is to close the producer and exit. producer.close(); } catch (KafkaException e) { // For all other exceptions, just abort the transaction and try again. producer.abortTransaction(); } producer.close(); } } 消费端 public class ConsumerTest { public static void main(String[] args) throws InterruptedException { Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;#34;192.168.31.186:9092&amp;#34;); props.put(ConsumerConfig.GROUP_ID_CONFIG, &amp;#34;test&amp;#34;); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OffsetResetStrategy.NONE.toString().toLowerCase(Locale.ROOT)); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &amp;#34;false&amp;#34;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &amp;#34;org.apache.kafka.common.serialization.StringDeserializer&amp;#34;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, &amp;#34;org.apache.kafka.common.serialization.StringDeserializer&amp;#34;); props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT)); KafkaConsumer&amp;lt;String, String&amp;gt; consumer = new KafkaConsumer&amp;lt;&amp;gt;(props); consumer.subscribe(Arrays.asList(&amp;#34;my-topic&amp;#34;)); while (true) { ConsumerRecords&amp;lt;String, String&amp;gt; records = consumer.poll(100); if (!records.isEmpty()) { for (ConsumerRecord&amp;lt;String, String&amp;gt; record : records) { System.out.printf(&amp;#34;offset = %d, key = %s, value = %s%n&amp;#34;, record.offset(), record.key(), record.value()); //Manually commit each record consumer.commitSync(Collections.singletonMap(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1))); } } } } }</description></item><item><title>暴力停止ExecutorService的线程</title><link>https://atbug.com/stop-a-thread-of-executor-service/</link><pubDate>Wed, 19 Jul 2017 22:25:19 +0000</pubDate><guid>https://atbug.com/stop-a-thread-of-executor-service/</guid><description>
停止，stop，这里说的是真的停止。如何优雅的结束，这里就不提了。 这里要用Thread.stop()。众所周知，stop()方法在JDK中是废弃的。 该方法天生是不安全的。使用thread.stop()停止一个线程，导致释放（解锁）所有该线程已经锁定的监视器（因沿堆栈向上传播的未检查异常ThreadDeath而解锁）。如果之前受这些监视器保护的任何对象处于不一致状态，则不一致状态的对象（受损对象）将对其他线程可见，这可能导致任意的行为。 有时候我们会有这种需求，不需要考虑线程执行到哪一步。一般这种情况是外部执行stop，比如执行业务的线程因为各种原因假死或者耗时较长，由于设计问题又无法响应优雅的停</description></item><item><title>私有构造函数捕获模式</title><link>https://atbug.com/private-constructor-capture-idiom/</link><pubDate>Wed, 24 May 2017 06:50:44 +0000</pubDate><guid>https://atbug.com/private-constructor-capture-idiom/</guid><description>
《Java并发编程实践》的注解中有提到这一概念。 The private constructor exists to avoid the race condition that would occur if the copy constructor were implemented as this (p.x, p.y); this is an example of the private constructor capture idiom (Bloch and Gafter, 2005). 结合原文代码： @ThreadSafe public class SafePoint{ @GuardedBy(&amp;#34;this&amp;#34;) private int x,y; private SafePoint (int [] a) { this (a[0], a[1]); } public SafePoint(SafePoint p) { this (p.get()); } public SafePoint(int x, int y){ this.x = x; this.y = y; } public synchronized int[] get(){ return new int[] {x,y}; } public synchronized void set(int x, int y){ this.x = x; this.y = y; } } 这里的构造器public SafePoint(SafePoint p) { this (p.get()); }是为了捕获另一个实例的状态。get()方法是一个同步方法，为了避免竞态没有分别提供x、y的公有getter方法。 为了保证SafePoint的多线程安全性，在使用另一个实例构造新的实例时，使用了一个私有的构造器。 首先为什么不用下面这种，还是为了避免竞态（p.x和p.y调用不是原子操作）。 public SafePoint(SafePoint p) { this(p.x, p.y) } 同理，这</description></item><item><title>Docker快速构建Cassandra和Java操作</title><link>https://atbug.com/java-operate-cassandra-deployed-in-docker/</link><pubDate>Thu, 18 May 2017 23:33:24 +0000</pubDate><guid>https://atbug.com/java-operate-cassandra-deployed-in-docker/</guid><description>
搭建Cassandra 使用docker创建Cassandra，方便快捷 docker pull cassandra:latest docker run -d --name cassandra -p 9042:9042 cassandra docker exec -it cassandra bash 创建keyspace、table #cqlsh&amp;gt;#createkeyspaceCREATEKEYSPACEcontactsWITHREPLICATION={&amp;#39;class&amp;#39;:&amp;#39;SimpleStrategy&amp;#39;,&amp;#39;replication_factor&amp;#39;:1};#useUSEcontacts;#createtableCREATETABLEcontact(idUUID,emailTEXTPRIMARYKEY);查看表数据 cqlsh:contacts&amp;gt; SELECT * FROM contact; email | id -------+---- (0 rows) Java客</description></item><item><title>MetaspaceSize的坑</title><link>https://atbug.com/java8-metaspace-size-issue/</link><pubDate>Thu, 13 Apr 2017 11:55:14 +0000</pubDate><guid>https://atbug.com/java8-metaspace-size-issue/</guid><description>
这几天生产上有台机器的Metaspace一直在告警，Metaspace使用达到了97%。使用-XX:MetaspaceSize=512m，告警也还在在持续，查看MC只有81536.0，显然这个参数没起作用。 也有人遇到类似的问题，并在openjdk上提过类似的bug，其实是一个注释的bug，最终在JDK-8151845中修复了。 Class metadata is deallocated when the corresponding Java class is unloaded. Java classes are unloaded as a result of garbage collection, and garbage collections may be induced in order to unload classes and deallocate class metadata. When the space committed for class metadata reaches a certain level (a high-water mark), a garbage collection is induced. After the garbage collection, the high-water mark may be raised or lowered depending on the amount of space freed from class metadata. The high-water mark would be raised so as not to induce another garbage collection too soon. The high-water mark is initially set to the value of the command-line option MetaspaceSize. It is raised or lowered based on the options MaxMetaspaceFreeRatio and MinMetaspaceFreeRatio. If the committed space available for class metadata as a percentage of the total committed space for</description></item><item><title>一个Tomcat类加载问题</title><link>https://atbug.com/one-tomcat-class-load-issue/</link><pubDate>Wed, 12 Apr 2017 10:40:01 +0000</pubDate><guid>https://atbug.com/one-tomcat-class-load-issue/</guid><description>
背景 一个Tomcat实例中运行了三个应用，其中一个对接了Apereo的CAS系统。现在要求另外两个系统也对接CAS系统，问题就出现了： 应用启动后打开其中两个应用的任何一个，登录完成后系统都没有问题。唯独首选打开第三个，其他两个报错ClassNotFoundException: org.apache.xerces.parsers.SAXParser。 发现这个类来自xerces:xercesImpl:jar:2.6.2，使用mvn dependency:tree发现是被xom:xom:1.1简洁引用。 分析 CAS client jar中使用XMLReaderFactory创建XMLReader，首次创建会从classpa</description></item><item><title>GreenPlum JDBC和C3P0数据源</title><link>https://atbug.com/greenplum-jdbc-and-c3p0-datasource/</link><pubDate>Mon, 10 Apr 2017 08:29:00 +0000</pubDate><guid>https://atbug.com/greenplum-jdbc-and-c3p0-datasource/</guid><description>
在网上搜索GreenPlum（GPDB）的数据源配置的时候，发现搜索结果都是用postgresql的配置。 import com.mchange.v2.c3p0.DataSources; import javax.sql.DataSource; import java.sql.*; import java.util.Properties; /** * Created by addo on 2017/4/10. */ public class JDBCTest { private static String POSTGRESQL_URL = &amp;#34;jdbc:postgresql://192.168.56.101:5432/example&amp;#34;; private static String POSTGRESQL_USERNAME = &amp;#34;dbuser&amp;#34;; private static String POSTGRESQL_PASSWORD = &amp;#34;password&amp;#34;; private static String GPDB_URL = &amp;#34;jdbc:pivotal:greenplum://192.168.56.101:5432;DatabaseName=test&amp;#34;; private static String GPDB_USERNAME = &amp;#34;dbuser&amp;#34;; private static String GPDB_PASSWORD = &amp;#34;password&amp;#34;; /** * Postgresql Connection * * @return * @throws ClassNotFoundException * @throws SQLException */ public static Connection postgresqlConnection() throws ClassNotFoundException, SQLException { Class.forName(&amp;#34;org.postgresql.Driver&amp;#34;); return DriverManager.getConnection(POSTGRESQL_URL, POSTGRESQL_USERNAME, POSTGRESQL_PASSWORD); } /** * GreenPlum Connection * * @return * @throws ClassNotFoundException * @throws SQLException */ public static Connection gpdbConnection() throws ClassNotFoundException, SQLException { Class.forName(&amp;#34;com.pivotal.jdbc.GreenplumDriver&amp;#34;); return DriverManager.getConnection(GPDB_URL, GPDB_USERNAME, GPDB_PASSWORD); } /** * GreenPlud C3P0 Datasource Connection * * @return * @throws SQLException */ public static Connection gpdbC3P0Connection() throws SQLException { Properties c3p0Props = new Properties(); c3p0Props.setProperty(&amp;#34;driverClass&amp;#34;, &amp;#34;com.pivotal.jdbc.GreenplumDriver&amp;#34;); c3p0Props.setProperty(&amp;#34;jdbcUrl&amp;#34;, GPDB_URL); c3p0Props.setProperty(&amp;#34;user&amp;#34;, GPDB_USERNAME); c3p0Props.setProperty(&amp;#34;password&amp;#34;, GPDB_PASSWORD); c3p0Props.setProperty(&amp;#34;acquireIncrement&amp;#34;, &amp;#34;5&amp;#34;); c3p0Props.setProperty(&amp;#34;initialPoolSize1&amp;#34;, &amp;#34;1&amp;#34;); c3p0Props.setProperty(&amp;#34;maxIdleTime&amp;#34;, &amp;#34;60&amp;#34;); c3p0Props.setProperty(&amp;#34;maxPoolSize&amp;#34;, &amp;#34;50&amp;#34;); c3p0Props.setProperty(&amp;#34;minPoolSize&amp;#34;, &amp;#34;1&amp;#34;); c3p0Props.setProperty(&amp;#34;idleConnectionTestPeriod&amp;#34;, &amp;#34;60&amp;#34;); return DataSources.unpooledDataSource(GPDB_URL, c3p0Props).getConnection(); } public static void main(String[] args) throws ClassNotFoundException, SQLException { Connection[] connections = new Connection[]{postgresqlConnection(), gpdbConnection(), gpdbC3P0Connection()}; for (Connection connection : connections) { CallableStatement callableStatement = connection.prepareCall(&amp;#34;select * from user&amp;#34;); boolean execute = callableStatement.execute(); ResultSet resultSet = callableStatement.getResultSet(); while (resultSet.next()) { System.out.println(resultSet.getString(&amp;#34;current_user&amp;#34;)); } callableStatement.close(); connection.close(); } } } 源代码</description></item><item><title>Key长度对Redis性能影响</title><link>https://atbug.com/redis-performance-key-length/</link><pubDate>Thu, 16 Mar 2017 10:37:03 +0000</pubDate><guid>https://atbug.com/redis-performance-key-length/</guid><description>
最近Redis的使用中用的到key可能比较长，但是Redis的官方文档没提到key长度对性能的影响，故简单做了个测试。 环境 Redis和测试程序都是运行在本地，不看单次的性能，只看不同的长度堆读写性能的影响。 测试方法 使用长度分别为10, 100, 500, 1000, 2500, 5000, 7500, 10,000, and 20,000的key，value长度1000，读写1000次。 结果 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/14896309668401.jpg 链接到文件: /static//media/14896309668401.jpg 使用 Page Bundles: false Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/14896309585857.jpg 链接到文件: /static//media/14896309585857.jpg 使用 Page Bundles: false 从结果来看随着长度的增加，读写的耗时都随之增加。 长度为10：写平均耗时0.053ms，读0.040ms 长度为20000：写平均耗时0.352</description></item><item><title>遍历Collection时删除元素</title><link>https://atbug.com/remove-element-while-looping-collection/</link><pubDate>Sun, 05 Mar 2017 22:04:58 +0000</pubDate><guid>https://atbug.com/remove-element-while-looping-collection/</guid><description>
其实标题我想用《为什么foreach边循环边移除元素要用Iterator？》可是太长。 不用Iterator，用Collection.remove()，会报ConcurrentModificationException错误。 for(Integer i : list) { list.remove(i); //Throw ConcurrentModificationException } 其实使用foreach的时候，会自动生成一个Iterator来遍历list。不只是remove，使用add、clear等方法一样会出错。 拿ArrayList来说，它有一个私有的Iterator接口的内部类Itr： private class Itr implements Iterator&amp;lt;E&amp;gt; { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; //sevrval methods } 使用Iterator来遍历ArrayList实际上是通过两个指针来遍历Arr</description></item><item><title>Java Volatile关键字</title><link>https://atbug.com/deep-in-java-volatile-keywork/</link><pubDate>Thu, 02 Mar 2017 08:30:29 +0000</pubDate><guid>https://atbug.com/deep-in-java-volatile-keywork/</guid><description>
volatile通过保证对变量的读或写都是直接从内存中读取或直接写入内存中，保证了可见性；但是volatile并不足以保证线程安全，因为无法保证原子性，如count++操作： 将值从内存读入寄存器中 进行加1操作，内存保存到寄存器中 结果从寄存器flush到内存中 借用一张图来看： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://tutorials.jenkov.com/images/java-concurrency/java-volatile-2.png 链接到文件: /static/http://tutorials.jenkov.com/images/java-concurrency/java-volatile-2.png 使用 Page Bundles: false 不是volatile的变量的指令执行顺序是1-&amp;gt;2-&amp;gt;3；而声明为volatile的变量，顺序是1-&amp;gt;23。从这里看，volatile保证了一个线程修改了volatile修饰的变量，变化会马上体现在内存中。线程间看到的值是一样的。 上面说</description></item><item><title>mybatis报错“Result Maps collection already contains value for ***”</title><link>https://atbug.com/duplicate-resultmap-in-mybatis-mapper/</link><pubDate>Wed, 22 Feb 2017 14:12:18 +0000</pubDate><guid>https://atbug.com/duplicate-resultmap-in-mybatis-mapper/</guid><description>
这是工作中遇到的一个问题：测试环境部署出错，报了下面的问题。 Caused by: java.lang.IllegalArgumentException: Result Maps collection already contains value for xxx.xxx.xxxRepository.BaseResultMap at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:802) at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:774) at org.apache.ibatis.session.Configuration.addResultMap(Configuration.java:556) at org.apache.ibatis.builder.MapperBuilderAssistant.addResultMap(MapperBuilderAssistant.java:217) at org.apache.ibatis.builder.ResultMapResolver.resolve(ResultMapResolver.java:47) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:285) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:252) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElements(XMLMapperBuilder.java:244) at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:116) 检查了对应的mapper文件和java文件，已经8个多月没有修改过了。也检查了内容，没有发现重复的BaseResultMap；select中也resultMap的引用也都正确。 其实到最后发现跟代码一丁点关系都没有，是部署的时候没有删除旧版本的代码导致两个不同版本的jar同时存在，相应的mapper文件也有两个。 看了下源码，mybatis在创建SessionFactoryBean解析xml时候，会把xml中的resultMap放入到一个HashMap的子类StrictMap中，k</description></item><item><title>消费时offset被重置导致重复消费</title><link>https://atbug.com/offset-be-reset-when-consuming/</link><pubDate>Mon, 20 Feb 2017 13:23:49 +0000</pubDate><guid>https://atbug.com/offset-be-reset-when-consuming/</guid><description>
这是实际使用时遇到的问题：kafka api的版本是0.10，发现有重复消费问题；检查log后发现在commit offset的时候发生超时。 Auto offset commit failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records. 15:12:12.364 [main] WARN o.a.k.c.c.i.ConsumerCoordinator - Auto offset commit failed for group test: Commit offsets failed with retriable exception. You should retry committing offsets. 看了Kafka的API文档，发现0.10中提供了新的配置max.poll.records： The maximum number of records returned in a single call to poll(). type: int default: 2147483647 如果生产端写入很快，消费端处理耗时。一个batch的处理时间大于session.timeout.ms，会导致session time out，引起of</description></item><item><title>TheadPoolExecutor源码分析</title><link>https://atbug.com/threadpoolexecutor-sourcecode-analysis/</link><pubDate>Mon, 20 Feb 2017 09:56:07 +0000</pubDate><guid>https://atbug.com/threadpoolexecutor-sourcecode-analysis/</guid><description>
TheadPoolExecutor源码分析 ThreadPoolExecutor是多线程中经常用到的类，其使用一个线程池执行提交的任务。 实现 没有特殊需求的情况下，通常都是用Executors类的静态方法如newCachedThreadPoll来初始化ThreadPoolExecutor实例： public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&amp;lt;Runnable&amp;gt;()); } 从Executors的方法实现中看出，BlockingQueue使用的SynchronousQueue，底层使用了栈的实现。值得注意的是，这个SynchronousQueue是没有容量限制的，Executors也将maximumPoolSize设为Integer.MAX_VALUE</description></item><item><title>Kafka Java生产者模型</title><link>https://atbug.com/kafka-java-producer-model/</link><pubDate>Wed, 04 Jan 2017 16:33:02 +0000</pubDate><guid>https://atbug.com/kafka-java-producer-model/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/14835174309242.jpg 链接到文件: /static//media/14835174309242.jpg 使用 Page Bundles: false Producer初始化 初始化KafkaProducer实例，同时通过Config数据初始化MetaData、NetWorkClient、Accumulator和Sender线程。启动Sender线程。 MetaData信息 记录Cluster的相关信息，第一次链接使用Config设置，之后会从远端poll信息回来，比如host.name等信息。 Accumulator实例 Accumulator持有一个Map实例，key为TopicPartition（封装了topic和partition信息）对象，Value为RecordBatc</description></item><item><title>Redis清理缓存</title><link>https://atbug.com/clean-speicified-keys-in-redis/</link><pubDate>Tue, 13 Dec 2016 16:54:41 +0000</pubDate><guid>https://atbug.com/clean-speicified-keys-in-redis/</guid><description>
最近有个需求需要主动的去清理部分缓存，考虑的原子性的问题，用Lua脚本进行实现。 Lua脚本 local count = 0 for _,k in ipairs(redis.call(&amp;#39;KEYS&amp;#39;, ARGV[1])) do redis.call(&amp;#39;DEL&amp;#39;, k) count = count + 1 end return count shell运行 redis-cli --eval file.lua ,[KEY PATTERN] #sample: 清理所有key以Test开头的记录 redis-cli --eval clear.lua , Test* Java Jedis jedis = new Jedis(&amp;#34;127.0.0.1&amp;#34;, 6379); URL resource = Resources.getResource(&amp;#34;META-INF/scripts/clear.lua&amp;#34;); String lua = Resources.toString(resource, Charsets.UTF_8); Object eval = jedis.eval(lua, 0, &amp;#34;Name*&amp;#34;); System.out.println(eval);</description></item><item><title>探索Rabbitmq的Java客户端</title><link>https://atbug.com/deep-in-rabbitmq-java-client/</link><pubDate>Sun, 09 Oct 2016 09:20:07 +0000</pubDate><guid>https://atbug.com/deep-in-rabbitmq-java-client/</guid><description>
AMQPConnection 实例初始化 创建Connection时会通过FrameHandlerFacotry创建一个SocketFrameHandler，SocketFrameHandler对Socket进行了封装。 public AMQConnection(ConnectionParams params, FrameHandler frameHandler) { checkPreconditions(); this.username = params.getUsername(); this.password = params.getPassword(); this._frameHandler = frameHandler; this._virtualHost = params.getVirtualHost(); this._exceptionHandler = params.getExceptionHandler(); this._clientProperties = new HashMap&amp;lt;String, Object&amp;gt;(params.getClientProperties()); this.requestedFrameMax = params.getRequestedFrameMax(); this.requestedChannelMax = params.getRequestedChannelMax(); this.requestedHeartbeat = params.getRequestedHeartbeat(); this.shutdownTimeout = params.getShutdownTimeout(); this.saslConfig = params.getSaslConfig(); this.executor = params.getExecutor(); this.threadFactory = params.getThreadFactory(); this._channelManager = null; this._brokerInitiatedShutdown = false; this._inConnectionNegotiation = true; // we start out waiting for the first protocol response } 启动连接 初始化WorkService和HeartBeatSender。 创建一个channel0的AMQChannel，这个channel不会被ChannelManager管理。 首先channel0会将一个BlockingRpcContinuation作为当前未完成的Rpc请</description></item><item><title>深入剖析HashSet和HashMap实现</title><link>https://atbug.com/deep-in-implementation-of-hashset/</link><pubDate>Mon, 11 Jul 2016 14:57:16 +0000</pubDate><guid>https://atbug.com/deep-in-implementation-of-hashset/</guid><description>
HashSet是一个包含非重复元素的集合，如何实现的，要从底层实现代码看起。 背景 首先非重复元素如何定义，看Set的描述： More formally, sets contain no pair of elements e1 and e2 such that e1.equals(e2), and at most one null element. Set不会找到两个元素，并且两个元素满足e1.equals(e2)为true；并且最多只有一个null元素。 如果没有重写equals方法，查看Object类中equal方法的实现，==比较的其实是两个对象在内存中的地址。 public boolean equals(Object obj) { return (this == obj); } 说起equals方法，就不得不说hashCode方法了。Java中对于hashCode有个常规协定 The general contract of hashCode is: Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent</description></item><item><title>多线程下的单例模式+反汇编</title><link>https://atbug.com/singleton-in-multi-threads-programming/</link><pubDate>Wed, 06 Jul 2016 16:57:09 +0000</pubDate><guid>https://atbug.com/singleton-in-multi-threads-programming/</guid><description>
多线程下的单例模式的实现，顺便做了反汇编。 public class MySingleton { private static MySingleton INSTANCE; private MySingleton() { } public static MySingleton getInstance() { if (INSTANCE == null) { synchronized (MySingleton.class) { INSTANCE = new MySingleton(); } } return INSTANCE; } } Compiled from &amp;#34;MySingleton.java&amp;#34; public class MySingleton { public static MySingleton getInstance(); Code: 0: getstatic #2 // Field INSTANCE:LMySingleton; //+获得类的指定域，并压入栈顶 3: ifnonnull 32 //+不为null时跳转到行号32 6: ldc_w #3 // class MySingleton //+常量值从常量池中推送至栈顶（宽索引），推送的为地址 9: dup //+复制栈顶数值，并且复制值进栈 10: astore_0 //+将栈顶数值（objectref）存入当前 frame的局部变量数组中指定下标(index）处的变量中，栈顶数值出栈。这里存的是MySingleton类定义的地址 11: monitorenter //+获得对象锁即MySingleton地址 12: new #3 // class MySingleton //+创建一个对象，并且其引用进栈 15: dup //+复</description></item><item><title>使用Kryo替换spring amqp的Java序列化</title><link>https://atbug.com/use-kryo-in-spring-amqp-serialization/</link><pubDate>Wed, 29 Jun 2016 05:29:14 +0000</pubDate><guid>https://atbug.com/use-kryo-in-spring-amqp-serialization/</guid><description>
spring amqp的原生并没有对Kryo加以支持，Kryo的优点就不多说了。 git地址：https://github.com/addozhang/spring-kryo-messaeg-converter public class KryoMessageConverter extends AbstractMessageConverter { public static final String CONTENT_TYPE = &amp;#34;application/x-kryo&amp;#34;; public static final String DEFAULT_CHARSET = &amp;#34;UTF-8&amp;#34;; private String defaultCharset = DEFAULT_CHARSET; private KryoFactory kryoFactory = new DefaultKryoFactory(); /** * Crate a message from the payload object and message properties provided. The message id will be added to the * properties if necessary later. * * @param object the payload * @param messageProperties the message properties (headers) * @return a message */ @Override protected Message createMessage(Object object, MessageProperties messageProperties) { byte[] bytes = null; Kryo kryo = kryoFactory.create(); Output output = new ByteBufferOutput(4096, 1024 * 1024); try { kryo.writeClassAndObject(output, object); bytes = output.toBytes(); } finally { output.close(); } messageProperties.setContentType(CONTENT_TYPE); if (messageProperties.getContentEncoding() == null) { messageProperties.setContentEncoding(defaultCharset); } return new Message(bytes, messageProperties); } @Override public Object fromMessage(Message message) throws MessageConversionException { Object content = null; MessageProperties properties = message.getMessageProperties(); if (properties != null) { if (properties.getContentType() != null &amp;amp;amp;&amp;amp;amp; properties.getContentType().contains(&amp;#34;x-kryo&amp;#34;)) { Kryo kryo = kryoFactory.create(); content = kryo.readClassAndObject(new ByteBufferInput(message.getBody())); } else { throw new MessageConversionException(&amp;#34;Converter not applicable to this message&amp;#34;); } } return content; } private class DefaultKryoFactory implements KryoFactory { @Override public Kryo create() { Kryo kryo = new Kryo(); return kryo; } } }</description></item><item><title>关于SLF4J</title><link>https://atbug.com/about-slf4j/</link><pubDate>Sat, 18 Apr 2015 11:16:26 +0000</pubDate><guid>https://atbug.com/about-slf4j/</guid><description>
Spring的功能越来越强大，同时也越来越臃肿。比如想快速搭建一个基于Spring的项目，解决依赖问题非常耗时。Spring的项目模板的出现就解决了这个问题，通过这个描述文件，可以快速的找到你所需要的模板。 第一次认识SLF4J就是在这些项目模板里，它的全称是Simple Logging Facade for Java。从字面上可以看出它只是一个Facade，不提供具体的日志解决方案，只服务于各个日志系统。简单说有了它，我们就可以随意的更换日志系统（如java.util.logging、logback、log4j）。比如在开发的时候使用logback，部署的时候可以切换到log4j；如果关闭所有的log，切换到NOP就可以了。只</description></item></channel></rss>