<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on 乱世浮生</title><link>https://atbug.com/tags/kubernetes/</link><description>Recent content in Kubernetes on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 07 Feb 2022 01:00:29 +0800</lastBuildDate><atom:link href="https://atbug.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>探秘 k8e：极简 Kubernetes 发行版</title><link>https://atbug.com/explore-simple-kubernetes-distribution/</link><pubDate>Mon, 07 Feb 2022 01:00:29 +0800</pubDate><guid>https://atbug.com/explore-simple-kubernetes-distribution/</guid><description>
TL;DR 本文介绍并安装体验了极简 Kubernetes 发行版，也顺便分析学习下编译的流程。 背景 k8e 本意为 kuber easy，是一个 Kubernetes 的极简发行版，意图让云原生落地部署 Kubernetes 更轻松。k8e 是基于另一个发行版 k3s ，经过裁剪（去掉了 Edge/IoT 相关功能、traefix等）、扩展（加入 ingress、sidecar 实现、cilium等）而来。 k8e 具有以下特性： 单二进制文件，集成了 k8s 的各种组件、containerd、runc、kubectl、nerdctl 等 使用 cilium 作为 cni 的实现，方便 eBPF 的快速落地 支持基于 Pipy 的 ingress、sidecar proxy，实现应用流量一站式管理 只维护一个 k8s 版本，目前是 1.21 按照私有云的经验增加、优化代码 得益于这些特性，k8e 非常适合CI</description></item><item><title>追踪 Kubernetes 中的网络流量</title><link>https://atbug.com/tracing-path-of-kubernetes-network-packets/</link><pubDate>Sat, 22 Jan 2022 10:15:31 +0800</pubDate><guid>https://atbug.com/tracing-path-of-kubernetes-network-packets/</guid><description>
译者注： 这篇文章很全面的罗列出了 Kubernetes 中涉及的网络知识，从 Linux 内核的网络内容，到容器、Kubernetes，一一进行了详细的说明。 ​文章篇幅有点长，不得不说，网络是很复杂很麻烦的一层，但恰恰这层多年来变化不大。希望翻译的内容对大家能有所帮助，有误的地方，也欢迎大家指正。 本文翻译获得 Learnk8s 的授权，原文 Tracing the path of network traffic in Kubernetes 作者 Kristijan Mitevski。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427680911539.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427680911539.jpg 使用 Page Bundles: false TL;DR： 本文将代理了解 Kubernetes 集群内外的数据流转。从最初的 Web 请求开始，一直到托管应用程序的容器。 目录 目录 Kubernetes 网络要求 Linux 网络命名空间如果在 pod 中工作 Pause 容器创建 Pod 中的网络命名空间 为 Pod 分配了 IP 地址 检查集群中 pod 到</description></item><item><title>Kubernetes HPA 基于 Prometheus 自定义指标的可控弹性伸缩</title><link>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</link><pubDate>Tue, 18 Jan 2022 12:03:59 +0800</pubDate><guid>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</guid><description>
在《Kubernetes 的自动伸缩你用对了吗？》 一文中详细说明了如何使用 Kubernetes 的自动伸缩。在 Kubernetes 中弹性伸缩主要有三种：HPA、VPA、CA。本文不再详细说明，有兴趣的可以看那篇文章。这里主要来说下 Pod 水平缩放 HPA。 随着 Kubernetes v1.23 的发布，HPA 的 API 来到了稳定版 autoscaling/v2： 基于自定义指标的伸缩 基于多项指标的伸缩 可配置的伸缩行为 从最初的 v1 版本 HPA 只支持 CPU、内存利用率的伸缩，到后来的自定义指标、聚合层 API 的支持，到了 v1.18 版本又加入了配置伸缩行为的支持，HPA 也越来越好用、可靠。 依靠 CPU 或者内存指标的扩容并非使用所有系统，看起来也没那么可靠。对大部分的 web 后端系统来说，基于 RPS（每秒请求数）的弹性伸缩</description></item><item><title>Colima：MacOS 上的极简容器运行时和 Kubernetes（支持 m1）</title><link>https://atbug.com/containers-runtime-on-macos-with-colima/</link><pubDate>Sun, 26 Dec 2021 12:31:16 +0800</pubDate><guid>https://atbug.com/containers-runtime-on-macos-with-colima/</guid><description>
Colima 是一个以最小化设置来在MacOS上运行容器运行时和 Kubernetes 的工具。支持 m1（文末讨论），同样也支持 Linux。 Colima 的名字取自 Container on Lima。Lima 是一个虚拟机工具，可以实现自动的文件共享、端口转发以及 containerd。 Colima 实际上是通过 Lima 启动了名为 colima 的虚拟机，使用虚拟机中的 containerd 作为容器运行时。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/26/colima.gif 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/26/colima.gif 使用 Page Bundles: false 使用 Colima 的使用很简单，执行下面的命令就可以创建虚拟机，默认是 Docker 的运行时。 初次运行需要下载虚拟机镜像创建虚拟机，耗时因网络情况有所差异。之后，启动虚拟机就只需要 30s 左右的时间。 colima start INFO[0000] starting colima INFO[0000] creating and starting ... context=vm INFO[0119] provisioning ... context=docker INFO[0119] provisioning in VM ... context=docker INFO[0133] restarting VM to complete setup ... context=docker INFO[0133] stopping ... context=vm INFO[0136] starting ...</description></item><item><title>OpenFaaS - 以自己的方式运行容器化函数</title><link>https://atbug.com/openfaas-case-study-zh/</link><pubDate>Fri, 17 Dec 2021 09:13:59 +0800</pubDate><guid>https://atbug.com/openfaas-case-study-zh/</guid><description>
译者注： 本文篇幅较长，有助于了解 FaaS 和 OpenFaaS。作者分别从开发人员和运维人员的视角来了解 OpenFaaS，对了解新的技术是个很好的方式。 本文翻译自 Ivan Velichko 的 OpenFaaS - Run Containerized Functions On Your Own Terms。 长期以来，无服务器（serverless） 对我来说无非就是 AWS Lambda 的代名词。Lambda 提供了一种方便的途径，可以将任意代码附加到平台事件（云实例的状态变更、DynamoDB 记录的更新或新的 SNS 消息）中。但是，我时不时会想到某个逻辑，但其又没大到足以有自己的服务，同时有不适合任何现有服务的范围。因此，我经常将其放入函数中，以便日后使用 CLI 命令或者 HTTP 调用来调用它。 几年前，我来开了 AWS，自那以后，我一直怀念部署无服务器功</description></item><item><title>策略即代码：为了 OpenPolicyAgent 学 Rego？试试 Javascript</title><link>https://atbug.com/policy-as-code-with-pipy/</link><pubDate>Wed, 08 Dec 2021 07:49:37 +0800</pubDate><guid>https://atbug.com/policy-as-code-with-pipy/</guid><description>
距离上个版本 用 Pipy 实现 OPA，已经过去快半年了。当初使用Pipy 实现了可信镜像仓库的检查，那时的版本实现起来会稍微复杂，从策略仓库到证书创建到Admission Webhook 的创建都需要大量的人工操作，配置和逻辑也还是耦合在一起。 这个版本安装和使用起来会更加简单。 当初我用“不务正业”来形容 Pipy 实现准入控制，等看完这篇文章，欢迎留言说说你的看法。 架构 还是继续上次的场景，在 Pod 创建时对 Pod 使用的镜像所在仓库进行检查，以及检查镜像的 tag 是否合法。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/08/untitled.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/08/untitled.jpg 使用 Page Bundles: false 这里借助 Pipy Repo 的能力，将代表策略的脚本和配置交由 Repo 进行管理；Pipy 实例实时从 Pipy Repo 同步策略，并进行动态加载。</description></item><item><title>Kubernetes Deployment 的故障排查可视化指南（2021 中文版）</title><link>https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/</link><pubDate>Sat, 20 Nov 2021 18:29:19 +0800</pubDate><guid>https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/</guid><description>
将应用部署到 Kubernetes 时通常会使用 Deployment、Service、Ingress，整个应用从部署到正常运行，经历的流程很长。从 kubectl apply YAML 文件，经过 apiserver、controller manager、scheduler、kubelet、以及 CRI、CNI 等众多组件的协同工作。 漫长的“行程”，Pod 也经历各种正常和不正常的状态变化，即使正常运行也会出现服务无法访问的问题。对于刚开始在 Kubernetes 平台开展工作的同学来说，故障的排查确实棘手。之前工作的时候，经常要协助排查各种问题。去年在 Learnk8s 上看到了关于 Deployment 故障排查的视图，我还参考做了当时整个平台的故障排查视图，包括了从项目源码、CICD 流水线、部署整个流程的故障排查</description></item><item><title>Kubernetes 上调试 distroless 容器</title><link>https://atbug.com/debug-distroless-container-on-kubernetes/</link><pubDate>Wed, 03 Nov 2021 07:40:40 +0800</pubDate><guid>https://atbug.com/debug-distroless-container-on-kubernetes/</guid><description>
TL;DR 本文内容： 介绍 distroless 镜像、作用以及简单的使用 如何针对 distroless 容器的进行调试 临时容器(v.1.18+)的使用 Distroless 镜像 Distroless 容器，顾名思义使用 Distroless 镜像作为基础镜像运行的容器。 &amp;ldquo;Distroless&amp;rdquo; 镜像只包含了你的应用程序以及其运行时所需要的依赖。不包含你能在标准 Linxu 发行版里的可以找到的包管理器、shells 或者其他程序。 GoogleContainerTools/distroless 针对不同语言提供了 distroless 镜像： gcr.io/distroless/static-debian11 gcr.io/distroless/base-debian11 gcr.io/distroless/java-debian11 gcr.io/distroless/cc-debian11 gcr.io/distroless/nodejs-debian11 gcr.io/distroless/python3-debian11 Distroless 镜像有什么用？ 那些可能是构建镜像时需要的，但大部分并不是运行时需要的。这也是为什么上篇文章介绍 Buildpacks 时说的一个 builder 的 stack 镜像包含构建时基础镜像和运行时基础镜像，这样可以做到镜像的最小化。 其实控制体积并不是 distroless 镜像的主要作用。将运行时容器中的内容限制为应用程序所需的依赖，此外不应该安装任何</description></item><item><title>自动替换 Kubernetes 镜像</title><link>https://atbug.com/kubernetes-images-swapper/</link><pubDate>Wed, 06 Oct 2021 08:01:41 +0800</pubDate><guid>https://atbug.com/kubernetes-images-swapper/</guid><description>
前几天有朋友在问如何在某云上拉取 Tekton 的镜像，这种情况其实比较普遍不只是某云。工作中经常要用到过某些靠运气才能拉取到的镜像，这对工作来说真是极度的不友好。 因此也萌生了个想法，维护一个后网络友好的仓库镜像，在 Pod 创建时将镜像仓库切换到自维护的仓库，从自维护的仓库拉取镜像。 前几天体验了极狐Gitlab 的容器镜像库，便是为这个想法做的准备。当然其他的云厂商也有提供针对个人版的免费镜像仓库和企业版仓库。 正好 Pipy 作为策略引擎，非常适合实现这种策略的执行。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005212733.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005212733.png 使用 Page Bundles: false 实现思路 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005210725.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005210725.png 使用 Page Bundles: false Admission Webhook Kubernetes 动</description></item><item><title>ARM64 平台基于 openEuler + iSula 环境部署 Kubernetes</title><link>https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/</link><pubDate>Thu, 02 Sep 2021 20:41:06 +0800</pubDate><guid>https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/</guid><description>
为什么要在 arm64 平台上部署 Kubernetes，而且还是鲲鹏 920 的架构。说来话长 。。。 此处省略5000 字。 介绍下系统信息； 架构：鲲鹏 920(Kunpeng920) OS：openEuler 20.03 (LTS-SP1) CPU：4c 内存：16G 硬盘：若干 整个过程虽然参考了鲲鹏论坛的帖子，不过还是颇费周折。 TL;DR 整个过程中要注意 arm64 平台上安装 Kubernetes 及网络组件，需要使用 arm64 版本的镜像。 环境配置 1.关闭 selinux #临时关闭 setenforce 0 #永久关闭 SELINUX=disabled vim /etc/sysconfig/selinux 2. 关闭swap分区 #临时关闭 swapoff -a #永久关闭 注释 swap 行 vim /etc/fstab 3. 关闭防火墙 systemctl stop firewalld ssystemctl disable firewalld 4. 网络配置 对iptables内部的nf-call需要打开的内生的桥接功能 vim /etc/sysctl.d/k8s.conf 修改如下内容： net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm_swappiness=0 修改完成后执行： modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf 5. 添加 Kubernetes 源 在文件 /etc/yum.repos.d/openEuler.repo 中追加如下内容： [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64/ enabled=1</description></item><item><title>Kubernetes 必备工具：2021</title><link>https://atbug.com/translation-kuberletes-essential-tools-2021/</link><pubDate>Thu, 15 Jul 2021 08:10:22 +0800</pubDate><guid>https://atbug.com/translation-kuberletes-essential-tools-2021/</guid><description>
有别于前些天的文章 - 常用的几款工具让 Kubernetes 集群上的工作更容易 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。 文档翻译自 Kubernetes Essential Tools: 2021，篇幅较长，做了部分增删。 介绍 在本文中，我将尝试总结我最喜欢的 Kubernetes 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。 这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。 K3D K3D 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常轻巧且速</description></item><item><title>Rego 不好用？用 Pipy 实现 OPA</title><link>https://atbug.com/pipy-implement-kubernetes-admission-control/</link><pubDate>Tue, 13 Jul 2021 08:44:56 +0800</pubDate><guid>https://atbug.com/pipy-implement-kubernetes-admission-control/</guid><description>
还不知道 Pipy 是什么的同学可以看下 GitHub 。 Pipy 是一个轻量级、高性能、高稳定、可编程的网络代理。Pipy 核心框架使用 C++ 开发，网络 IO 采用 ASIO 库。 Pipy 的可执行文件仅有 5M 左右，运行期的内存占用 10M 左右，因此 Pipy 非常适合做 Sidecar proxy。 Pipy 内置了自研的 pjs 作为脚本扩展，使得Pipy 可以用 JS 脚本根据特定需求快速定制逻辑与功能。 Pipy 采用了模块化、链式的处理架构，用顺序执行的模块来对网络数据块进行处理。这种简单的架构使得 Pipy 底层简单可靠，同时具备了动态编排流量的能力，兼顾了简单和灵活。通过使用 REUSE_PORT 的机制（主流 Linux 和 BSD 版本都支持该功能），Pipy 可以以多进程模式运行，使得 Pipy 不仅适用于 Sidecar 模式，也适用于大规模的流量处理场景。 在实践中，Pipy 独</description></item><item><title>Kubernetes 的魔力在于企业标准化，而不是应用程序的可移植性</title><link>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</link><pubDate>Sun, 11 Jul 2021 08:05:42 +0800</pubDate><guid>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</guid><description>
笔者：Kubernetes 抽象了资源和工作负载的操作模式，统一了工具集，实现人机接口的标准化。正如类 Docker 工具提供了应用运行时的操作模式；Spring Framework 提供了 Java 应用的开发模式。 Kubernetes 是关于跨云的技能、工具和实践的可移植性。不是工作负载的可移植性。 &amp;ndash; Bilgin Lbryam @bibryam 本文翻译自 Kubernetes magic is in enterprise standardization, not app portability Kubernetes 不会神奇地使你的应用程序具有可移植性，但它可能会给你带来更好的东西。 云为企业提供了看似无限的选择。然而，根据 Canonical-sponsored 的一项调查，这并不是大多数企业采用 Kubernetes 等云友好技术的原因。相反，Kubernetes 的主要目标是标准化——外观和操作与其他人一样。 可移植性不是目标 我之前已经讨论过这个问题，参考了 Gartner 关于 Kubernetes 和可移植性的指南。许多人认为 K</description></item><item><title>Kubernetes CKA 证书备考笔记</title><link>https://atbug.com/notes-for-cka-preparation/</link><pubDate>Fri, 02 Jul 2021 08:02:15 +0800</pubDate><guid>https://atbug.com/notes-for-cka-preparation/</guid><description>
Kubernetes 使用有好几年了，但在今年 5 月才完成 CKA 的考试。虽说用了几年，还是提前刷了部分题熟悉下。 绝大部分题都是有在 minikube 的环境上操作过，只有部分比如升级集群受限于环境问题没有实地操作。 写在最前 保存常用文档进书签，如果有 Alfred 启用浏览器书签 workflow。效果见下图 kubectl 自动补全 echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc; source ~/.bashrc 每道题开始前要切换 context 和 namespace，直接复制题目里的命令即可 必要的 alias 善用 --dry-run=client -o yaml 避免手动敲太多 善用 kubectl explain [resource[.field]] 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 书签地址：K8s-CKA-CAKD-Bookmarks</description></item><item><title>常用的几款工具让 Kubernetes 集群上的工作更容易</title><link>https://atbug.com/tools-accelerate-work-on-kubernetes-cluster/</link><pubDate>Sat, 26 Jun 2021 12:16:28 +0800</pubDate><guid>https://atbug.com/tools-accelerate-work-on-kubernetes-cluster/</guid><description>
之前写过一篇 介绍了工具加速云原生 Java 开发。 其实日常工作中在集群上的操作也非常多，今天就来介绍我所使用的工具。 kubectl-alias 使用频率最高的工具，我自己稍微修改了一下，加入了 StatefulSet 的支持。 这个是我的 https://github.com/addozhang/kubectl-aliases，基于 https://github.com/ahmetb/kubectl-aliases。 比如输出某个 pod 的 json，kgpoojson xxx 等同于 kubectl get pod xxx -o json。 结合 jq 使用效果更好 😂。 语法解读 k=kubectl sys=--namespace kube-system commands: g=get d=describe rm=delete a:apply -f ak:apply -k k:kustomize ex: exec -i -t lo: logs -f resources: po=pod, dep=deployment, ing=ingress, svc=service, cm=configmap, sec=secret,ns=namespace, no=node flags: output format: oyaml, ojson, owide all: --all or --all-namespaces depending on the command sl: --show-labels w=-w/--watch value flags (should be at the end): n=-n/--namespace f=-f/--filename l=-l/--selector kubectx + kubens 安装看这里 kubectx 用于在不同的集群间</description></item><item><title>Jenkins 如何与 Kubernetes 集群的 Tekton Pipeline 交互？</title><link>https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/</link><pubDate>Wed, 23 Jun 2021 07:58:45 +0800</pubDate><guid>https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/</guid><description>
本文详细介绍了 Jenkins 如何通过 tekton-client-plugin 实现与 Kubernetes 上的 Tekton Pipeline 交互，包括 Kubernetes 上安装 Jenkins、Tekton Pipelines 等。 关于如何使用 Tekton Pipeline 实现 CICD 可以看这篇文章 云原生 CICD: Tekton Pipeline 实战 本文用于构建的项目以及所有 manifest yaml 都在可以在这里下载。 TL;DR 惯例，先上总结。tekton-client-plugin 虽然还是处于初期阶段，但是 其价值非常明显，尤其是对先用使用 Jenkins 作为 CICD 实现的用户来说。从 Jenkins 迁移到云原生的 Tekton 时，可以省掉用户界面的开发成本，而且尽可能少的改变用户习惯 ，依靠版本管理可以控制迁移的节奏。 tekton-client-plugin 在今年 5 月 7 日发布的 1.0.0 版本，目前为 1.0.02。目前还处于初期阶段，我个人感觉目前仅仅算是打通 Jenkins 与 Tekton 交互这条路，扩展性还不够好。 比如目前仅仅支持如下几</description></item><item><title>云原生 CICD: Tekton Pipeline 实战</title><link>https://atbug.com/tekton-pipeline-practice/</link><pubDate>Tue, 22 Jun 2021 07:19:33 +0800</pubDate><guid>https://atbug.com/tekton-pipeline-practice/</guid><description>
更新历史： v1：2020.1.21 基于 Tekton Pipline v0.9.0 v2（当前）：2021.6.22 基于 Tekton Pipeline v0.25.0 Tekton 是 Google 开源的 Kubernetes 原生CI/CD 系统, 功能强大扩展性强. 前身是 Knavite 里的 build-pipeline 项目, 后期孵化成独立的项目. 并成为 CDF 下的四个项目之一, 其他三个分别是 Jenkins, Jenkins X, Spinnaker. 为什么说 Tekton 是 Kubernetes 原生的, 以内其基于 Kubernetes 的 CRD 定义了 Pipeline 流水线. Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/uPic/bquuTV.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/uPic/bquuTV.jpg 使用 Page Bundles: false CRD 及说明: Task: 构建任务, 可以定义一些列的 steps. 每个 step 由一个 container 执行. TaskRun: task 实际的执行, 并提供执行所需的参数. 这个对象创建后, 就会有 pod 被创建. Pipeline: 定义一个或者多个 task 的执行, 以及 PipelineResource 和各种定义参数的集合 PipelineRun: 类似 task 和 taskrun 的关系: 一个定义一个执行. PipelineRun 则是 pipeline 的实际执行. 创建</description></item><item><title>源码解析：一文读懂 Kubelet</title><link>https://atbug.com/kubelet-source-code-analysis/</link><pubDate>Tue, 15 Jun 2021 08:25:25 +0800</pubDate><guid>https://atbug.com/kubelet-source-code-analysis/</guid><description>
本文主要介绍 kubelet 功能、核心组件，以及启动流程的源码分析，总结了 kubelet 的工作原理。 kubelet 简介 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/15/20210613091144.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/15/20210613091144.png 使用 Page Bundles: false 从官方的架构图中很容易就能找到 kubelet 执行 kubelet -h 看到 kubelet 的功能介绍： kubelet 是每个 Node 节点上都运行的主要“节点代理”。使用如下的一个向 apiserver 注册 Node 节点：主机的 hostname；覆盖 host 的参数；或者云提供商指定的逻辑。 kubelet 基于 PodSpec 工作。PodSpec 是用 YAML 或者 JSON 对象来描述 Pod。Kubelet 接受通过各种机制（主要是 apiserver）提供的一组 PodSpec，并确保里面描述的容器良好运行。 除了由 apiserver 提供 PodSpec，还可以通过以下方式提供： 文件 HTTP 端点 HTTP 服务器 kubelet 功能归纳一下</description></item><item><title>Kubernetes 上如何控制容器的启动顺序？</title><link>https://atbug.com/k8s-1.18-container-start-sequence-control/</link><pubDate>Fri, 30 Apr 2021 07:43:54 +0800</pubDate><guid>https://atbug.com/k8s-1.18-container-start-sequence-control/</guid><description>
去年写过一篇博客：控制 Pod 内容器的启动顺序，分析了 TektonCD 的容器启动控制的原理。 为什么要做容器启动顺序控制？我们都知道 Pod 中除了 init-container 之外，是允许添加多个容器的。类似 TektonCD 中 task 和 step 的概念就分别与 pod 和 container 对应，而 step 是按照顺序执行的。此外还有服务网格的场景，sidecar 容器需要在服务容器启动之前完成配置的加载，也需要对容器的启动顺序加以控制。否则，服务容器先启动，而 sidecar 还无法提供网络上的支持。 现实 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle1.gif 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle1.gif 使用 Page Bundles: false 期望 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle2.gif 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle2.gif 使用 Page Bundles: false 到了这里肯定有同学会问，spec.containers[] 是一个数组，数组是</description></item><item><title>服务网格平稳落地：Istio 中精准控制 Sidecar 的注入</title><link>https://atbug.com/how-to-control-istio-sidecar-injection/</link><pubDate>Wed, 21 Apr 2021 08:13:04 +0800</pubDate><guid>https://atbug.com/how-to-control-istio-sidecar-injection/</guid><description>
为什么 说起服务网格，这幅图大家肯定不会陌生。这就是服务网格的网络，也是网格架构的终极形态。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2314142x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2314142x.png 使用 Page Bundles: false 那在迁移到网格架构之前，我们的系统是什么样的？ Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2316432x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2316432x.png 使用 Page Bundles: false 我们的系统在演进的过程中，不可避免的会遇到各种 0 到 1 过程中的中间态。比如下面这种，可以比较直观的看出 Istio 或者网格是部分覆盖的。这个过程中，我们需要平滑、可控的推进，才能在保障系统可用性的前提下进行架构的演进。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2318242x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2318242x.png 使用 Page Bundles: false 怎么做 Sidecar 的注入分两种：手动和自动。 手动 手动</description></item><item><title>分布式系统在 Kubernetes 上的进化</title><link>https://atbug.com/translation-distributed-systems-kubernetes/</link><pubDate>Mon, 29 Mar 2021 23:11:25 +0800</pubDate><guid>https://atbug.com/translation-distributed-systems-kubernetes/</guid><description>
本文译自 The Evolution of Distributed Systems on Kubernetes 在 3 月份的 QCon 上，我做了一个关于 Kubernetes 的分布式系统进化的演讲。首先，我想先问一个问题，微服务之后是什么？我相信大家都有各自的答案，我也有我的答案。你会在最后发现我的想法是什么。为了达到这个目的，我建议大家看看分布式系统的需求是什么？以及这些需求在过去是如何发展的，从单体应用开始到 Kubernetes，再到最近的 Dapr、Istio、Knative 等项目，它们是如何改变我们做分布式系统的方式。我们将尝试对未来做一些预测。 现代分布式应用 为了给这个话题提供更多的背景信息，我认为的分布式系统是由数百个组件组成的系统。这些组件可以是有状态的、无状态的或者无服务器的。此外，这些组件可以用不</description></item><item><title>Kubernetes 源码解析 - Informer</title><link>https://atbug.com/kubernetes-source-code-how-informer-work/</link><pubDate>Sun, 16 Aug 2020 23:32:38 +0800</pubDate><guid>https://atbug.com/kubernetes-source-code-how-informer-work/</guid><description>
上篇扒了 HPA 的源码，但是没深入细节，今天往细节深入。 开局先祭出一张图： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/08/16/15975802542217.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/08/16/15975802542217.png 使用 Page Bundles: false 为什么要有 Informer？ Kubernetes 中的持久化数据保存在 etcd中，各个组件并不会直接访问 etcd，而是通过 api-server暴露的 RESTful 接口对集群进行访问和控制。 资源的控制器（图中右侧灰色的部分）读取数据也并不会直接从 api-server 中获取资源信息（这样会增加 api-server 的压力），而是从其“本地缓存”中读取。这个“本地缓存”只是表象的存在，加上缓存的同步逻辑就是今天要是说的Informer（灰色区域中的第一个蓝色块）所提供的功能。 从图中可以看到 Informer 的几个组件： Reflector：与 api</description></item><item><title>Kubernetes 源码解析 - HPA 水平自动伸缩如何工作</title><link>https://atbug.com/kubernetes-source-code-how-hpa-work/</link><pubDate>Sat, 15 Aug 2020 02:09:37 +0800</pubDate><guid>https://atbug.com/kubernetes-source-code-how-hpa-work/</guid><description>
HPA - Horizontal Pod Autoscaler 的缩写，Pod 水平自动伸缩。通过对 Pod 负载的监控，来自动增加或者减少 Pod 的副本数量。 从字面意思来看，其主要包含了两部分： 监控 Pod 的负载 控制 Pod 的副本数量 那具体是如何实现的呢？以下基于1.17 源码，来分析下 HPA 如何工作。 注意：文章中的代码在源码的基础上进行了精简：删掉了注释、序列化等信息，或保留了部分核心代码，加上新的注释。 资源 HPA 的资源是HorizontalPodAutoscaler，在v1版本中，只支持基于 CPU 指标的计算；在v2beta2版本中加入了基于内存和自定义指标的计算。 v1 //staging/src/k8s.io/api/autoscaling/v1/types.go type HorizontalPodAutoscaler struct { metav1.TypeMeta metav1.ObjectMeta Spec HorizontalPodAutoscalerSpec Status HorizontalPodAutoscalerStatus } type HorizontalPodAutoscalerSpec struct { ScaleTargetRef CrossVersionObjectReference //监控的目标资源 MinReplicas *int32 //最小副本数 MaxReplicas int32 //最大副本数 TargetCPUUtilizationPercentage *int32 //触发调整的CPU 使用</description></item><item><title>Tekton 的工作原理</title><link>https://atbug.com/how-tekton-works/</link><pubDate>Sat, 23 May 2020 22:47:14 +0800</pubDate><guid>https://atbug.com/how-tekton-works/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/24/tekton.jpeg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/24/tekton.jpeg 使用 Page Bundles: false 这篇文章是基于 Tekton Pipeline 的最新版本v0.12.1版本。 快速入门请参考：云原生 CICD: Tekton Pipeline 实战 ，实战是基于版本 v0.10.x。 Pipeline CRD 与核心资源的关系 $ k api-resources --api-group=tekton.dev NAME SHORTNAMES APIGROUP NAMESPACED KIND clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task Tekton Pipelines提供了上面的CRD，其中部分CRD与k8s core中资源相对应 Task =&amp;gt; Pod Task.Step =&amp;gt; Container Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902164552270.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902164552270.jpg 使用 Page Bundles: false 工作原理 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902280074872.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902280074872.jpg 使用 Page Bundles: false (图片来自) Tekton Pipeline 是基于 Knative 的实现，pod tekton-pipelines-controller 中有两个 Knative Controller的</description></item><item><title>控制 Pod 内容器的启动顺序</title><link>https://atbug.com/control-process-order-of-pod-containers/</link><pubDate>Thu, 12 Mar 2020 22:05:16 +0800</pubDate><guid>https://atbug.com/control-process-order-of-pod-containers/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/screenshot-20210430-at-092623.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/screenshot-20210430-at-092623.png 使用 Page Bundles: false 2021.4.30 更新： 最新的方案，请跳转新篇 Kubernetes 上如何控制容器的启动顺序。 背景 众所周知, Kubernetes Pod 内有两种容器: 初始化容器(init container)和应用容器(app container). 其中初始化容器的执行先于应用容器, 并且初始化容器和应用容器的个数分别为 0~n 和 1~n. 初始化容器会按照顺序执行, 顺序执行的前提是初始化容器始终会运行到完成(completed)状态. 而应用容器恰好相反: 启动顺序随机, 并始终保持运行(running)状态. 问题 工作中有个架构的方案使用到了 sidecar 容器: 将基础组件功能从容器转移到 sidecar 容器中, 其中有个功能是从远程配置中心获取配置并保持实时更新. 保证实</description></item><item><title>Tekton Trigger 介绍</title><link>https://atbug.com/tekton-trigger-glance/</link><pubDate>Wed, 05 Feb 2020 18:03:15 +0800</pubDate><guid>https://atbug.com/tekton-trigger-glance/</guid><description>
背景 Tekton 的介绍请参考Tekton Pipeline 实战. 通常, CI/CD 事件应该包含如下信息: 确定事件的类型(比如 GitHub Push, GitLab Issue, Docker Hub Webhook 等) 可从特定管道访问并映射到特定管道 (从事件负载中获取 SHA 信息, 然后在管道中使用) 准确地触发管道 (基于有效负载值触发管道) Tekton API 的设计分离了配置(比如 PipelineRun VS Pipeline), 保证了 step 可以被重用. 但是没有提供动态封装配置的机制来生成资源(尤其是 PipelineRun 和 PipelineResource). Triggers 通过下面的 CRDs 在架构上对 Tekton 进行了扩展: TriggerTemplate: 创建资源的模板(比如用来创建 PipelineResource 和 PipelineRun) TriggerBinding: 校验事件并提取负载字段 EventListener: 连接TriggerBinding和TriggerTemplate到可寻址的端点(事件接收器). 使用从各个 TriggerBinding中提取的参数来创建TriggerTemp</description></item><item><title>Tekton Dashboard 安装</title><link>https://atbug.com/tekton-dashboard-installation/</link><pubDate>Sat, 01 Feb 2020 12:39:28 +0800</pubDate><guid>https://atbug.com/tekton-dashboard-installation/</guid><description>
Tekton 提供了dashboard方便用户管理和查看 Tekton PipelineRun 和 TaskRun 以及创建, 执行和完成过程中涉及的资源. 它还允许按标签过滤 PipelineRun 和 TaskRun. 安装方法 kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/v0.4.1/dashboard_latest_release.yaml 检查dashboard的运行情况, STATUS为Running的话则说明运行成功. kubectl get pods --namespace tekton-pipelines 访问 访问Tekton的Dashboard有两种方式, 一种是通过port-forward, 另一种是通过ingress来访问. port-forward kubectl port-forward svc/tekton-dashboard 9097 ingress 先检查ingress是否开启. minikube addon list ... - ingress: enabled ... 如果是disabled的话, 通过命令minikube addons enable ingress. 注意: 这里拉取quay.io/kubernetes-ingress-controller/nginx-ingress-controller镜</description></item><item><title>Tekton安装及Hello world</title><link>https://atbug.com/tekton-installation-and-sample/</link><pubDate>Fri, 17 Jan 2020 19:17:14 +0800</pubDate><guid>https://atbug.com/tekton-installation-and-sample/</guid><description>
安装 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 检查安装的tekton相关的CRD: kubectl api-resources | grep tekton clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task tekton的两个pod: kubectl get pods --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-556d8f4494-2qthv 1/1 Running 0 11m tekton-pipelines-webhook-849cff5cf-8m5qq 1/1 Running 0 11m 安装CLI cli: https://github.com/tektoncd/cli#installing-tkn brew install tektoncd-cli Tekton: hello world 创建一个简单的Task, 只有一个step就是打印出&amp;quot;hello world&amp;quot; apiVersion:tekton.dev/v1alpha1kind:Taskmetadata:name:echo-hello-worldspec:steps:- name:echoimage:alpinecommand:- echoargs:- &amp;#34;hello world&amp;#34;创建一个TaskRun执行上面的Task apiVersion:tekton.dev/v1alpha1kind:TaskRunmetadata:name:echo-hello-world-task-runspec:taskRef:name:echo-hello-world运行task: kubectl apply</description></item><item><title>Minikube安装istio</title><link>https://atbug.com/install-istio-on-minikube/</link><pubDate>Fri, 17 Jan 2020 08:02:42 +0800</pubDate><guid>https://atbug.com/install-istio-on-minikube/</guid><description>
准备 注意: istioctl的安装要使用安装里的, 不要是用homebrew里的. github issue curl -L https://istio.io/downloadIstio | sh - cd istio-1.4.2 cp bin/istioctl /usr/local/bin/istioctl 安装前检查 istioctl verify-install 如果检查没问题, 会看到Install Pre-Check passed! The cluster is ready for Istio installation. 安装 istio有5种内建的安装配置1: remote, sds, default, demo, minimal istioctl profile list minimal: 使用istio的流量管理所需组件的最小化安装 default: 根据IstioControlPlane API的默认设置(建议用于生产部署)启用组件. 您可以通过运行命令istioctl profile dump显示默认设置. demo: 几乎安装所有的特性, 包括logging和tracing的比例为100%. 不适合生产环境, 负载太重 default demo minimal sds remote Core components istio-citadel X X X X istio-egressgateway X istio-galley X X X istio-ingressgateway X X X istio-nodeagent X istio-pilot X X X X istio-policy X X X istio-sidecar-injector X X X X istio-telemetry X X X Addons</description></item><item><title>加速云原生的 Java 开发</title><link>https://atbug.com/speed-up-java-development-on-kubernetes/</link><pubDate>Sat, 21 Dec 2019 20:45:22 +0800</pubDate><guid>https://atbug.com/speed-up-java-development-on-kubernetes/</guid><description>
今天来说说日常在Kubernetes开发Java项目遇到的问题. 当我们新建一个项目的时候, 总是面临需要新建manifest, 平时都是copy+paste+modify. 能否以变成的方式来生成? 开发时的步骤也比较繁琐: docker build, docker push, kubectl apple, kubectl delete pod. 对于一个Java应用来说还多了一步编译. 操作一次还ok, 但是一天十几次总会有想吐的感觉. 这些步骤能否简化成一个命令, 甚至修改了代码自动就完成上面一系列的操作? 实现这些我们需要几个工具: dekorate, Jib, Skaffold. 其中Jib也在上一篇文章使用Jib为Java应用构建镜像中介绍过. dekorate Dekorate is a collection of Java compile-time generators and decorators for Kubernetes/OpenShift manifests. Dekorate是Java编译时生成和装饰Kubernetes/OpenShift</description></item><item><title>Kubernetes中的Nginx动态解析</title><link>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</link><pubDate>Wed, 30 May 2018 12:10:32 +0000</pubDate><guid>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</guid><description>
背景 Nginx运行在kubernets中, 反向代理service提供服务. kubernetes版本v1.9.1+a0ce1bc657. 问题: 配置如下: location ^~/info { proxy_pass: http://serviceName:port; } 删除并重建Service的时候, nginx会出现下面的问题: connect() failed (113: No route to host) &amp;hellip; upstream: &amp;ldquo;xxxxx&amp;rdquo; 分析 通过google发现, 是nginx的dns解析方案的问题. nginx官方的说明: If the domain name can’t be resolved, NGINX fails to start or reload its configuration. NGINX caches the DNS records until the next restart or configuration reload, ignoring the records’ TTL values. We can’t specify another load‑balancing algorithm, nor can we configure passive health checks or other features defined by parameters to the server directive, which we’ll describe in the next section. 意思是说, nginx在启动的时候就会解析proxy_pass后的域名, 并把ip缓存下来</description></item><item><title>Kubernetes — 持久卷</title><link>https://atbug.com/kubernetes-persistent-volumes/</link><pubDate>Sun, 20 Aug 2017 22:25:40 +0000</pubDate><guid>https://atbug.com/kubernetes-persistent-volumes/</guid><description>
Persistent Volume 译自Persistent Volumes 介绍 管理存储是管理计算的独特问题。 PersistentVolume子系统为用户和管理员提供了一个API，其中提供了如何从如何使用存储提供存储的详细信息。为此，我们介绍两种新的API资源：PersistentVolume和PersistentVolumeClaim。 PersistentVolume（PV）是由管理员配置的集群中的一段存储。它是集群中的一种资源就像一个节点是一个集群的资源。 PV是类似Volumes的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 Persistent</description></item><item><title>Kubernetes学习 — Macos安装Kubernetes</title><link>https://atbug.com/install-kubernetes-on-macos/</link><pubDate>Thu, 17 Aug 2017 09:44:17 +0000</pubDate><guid>https://atbug.com/install-kubernetes-on-macos/</guid><description>
Kubernetes 安装 macos 检查环境 sysctl -a | grep machdep.cpu.features | grep VMX 安装VirtualBox http://download.virtualbox.org/virtualbox/5.1.26/Oracle_VM_VirtualBox_Extension_Pack-5.1.26-117224.vbox-extpack 安装minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.21.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ 创建集群 默认使用virtualbox。 主机的ip是192.168.31.186， 1087是proxy的端口。需要将ss的http代理监听地址从127.0.0.1改为主机的ip。 #启动 minikube start #使用私有库 minikube start --insecure-registry=&amp;#34;192.168.31.34&amp;#34; #使用proxy，用于获取镜像 minikube start --docker-env HTTP_PROXY=&amp;#34;192.168.31.186:1087&amp;#34; --docker-env HTTPS_PROXY=&amp;#34;192.168.31.186:1087&amp;#34; --docker-env NO_PROXY=192.168.99.0/24 安装kubectl curl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.7.3/bin/darwin/amd64/kubectl &amp;amp;&amp;amp; chmod +x kubectl &amp;amp;&amp;amp; sudo mv kubectl /usr/local/bin/ oh-my-zsh tab completion vi ~/.zshrc 添加到plugin部分 plugins=(git zsh-completions kubectl) 使用 minikube 检查版本 minikube version #minikube version: v0.21.0 kubectl version #Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;3&amp;#34;, GitVersion:&amp;#34;v1.3.0&amp;#34;, GitCommit:&amp;#34;283137936a498aed572ee22af6774b6fb6e9fd94&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2016-07-01T19:26:38Z&amp;#34;, GoVersion:&amp;#34;go1.6.2&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;} #Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;7&amp;#34;, GitVersion:&amp;#34;v1.7.0&amp;#34;, GitCommit:&amp;#34;d3ada0119e776222f11ec7945e6d860061339aad&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2017-07-26T00:12:31Z&amp;#34;, GoVersion:&amp;#34;go1.8.3&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} 获取集群地址 minikube ip 192.168.99.100 获取服务列表 minikube service list 打开dashboard minikube dashboard kubectl 部署Dashboard UI 默认</description></item></channel></rss>