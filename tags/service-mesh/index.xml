<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Service Mesh on 乱世浮生</title><link>https://atbug.com/tags/service-mesh/</link><description>Recent content in Service Mesh on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 08 Dec 2023 07:15:43 +0800</lastBuildDate><atom:link href="https://atbug.com/tags/service-mesh/index.xml" rel="self" type="application/rss+xml"/><item><title>探索服务网格与 OpenTelemetry 的协同之分布式跟踪</title><link>https://atbug.com/integrate-service-mesh-with-opentelemetry-for-distributed-tracing/</link><pubDate>Fri, 08 Dec 2023 07:15:43 +0800</pubDate><guid>https://atbug.com/integrate-service-mesh-with-opentelemetry-for-distributed-tracing/</guid><description>在上一篇文章中，介绍了 如何在 k8s 中无侵入安装 Otel 探针 并实现了无侵入（某些语言还无法实现，比如 Go 的 eBPF 对内核的苛刻要求）的分布式跟踪。
这篇文章发出后有读者评论 javaagent 的“无侵入”一说，这里有必要解释下。“无侵入”主要指的是不需要修改应用程序的业务逻辑代码就能实现的功能，对应用程序透明无感知，让开发者专注于业务开发；同时由于无需修改应用程序代码，更易于集成；同时还维护简单，在多种语言、框架间保证功能的一致性。
而 Java Agent 在 JVM 启动时加载，它在运行时修改字节码来注入跟踪代码，而不是在应用程序的源代码层面上进行修改。
背景 分布式跟踪 分布式跟踪是监控和诊断微服务请求流程的关键技术，也是可观测性的关键组成部分，提供了对微服务架构中复杂交互和性能问题的深入洞察。它通过提供服务间请求链路的清晰视图来管理复杂性，并帮助识别性能瓶颈、优化资源分配、快速定位和解决故障，提高系统的整体可靠性。
服务网格的无侵入式分布式跟踪 又是无侵入性！服务网格中的代理自动处理所有入站和出站的网络通信，自动捕获、记录和分析服务间的请求和响应的详细细心，如请求时间、持续时间、状态代码和其他元数据。这种 实现方式 对应用程序本身透明，并且较 Java Agent 在运行时修改字节码更加彻底。
这里有个前提是应用程序能够在请求中传递上下文信息，这样 sidecar 代理生成和发送的跟踪信息最终可以串联在一起，不会发生断链。
网格的无侵入式分布式跟踪虽然为我们展示了请求的链路，但是如上图所示每个跨度（span）都是 sidecar 代理的信息。
紧跟上篇文章之后，我们今天将探索 服务网格 FSM 与 OpenTelemetry 的集成，实现应用、网格的全链路分布式跟踪。</description></item><item><title>Netflix 零配置服务网格与按需集群发现</title><link>https://atbug.com/translation-zero-configuration-service-mesh-with-on-demand-cluster-discovery/</link><pubDate>Mon, 25 Sep 2023 12:39:05 +0800</pubDate><guid>https://atbug.com/translation-zero-configuration-service-mesh-with-on-demand-cluster-discovery/</guid><description>本文翻译自由 David Vroom, James Mulcahy, Ling Yuan, Rob Gulewich 编写的 Netflix 博客 Zero Configuration Service Mesh with On-Demand Cluster Discovery。
Netflix 相信大家并不陌生，在 Spring Cloud 生态中就有 Netflix 全家桶。多年前，我也曾将基于 Netflix OSS 构建的微服务架构搬上了 Kubernetes 平台，并持续折腾好几年。
Spring Cloud Netflix 有着庞大的用户群以及用户场景，其提供了微服务治理的一整套解决方案：服务发现 Eureka、客户端负载均衡 Ribbon、断路器 Hystrix、微服务网格 Zuul。</description></item><item><title>探索 Gateway API 在 Service Mesh 中的工作机制</title><link>https://atbug.com/explore-how-gateway-api-works-in-service-mesh/</link><pubDate>Thu, 07 Sep 2023 12:57:26 +0800</pubDate><guid>https://atbug.com/explore-how-gateway-api-works-in-service-mesh/</guid><description>前几天 Gateway API 宣布在 0.8.0 中支持服务网格，这意味着 GAMMA（Gateway API for Mesh Management and Administration）有了新进展，虽然目前还是实验阶段。去年 6 月 Gateway API 发布 0.5.0 时，我还写了一篇 SMI 与 Gateway API 的 GAMMA 倡议意味着什么？。如今，SMI 作为 sandbox 项目的年度审查已经 过了几个月仍未提交，唏嘘。
废话不多说，我们来看下 0.8.0 下的 Gateway API 如何在 Service Mesh 中工作。</description></item><item><title>【译】分拆：技术栈的自然演进</title><link>https://atbug.com/the-unbundling-of-tech-stack-translation/</link><pubDate>Sun, 12 Feb 2023 20:26:32 +0800</pubDate><guid>https://atbug.com/the-unbundling-of-tech-stack-translation/</guid><description>本文翻译自 Bilgin Lbryam 的 Unbundling: The Natural Evolution of Tech Stacks，翻译难免有所疏漏，有建议请反馈。
“unbundling” 如何翻译，有点纠结，我一度将其翻译成“解耦”，但解耦是 “decoupling” 的翻译。这里我将其翻译成分拆，如果你有更好的翻译请告知。
译者注 作者应该是去年 7 月离开红帽加入了基于 Dapr 的创业公司 Diagrid，曾写过 Multi-Runtime Microservices Architecture 介绍多运行时，多运行时实际上也是分拆的体现。
作者从多种技术和团队触发，介绍在演进中分拆的体现。除了文中提到，我认为可以分拆的是计算资源。将计算资源拆分：虚拟机、多租户、多集群、多云、混合云，以降低成本、避免供应商绑定、提升性能和可靠性。在计算资源拆分过程中，也衍生出了与之配套的技术来解决拆分后带来的不便。
随着 IT 领域的不断发展，新的软件架构、开发技术和工具层出不穷。包括微服务、微前端、零信任、服务网格和数据网格，并将其网格化。尽管这些技术和方法间存在着明显的不同，但它们都被一个共同趋势联系在一起：技术栈和团队的分拆。这种趋势包括将系统分解成更小的、独立的组件，并将工作组织成更小、更专注的团队，以实现更高的灵活性和模块化。
他们都是如何体现分拆的？
微服务 的出现是为了应对单体架构的局限性，随着应用程序的增长单体架构灵活性不足，并且扩展和维护困难。通过将单体应用程序分解为更小的、独立服务，就可以独立开发、部署和扩展应用程序的每一部分，从而缩短开发周期并提高灵活性。 六边形架构 的出现是为了通过将组件解耦并提供与它们交互的标准接口来提高 3 层应用程序的灵活性和可维护性。 领域驱动设计 (DDD) 是一种软件开发方法，可以帮助将整体应用程序分解成更小的、松耦合的、代表不同的业务领域或上下文的模块。 微前端 架构是一种设计方法，是将大型单体前端应用程序分解为较小的、独立的、可以单独开发和部署的模块。 JAMstack 通过将构成用户界面的 HTML、CSS 和 JavaScript 与为应用程序提供支持的服务器端代码和数据库分离，实现应用程序的前端和后端分离。由于系统的一部分的变更无需变更其他部分，从而可以更轻松地维护应用。 服务网格 将分布式应用程序的网络职责（例如路由、负载平衡和服务发现）与应用程序本身分离，使开发人员可以专注于构建业务逻辑和功能，而无需担心底层网络基础设施。 与微服务类似，数据网格 将大型复杂系统分解为更小的独立组件。它将数据治理和管理实践分解为更小的、独立组件，这些组件可以跨不同的数据源和系统一致地实现和执行。 2 个比萨团队 模型是一种在组织中组织团队和工作的策略，它提倡更小的团队能够更快地响应变化、沟通和协作，并可以更快地做出决策并更有效地解决问题。 每种技术趋势的最终结果都是分拆。将技术栈分解为独立的组件，将团队分解为更小、更专注的团队，这些团队可能会扩展到所有其他领域。在前端、数据、网络、安全之后，下一个拆分领域你认为会是什么？ 和我一起 致力于 Dapr 和分拆集成。 也可以在 @bibryam 上关注我，并大声说出关于 分拆 主题的任何想法和评论。</description></item><item><title>【译】eBPF 和服务网格：还不能丢掉 Sidecar</title><link>https://atbug.com/translate-ebpf-service-mesh/</link><pubDate>Mon, 31 Oct 2022 20:51:17 -0400</pubDate><guid>https://atbug.com/translate-ebpf-service-mesh/</guid><description>服务网格以典型的 sidecar 模型为人熟知，将 sidecar 容器与应用容器部署在同一个 Pod 中。虽说 sidecar 并非很新的模型（操作系统的 systemd、initd、cron 进程；Java 的多线程），但是以这种与业务逻辑分离的方式来提供服务治理等基础能力的设计还是让人一亮。
随着 eBPF 等技术的引入，最近关于服务网格是否需要 sidecar （也就是 sidecarless）的讨论渐增。
笔者认为任何问题都有其起因，长久困扰服务网格的不外乎性能和资源占用。这篇文章翻译自 Buoyant 的 Flynn 文章 eBPF and the Service Mesh: Don&amp;rsquo;t Dismiss the Sidecar Yet。希望这篇文章能帮助大家穿透迷雾看透事物的本质。
本文要点 eBPF 是一个旨在通过（谨慎地）允许在内核中运行一些用户代码来提高性能的工具。 在可预见的未来，服务网格所需的第 7 层处理在 eBPF 中不太可能实现，这意味着网格仍然需要代理。 与 sidecar 代理相比，每个主机代理增加了操作复杂性并降低了安全性。 可以通过更小、更快的 Sidecar 代理来解决有关 Sidecar 代理的典型性能问题。 目前，sidecar 模型对服务网格仍是最有意义的。 关于 eBPF 的故事已经在云原生世界中泛滥了一段时间，有时将其描述为自切片面包以来最伟大的事物，有时则嘲笑它是对现实世界的无用干扰。当然，现实要微妙得多，因此仔细研究一下 eBPF 能做什么和不能做什么似乎是有必要的——技术毕竟只是工具，使用的工具应该适合手头的任务。</description></item><item><title>一文搞懂 Kubernetes Gateway API 的 Policy Attachment</title><link>https://atbug.com/explore-k8s-gateway-api-policy-attachment/</link><pubDate>Sun, 30 Oct 2022 18:01:30 -0400</pubDate><guid>https://atbug.com/explore-k8s-gateway-api-policy-attachment/</guid><description>背景 在《SMI 与 Gateway API 的 GAMMA 倡议意味着什么？》的一文中，介绍过 Kubernetes Gateway API 处理东西向（网格）流量的可能性。让我们重新看下 Gateway API 中对流量定义（路由）的定义，以 HTTP 路由为例：
apiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: my-route namespace: gateway-api-example-ns2 spec: parentRefs: - kind: Gateway name: foo-gateway namespace: gateway-api-example-ns1 hostnames: - foo.example.com rules: - backendRefs: - name: foo-svc port: 8080 上面的 YAML 中，使用 parentRefs 字段将路由策略附加到了网关资源 foo-gateway 上。网关 foo-gateway 会根据请求头部的 HOST 来选择合适的 HTTPRoute，然后将流量代理到上游 Service foo-svc。简单来说，就是定义了“网关 -&amp;gt; 路由 -&amp;gt; 后端”的映射。</description></item><item><title>使用 Argo Rollouts 和服务网格实现自动可控的金丝雀发布</title><link>https://atbug.com/canary-release-via-argo-rollouts-and-service-mesh/</link><pubDate>Wed, 21 Sep 2022 23:37:43 +0800</pubDate><guid>https://atbug.com/canary-release-via-argo-rollouts-and-service-mesh/</guid><description>金丝雀发布是服务治理中的重要功能，在发布时可以可控地将部分流量导入新版本的服务中；其余的流量则由旧版本处理。发布过程中，可以逐步增加新版本服务的流量。通过测试，可以决定是回滚还是升级所有实例，停用旧版本。
有了金丝雀发布，使用真实流量对服务进行测试，通过对流量的控制可以有效的降低服务发布的风险。
本文将介绍如何将使用 Argo Rollouts 和服务网格 osm-edge 来进行应用的自动、可控的金丝雀发布，不涉及工作原理的分析。对工作原理有兴趣的同学可以留言，可以再做一篇原理的介绍。
Argo Rollouts Argo Rollouts 包括一个 Kubernetes控制器 和一组 CRD，提供如蓝绿色、金丝雀、金丝雀分析、体验等高级部署功能和 Kubernetes 的渐进交付功能。
Argo Rollouts 与 入口控制器 和服务网格集成，利用其流量管理能力，在发布期间逐步将流量转移到新版本。此外，Rollouts 可以查询和解析来自各种提供商的指标，以验证关键 KPI，并在更新期间推动自动推进或回滚。
Argo Rollouts 支持服务网格标准 SMI（Service Mesh Interface） 的 TrafficSplit API，通过 TrafficSplit API 的使用来控制金丝雀发布时的流量控制。
服务网格 osm-edge 服务网格是处理服务间网络通信的基础设施组件，旨在从平台层面提供可观性、安全以及可靠性特性，以进程外的方式提供原本由部分应用层逻辑承载的基础能力，真正实现与业务逻辑的分离。</description></item><item><title>另眼旁观 Linkerd 2.12 的发布：服务网格标准的曙光？</title><link>https://atbug.com/thoughts-on-linkerd-release/</link><pubDate>Sun, 28 Aug 2022 10:03:12 +0800</pubDate><guid>https://atbug.com/thoughts-on-linkerd-release/</guid><description>8 月 24 日，发明服务网格的公司 Buoyant 发布了 Linkerd 2.12，这是时隔近一年的版本发布。不知道大家的对新版本期待如何，在我来看新版本中的功能对于一年的时间来说确实不算多，但是我想说的是 Linkerd 还是那个 Linkerd，还是秉持着一贯的 “Keep it simple” 的设计理念。
新版本的内容不一一介绍，其中最主要的功能：per-route 策略，相比上个版本基于端口的策略，扩展到了 per-route。
而我想说的特别之处也就是在 per-route 和策略上。
per-route 和策略有何特别之处？ 在《SMI 与 Gateway API 的 GAMMA 倡议意味着什么？》 层有过猜想：未来服务网格的标准有可能全部或者部分以 Kubernetes 原生 API 的方式存在。
“网关 API，之于集群是入口/出口网关，之于 Pod 是 inbound/outbound sidecar。”</description></item><item><title>SMI 与 Gateway API 的 GAMMA 倡议意味着什么？</title><link>https://atbug.com/why-smi-collaborating-in-gateway-api-gamma/</link><pubDate>Fri, 22 Jul 2022 22:34:43 +0800</pubDate><guid>https://atbug.com/why-smi-collaborating-in-gateway-api-gamma/</guid><description>就在上周 Gateway API 发布版本 0.5.0，其中几个最重要的 CRD Gateway、GatewayClass 以及 HTTPRoute 第一次升级到了 beta 版本。升级的详细内容这里不做详谈，我也想说说的是与版本一同发布的，也是很容易被忽略的社区合作倡议 GAMMA。
GAMMA 是 Gateway API Mesh Management and Administration 的简写，这个计划是 Gateway API 子项目中的一个专用工作流。这个小组的目标是调研、设计和跟踪 Gateway API 资源、语义和其他与服务网格技术和用例相关的组件。此外，小组还努力倡导服务网格项目的 Gateway API 实现之间的一致性，无论使用何种技术栈或者代理。
可能你会有疑问如此简单一个倡议，有什么特别？有两点：
规范的参与：这个倡议由 SIG Network（Gateway API 所在的 Kubernetes SIG）与服务网格社区共同发起，服务网格社区有来自 Cilium Service Mesh、Consul、Istio、Kuma、Linkerd、SMI、NGINX Service Mesh 和 Open Service Mesh 的代表。SMI 与其他几个不同，它是服务网格的规范 API，并非是实现，Gateway API 也一样。 面向东西向流量：小组的首个工作探索使用 Gateway API 处理东西向流量已经开始。东西向流量，也就是服务网格中服务到服务的流量。 可见，服务网格部分 API 的统一将迈进一步，且成为 Kubernetes 原生 API 的一员。当然现在还言之过早，还需看各个厂商的跟进程度。</description></item><item><title>开放服务网格 Open Service Mesh 如何开放？</title><link>https://atbug.com/how-open-presented-in-open-service-mesh/</link><pubDate>Wed, 13 Apr 2022 06:50:56 +0800</pubDate><guid>https://atbug.com/how-open-presented-in-open-service-mesh/</guid><description>TL;DR 本文从服务网格发展现状、到 Open Service Mesh 源码，分析开放服务网格中的开放是什么以及如何开放。笔者总结其开放体现在以下几点：
资源提供者（Provider）接口和资源的重新封装：通过资源提供者接口抽象计算平台的资源，并封装成平台、代理无关的结构化类型，并由统一的接口 MeshCataloger 对外提供访问。虽然目前只有 Kubernetes 相关资源的 Provider 实现，但是通过抽象出的接口，可以兼容其他平台，比如虚拟机、物理机。 服务网格能力接口：对服务网格能力的抽象，定义服务网格的基础功能集。 代理控制面接口：这一层与反向代理，也就是 sidecar 的实现相关。实际上是反向代理所提供的接口，通过对接口的实现，加上 MeshCataloger 对资源的访问，生成并下发代理所需的配置。 背景 服务网格是什么 服务网格是在 2017年由 Buoyant 的 Willian Morgan 在 What’s a service mesh? And why do I need one? 给出了解释。
服务网格是处理服务间网络通信的基础设施组件，旨在从平台层面提供可观性、安全以及可靠性特性，以进程外的方式提供原本由部分应用层逻辑承载的基础能力，真正实现与业务逻辑的分离。典型的实现是与应用程序一起部署的可扩展网络代理（通常称为 sidecar 模型），来代理服务间的网络通信，也是服务网格特性的接入点。这些代理就组成了服务网格的数据平面，并由控制平面进行统一的管理。</description></item><item><title>eBPF 和 Wasm：探索服务网格数据平面的未来</title><link>https://atbug.com/ebpf-wasm-service-mesh/</link><pubDate>Tue, 11 Jan 2022 10:40:56 +0800</pubDate><guid>https://atbug.com/ebpf-wasm-service-mesh/</guid><description>本文翻译自 Vivian Hu 的 《eBPF and Wasm: Exploring the Future of the Service Mesh Data Plane》。
在 2021 年 12 月 2 日，Cilium 项目宣布了 Cilium Service Mesh 项目的测试版。在 2020 年 8 月 Google Cloud 宣布基于 eBPF 的 Google Kubernetes 服务（GKS）的数据平面 V2 的一年后，Cilium Service Mesh 带来了 “无边车服务网格”（sidecarless service mesh）的想法。它扩展了 Cilium eBPF 产品来处理服务网格中的大部分边车代理功能，包括 7 层路由和负载均衡、TLS 终止、访问策略、健康检查、日志和跟踪，以及内置的 Kubernetes Ingress。</description></item><item><title>低复杂度 - 服务网格的下一站</title><link>https://atbug.com/service-mesh-unnecessary-complexity/</link><pubDate>Fri, 15 Oct 2021 07:58:25 +0800</pubDate><guid>https://atbug.com/service-mesh-unnecessary-complexity/</guid><description>译者：
作为一个曾经在制造业企业的基础架构团队任职，为支持公司的“互联网基因”和“数字化转型”落地了云原生基础设施平台，并在尝试采用服务网格未成的我来说，看到这篇文章深有感触。尤其是文中所说的“人少，问题多，需要快速输出价值”，直戳到了痛处。有限的人手有限的时间，我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上，要想很好的运行复杂的系统是非常困难的。
服务网格是一个新的基础设施层，可以承载很多的功能，未来还会有更大的想象空间和光明的未来。
以上的种种原因，也促使我后来选择进入一家提供服务网格的产品企业，也希望服务网格可以被更简单的使用。
“道阻且长，行则将至！”
本文翻译自 Chris Campbell 的 How Unnecessary Complexity Gave the Service Mesh a Bad Name
关键要点 采用服务网格有巨大的价值，但必须以轻量级的方式进行，以避免不必要的复杂性。 在实施服务网时，要采取务实的方法，与技术的核心功能保持一致，并小心干扰（译者：注意力的分散）。 服务网格的一些核心特性包括标准化监控、自动加密和身份识别、智能路由、可靠的重试和网络可扩展性。 服务网格可以提供强大的功能，但这些功能会分散本应对核心优势的关注，并且这些功能也不是实施服务网格的主要原因。 在初始实施服务网格时没有必要去关注那些明显会分散注意力的功能，比如复杂的控制平面、多集群支持、Envoy、WASM 和 A/B 测试。 服务网格是 Kubernetes 世界中的一个热门话题，但许多潜在的采用者已经有些失望了。服务网格的落地受到压倒性的复杂性和看似无穷无尽的供应商解决方案的限制。在我亲自浏览了这个领域之后，我发现采用服务网格具有巨大的价值，但它必须以轻量级的方式完成，以避免不必要的复杂性。尽管普遍存在幻灭感，但服务网格的未来依然光明。
在工作中学习 我进入服务网格的世界始于我在一家老牌的财富 500 强技术公司担任云计算架构师的角色。在开始我们的服务网格之旅时，我身边有许多强大的工程师，但大多数人几乎没有云计算开发经验。我们的组织诞生于云计算之前，完全实现云计算的价值需要时间。我们的传统业务线主要集中在技术栈的硬件元素上，云计算的决策最初是由为运送硬件或为该硬件提供固件和驱动程序而开发的流程驱动的。
随着该组织经历其“数字化转型”，它越来越依赖于提供高质量的软件服务，并逐渐开发出更好的方法。但作为云计算架构师，我仍在为优先考虑硬件的业务流程，以及具有不同技能、流程和信念的工程团队导航。随着时间的推移，我和我的团队在将 .NET 应用程序迁移到 Linux、采用 Docker、迁移到 AWS 以及与之相关的最佳实践（如持续集成、自动化部署、不可变基础设施、基础设施即代码、监控等）方面变得熟练并成功。但挑战依然存在。</description></item><item><title>使用 Flomesh 进行 Dubbo 服务治理</title><link>https://atbug.com/enhance-dubbo-service-governance-with-flomesh/</link><pubDate>Wed, 18 Aug 2021 09:50:28 +0800</pubDate><guid>https://atbug.com/enhance-dubbo-service-governance-with-flomesh/</guid><description>写在最前 和上一篇《使用 Flomesh 强化 Spring Cloud 服务治理》一样，这次同样是在无代码侵入的情况下对 Dubbo 服务治理的提升。
更多治理场景陆续添加中，有兴趣的可关注 https://github.com/flomesh-io/service-mesh-dubbo-demo。
开源的 Pipy 作为 Flomesh 的核心，得益于其轻量及灵活性可以通过编程的方式轻松快速的支持多中平台的服务发现机制，比如 Eureka、Consul、Nacos 等。
概览 细节 环境搭建 搭建 Kubernetes 环境，可以选择 kubeadm 进行集群搭建。也可以选择 minikube、k3s、Kind 等，本文使用 k3s。
使用 k3d 安装 k3s。k3d 将在 Docker 容器中运行 k3s，因此需要保证已经安装了 Docker。
$ k3d cluster create dubbo-demo -p &amp;#34;80:80@loadbalancer&amp;#34; --k3s-server-arg &amp;#39;--no-deploy=traefik&amp;#39; 安装 Flomesh 从仓库 https://github.</description></item><item><title>使用 Flomesh 强化 Spring Cloud 服务治理</title><link>https://atbug.com/enhance-springcloud-service-governance-with-flomesh/</link><pubDate>Tue, 17 Aug 2021 18:47:33 +0800</pubDate><guid>https://atbug.com/enhance-springcloud-service-governance-with-flomesh/</guid><description>写在最前 这篇是关于如何使用 Flomesh 服务网格来强化 Spring Cloud 的服务治理能力，降低 Spring Cloud 微服务架构落地服务网格的门槛，实现“自主可控”。
文档在 github 上持续更新，欢迎大家一起讨论：https://github.com/flomesh-io/flomesh-bookinfo-demo。
架构 环境搭建 搭建 Kubernetes 环境，可以选择 kubeadm 进行集群搭建。也可以选择 minikube、k3s、Kind 等，本文使用 k3s。
使用 k3d 安装 k3s。k3d 将在 Docker 容器中运行 k3s，因此需要保证已经安装了 Docker。
$ k3d cluster create spring-demo -p &amp;#34;81:80@loadbalancer&amp;#34; --k3s-server-arg &amp;#39;--no-deploy=traefik&amp;#39; 安装 Flomesh 从仓库 https://github.</description></item><item><title>Kubernetes 必备工具：2021</title><link>https://atbug.com/translation-kuberletes-essential-tools-2021/</link><pubDate>Thu, 15 Jul 2021 08:10:22 +0800</pubDate><guid>https://atbug.com/translation-kuberletes-essential-tools-2021/</guid><description>有别于前些天的文章 - 常用的几款工具让 Kubernetes 集群上的工作更容易 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。
文档翻译自 Kubernetes Essential Tools: 2021，篇幅较长，做了部分增删。
介绍 在本文中，我将尝试总结我最喜欢的 Kubernetes 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。
这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。
K3D K3D 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常轻巧且速度非常快。它是使用 Docker 围绕 K3S 的包装器。所以，你只需要 Docker 来运行它并且资源使用率非常低。唯一的问题是它不完全符合 K8s 标准，但这不应该是本地开发的问题。对于测试环境，你可以使用其他解决方案。K3D 比 Kind 快，但 Kind 完全兼容。
备选 K3S 物联网或者边缘计算 Kind 完全兼容 Kubernetes 的备选 MicroK8s MiniKube Krew Krew 是管理的必备工具 Kubectl 插件，这是一个必须有任何 K8S 用户。我不会详细介绍超过 145 个可用插件，但至少安装 kubens 和 kubectx。</description></item><item><title>服务网格平稳落地：Istio 中精准控制 Sidecar 的注入</title><link>https://atbug.com/how-to-control-istio-sidecar-injection/</link><pubDate>Wed, 21 Apr 2021 08:13:04 +0800</pubDate><guid>https://atbug.com/how-to-control-istio-sidecar-injection/</guid><description>为什么 说起服务网格，这幅图大家肯定不会陌生。这就是服务网格的网络，也是网格架构的终极形态。
那在迁移到网格架构之前，我们的系统是什么样的？
我们的系统在演进的过程中，不可避免的会遇到各种 0 到 1 过程中的中间态。比如下面这种，可以比较直观的看出 Istio 或者网格是部分覆盖的。这个过程中，我们需要平滑、可控的推进，才能在保障系统可用性的前提下进行架构的演进。
怎么做 Sidecar 的注入分两种：手动和自动。
手动 手动就是利用 Istio 的 cli 工具 istioctl kube-inject 对资源 yaml 进行修改：
$ istioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f - serviceaccount/sleep created service/sleep created deployment.apps/sleep created 手动的方式比较适合开发阶段使用。</description></item><item><title>分布式系统在 Kubernetes 上的进化</title><link>https://atbug.com/translation-distributed-systems-kubernetes/</link><pubDate>Mon, 29 Mar 2021 23:11:25 +0800</pubDate><guid>https://atbug.com/translation-distributed-systems-kubernetes/</guid><description>本文译自 The Evolution of Distributed Systems on Kubernetes
在 3 月份的 QCon 上，我做了一个关于 Kubernetes 的分布式系统进化的演讲。首先，我想先问一个问题，微服务之后是什么？我相信大家都有各自的答案，我也有我的答案。你会在最后发现我的想法是什么。为了达到这个目的，我建议大家看看分布式系统的需求是什么？以及这些需求在过去是如何发展的，从单体应用开始到 Kubernetes，再到最近的 Dapr、Istio、Knative 等项目，它们是如何改变我们做分布式系统的方式。我们将尝试对未来做一些预测。
现代分布式应用 为了给这个话题提供更多的背景信息，我认为的分布式系统是由数百个组件组成的系统。这些组件可以是有状态的、无状态的或者无服务器的。此外，这些组件可以用不同的语言创建，运行在混合环境上，并开发开源技术、开放标准和互操作性。我相信你可以使用闭源软件来构建这样的系统，也可以在 AWS 和其他地方构建。具体到这次演讲，我将关注 Kubernetes 生态系统，以及你如何在 Kubernetes 平台上构建这样一个系统。
我们从分布式系统的需求讲起。我认为是我们要创建一个应用或者服务，并写一些业务逻辑。那从运行时的平台到构建分布式系统，我们还需要什么呢？在底层，最开始是我们要一些生命周期的能力。当你用任一语言开发你的应用时，我们希望有能力把这个应用可靠地打包和部署、回滚、健康检查。并且能够把应用部署到不同的节点上，并实现资源隔离、扩展、配置管理，以及所有这些。这些都是你创建分布式应用所需要的第一点。
第二点是围绕网络。我们有了应用之后，我们希望它能够可靠地连接到其他服务，无论该服务是在集群内部还是在外部。我们希望其具有服务发现、负载均衡的能力。为了不同的发布策略或是其他的一些原因的我们希望有流量转移的能力。然后我们还希望其具有与其他系统进行弹性通信的能力，无论是通过重试、超时还是断路器。要有适当的安全保障，并且要有足够的监控、追踪、可观察性等等。
我们有了网络之后，接下来就是我们希望有能力与不同的 API 和端点交互，即资源绑定&amp;ndash;与其他协议和不同的数据格式交互。甚至能够从一种数据格式转换成另一种数据格式。我还会在这里加入诸如滤光的功能，也就是说，当我们订阅一个主题时，我们也许只对某些事件感兴趣。
你认为最后一类是什么？是状态。当我在说状态和有状态的抽象时，我并不是在谈论实际的状态管理，比如数据库或者文件系统的功能。我要说的更多是有关幕后依赖状态的开发人员抽象。可能，你需要具有工作流管理的能力。也许你想管理运行时间长的进程或者做临时调度或者某些定时任务来定期运行服务。也许你还想进行分布式缓存，具有幂等性或者支持回滚。所有这些都是开发人员级的原语，但在幕后，它们依赖于具有某种状态。你想随意使用这些抽象俩创建完善的分布式系统。
我们将使用这个分布式系统原语的框架来评估它们在 Kubernetes 和其他项目上的变化情况。
单体架构 &amp;ndash; 传统中间件功能 假设我们从单体架构以及如何获得这些能力开始。在那种情况下，首先是当我说单体的时候，在分布式应用的情况下我想到的是 ESB。ESB 是相当强大的，当我们检查我们的需求列表时，我们会说 ESB 对所有有状态的抽象有很好的支持。</description></item><item><title>Minikube安装istio</title><link>https://atbug.com/install-istio-on-minikube/</link><pubDate>Fri, 17 Jan 2020 08:02:42 +0800</pubDate><guid>https://atbug.com/install-istio-on-minikube/</guid><description>准备 注意: istioctl的安装要使用安装里的, 不要是用homebrew里的. github issue
curl -L https://istio.io/downloadIstio | sh - cd istio-1.4.2 cp bin/istioctl /usr/local/bin/istioctl 安装前检查 istioctl verify-install 如果检查没问题, 会看到Install Pre-Check passed! The cluster is ready for Istio installation.
安装 istio有5种内建的安装配置1: remote, sds, default, demo, minimal
istioctl profile list minimal: 使用istio的流量管理所需组件的最小化安装 default: 根据IstioControlPlane API的默认设置(建议用于生产部署)启用组件.</description></item></channel></rss>