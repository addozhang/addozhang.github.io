<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DevOps on 乱世浮生</title><link>https://atbug.com/tags/devops/</link><description>Recent content in DevOps on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 18 Jan 2022 12:03:59 +0800</lastBuildDate><atom:link href="https://atbug.com/tags/devops/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubernetes HPA 基于 Prometheus 自定义指标的可控弹性伸缩</title><link>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</link><pubDate>Tue, 18 Jan 2022 12:03:59 +0800</pubDate><guid>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</guid><description>在《Kubernetes 的自动伸缩你用对了吗？》 一文中详细说明了如何使用 Kubernetes 的自动伸缩。在 Kubernetes 中弹性伸缩主要有三种：HPA、VPA、CA。本文不再详细说明，有兴趣的可以看那篇文章。这里主要来说下 Pod 水平缩放 HPA。
随着 Kubernetes v1.23 的发布，HPA 的 API 来到了稳定版 autoscaling/v2：
基于自定义指标的伸缩 基于多项指标的伸缩 可配置的伸缩行为 从最初的 v1 版本 HPA 只支持 CPU、内存利用率的伸缩，到后来的自定义指标、聚合层 API 的支持，到了 v1.18 版本又加入了配置伸缩行为的支持，HPA 也越来越好用、可靠。
依靠 CPU 或者内存指标的扩容并非使用所有系统，看起来也没那么可靠。对大部分的 web 后端系统来说，基于 RPS（每秒请求数）的弹性伸缩来处理突发的流量则会更加靠谱。
Prometheus 也是当下流行开源监控系统，通过 Prometheus 可以获取到系统的实时流量负载指标，今天我们就来尝试下基于 Prometheus 的自定义指标进行弹性伸缩。</description></item><item><title>自动替换 Kubernetes 镜像</title><link>https://atbug.com/kubernetes-images-swapper/</link><pubDate>Wed, 06 Oct 2021 08:01:41 +0800</pubDate><guid>https://atbug.com/kubernetes-images-swapper/</guid><description>前几天有朋友在问如何在某云上拉取 Tekton 的镜像，这种情况其实比较普遍不只是某云。工作中经常要用到过某些靠运气才能拉取到的镜像，这对工作来说真是极度的不友好。
因此也萌生了个想法，维护一个后网络友好的仓库镜像，在 Pod 创建时将镜像仓库切换到自维护的仓库，从自维护的仓库拉取镜像。
前几天体验了极狐Gitlab 的容器镜像库，便是为这个想法做的准备。当然其他的云厂商也有提供针对个人版的免费镜像仓库和企业版仓库。
正好 Pipy 作为策略引擎，非常适合实现这种策略的执行。
实现思路 Admission Webhook Kubernetes 动态准备控制 的 MutatingWebhookConfiguration 可以 hook Pod 的创建或者更新，然后调用目标服务对 Pod 资源对象进行 patch 操作。
策略引擎 Pipy 作为应用的核心，也就是 MutatingWebhookConfiguration 的目标服务，以策略引擎的角色完成策略的执行。
Pipy 支持从文件或者 HTTP 地址加载脚本，这里为了便于策略的更新，使用了后者。
对于从 HTTP 地址加载脚本，HTTP 地址返回内容的第一行会作为 Pipy 的主脚本，Pipy 启动时会加载主脚本，其他的文件也会被缓存到内存中。</description></item><item><title>极狐GitLab SaaS 内测轻度体验</title><link>https://atbug.com/jihu-gitlab-experience/</link><pubDate>Fri, 01 Oct 2021 08:18:09 +0800</pubDate><guid>https://atbug.com/jihu-gitlab-experience/</guid><description>感谢极狐团队为 GitLab（SaaS）本地化的努力，同时也感谢小马哥提供的内测资格。
最近突然想到了个点子，需要使用一个私有的镜像仓库。极狐GitLab 有提供容器镜像库，正好和 CICD 一起做个轻度体验。
容器镜像库 Container Registry 文档介绍在这里，目前还是英文。（应该本地化的工作量很大，文档还没翻译。）
容器镜像库可以作为独立镜像仓库使用（为什么要这么用，卖个关子下篇文章见），就是使用 docker 命令将构建好的镜像推送到 容器镜像库。
当然也可以同 CICD 流水线结合使用，后文也会介绍。
独立使用 本地登录 Container Registry 有两种验证方式：
使用用户名和密码 开启了双重身份验证，可以使用访问个人访问令牌 其实，不管是否开始双重验证，都建议使用访问令牌。
docker login registry.gitlab.cn #根据提示输入用户名和密码或者令牌 image 的名字最多有三层，即 registry.example.com/[namespace] 之后的内容最多有 3 层。比如下面的 image 名字 myproject/my/image
registry.example.com/mynamespace/myproject/my/image:rc1 其次 image 名字的第一层必须是镜像名，如上面的 myproject。</description></item><item><title>Kubernetes 必备工具：2021</title><link>https://atbug.com/translation-kuberletes-essential-tools-2021/</link><pubDate>Thu, 15 Jul 2021 08:10:22 +0800</pubDate><guid>https://atbug.com/translation-kuberletes-essential-tools-2021/</guid><description>有别于前些天的文章 - 常用的几款工具让 Kubernetes 集群上的工作更容易 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。
文档翻译自 Kubernetes Essential Tools: 2021，篇幅较长，做了部分增删。
介绍 在本文中，我将尝试总结我最喜欢的 Kubernetes 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。
这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。
K3D K3D 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常轻巧且速度非常快。它是使用 Docker 围绕 K3S 的包装器。所以，你只需要 Docker 来运行它并且资源使用率非常低。唯一的问题是它不完全符合 K8s 标准，但这不应该是本地开发的问题。对于测试环境，你可以使用其他解决方案。K3D 比 Kind 快，但 Kind 完全兼容。
备选 K3S 物联网或者边缘计算 Kind 完全兼容 Kubernetes 的备选 MicroK8s MiniKube Krew Krew 是管理的必备工具 Kubectl 插件，这是一个必须有任何 K8S 用户。我不会详细介绍超过 145 个可用插件，但至少安装 kubens 和 kubectx。</description></item><item><title>Jenkins 如何与 Kubernetes 集群的 Tekton Pipeline 交互？</title><link>https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/</link><pubDate>Wed, 23 Jun 2021 07:58:45 +0800</pubDate><guid>https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/</guid><description>本文详细介绍了 Jenkins 如何通过 tekton-client-plugin 实现与 Kubernetes 上的 Tekton Pipeline 交互，包括 Kubernetes 上安装 Jenkins、Tekton Pipelines 等。
关于如何使用 Tekton Pipeline 实现 CICD 可以看这篇文章 云原生 CICD: Tekton Pipeline 实战
本文用于构建的项目以及所有 manifest yaml 都在可以在这里下载。
TL;DR 惯例，先上总结。tekton-client-plugin 虽然还是处于初期阶段，但是 其价值非常明显，尤其是对先用使用 Jenkins 作为 CICD 实现的用户来说。从 Jenkins 迁移到云原生的 Tekton 时，可以省掉用户界面的开发成本，而且尽可能少的改变用户习惯 ，依靠版本管理可以控制迁移的节奏。</description></item><item><title>云原生 CICD: Tekton Pipeline 实战</title><link>https://atbug.com/tekton-pipeline-practice/</link><pubDate>Tue, 22 Jun 2021 07:19:33 +0800</pubDate><guid>https://atbug.com/tekton-pipeline-practice/</guid><description>更新历史：
v1：2020.1.21 基于 Tekton Pipline v0.9.0 v2（当前）：2021.6.22 基于 Tekton Pipeline v0.25.0 Tekton 是 Google 开源的 Kubernetes 原生CI/CD 系统, 功能强大扩展性强. 前身是 Knavite 里的 build-pipeline 项目, 后期孵化成独立的项目. 并成为 CDF 下的四个项目之一, 其他三个分别是 Jenkins, Jenkins X, Spinnaker.
为什么说 Tekton 是 Kubernetes 原生的, 以内其基于 Kubernetes 的 CRD 定义了 Pipeline 流水线.</description></item><item><title>Kubernetes 源码解析 - HPA 水平自动伸缩如何工作</title><link>https://atbug.com/kubernetes-source-code-how-hpa-work/</link><pubDate>Sat, 15 Aug 2020 02:09:37 +0800</pubDate><guid>https://atbug.com/kubernetes-source-code-how-hpa-work/</guid><description>HPA - Horizontal Pod Autoscaler 的缩写，Pod 水平自动伸缩。通过对 Pod 负载的监控，来自动增加或者减少 Pod 的副本数量。
从字面意思来看，其主要包含了两部分：
监控 Pod 的负载 控制 Pod 的副本数量 那具体是如何实现的呢？以下基于1.17 源码，来分析下 HPA 如何工作。
注意：文章中的代码在源码的基础上进行了精简：删掉了注释、序列化等信息，或保留了部分核心代码，加上新的注释。
资源 HPA 的资源是HorizontalPodAutoscaler，在v1版本中，只支持基于 CPU 指标的计算；在v2beta2版本中加入了基于内存和自定义指标的计算。
v1 //staging/src/k8s.io/api/autoscaling/v1/types.go type HorizontalPodAutoscaler struct { metav1.TypeMeta metav1.ObjectMeta Spec HorizontalPodAutoscalerSpec Status HorizontalPodAutoscalerStatus } type HorizontalPodAutoscalerSpec struct { ScaleTargetRef CrossVersionObjectReference //监控的目标资源 MinReplicas *int32 //最小副本数 MaxReplicas int32 //最大副本数 TargetCPUUtilizationPercentage *int32 //触发调整的CPU 使用率 } v2 //staging/src/k8s.</description></item><item><title>Tekton 的工作原理</title><link>https://atbug.com/how-tekton-works/</link><pubDate>Sat, 23 May 2020 22:47:14 +0800</pubDate><guid>https://atbug.com/how-tekton-works/</guid><description>这篇文章是基于 Tekton Pipeline 的最新版本v0.12.1版本。
快速入门请参考：云原生 CICD: Tekton Pipeline 实战 ，实战是基于版本 v0.10.x。
Pipeline CRD 与核心资源的关系 $ k api-resources --api-group=tekton.dev NAME SHORTNAMES APIGROUP NAMESPACED KIND clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.</description></item><item><title>云原生CICD: Tekton Trigger 实战</title><link>https://atbug.com/tekton-trigger-practice/</link><pubDate>Wed, 12 Feb 2020 21:30:03 +0800</pubDate><guid>https://atbug.com/tekton-trigger-practice/</guid><description>Trigger的介绍看 这里.
接上文 Tekton Pipeline 实战 , 我们为某个项目创建了一个Pipeline, 但是执行时通过 PipelineRun 来完成的. 在 PipelineRun 中我们制定了 Pipepline 以及要使用的 PipelineResource. 但是日常的开发中, 我们更多希望在提交了代码之后开始 Pipeline 的执行. 这时我们就要用到 Tekton Trigger 了.
思路是这样: 代码提交后将Push Event发送给Tekton Trigger EventController(以下简称 Controller), 然后 Controller 基于的TriggerBinding的配置从 payload 中提取信息, 装载在&amp;quot;Params&amp;quot;中作为TriggerTemplate的入参. 最后 Controller 创建PipelineRun.</description></item><item><title>Tekton Trigger 介绍</title><link>https://atbug.com/tekton-trigger-glance/</link><pubDate>Wed, 05 Feb 2020 18:03:15 +0800</pubDate><guid>https://atbug.com/tekton-trigger-glance/</guid><description>背景 Tekton 的介绍请参考Tekton Pipeline 实战.
通常, CI/CD 事件应该包含如下信息:
确定事件的类型(比如 GitHub Push, GitLab Issue, Docker Hub Webhook 等) 可从特定管道访问并映射到特定管道 (从事件负载中获取 SHA 信息, 然后在管道中使用) 准确地触发管道 (基于有效负载值触发管道) Tekton API 的设计分离了配置(比如 PipelineRun VS Pipeline), 保证了 step 可以被重用. 但是没有提供动态封装配置的机制来生成资源(尤其是 PipelineRun 和 PipelineResource). Triggers 通过下面的 CRDs 在架构上对 Tekton 进行了扩展:</description></item><item><title>Tekton Dashboard 安装</title><link>https://atbug.com/tekton-dashboard-installation/</link><pubDate>Sat, 01 Feb 2020 12:39:28 +0800</pubDate><guid>https://atbug.com/tekton-dashboard-installation/</guid><description>Tekton 提供了dashboard方便用户管理和查看 Tekton PipelineRun 和 TaskRun 以及创建, 执行和完成过程中涉及的资源. 它还允许按标签过滤 PipelineRun 和 TaskRun.
安装方法 kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/v0.4.1/dashboard_latest_release.yaml 检查dashboard的运行情况, STATUS为Running的话则说明运行成功.
kubectl get pods --namespace tekton-pipelines 访问 访问Tekton的Dashboard有两种方式, 一种是通过port-forward, 另一种是通过ingress来访问.
port-forward kubectl port-forward svc/tekton-dashboard 9097 ingress 先检查ingress是否开启.
minikube addon list ... - ingress: enabled .</description></item><item><title>Tekton 0.9.0 更新</title><link>https://atbug.com/tekton-0.9.0-release/</link><pubDate>Sun, 19 Jan 2020 14:33:17 +0800</pubDate><guid>https://atbug.com/tekton-0.9.0-release/</guid><description>翻译整理自 What’s New in Tekton 0.9
功能及Bug修复 脚本模式 以前如果要在容器里运行个简单的 bash 脚本, 需要这么写:
- name: hello image: ubuntu command: [&amp;#39;bash&amp;#39;] args: - -c - | set -ex echo &amp;#34;hello&amp;#34; 在 0.9 之后, 可以更加简单, 不需要再写command 和讨厌的-c.
- name: hello image: ubuntu script: | #!</description></item><item><title>Tekton安装及Hello world</title><link>https://atbug.com/tekton-installation-and-sample/</link><pubDate>Fri, 17 Jan 2020 19:17:14 +0800</pubDate><guid>https://atbug.com/tekton-installation-and-sample/</guid><description>安装 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 检查安装的tekton相关的CRD:
kubectl api-resources | grep tekton clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task tekton的两个pod:</description></item><item><title>Nginx实现Elasticsearch的HTTP基本认证</title><link>https://atbug.com/elasticsearch-http-basic-authentication-via-nginx/</link><pubDate>Tue, 06 Nov 2018 09:24:24 +0000</pubDate><guid>https://atbug.com/elasticsearch-http-basic-authentication-via-nginx/</guid><description>Elasticssearch的HTTP基本认证实现有两种方案: x-pack和nginx反向代理. 前者收费, 后者不太适合生产使用. 如果仅仅是开发测试, 第二种完全足够.
创建密码 htpasswd -bc ./passwd [username] [password] Docker compose version: &amp;#39;3&amp;#39; services: elasticsearch: image: elasticsearch:5.5.2 container_name: elasticsearch restart: unless-stopped volumes: - /tmp/elasticsearch:/usr/share/elasticsearch/data nginx: image: nginx:latest container_name: elasticsearch-proxy ports: - 9200:9200 links: - elasticsearch volumes: - .</description></item><item><title>Kubernetes 中的 Nginx 动态解析</title><link>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</link><pubDate>Wed, 30 May 2018 12:10:32 +0000</pubDate><guid>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</guid><description>背景 Nginx运行在kubernets中, 反向代理service提供服务.
kubernetes版本v1.9.1+a0ce1bc657.
问题: 配置如下:
location ^~/info { proxy_pass: http://serviceName:port; } 删除并重建Service的时候, nginx会出现下面的问题:
connect() failed (113: No route to host) &amp;hellip; upstream: &amp;ldquo;xxxxx&amp;rdquo;
分析 通过google发现, 是nginx的dns解析方案的问题.
nginx官方的说明:
If the domain name can’t be resolved, NGINX fails to start or reload its configuration.</description></item><item><title>Jenkins CI/CD (一) 基于角色的授权策略</title><link>https://atbug.com/using-role-based-authorization-strategy-in-jenkins/</link><pubDate>Fri, 20 Apr 2018 12:18:46 +0000</pubDate><guid>https://atbug.com/using-role-based-authorization-strategy-in-jenkins/</guid><description>&lt;p>最近开始客串运维做CI/CD的规划设计, 主要是基于&amp;rsquo;Pipeline as Code in Jenkins&amp;rsquo;. 整理了下思路和技术点, 慢慢的写.&lt;/p>
&lt;p>这一篇是关于基于角色的授权策略, 用的是&lt;code>Role-Based Authorization Strategy Plugin&lt;/code>.&lt;/p>
&lt;p>授权在CI/CD流程中比较常见, 比如我们只让某些特定用户才可以构建Pre-Release的Job. 而更高级的Release发布, 又会需要某些用户的审批才可以进行. 需要授权时, 可能还需要发邮件提醒用户.&lt;/p>
&lt;p>UI上如何使用就不提了, 这里只说Pipeline as Code. 后面的几篇也会是这个背景.&lt;/p>
&lt;p>参考的这篇&lt;a href="https://www.avioconsulting.com/blog/using-role-based-authorization-strategy-jenkins">文章&lt;/a>, 文章里的代码运行失败, 做了修复.&lt;/p>
&lt;h2 id="配置">配置&lt;/h2>
&lt;p>安装完插件, 需要开始&lt;code>基于角色的授权策略&lt;/code>. 同时添加角色和为用户分配角色.&lt;/p>
&lt;h3 id="使用role-based-strategy作为验证方式">使用&lt;code>Role-Based Strategy&lt;/code>作为验证方式&lt;/h3>
&lt;p>&lt;code>Manage Jenkins / Configure Global Security / Configure Global Security&lt;/code>&lt;/p>
&lt;p>&lt;img src="http://7xvxng.com1.z0.glb.clouddn.com/15241955282214.jpg" alt="">&lt;/p></description></item><item><title>启用Jenkins CLI</title><link>https://atbug.com/jenkins-cli-enable/</link><pubDate>Mon, 09 Apr 2018 11:16:38 +0000</pubDate><guid>https://atbug.com/jenkins-cli-enable/</guid><description>Jenkins CLI提供了SSH和Client模式.
Docker运行Jenkins
version: &amp;#39;3&amp;#39; services: jenkins: image: jenkins/jenkins:alpine ports: - 8080:8080 - 50000:50000 - 46059:46059 volumes: - &amp;#34;/Users/addo/DevApps/Docker/data/jenkins:/var/jenkins_home&amp;#34; note: 以为是docker运行, ssh端口设置选用了固定端口.
Client 从http://JENKINS_URL/cli页面下载client jar
使用方法:
java -jar jenkins-cli.jar -s http://localhost:8080/ help 构建:
java -jar jenkins-cli.jar -s http://localhost:8080/ build JOB [-c] [-f] [-p] [-r N] [-s] [-v] [-w] Starts a build, and optionally waits for a completion.</description></item><item><title>Jenkins - 解决execute shell中启动的进程被在Job退出时被杀死问题</title><link>https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/</link><pubDate>Thu, 15 Mar 2018 17:00:25 +0000</pubDate><guid>https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/</guid><description>因为ProcessTreeKiller的存在, 构建过程中使用shell启动的进程在Job完成时都会被kill掉.
各种搜索以及ProcessTreeKiller提供的解决方式是修改BUILD_ID和添加 -Dhudson.util.ProcessTree.disable=true都无法解决.
最后参考StackOverflow和Jenkins JIRA, 修改JENKINS_NODE_COOKIE为任何值, 如dontKillMe. 这种方法可以解决, 记录一下. (搜索排名靠前的结果都不对).</description></item><item><title>Centos 编译安装 Redis</title><link>https://atbug.com/install-redis-on-centos/</link><pubDate>Fri, 07 Apr 2017 16:48:46 +0000</pubDate><guid>https://atbug.com/install-redis-on-centos/</guid><description>版本 Centos7
Redis3.2.8
编译安装 wget http://download.redis.io/releases/redis-3.2.8.tar.gz tar -zxvf redis-3.2.8.tar.gz cd redis-3.2.8 sudo make test sudo make install 启动 redis-server 问题 /bin/sh: cc: command not found
**原因：**Centos安装时选择的类型是Infrastructure，没有c++的编译工具。
解决：sudo yum -y install gcc gcc-c++ libstdc++-devel
malloc.h:50:31: fatal error: jemalloc/jemalloc.h: No such file or directory</description></item><item><title>Centos 上安装 Postgresql</title><link>https://atbug.com/install-postgresql-on-centos/</link><pubDate>Thu, 06 Apr 2017 22:54:17 +0000</pubDate><guid>https://atbug.com/install-postgresql-on-centos/</guid><description>版本 Centos7
Postgresql9.2
Enable ssh service sshd start
Open firewall for 22 firewall-cmd —state
firewall-cmd —list-all
firewall-cmd —permanent —zone=public —add-port=22/tcp
firewall-cmd —reload
Install Postgresql yum install postgres
su postgres
postgres —version
默认会创建postgres:postgres用户和组
切换用户 su - postgres
初始化数据库 通过指定数据文件目录初始化db
initdb -D /var/lib/pgsql/data</description></item><item><title>Haproxy虚拟主机SSL</title><link>https://atbug.com/haproxy-multi-host-with-ssl/</link><pubDate>Mon, 27 Feb 2017 19:31:53 +0000</pubDate><guid>https://atbug.com/haproxy-multi-host-with-ssl/</guid><description>Haproxy为多个域名配置SSL
生成自签名证书 sudo mkdir /etc/ssl/atbug.com sudo openssl genrsa -out /etc/ssl/atbug.com/atbug.com.key 1024 sudo openssl req -new -key /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.csr sudo openssl x509 -req -days 365 -in /etc/ssl/atbug.com/atbug.com.csr -singkey /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.crt sudo openssl x509 -req -days 365 -in /etc/ssl/atbug.com/atbug.com.csr -signkey /etc/ssl/atbug.</description></item><item><title>Flume - FileChannel （一）</title><link>https://atbug.com/flume-filechannel-overview/</link><pubDate>Wed, 23 Nov 2016 09:23:57 +0000</pubDate><guid>https://atbug.com/flume-filechannel-overview/</guid><description>概述 当使用Flume的时候，每个流程都包含了输入源、通道和输出。一个典型的例子是一个web服务器将事件通过RPC（搬入AvroSource）写入输入源中，输入源将其写入MemoryChannel，最后HDFS Sink消费事件将其写入HDFS中。
MemeoryChannel提供了高吞吐量但是在系统崩溃或者断电时会丢失数据。因此需要开发一个可持久话通道。FileChannel是在FLUME-1085里实现的。目标是提供一个高可用高吞吐量的通道。FileChannle保证了在失误提交之后，在崩溃或者断电后不丢失数据。
需要注意的是FileChannel自己不做任何的数据复制，因此它只是和基本的磁盘一样高可用。使用FileChannle的用户需要购买配置更多的硬盘。硬盘最好是RAID、SAN或者类似的。
很多需要通过损失少量的数据（每隔几秒将内存数据fsync到硬盘）换取高吞吐量。Flume团队决定使用另一种方式实现FileChannel。Flue是一个事务型的系统，在一次存或取的事务中可以操作多个事件。通过改变批量大小来控制吞吐量。使用大的批量，Flume可以以比较高的吞吐量传送数据，同时不丢失数据。批量的大小完全由客户端控制。使用RDBMS的用户对这种方式会比较熟悉。
一个Flume事务由存或取组成，但不能同时做两种操作，同样提交和回滚也是一样。每个事务实现了存和取的方法。数据源将事件存入通道，输出从通道中将事件取出。
设计 FileChannel在WAL（预写式日志）的基础上添加了一个内存队列。每个事务都被写成一个基于事务类型（存或取）的WAL，内存队列也相应的被更新。每次是事务提交，正确的文件被fsync保证数据被真正地保存到磁盘上，同时该事件的指针也被保存到了内存队列中。这个队列提供的功能跟其他队列没有区别：管理那些还没有被输出消费的事件。在取的过程中，指针被从队列中删除。事件直接从WAL中读取。得益于当前大容量的RAM，从操作系统的文件缓存中读取很常见。
在系统崩溃之后，WAL可以被重现到队列中保持原来的状态，没有被提交的事务会丢失。重现WAL是耗时的，因此队列也被周期性地写到磁盘上。写队列到磁盘被称作checkpoint。崩溃后，从磁盘读取队列。只有队列保存到磁盘之后提交的事务被重现，这样可以显著的减少需要读取的WAL的数量。
例如，如下有两个事件的通道：
WAL包含了三个重要的元素：事务id、序列号和事件数据。每个事务都有一个唯一的事务id，每个事件都有一个唯一的序列号。事务id只被用来标识事务中的一组事件，序列号在重演日志的时候被用到。上面的例子中，事务id是1，序列号是1、2、3。
当队列被保存到硬盘后 &amp;ndash; 一次checkpoint &amp;ndash; 序列号自动增加并同样被保存。在重启时，队列最先被从硬盘上加载，所有序列号大于队列的WAL项被重现。在checkpoint操作时，channle被锁住以保证没有存取操作改变它的状态。如果允许修改，会导致保存到硬盘上的队列快照不一致。
上面例子中的队列，checkpoint发送在事务1提交之后，因此事件a、b的指针和序列号4被保存到硬盘。
之后，事件a在事务2中被从队列中取出：
如果这时系统崩溃，队列的checkpoint从硬盘中加载。注意这个checkpoint发生在事务2之前，事件a、b的指针存在队列中。因此WAL中序列号大于4的已提交的事务被重现，事件a指针被从队列中删除。
上面的设计有两点没提到。checkpoint时发生的存和取操作会丢失。假设checkpoint在取事件a之后发生：
如果这时系统崩溃，根据上面的设计，事件b指针保存在队列中，所有序列号大于5的WAL项被重现：事务2的回滚被重现。但是事务2的取操作不会被重现。因此事件a指针不会被放回队列因而导致数据丢失。存的场景也类似。因此在队列checkpoint的时候，进行中的事务操作也会被重现，这样这种情况能被正确处理。
实现 FileChannel被保存在flume项目的flume-file-channel模块中，他的java包名是org.apache.flume.channel.file。上面提到队列被叫做FlumeEventQueue，WAL被叫做 Log。队列是一个环形数组，使用Memory Mapped File。WAL是一组以LogFile或其子类序列化的文件。
总结 FileChannle在硬件、软件和系统故障下的持久化并同时保证高吞吐量。如果这亮点都看中的话，FileChannel是推荐使用的通道。
原文</description></item><item><title>Git回车换行</title><link>https://atbug.com/crlf-in-git/</link><pubDate>Wed, 14 Sep 2016 09:16:10 +0000</pubDate><guid>https://atbug.com/crlf-in-git/</guid><description>最近又个项目，checkout之后，没做任何改动前git status发现已经有modified了，通过git diff发现有两种改动：
- warning: CRLF will be replaced by LF in **
- 删除并添加的同样的行
使用git diff -w却没有改动；使用git diff –ws-error-highlight=new,old发现行尾有**^M**
我本人用的是Linux，其他同事有用Windows，问题就出在平台上。
Windows用CR LF来定义换行，Linux用LF。CR全称是Carriage Return ,或者表示为\r, 意思是回车。 LF全称是Line Feed，它才是真正意义上的换行表示符。
git config中关于CRLF有两个设定：core.autocrlf和core.safecrlf。
一、AutoCRLF
#提交时转换为LF，检出时转换为CRLF
git config –global core.autocrlf true
#提交时转换为LF，检出时不转换
git config –global core.</description></item></channel></rss>