<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>最佳实践 on 乱世浮生</title><link>https://atbug.com/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</link><description>Recent content in 最佳实践 on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 09 Jun 2021 00:34:25 +0800</lastBuildDate><atom:link href="https://atbug.com/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubernetes 的自动伸缩你用对了吗？</title><link>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</link><pubDate>Wed, 09 Jun 2021 00:34:25 +0800</pubDate><guid>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</guid><description>
&lt;p>本文翻译自 learnk8s 的 &lt;a href="https://learnk8s.io/kubernetes-autoscaling-strategies#when-autoscaling-pods-goes-wrong">Architecting Kubernetes clusters — choosing the best autoscaling strategy&lt;/a>，&lt;!-- raw HTML omitted -->有增删部分内容&lt;!-- raw HTML omitted -->。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2159402x.png" alt="">&lt;/p>
&lt;p>TL;DR: 在默认设置下，扩展 Kubernetes 集群中的 pod 和节点可能需要几分钟时间。了解如何调整集群节点的大小、配置水平和集群自动缩放器以及过度配置集群以加快扩展速度。&lt;/p>
&lt;h2 id="自动扩展器">自动扩展器&lt;/h2>
&lt;p>在 Kubernetes 中，常说的“自用扩展”有：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA：Pod 水平缩放器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VPA：Pod 垂直缩放器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">CA：集群自动缩放器&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>不同类型的自动缩放器，使用的场景不一样。&lt;/p>
&lt;h3 id="hpa">HPA&lt;/h3>
&lt;p>HPA 定期检查内存和 CPU 等指标，自动调整 Deployment 中的副本数，比如流量变化：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2206552x.png" alt="调整前">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2207122x.png" alt="调整后">&lt;/p>
&lt;h3 id="vpa">VPA&lt;/h3>
&lt;p>有些时候无法通过增加 Pod 数来扩容，比如数据库。这时候可以通过 VPA 增加 Pod 的大小，比如调整 Pod 的 CPU 和内存：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2212162x.png" alt="调整前">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2212342x.png" alt="调整后">&lt;/p>
&lt;h3 id="ca">CA&lt;/h3>
&lt;p>当集群资源不足时，CA 会自动配置新的计算资源并添加到集群中：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2213392x.png" alt="调整前">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2214182x.png" alt="调整后">&lt;/p>
&lt;h2 id="自动缩放-pod-出错时">自动缩放 Pod 出错时&lt;/h2>
&lt;p>比如一个应用需要 1.5 GB 内存和 0.25 个 vCPU。一个 8GB 和 2 个 vCPU 的节点，可以容纳 4 个这样的 Pod，完美！&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2216192x.png" alt="">&lt;/p>
&lt;p>做如下配置：&lt;/p>
&lt;ol>
&lt;li>HPA：每增加 10 个并发，增加一个副本。即 40 个并发的时候，自动扩展到 4 个副本。（这里使用自定义指标，比如来自 Ingress Controller 的 QPS）&lt;/li>
&lt;li>CA：在资源不足的时候，增加计算节点。&lt;/li>
&lt;/ol>
&lt;p>当并发达到 30 的时候，系统是下面这样。完美！HPA 工作正常，CA 没工作。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2223102x.png" alt="">&lt;/p>
&lt;p>当增加到 40 个并发的时候，系统是下面的情况：&lt;/p>
&lt;ol>
&lt;li>HPA 增加了一个 Pod&lt;/li>
&lt;li>Pod 挂起&lt;/li>
&lt;li>CA 增加了一个节点&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2224462x.png" alt="HPA 工作">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2225022x.png" alt="CA 工作">&lt;/p>
&lt;p>&lt;em>为什么 Pod 没有部署成功？&lt;/em>&lt;/p>
&lt;p>节点上的操作系统进程和 kubelet 也会消耗一部分资源，8G 和 2 vCPU 并不是全都可以提供给 Pod 用的。并且还有一个&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#eviction-thresholds">驱逐阈值&lt;/a>：在节点系统剩余资源达到阈值时，会驱逐 Pod，避免 OOM 的发生。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2230402x.png" alt="">&lt;/p>
&lt;p>当然上面的这些都是&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">可配置&lt;/a>的。&lt;/p>
&lt;p>&lt;em>那为什么在创建该 Pod 之前，CA 没有增加新的节点呢？&lt;/em>&lt;/p>
&lt;h2 id="ca-如何工作">CA 如何工作？&lt;/h2>
&lt;p>&lt;strong>CA 在触发自动缩放时，不会查看可用的内存或 CPU。&lt;/strong>&lt;/p>
&lt;p>CA 是面向事件工作的，并每 10 秒检查一次是否存在不可调度（Pending）的 Pod。&lt;/p>
&lt;p>当调度器无法找到可以容纳 Pod 的节点时，这个 Pod 是不可调度的。&lt;/p>
&lt;p>此时，CA 开始创建新节点：CA 扫描集群并检查是否有不可调度的 Pod。&lt;/p>
&lt;p>当集群有多种节点池，CA 会通过选择下面的一种策略：&lt;/p>
&lt;ul>
&lt;li>&lt;code>random&lt;/code>：默认的扩展器，随机选择一种节点池&lt;/li>
&lt;li>&lt;code>most-pods&lt;/code>：能够调度最多 Pod 的节点池&lt;/li>
&lt;li>&lt;code>least-waste&lt;/code>：选择扩展后，资源空闲最少的节点池&lt;/li>
&lt;li>&lt;code>price&lt;/code>：选择成本最低的节点池&lt;/li>
&lt;li>&lt;code>priority&lt;/code>：选择用户分配的具有最高优先级的节点池&lt;/li>
&lt;/ul>
&lt;p>确定类型后，CA 会调用相关 API 来创建资源。（云厂商会实现 API，比如 AWS 添加 EC2；Azure 添加 Virtual Machine；阿里云增加 ECS；GCP 增加 Compute Engine）&lt;/p>
&lt;p>计算资源就绪后，就会进行&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">节点的初始化&lt;/a>。&lt;/p>
&lt;p>注意，这里需要一定的耗时，通常比较慢。&lt;/p>
&lt;h2 id="探索-pod-自动缩放的前置时间">探索 Pod 自动缩放的前置时间&lt;/h2>
&lt;p>四个因素：&lt;/p>
&lt;ol>
&lt;li>HPA 的响应耗时&lt;/li>
&lt;li>CA 的响应耗时&lt;/li>
&lt;li>节点的初始化耗时&lt;/li>
&lt;li>Pod 的创建时间&lt;/li>
&lt;/ol>
&lt;p>默认情况下，&lt;a href="https://github.com/kubernetes/kubernetes/blob/2da8d1c18fb9406bd8bb9a51da58d5f8108cb8f7/pkg/kubelet/kubelet.go#L1855">kubelet 每 10 秒抓取一次 Pod 的 CPU 和内存占用情况&lt;/a>。&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-often-metrics-are-scraped">每分钟，Metrics Server 会将聚合的指标开放&lt;/a>给 Kubernetes API 的其他组件使用。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2256302x.png" alt="">&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-up-work">CA 每 10 秒排查不可调度的 Pod。&lt;/a>&lt;/p>
&lt;ul>
&lt;li>少于 100 个节点，且每个节点最多 30 个 Pod，时间不超过 30s。平均延迟大约 5s。&lt;/li>
&lt;li>100 到 1000个节点，不超过 60s。平均延迟大约 15s。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2300242x.png" alt="">&lt;/p>
&lt;p>节点的配置时间，取决于云服务商。通常在 3~5 分钟。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2301312x.png" alt="">&lt;/p>
&lt;p>容器运行时创建 Pod：启动容器的几毫秒和&lt;strong>下载镜像的几秒钟&lt;/strong>。如果不做镜像缓存，几秒到 1 分钟不等，取决于层的大小和梳理。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2302382x.png" alt="">&lt;/p>
&lt;p>对于小规模的集群，最坏的情况是 6 分 30 秒。对于 100 个以上节点规模的集群，可能高达 7 分钟。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">HPA delay: 1m30s +
CA delay: 0m30s +
Cloud provider: 4m +
Container runtime: 0m30s +
=========================
Total 6m30s
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>突发情况，比如流量激增，你是否愿意等这 7 分钟？&lt;/em>&lt;/p>
&lt;p>&lt;em>这 7 分钟，如何优化压缩？&lt;/em>&lt;/p>
&lt;ul>
&lt;li>HPA 的刷新时间，默认 15 秒，通过 &lt;code>--horizontal-pod-autoscaler-sync-period&lt;/code> 标志控制。&lt;/li>
&lt;li>Metrics Server 的指标抓取时间，默认 60 秒，通过 &lt;code>metric-resolution&lt;/code> 控制。&lt;/li>
&lt;li>CA 的扫描间隔，默认 10 秒，通过 &lt;code>scan-interval&lt;/code> 控制。&lt;/li>
&lt;li>节点上缓存镜像，比如 &lt;a href="https://github.com/senthilrch/kube-fledged">kube-fledged&lt;/a> 等工具。&lt;/li>
&lt;/ul>
&lt;p>即使调小了上述设置，依然会受云服务商的时间限制。&lt;/p>
&lt;p>&lt;em>那么，如何解决？&lt;/em>&lt;/p>
&lt;p>两种尝试：&lt;/p>
&lt;ol>
&lt;li>尽量避免被动创建新节点&lt;/li>
&lt;li>主动创建新节点&lt;/li>
&lt;/ol>
&lt;h2 id="为-kubernetes-选择最佳规格的节点">为 Kubernetes 选择最佳规格的节点&lt;/h2>
&lt;p>&lt;strong>这会对扩展策略产生巨大影响。&lt;/strong>&lt;/p>
&lt;p>&lt;em>这样的场景&lt;/em>&lt;/p>
&lt;p>应用程序需要 1GB 内存和 0.1 vCPU；有一个 4GB 内存和 1 个 vCPU 的节点。&lt;/p>
&lt;p>排除操作系统、kubelet 和阈值保留空间后，有 2.5GB 内存和 0.7 个 vCPU 可用。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2314032x.png" alt="">&lt;/p>
&lt;p>最多只能容纳 2 个 Pod，扩展副本时最长耗时 7 分钟（HPA、CA、云服务商的资源配置耗时）&lt;/p>
&lt;p>假如节点的规格是 64GB 内存和 16 个 vCPU，可用的资源为 58.32GB 和 15.8 个 vCPU。&lt;/p>
&lt;p>&lt;strong>这个节点可以托管 58 个 Pod。只有扩容第 59 个副本时，才需要创建新的节点。&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2316562x.png" alt="CleanShot 2021-06-08 at 23.16.56@2x">&lt;/p>
&lt;p>这样触发 CA 的机会更少。&lt;/p>
&lt;p>选择大规格的节点，还有另外一个好处：资源的利用率会更高。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2317562x.png" alt="">&lt;/p>
&lt;p>&lt;strong>节点上可以容纳的 Pod 数量，决定了效率的峰值。&lt;/strong>&lt;/p>
&lt;p>物极必反！更大的实例，并不是一个好的选择：&lt;/p>
&lt;ol>
&lt;li>爆炸半径（Blast radius）：节点故障时，少节点的集群和多节点的集群，前者影响更大。&lt;/li>
&lt;li>自动缩放的成本效益低：增加一个大容量的节点，其利用率会比较低（调度过去的 Pod 数少）&lt;/li>
&lt;/ol>
&lt;p>&lt;em>即使选择了正确规格的节点，配置新的计算单元时，延迟仍然存在。怎么解决？&lt;/em>&lt;/p>
&lt;p>&lt;em>能否提前创建节点？&lt;/em>&lt;/p>
&lt;h2 id="为集群过度配置节点">为集群过度配置节点&lt;/h2>
&lt;p>即为集群增加备用节点，可以：&lt;/p>
&lt;ol>
&lt;li>创建一个节点，并留空 （比如 SchedulingDisabled）&lt;/li>
&lt;li>一旦空节点中有了一个 Pod，马上创建新的空节点&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2325582x.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2326262x.png" alt="CleanShot 2021-06-08 at 23.26.26@2x">&lt;/p>
&lt;p>&lt;strong>这种会产生额外的成本，但是效率会提升。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CA 并不支持此功能 &amp;ndash; 总是保留一个空节点。&lt;/strong>&lt;/p>
&lt;p>但是，可以伪造。创建一个只占用资源，不使用资源的 Pod 占用整个 Node 节点。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2329322x.png" alt="">&lt;/p>
&lt;p>一旦有了真正的 Pod，驱逐占位的 Pod。
&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2330332x.png" alt="">&lt;/p>
&lt;p>待后台完成新的节点配置后，将“占位” Pod 再次占用整个节点。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2331062x.png" alt="">&lt;/p>
&lt;p>这个“占位”的 Pod 可以通过永久休眠来实现空间的保留。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">pause&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">k8s.gcr.io/pause&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;1739m&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;5.9G&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>使用&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/">优先级和抢占&lt;/a>，来实现创建真正的 Pod 后驱逐“占位”的 Pod。&lt;/p>
&lt;p>使用 &lt;code>PodPriorityClass&lt;/code> 在配置 Pod 优先级：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">scheduling.k8s.io/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">PriorityClass&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>-&lt;span class="m">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">#默认的是 0，这个表示比默认的低&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">globalDefault&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">description&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;Priority class used by overprovisioning.&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>为“占位” Pod 配置优先级：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">priorityClassName&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">#HERE&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">reserve-resources&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">k8s.gcr.io/pause&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;1739m&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;5.9G&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>已经做完过度配置，应用程序是否需要优化？&lt;/em>&lt;/p>
&lt;h2 id="为-pod-选择正确的内存和-cpu-请求">为 Pod 选择正确的内存和 CPU 请求&lt;/h2>
&lt;p>Kubernetes 是根据 Pod 的内存和 CPU 请求，为其分配节点。&lt;/p>
&lt;p>如果 Pod 的资源请求配置不正确，可能会过晚（或过早）触发自动缩放器。&lt;/p>
&lt;p>这样一个场景：&lt;/p>
&lt;ul>
&lt;li>应用程序平均负载下消耗 512MB 内存和 0.25 个 vCPU。&lt;/li>
&lt;li>高峰时，消耗 4GB 内存 和 1 个 vCPU。（即资源限制，Limit）&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2338292x.png" alt="">&lt;/p>
&lt;p>有三种请求的配置选择：&lt;/p>
&lt;ol>
&lt;li>远低于平均使用量&lt;/li>
&lt;li>匹配平均使用量&lt;/li>
&lt;li>尽量接近限制&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2344462x.png" alt="2">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2345002x.png" alt="2">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2346042x.png" alt="3">&lt;/p>
&lt;p>第一种的问题在于&lt;strong>超卖严重，过度使用节点&lt;/strong>。kubelet 负载高，稳定性差。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2346452x.png" alt="1">&lt;/p>
&lt;p>第三种，会造成资源的利用率低，浪费资源。这种通常被称为 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QoS：Quality of Service class&lt;/a> 中的 &lt;code>Guaranteed&lt;/code> 级别，Pod 不会被终止和驱逐。
&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2347382x.png" alt="3">&lt;/p>
&lt;p>&lt;em>如何在稳定性和资源使用率间做权衡？&lt;/em>&lt;/p>
&lt;p>这就是 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QoS：Quality of Service class&lt;/a> 中的 &lt;code>Burstable&lt;/code> 级别，即 Pod 偶尔会使用更多的内存和 CPU。&lt;/p>
&lt;ol>
&lt;li>如果节点中有可用资源，应用程序会在返回基线（baseline）前使用这些资源。&lt;/li>
&lt;li>如果资源不足，Pod 将竞争资源（CPU），kubelet 也有可能尝试驱逐 Pod（内存）。&lt;/li>
&lt;/ol>
&lt;p>在 &lt;code>Guaranteed&lt;/code> 和 &lt;code>Burstable&lt;/code> 之前如何做选择？取决于：&lt;/p>
&lt;ol>
&lt;li>想尽量减少 Pod 的重新调度和驱逐，应该是用 &lt;code>Guaranteed&lt;/code>。&lt;/li>
&lt;li>如果想充分利用资源时，使用 &lt;code>Burstable&lt;/code>。比如弹性较大的服务，Web 或者 REST 服务。&lt;/li>
&lt;/ol>
&lt;p>&lt;em>如何做出正确的配置？&lt;/em>&lt;/p>
&lt;p>应该分析应用程序，并测算空闲、负载和峰值时的内存和 CPU 消耗。&lt;/p>
&lt;p>甚至可以通过部署 VPA 来自动调整。&lt;/p>
&lt;h2 id="如何进行集群缩容">如何进行集群缩容？&lt;/h2>
&lt;p>&lt;strong>每 10 秒，当请求（request）利用率低于 50%时，CA 才会决定删除节点。&lt;/strong>&lt;/p>
&lt;p>CA 会汇总同一个节点上的所有 Pod 的 CPU 和内存请求。小于节点容量的一半，就会考虑对当前节点进行缩减。&lt;/p>
&lt;blockquote>
&lt;p>需要注意的是，CA 不考虑实际的 CPU 和内存使用或者限制（limit），只看请求（request）。&lt;/p>
&lt;/blockquote>
&lt;p>移除节点之前，CA 会：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node">检查 Pod&lt;/a> 确保可以调度到其他节点上。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#i-have-a-couple-of-nodes-with-low-utilization-but-they-are-not-scaled-down-why">检查节点&lt;/a>，避免节点被过早的销毁，比如两个节点的请求都低于 50%。&lt;/li>
&lt;/ol>
&lt;p>检查都通过之后，才会删除节点。&lt;/p>
&lt;h2 id="为什么不根据内存或-cpu-进行自动缩放">为什么不根据内存或 CPU 进行自动缩放？&lt;/h2>
&lt;p>&lt;strong>基于内存和 CPU 的自动缩放器，不关心 pod。&lt;/strong>&lt;/p>
&lt;p>比如配置缩放器在节点的 CPU 达到总量的 80%，就自动增加节点。&lt;/p>
&lt;p>当你创建 3 个副本的 Deployment，3 个节点的 CPU 达到了 85%。这时会创建一个节点，但你并不需要第 4 个副本，新的节点就空闲了。&lt;/p>
&lt;p>&lt;strong>不建议使用这种类型的自动缩放器。&lt;/strong>&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>定义和实施成功的扩展策略，需要掌握以下几点：&lt;/p>
&lt;ul>
&lt;li>节点的可分配资源。&lt;/li>
&lt;li>微调 Metrics Server、HPA 和 CA 的刷新间隔。&lt;/li>
&lt;li>设计集群和节点的规格。&lt;/li>
&lt;li>缓存容器镜像到节点。&lt;/li>
&lt;li>应用程序的基准测试和分析。&lt;/li>
&lt;/ul>
&lt;p>配合适当的监控工具，可以反复测试扩展策略并调整集群的缩放速度和成本。&lt;/p></description></item></channel></rss>