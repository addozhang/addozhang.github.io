<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>eBPF on 乱世浮生</title><link>https://atbug.com/tags/ebpf/</link><description>Recent content in eBPF on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 31 Mar 2024 19:01:44 +0800</lastBuildDate><atom:link href="https://atbug.com/tags/ebpf/index.xml" rel="self" type="application/rss+xml"/><item><title>抽丝剥茧：从 Linux 源码探索 eBPF 的实现</title><link>https://atbug.com/exploring-ebpf-implementation-through-linux-source-code/</link><pubDate>Sun, 31 Mar 2024 19:01:44 +0800</pubDate><guid>https://atbug.com/exploring-ebpf-implementation-through-linux-source-code/</guid><description>去年学习 eBPF，分享过 几篇 eBPF 方面的学习笔记，都是面向 eBPF 的应用。为了准备下一篇文章，这次决定从 Linux 源码入手，深入了解 eBPF 的工作原理。因此这篇又是一篇学习笔记，假如你对 eBPF 的工作原理也感兴趣，不如跟随我的脚步一起。文章中若有任何问题，请不吝赐教。
这里不会再对 eBPF 进行过多的介绍，可以参考我的另一篇 使用 eBPF 技术实现更快的网络数据包传输，结合 追踪 Kubernetes 中的数据包 可以了解 eBPF 的基本内容以及其在网络加速方面的应用。
接下来我们还是使用 eBPF sockops 中的程序 bpf_sockops 为例， 配合 Linux v6.8 源码探索 eBPF 的工作原理。
BPF 程序操作 在 load.</description></item><item><title>Cilium 如何处理 L7 流量</title><link>https://atbug.com/deep-dive-into-cilium-l7-packet-processing/</link><pubDate>Sun, 11 Jun 2023 15:35:15 +0800</pubDate><guid>https://atbug.com/deep-dive-into-cilium-l7-packet-processing/</guid><description>还记得在 使用 Cilium 增强 Kubernetes 网络安全 示例中，我们通过设置网络策略限制钛战机 tiefighter 访问死星 deathstar 的 /v1/exhaust-port 端点，但放行着陆请求 /v1/request-landing。在提起 Cilium 时，都说其是使用 eBPF 技术推动的用于提供、保护和观察容器工作负载之间的网络连接的开源软件。eBPF 可以处理 L3/4 的数据包，但是对复杂的 L7 的协议处理的成本比较高，并且无法应对 L7 协议策略的灵活性。Cilium 引入 Envoy Proxy（Cilium 定制的发行版）作为 L7 代理，来处理该场景。
那 Cilium 是如何处理 L7 流量的呢？今天就让我们一探究竟。
注，这篇的内容是基于目前最新的 Cilium 1.13.3 和 proxy 1.</description></item><item><title>深入探索 Cilium 的工作机制</title><link>https://atbug.com/deep-dive-into-cilium/</link><pubDate>Sat, 20 May 2023 18:27:39 +0800</pubDate><guid>https://atbug.com/deep-dive-into-cilium/</guid><description>这篇之前写 Kubernetes 网络学习之 Cilium 与 eBPF 记录的内容，隔了几个月终于想起把笔记完成，作为探索 Cilium 工作原理的入门，也还是 Cilium 冰山一角，像是高级的网络策略、网络加密、BGP 网络、服务网格等方面并没有深入。如果阅读过程中有发现任何问题，也烦请纠正。
本文基于 Cilium v1.12 及 Kubernetes v1.25。
实验环境 我们使用 k8e 创建集群，因为 k8e 使用 Cilium 作为默认的 CNI 实现。在我的 homelab 上做个双节点（ubuntu-test1: 192.168.1.21、ubuntu-test2: 192.168.1.22）的集群。
Master 节点
curl -sfL https://getk8e.com/install.sh | API_SERVER_IP=192.168.1.21 K8E_TOKEN=ilovek8e INSTALL_K8E_EXEC=&amp;#34;server --cluster-init --write-kubeconfig-mode 644 --write-kubeconfig ~/.</description></item><item><title>使用 eBPF 技术实现更快的网络数据包传输</title><link>https://atbug.com/accelerate-network-packets-transmission/</link><pubDate>Wed, 22 Mar 2023 07:03:28 +0800</pubDate><guid>https://atbug.com/accelerate-network-packets-transmission/</guid><description>在 上篇文章 用了整篇的内容来描述网络数据包在 Kubernetes 网络中的轨迹，文章末尾，我们提出了一种假设：同一个内核空间中的两个 socket 可以直接传输数据，是不是就可以省掉内核网络协议栈处理带来的延迟？
不论是同 pod 中的两个不同容器，或者同节点的两个 pod 间的网络通信，实际上都发生在同一个内核空间中，互为对端的两个 socket 也都位于同一个内存中。而在上篇文章的开头也总结了数据包的传输轨迹实际上是 socket 的寻址过程，可以进一步将问题展开：同一节点上的两个 socket 间的通信，如果可以 快速定位到对端的 socket &amp;ndash; 找到其在内存中的地址，我们就可以跳过内核协议栈的环节进而加速数据包的传输。
互为对端的两个 socket 也就是建立起连接的客户端 socket 和服务端 socket，他们可以通过 IP 地址和端口进行关联。客户端 socket 的本地地址和端口，是服务端 socket 的远端地址和端口；客户端 socket 的远端地址和端口，则是服务端 socket 的本地地址和端口。
当客户端和服务端在完成连接的建立之后，如果可以使用本地地址 + 端口和远端地址 + 端口端口的组合 指向 socket，仅需调换本地和远端的地址 + 端口，即可定位到对端的 socket，然后将数据直接写到对端 socket（实际是写入 socket 的接收队列 RXQ，这里不做展开），就可以避开内核网络栈（包括 netfilter/iptables）甚至是 NIC 的处理。</description></item><item><title>Kubernetes 网络学习之 Cilium 与 eBPF</title><link>https://atbug.com/learn-cilium-and-ebpf/</link><pubDate>Wed, 11 Jan 2023 18:12:58 +0800</pubDate><guid>https://atbug.com/learn-cilium-and-ebpf/</guid><description>这是 Kubernetes 网络学习的第五篇笔记，也是之前计划中的最后一篇。
深入探索 Kubernetes 网络模型和网络通信 认识一下容器网络接口 CNI 源码分析：从 kubelet、容器运行时看 CNI 的使用 从 Flannel 学习 Kubernetes VXLAN 网络 Kubernetes 网络学习之 Cilium 与 eBPF（本篇） &amp;hellip; 开始之前说点题外话，距离上一篇 Flannel CNI 的发布已经快一个月了。这篇本想趁着势头在去年底完成的，正好在一个月内完成计划的所有内容。但上篇发布后不久，我中招了花了一个多周的时间才恢复。然而，恢复后的状态让我有点懵，总感觉很难集中精力，很容易精神涣散。可能接近网上流传的“脑雾”吧，而且 Cilium 也有点类似一团迷雾。再叠加网络知识的不足，eBPF 也未从涉足，学习的过程中断断续续，我曾经一度怀疑这篇会不会流产。
文章中不免会有问题，如果有发现问题或者建议，望不吝赐教。
背景 去年曾经写过一篇文章 《使用 Cilium 增强 Kubernetes 网络安全》 接触过 Cilium，借助 Cilium 的网络策略从网络层面对 pod 间的通信进行限制。但当时我不曾深入其实现原理，对 Kubernetes 网络和 CNI 的了解也不够深入。这次我们通过实际的环境来探寻 Cilium 的网络。</description></item><item><title>【译】eBPF 和服务网格：还不能丢掉 Sidecar</title><link>https://atbug.com/translate-ebpf-service-mesh/</link><pubDate>Mon, 31 Oct 2022 20:51:17 -0400</pubDate><guid>https://atbug.com/translate-ebpf-service-mesh/</guid><description>服务网格以典型的 sidecar 模型为人熟知，将 sidecar 容器与应用容器部署在同一个 Pod 中。虽说 sidecar 并非很新的模型（操作系统的 systemd、initd、cron 进程；Java 的多线程），但是以这种与业务逻辑分离的方式来提供服务治理等基础能力的设计还是让人一亮。
随着 eBPF 等技术的引入，最近关于服务网格是否需要 sidecar （也就是 sidecarless）的讨论渐增。
笔者认为任何问题都有其起因，长久困扰服务网格的不外乎性能和资源占用。这篇文章翻译自 Buoyant 的 Flynn 文章 eBPF and the Service Mesh: Don&amp;rsquo;t Dismiss the Sidecar Yet。希望这篇文章能帮助大家穿透迷雾看透事物的本质。
本文要点 eBPF 是一个旨在通过（谨慎地）允许在内核中运行一些用户代码来提高性能的工具。 在可预见的未来，服务网格所需的第 7 层处理在 eBPF 中不太可能实现，这意味着网格仍然需要代理。 与 sidecar 代理相比，每个主机代理增加了操作复杂性并降低了安全性。 可以通过更小、更快的 Sidecar 代理来解决有关 Sidecar 代理的典型性能问题。 目前，sidecar 模型对服务网格仍是最有意义的。 关于 eBPF 的故事已经在云原生世界中泛滥了一段时间，有时将其描述为自切片面包以来最伟大的事物，有时则嘲笑它是对现实世界的无用干扰。当然，现实要微妙得多，因此仔细研究一下 eBPF 能做什么和不能做什么似乎是有必要的——技术毕竟只是工具，使用的工具应该适合手头的任务。</description></item><item><title>使用 Cilium 增强 Kubernetes 网络安全</title><link>https://atbug.com/enhance-kubernetes-network-security-with-cilium/</link><pubDate>Sun, 13 Feb 2022 05:03:48 +0800</pubDate><guid>https://atbug.com/enhance-kubernetes-network-security-with-cilium/</guid><description>TL;DR 在本篇，我们分别使用了 Kubernetes 原生的网络策略和 Cilium 的网络策略实现了 Pod 网络层面的隔离。不同的是，前者只提供了基于 L3/4 的网络策略；后者支持 L3/4、L7 的网络策略。
通过网络策略来提升网络安全，可以极大降低了实现和维护的成本，同时对系统几乎没有影响。
尤其是基于 eBPF 技术的 Cilium，解决了内核扩展性不足的问题，从内核层面为工作负载提供安全可靠、可观测的网络连接。
目录 TL;DR 目录 背景 示例应用 Kubernetes 网络策略 测试 思考 Cilium 网络策略 Cilium 简介 测试 背景 为什么说 Kubernetes 网络存在安全隐患？集群中的 Pod 默认是未隔离的，也就是 Pod 之间的网络是互通的，可以互相通信的。
这里就会有问题，比如由于数据敏感服务 B 只允许特定的服务 A 才能访问，而服务 C 无法访问 B。要禁止服务 C 对服务 B 的访问，可以有几种方案：</description></item><item><title>eBPF 和 Wasm：探索服务网格数据平面的未来</title><link>https://atbug.com/ebpf-wasm-service-mesh/</link><pubDate>Tue, 11 Jan 2022 10:40:56 +0800</pubDate><guid>https://atbug.com/ebpf-wasm-service-mesh/</guid><description>本文翻译自 Vivian Hu 的 《eBPF and Wasm: Exploring the Future of the Service Mesh Data Plane》。
在 2021 年 12 月 2 日，Cilium 项目宣布了 Cilium Service Mesh 项目的测试版。在 2020 年 8 月 Google Cloud 宣布基于 eBPF 的 Google Kubernetes 服务（GKS）的数据平面 V2 的一年后，Cilium Service Mesh 带来了 “无边车服务网格”（sidecarless service mesh）的想法。它扩展了 Cilium eBPF 产品来处理服务网格中的大部分边车代理功能，包括 7 层路由和负载均衡、TLS 终止、访问策略、健康检查、日志和跟踪，以及内置的 Kubernetes Ingress。</description></item></channel></rss>