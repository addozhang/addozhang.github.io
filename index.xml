<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>乱世浮生</title><link>https://atbug.com/</link><description>Recent content on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 13 Feb 2022 05:03:48 +0800</lastBuildDate><atom:link href="https://atbug.com/index.xml" rel="self" type="application/rss+xml"/><item><title>使用 Cilium 增强 Kubernetes 网络安全</title><link>https://atbug.com/enhance-kubernetes-network-security-with-cilium/</link><pubDate>Sun, 13 Feb 2022 05:03:48 +0800</pubDate><guid>https://atbug.com/enhance-kubernetes-network-security-with-cilium/</guid><description>
TL;DR 在本篇，我们分别使用了 Kubernetes 原生的网络策略和 Cilium 的网络策略实现了 Pod 网络层面的隔离。不同的是，前者只提供了基于 L3/4 的网络策略；后者支持 L3/4、L7 的网络策略。 通过网络策略来提升网络安全，可以极大降低了实现和维护的成本，同时对系统几乎没有影响。 尤其是基于 eBPF 技术的 Cilium，解决了内核扩展性不足的问题，从内核层面为工作负载提供安全可靠、可观测的网络连接。 目录 TL;DR 背景 示例应用 Kubernetes 网络策略 测试 思考 Cilium 网络策略 Cilium 简介 测试 背景 为什么说 Kubernetes 网络存在安全隐患？集群中的 Pod 默认是未隔离的，也就是 Pod 之间的网络是互通的，可以互相通信的。 这里就会有问题，比如由于数据敏感服务 B 只允许特定的服务 A 才能访问，而服务 C 无法访问 B。要禁止服</description></item><item><title>探秘 k8e：极简 Kubernetes 发行版</title><link>https://atbug.com/explore-simple-kubernetes-distribution/</link><pubDate>Mon, 07 Feb 2022 01:00:29 +0800</pubDate><guid>https://atbug.com/explore-simple-kubernetes-distribution/</guid><description>
TL;DR 本文介绍并安装体验了极简 Kubernetes 发行版，也顺便分析学习下编译的流程。 背景 k8e 本意为 kuber easy，是一个 Kubernetes 的极简发行版，意图让云原生落地部署 Kubernetes 更轻松。k8e 是基于另一个发行版 k3s ，经过裁剪（去掉了 Edge/IoT 相关功能、traefix等）、扩展（加入 ingress、sidecar 实现、cilium等）而来。 k8e 具有以下特性： 单二进制文件，集成了 k8s 的各种组件、containerd、runc、kubectl、nerdctl 等 使用 cilium 作为 cni 的实现，方便 eBPF 的快速落地 支持基于 Pipy 的 ingress、sidecar proxy，实现应用流量一站式管理 只维护一个 k8s 版本，目前是 1.21 按照私有云的经验增加、优化代码 得益于这些特性，k8e 非常适合CI</description></item><item><title>使用 sdkman 在 M1 Mac 上 安装 graalvm jdk</title><link>https://atbug.com/install-graalvm-on-m1-mac-with-sdkman/</link><pubDate>Thu, 27 Jan 2022 11:07:34 +0800</pubDate><guid>https://atbug.com/install-graalvm-on-m1-mac-with-sdkman/</guid><description>
&lt;p>SDKMAN 是一款管理多版本 SDK 的工具，可以实现在多个版本间的快速切换。安装和使用非常简单：&lt;/p></description></item><item><title>追踪 Kubernetes 中的网络流量</title><link>https://atbug.com/tracing-path-of-kubernetes-network-packets/</link><pubDate>Sat, 22 Jan 2022 10:15:31 +0800</pubDate><guid>https://atbug.com/tracing-path-of-kubernetes-network-packets/</guid><description>
译者注： 这篇文章很全面的罗列出了 Kubernetes 中涉及的网络知识，从 Linux 内核的网络内容，到容器、Kubernetes，一一进行了详细的说明。 ​文章篇幅有点长，不得不说，网络是很复杂很麻烦的一层，但恰恰这层多年来变化不大。希望翻译的内容对大家能有所帮助，有误的地方，也欢迎大家指正。 本文翻译获得 Learnk8s 的授权，原文 Tracing the path of network traffic in Kubernetes 作者 Kristijan Mitevski。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427680911539.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427680911539.jpg 使用 Page Bundles: false TL;DR： 本文将代理了解 Kubernetes 集群内外的数据流转。从最初的 Web 请求开始，一直到托管应用程序的容器。 目录 目录 Kubernetes 网络要求 Linux 网络命名空间如果在 pod 中工作 Pause 容器创建 Pod 中的网络命名空间 为 Pod 分配了 IP 地址 检查集群中 pod 到</description></item><item><title>Kubernetes HPA 基于 Prometheus 自定义指标的可控弹性伸缩</title><link>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</link><pubDate>Tue, 18 Jan 2022 12:03:59 +0800</pubDate><guid>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</guid><description>
在《Kubernetes 的自动伸缩你用对了吗？》 一文中详细说明了如何使用 Kubernetes 的自动伸缩。在 Kubernetes 中弹性伸缩主要有三种：HPA、VPA、CA。本文不再详细说明，有兴趣的可以看那篇文章。这里主要来说下 Pod 水平缩放 HPA。 随着 Kubernetes v1.23 的发布，HPA 的 API 来到了稳定版 autoscaling/v2： 基于自定义指标的伸缩 基于多项指标的伸缩 可配置的伸缩行为 从最初的 v1 版本 HPA 只支持 CPU、内存利用率的伸缩，到后来的自定义指标、聚合层 API 的支持，到了 v1.18 版本又加入了配置伸缩行为的支持，HPA 也越来越好用、可靠。 依靠 CPU 或者内存指标的扩容并非使用所有系统，看起来也没那么可靠。对大部分的 web 后端系统来说，基于 RPS（每秒请求数）的弹性伸缩</description></item><item><title>eBPF 和 Wasm：探索服务网格数据平面的未来</title><link>https://atbug.com/ebpf-wasm-service-mesh/</link><pubDate>Tue, 11 Jan 2022 10:40:56 +0800</pubDate><guid>https://atbug.com/ebpf-wasm-service-mesh/</guid><description>
本文翻译自 Vivian Hu 的 《eBPF and Wasm: Exploring the Future of the Service Mesh Data Plane》。 在 2021 年 12 月 2 日，Cilium 项目宣布了 Cilium Service Mesh 项目的测试版。在 2020 年 8 月 Google Cloud 宣布基于 eBPF 的 Google Kubernetes 服务（GKS）的数据平面 V2 的一年后，Cilium Service Mesh 带来了 “无边车服务网格”（sidecarless service mesh）的想法。它扩展了 Cilium eBPF 产品来处理服务网格中的大部分边车代理功能，包括 7 层路由和负载均衡、TLS 终止、访问策略、健康检查、日志和跟踪，以及内置的 Kubernetes Ingress。 Cilium 的创始人 Isovalent 在一篇名为 “How eBPF will solve Service Mesh - Goodbye Sidecars” 的文章中解释了 eBPF 如何替代边车代理。 它将我们从边车模型中解放处理，并允许我们将现有的代理技术集成到现有的内核命名空间概念中</description></item><item><title>快速搭建实验环境：使用 Terraform 部署 Proxmox 虚拟机</title><link>https://atbug.com/deploy-vm-on-proxmox-with-terraform/</link><pubDate>Mon, 03 Jan 2022 12:07:35 +0800</pubDate><guid>https://atbug.com/deploy-vm-on-proxmox-with-terraform/</guid><description>
自从用上 m1 的电脑，本地开发环境偶尔会遇到兼容性的问题。比如之前尝试用 Colima 在虚拟机中运行容器运行时和 Kubernetes，其实际使用的还是 aarch64 虚拟机，实际使用还是会有些差异。 手上有台之前用的黑苹果小主机，吃灰几个月了，实属浪费。正好元旦假期，有时间折腾一下。 CPU: Intel 8700 6C12T MEM: 64G DDR4 DISK: 1T SSD 折腾的目的： 将平台虚拟化 提供多套实验环境 快速创建销毁实验环境 体验基础设施即代码 IaaS 主要用到的工具： 虚拟化工具 Proxmox VE Terraform：开源的基础设施即代码工具 terraform-provider-proxmox：Terraform Proxmox Provider，通过 Proxmox VE 的 REST API 在创建虚拟机。 安装 Proxmox 虚拟化工具 从官网 下载 ISO 镜像，写入到 U 盘中。mac</description></item><item><title>再见，2021</title><link>https://atbug.com/farewell-2021/</link><pubDate>Thu, 30 Dec 2021 23:48:32 +0800</pubDate><guid>https://atbug.com/farewell-2021/</guid><description>
没错，这是一篇 2021 年的盘点。 2021 年马上结束，翻看幕布，看到了年初给自己定下的目标。记性不好，好像把这个事情给忘记了。 工作 工作这么多年，已然跨过的 35 岁这个坎。没有想象中的困难，唯有一路的坚持。 职业生涯 自己一直坚持走技术路线，不愿走上管理这条路。就像有些事情，不是不会也不是不能，而是不愿。可能是自己性格使然，也唯有技术一条路可走，所以只有坚持，尤其是自己也喜欢搞搞技术。 过去的自己埋头写代码、学习，鲜有与外界的沟通。正好借着去年工作的事情走出去，参与社区。见识到了技术人的简单直接，也结交到了志同道合的朋友。 正职 年初太太跟我说，本命年少折腾，工作稳定点就行，不求其他。但还是在年中的时候换了工作，感谢上家公</description></item><item><title>Colima：MacOS 上的极简容器运行时和 Kubernetes（支持 m1）</title><link>https://atbug.com/containers-runtime-on-macos-with-colima/</link><pubDate>Sun, 26 Dec 2021 12:31:16 +0800</pubDate><guid>https://atbug.com/containers-runtime-on-macos-with-colima/</guid><description>
Colima 是一个以最小化设置来在MacOS上运行容器运行时和 Kubernetes 的工具。支持 m1（文末讨论），同样也支持 Linux。 Colima 的名字取自 Container on Lima。Lima 是一个虚拟机工具，可以实现自动的文件共享、端口转发以及 containerd。 Colima 实际上是通过 Lima 启动了名为 colima 的虚拟机，使用虚拟机中的 containerd 作为容器运行时。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/26/colima.gif 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/26/colima.gif 使用 Page Bundles: false 使用 Colima 的使用很简单，执行下面的命令就可以创建虚拟机，默认是 Docker 的运行时。 初次运行需要下载虚拟机镜像创建虚拟机，耗时因网络情况有所差异。之后，启动虚拟机就只需要 30s 左右的时间。 colima start INFO[0000] starting colima INFO[0000] creating and starting ... context=vm INFO[0119] provisioning ... context=docker INFO[0119] provisioning in VM ... context=docker INFO[0133] restarting VM to complete setup ... context=docker INFO[0133] stopping ... context=vm INFO[0136] starting ...</description></item><item><title>OpenFaaS - 以自己的方式运行容器化函数</title><link>https://atbug.com/openfaas-case-study-zh/</link><pubDate>Fri, 17 Dec 2021 09:13:59 +0800</pubDate><guid>https://atbug.com/openfaas-case-study-zh/</guid><description>
译者注： 本文篇幅较长，有助于了解 FaaS 和 OpenFaaS。作者分别从开发人员和运维人员的视角来了解 OpenFaaS，对了解新的技术是个很好的方式。 本文翻译自 Ivan Velichko 的 OpenFaaS - Run Containerized Functions On Your Own Terms。 长期以来，无服务器（serverless） 对我来说无非就是 AWS Lambda 的代名词。Lambda 提供了一种方便的途径，可以将任意代码附加到平台事件（云实例的状态变更、DynamoDB 记录的更新或新的 SNS 消息）中。但是，我时不时会想到某个逻辑，但其又没大到足以有自己的服务，同时有不适合任何现有服务的范围。因此，我经常将其放入函数中，以便日后使用 CLI 命令或者 HTTP 调用来调用它。 几年前，我来开了 AWS，自那以后，我一直怀念部署无服务器功</description></item><item><title>策略即代码：为了 OpenPolicyAgent 学 Rego？试试 Javascript</title><link>https://atbug.com/policy-as-code-with-pipy/</link><pubDate>Wed, 08 Dec 2021 07:49:37 +0800</pubDate><guid>https://atbug.com/policy-as-code-with-pipy/</guid><description>
距离上个版本 用 Pipy 实现 OPA，已经过去快半年了。当初使用Pipy 实现了可信镜像仓库的检查，那时的版本实现起来会稍微复杂，从策略仓库到证书创建到Admission Webhook 的创建都需要大量的人工操作，配置和逻辑也还是耦合在一起。 这个版本安装和使用起来会更加简单。 当初我用“不务正业”来形容 Pipy 实现准入控制，等看完这篇文章，欢迎留言说说你的看法。 架构 还是继续上次的场景，在 Pod 创建时对 Pod 使用的镜像所在仓库进行检查，以及检查镜像的 tag 是否合法。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/08/untitled.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/08/untitled.jpg 使用 Page Bundles: false 这里借助 Pipy Repo 的能力，将代表策略的脚本和配置交由 Repo 进行管理；Pipy 实例实时从 Pipy Repo 同步策略，并进行动态加载。</description></item><item><title>沙盒化容器：是容器还是虚拟机</title><link>https://atbug.com/sandboxed-container/</link><pubDate>Tue, 07 Dec 2021 07:55:16 +0800</pubDate><guid>https://atbug.com/sandboxed-container/</guid><description>
随着 IT 技术的发展，AI、区块链和大数据等技术提升了对应用毫秒级扩展的需求，开发人员也面临着的功能快速推出的压力。混合云是新常态，数字化转型是保持竞争力的必要条件，虚拟化成为这些挑战的基本技术。 在虚拟化的世界，有两个词耳熟能详：虚拟机和容器。前者是对硬件的虚拟化，后者则更像是操作系统的虚拟化。两者都提供了沙箱的能力：虚拟机通过硬件级抽象提供，而容器则使用公共内核提供进程级的隔离。有很多人将容器看成是“轻量化的虚拟机”，通常情况下我们认为容器是安全的，那到底是不是跟我们想象的一样？ 容器：轻量化的虚拟机？ 容器是打包、共享和部署应用的现代化方式，帮助企业实现快速、标准、灵活地完成服务交互。容器化是建立</description></item><item><title>从 Docker 的信号机制看容器的优雅停止</title><link>https://atbug.com/gracefully-stopping-docker-containers-with-correct-command/</link><pubDate>Mon, 29 Nov 2021 07:30:43 +0800</pubDate><guid>https://atbug.com/gracefully-stopping-docker-containers-with-correct-command/</guid><description>
此文是前段时间笔记的整理，之前自己对这方面的关注不够，因此做下记录。 有太多的文章介绍如何运行容器，然而如何停止容器的文章相对少很多。 根据运行的应用类型，应用的停止过程非常重要。如果应用要写文件，停止前要保证正确刷新数据并关闭文件；如果是 HTTP 服务，要确保停止前处理所有未完成的请求。 信号 信号是 Linux 内核与进程以及进程间通信的一种方式。针对每个信号进程都有个默认的动作，不过进程可以通过定义信号处理程序来覆盖默认的动作，除了 SIGSTOP 和 SIGKILL。二者都不能被捕获或重写，前者用来将进程暂停在当前状态，而后者则是从内核层面立即杀掉进程。 有两个比较重要的进程 SIGTERM 和 SIGKILL。SIGTERM 是优雅地关闭命令，SIG</description></item><item><title>Kubernetes Deployment 的故障排查可视化指南（2021 中文版）</title><link>https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/</link><pubDate>Sat, 20 Nov 2021 18:29:19 +0800</pubDate><guid>https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/</guid><description>
将应用部署到 Kubernetes 时通常会使用 Deployment、Service、Ingress，整个应用从部署到正常运行，经历的流程很长。从 kubectl apply YAML 文件，经过 apiserver、controller manager、scheduler、kubelet、以及 CRI、CNI 等众多组件的协同工作。 漫长的“行程”，Pod 也经历各种正常和不正常的状态变化，即使正常运行也会出现服务无法访问的问题。对于刚开始在 Kubernetes 平台开展工作的同学来说，故障的排查确实棘手。之前工作的时候，经常要协助排查各种问题。去年在 Learnk8s 上看到了关于 Deployment 故障排查的视图，我还参考做了当时整个平台的故障排查视图，包括了从项目源码、CICD 流水线、部署整个流程的故障排查</description></item><item><title>Monterey 12.0.1 上的 bug</title><link>https://atbug.com/bug-with-m1-pro-and-monterey/</link><pubDate>Thu, 18 Nov 2021 08:54:44 +0800</pubDate><guid>https://atbug.com/bug-with-m1-pro-and-monterey/</guid><description>
最近换上了 MacBook Pro 2021，也慢慢将工作转到新的电脑上。结束了一年多的黑白配，之前工作主力机是我的黑苹果，配置以及 OpenCore 的引导放在这里了。 为了稳定性，系统一直停留在了 10.15.4。新的电脑拿到手就是 12.0.1，之前也就在我另一台 2016 款的 macbook pro 上用过几个周的 12.0.0。 新的系统加上新的架构，不免有有些 bug，今天就说下我所遇到的两个比较棘手的。 1. 安全相关 EXC_BAD_ACCESS (SIGKILL (Code Signature Invalid)) 公司的核心产品是用 c++ 开发的，编译之后我都习惯性的放到 /usr/local/bin 目录中。在新系统中，就出现了上面的错误（从控制台获取），运行的时候进程直接被 kill。 网上查了下，说与 kernel cache 有关，重启可解决。 This was caused by kernel caching of previously signed binaries and my replacing those binaries with newly compiled binaries which weren&amp;rsquo;t part of a signed package. By deleting the existing binaries, rebooting</description></item><item><title>Kubernetes 上调试 distroless 容器</title><link>https://atbug.com/debug-distroless-container-on-kubernetes/</link><pubDate>Wed, 03 Nov 2021 07:40:40 +0800</pubDate><guid>https://atbug.com/debug-distroless-container-on-kubernetes/</guid><description>
TL;DR 本文内容： 介绍 distroless 镜像、作用以及简单的使用 如何针对 distroless 容器的进行调试 临时容器(v.1.18+)的使用 Distroless 镜像 Distroless 容器，顾名思义使用 Distroless 镜像作为基础镜像运行的容器。 &amp;ldquo;Distroless&amp;rdquo; 镜像只包含了你的应用程序以及其运行时所需要的依赖。不包含你能在标准 Linxu 发行版里的可以找到的包管理器、shells 或者其他程序。 GoogleContainerTools/distroless 针对不同语言提供了 distroless 镜像： gcr.io/distroless/static-debian11 gcr.io/distroless/base-debian11 gcr.io/distroless/java-debian11 gcr.io/distroless/cc-debian11 gcr.io/distroless/nodejs-debian11 gcr.io/distroless/python3-debian11 Distroless 镜像有什么用？ 那些可能是构建镜像时需要的，但大部分并不是运行时需要的。这也是为什么上篇文章介绍 Buildpacks 时说的一个 builder 的 stack 镜像包含构建时基础镜像和运行时基础镜像，这样可以做到镜像的最小化。 其实控制体积并不是 distroless 镜像的主要作用。将运行时容器中的内容限制为应用程序所需的依赖，此外不应该安装任何</description></item><item><title>无需 Dockerfile 的镜像构建：BuildPack vs Dockerfile</title><link>https://atbug.com/build-docker-image-without-dockerfile/</link><pubDate>Fri, 29 Oct 2021 07:36:43 +0800</pubDate><guid>https://atbug.com/build-docker-image-without-dockerfile/</guid><description>
过去的工作中，我们使用微服务、容器化以及服务编排构建了技术平台。为了提升开发团队的研发效率，我们同时还提供了 CICD 平台，用来将代码快速的部署到 Openshift（企业级的 Kubernetes） 集群。 部署的第一步就是应用程序的容器化，持续集成的交付物从以往的 jar 包、webpack 等变成了容器镜像。容器化将软件代码和所需的所有组件（库、框架、运行环境）打包到一起，进而可以在任何环境任何基础架构上一致地运行，并与其他应用“隔离”。 我们的代码需要从源码到编译到最终可运行的镜像，甚至部署，这一切在 CICD 的流水线中完成。最初，我们在每个代码仓库中都加入了三个文件，也通过项目生成器（类似 Spring Initializer）在新</description></item><item><title>低复杂度 - 服务网格的下一站</title><link>https://atbug.com/service-mesh-unnecessary-complexity/</link><pubDate>Fri, 15 Oct 2021 07:58:25 +0800</pubDate><guid>https://atbug.com/service-mesh-unnecessary-complexity/</guid><description>
译者： 作为一个曾经在制造业企业的基础架构团队任职，为支持公司的“互联网基因”和“数字化转型”落地了云原生基础设施平台，并在尝试采用服务网格未成的我来说，看到这篇文章深有感触。尤其是文中所说的“人少，问题多，需要快速输出价值”，直戳到了痛处。有限的人手有限的时间，我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上，要想很好的运行复杂的系统是非常困难的。 服务网格是一个新的基础设施层，可以承载很多的功能，未来还会有更大的想象空间和光明的未来。 以上的种种原因，也促使我后来选择进入一家提供服务网格的产品企业，也希望服务网格可以被更简单的使用。 “道阻且长，行则将至！” 本文翻译自 Chris Campbell 的 How Unnecessary Complexity Gave the Service Mesh a</description></item><item><title>自动替换 Kubernetes 镜像</title><link>https://atbug.com/kubernetes-images-swapper/</link><pubDate>Wed, 06 Oct 2021 08:01:41 +0800</pubDate><guid>https://atbug.com/kubernetes-images-swapper/</guid><description>
前几天有朋友在问如何在某云上拉取 Tekton 的镜像，这种情况其实比较普遍不只是某云。工作中经常要用到过某些靠运气才能拉取到的镜像，这对工作来说真是极度的不友好。 因此也萌生了个想法，维护一个后网络友好的仓库镜像，在 Pod 创建时将镜像仓库切换到自维护的仓库，从自维护的仓库拉取镜像。 前几天体验了极狐Gitlab 的容器镜像库，便是为这个想法做的准备。当然其他的云厂商也有提供针对个人版的免费镜像仓库和企业版仓库。 正好 Pipy 作为策略引擎，非常适合实现这种策略的执行。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005212733.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005212733.png 使用 Page Bundles: false 实现思路 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005210725.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005210725.png 使用 Page Bundles: false Admission Webhook Kubernetes 动</description></item><item><title>极狐GitLab SaaS 内测轻度体验</title><link>https://atbug.com/jihu-gitlab-experience/</link><pubDate>Fri, 01 Oct 2021 08:18:09 +0800</pubDate><guid>https://atbug.com/jihu-gitlab-experience/</guid><description>
感谢极狐团队为 GitLab（SaaS）本地化的努力，同时也感谢小马哥提供的内测资格。 最近突然想到了个点子，需要使用一个私有的镜像仓库。极狐GitLab 有提供容器镜像库，正好和 CICD 一起做个轻度体验。 容器镜像库 Container Registry 文档介绍在这里，目前还是英文。（应该本地化的工作量很大，文档还没翻译。） 容器镜像库可以作为独立镜像仓库使用（为什么要这么用，卖个关子下篇文章见），就是使用 docker 命令将构建好的镜像推送到 容器镜像库。 当然也可以同 CICD 流水线结合使用，后文也会介绍。 独立使用 本地登录 Container Registry 有两种验证方式： 使用用户名和密码 开启了双重身份验证，可以使用访问个人访问令牌 其实，不管是否开始双重验证，都建议使用访问令牌。 docker login registry.gitlab.cn #根据</description></item><item><title>容器神话 Docker 是如何一分为二的</title><link>https://atbug.com/how-docker-broke-in-half/</link><pubDate>Mon, 20 Sep 2021 08:01:30 +0800</pubDate><guid>https://atbug.com/how-docker-broke-in-half/</guid><description>
译者点评： 最近听了很多资深的人士关于开源，以及商业化的分析。开源与商业化，听起来就是一对矛盾的所在，似乎大家都在尝试做其二者的平衡。是先有开源，还是先有商业化？俗话说“谈钱不伤感情”，近几年背靠开源的创业公司如雨后春笋般涌现，即使是开发人员也是需要生活的。 容器神话 Docker 曾经无比风光，盛极一时。即使这样一个备受瞩目，大获风投的热捧的独角兽也未能免俗，并付出了不小的代价。 今天这篇文章讲述了 Docker 这家公司从诞生到巅峰到没落，这一路上所做的抉择，并最终做了开源与商业的分离，再一次从开源踏上找寻商业化之路。这些都是值得我们参考和思考的，不管是已经开源或者准备从事开源的。 这篇文章翻译自How Docker broke in half 这家改变游戏规</description></item><item><title>ARM64 平台基于 openEuler + iSula 环境部署 Kubernetes</title><link>https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/</link><pubDate>Thu, 02 Sep 2021 20:41:06 +0800</pubDate><guid>https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/</guid><description>
为什么要在 arm64 平台上部署 Kubernetes，而且还是鲲鹏 920 的架构。说来话长 。。。 此处省略5000 字。 介绍下系统信息； 架构：鲲鹏 920(Kunpeng920) OS：openEuler 20.03 (LTS-SP1) CPU：4c 内存：16G 硬盘：若干 整个过程虽然参考了鲲鹏论坛的帖子，不过还是颇费周折。 TL;DR 整个过程中要注意 arm64 平台上安装 Kubernetes 及网络组件，需要使用 arm64 版本的镜像。 环境配置 1.关闭 selinux #临时关闭 setenforce 0 #永久关闭 SELINUX=disabled vim /etc/sysconfig/selinux 2. 关闭swap分区 #临时关闭 swapoff -a #永久关闭 注释 swap 行 vim /etc/fstab 3. 关闭防火墙 systemctl stop firewalld ssystemctl disable firewalld 4. 网络配置 对iptables内部的nf-call需要打开的内生的桥接功能 vim /etc/sysctl.d/k8s.conf 修改如下内容： net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm_swappiness=0 修改完成后执行： modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf 5. 添加 Kubernetes 源 在文件 /etc/yum.repos.d/openEuler.repo 中追加如下内容： [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64/ enabled=1</description></item><item><title>使用 Flomesh 进行 Dubbo 服务治理</title><link>https://atbug.com/enhance-dubbo-service-governance-with-flomesh/</link><pubDate>Wed, 18 Aug 2021 09:50:28 +0800</pubDate><guid>https://atbug.com/enhance-dubbo-service-governance-with-flomesh/</guid><description>
写在最前 和上一篇《使用 Flomesh 强化 Spring Cloud 服务治理》一样，这次同样是在无代码侵入的情况下对 Dubbo 服务治理的提升。 更多治理场景陆续添加中，有兴趣的可关注 https://github.com/flomesh-io/service-mesh-dubbo-demo。 开源的 Pipy 作为 Flomesh 的核心，得益于其轻量及灵活性可以通过编程的方式轻松快速的支持多中平台的服务发现机制，比如 Eureka、Consul、Nacos 等。 概览 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/08/18/16292498443406.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/08/18/16292498443406.jpg 使用 Page Bundles: false 细节 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/08/18/16292498964245.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/08/18/16292498964245.jpg 使用 Page Bundles: false 环境搭建 搭建 Kubernetes 环境，可以选择 kubeadm 进行集群搭建。也可以选择 min</description></item><item><title>使用 Flomesh 强化 Spring Cloud 服务治理</title><link>https://atbug.com/enhance-springcloud-service-governance-with-flomesh/</link><pubDate>Tue, 17 Aug 2021 18:47:33 +0800</pubDate><guid>https://atbug.com/enhance-springcloud-service-governance-with-flomesh/</guid><description>
写在最前 这篇是关于如何使用 Flomesh 服务网格来强化 Spring Cloud 的服务治理能力，降低 Spring Cloud 微服务架构落地服务网格的门槛，实现“自主可控”。 文档在 github 上持续更新，欢迎大家一起讨论：https://github.com/flomesh-io/flomesh-bookinfo-demo。 架构 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/08/17/springdemotypology.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/08/17/springdemotypology.png 使用 Page Bundles: false 环境搭建 搭建 Kubernetes 环境，可以选择 kubeadm 进行集群搭建。也可以选择 minikube、k3s、Kind 等，本文使用 k3s。 使用 k3d 安装 k3s。k3d 将在 Docker 容器中运行 k3s，因此需要保证已经安装了 Docker。 $ k3d cluster create spring-demo -p &amp;#34;81:80@loadbalancer&amp;#34; --k3s-server-arg &amp;#39;--no-deploy=traefik&amp;#39; 安装 Flomesh 从仓库 https://github.com/flomesh-io/flomesh-bookinfo-demo.git 克隆代码。进入到 flomesh-book</description></item><item><title>开源评估框架</title><link>https://atbug.com/a-framework-for-open-source-evaluation/</link><pubDate>Mon, 09 Aug 2021 08:36:21 +0800</pubDate><guid>https://atbug.com/a-framework-for-open-source-evaluation/</guid><description>
本文由本人翻译自 Bilgin Ibryam 的 A Framework for Open Source Evaluation，首发在云原生社区博客。 如今，真假开源无处不在。最近开源项目转为闭源的案例越来越多，同时也有不少闭源项目（按照 OSI 定义）像开源一样构建社区的例子。这怎么可能，开源项目不应该始终如此吗？ 开源不是非黑即白，它具有开放性、透明、协作性和信任性的多个维度。有些开源是 Github 上的任何项目，有些必须通过 OSI 定义，有些是必须遵守不成文但普遍接受的开源规范。这里通过看一些商业和技术方面，再讨论社区管理习惯，来同大家分享一下我对评估开源项目的看法。 免责声明 这些是我的个人观点，与我的雇主或我所属的软件基金会和项目无关。 这不是法律或专业意见（我不是律师，也不是专门从事 OSS 评估</description></item><item><title>游记 - 2021 北疆之行</title><link>https://atbug.com/2021-xinjiang-trip/</link><pubDate>Sat, 07 Aug 2021 09:51:22 +0800</pubDate><guid>https://atbug.com/2021-xinjiang-trip/</guid><description>
趁着换工作的间隙，来一场“蓄谋已久”、“说走就走”的旅行。 新疆，是一个美丽而遥远的地方。前段时间看了李娟的《冬牧场》，更是提起了我对新疆对游牧哈萨克族的兴趣。以至后来梦中见到一望无际的戈壁滩，让自己下定决心做出改变。 背景 这次北疆之行准备仓促，从路线方案到寻找同伴，都是出发前一周搞定的。 同伴找的是云原生社区的 宋净超 Jimmy，同样也是老乡（两人在外漂多年，沟通全是普通话。。。），他从北京出发。由于他的工作性质特别&amp;ndash;远程工作，所以当初临时找同伴的时候想到了他，而且还也在有驾照在手。也由于他需要远程工作，这次我们选择了房车自驾，正好本人也想体验一下房车。 路书发在了这里，全程 2800km+</description></item><item><title>Kubernetes 必备工具：2021</title><link>https://atbug.com/translation-kuberletes-essential-tools-2021/</link><pubDate>Thu, 15 Jul 2021 08:10:22 +0800</pubDate><guid>https://atbug.com/translation-kuberletes-essential-tools-2021/</guid><description>
有别于前些天的文章 - 常用的几款工具让 Kubernetes 集群上的工作更容易 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。 文档翻译自 Kubernetes Essential Tools: 2021，篇幅较长，做了部分增删。 介绍 在本文中，我将尝试总结我最喜欢的 Kubernetes 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。 这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。 K3D K3D 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常轻巧且速</description></item><item><title>Rego 不好用？用 Pipy 实现 OPA</title><link>https://atbug.com/pipy-implement-kubernetes-admission-control/</link><pubDate>Tue, 13 Jul 2021 08:44:56 +0800</pubDate><guid>https://atbug.com/pipy-implement-kubernetes-admission-control/</guid><description>
还不知道 Pipy 是什么的同学可以看下 GitHub 。 Pipy 是一个轻量级、高性能、高稳定、可编程的网络代理。Pipy 核心框架使用 C++ 开发，网络 IO 采用 ASIO 库。 Pipy 的可执行文件仅有 5M 左右，运行期的内存占用 10M 左右，因此 Pipy 非常适合做 Sidecar proxy。 Pipy 内置了自研的 pjs 作为脚本扩展，使得Pipy 可以用 JS 脚本根据特定需求快速定制逻辑与功能。 Pipy 采用了模块化、链式的处理架构，用顺序执行的模块来对网络数据块进行处理。这种简单的架构使得 Pipy 底层简单可靠，同时具备了动态编排流量的能力，兼顾了简单和灵活。通过使用 REUSE_PORT 的机制（主流 Linux 和 BSD 版本都支持该功能），Pipy 可以以多进程模式运行，使得 Pipy 不仅适用于 Sidecar 模式，也适用于大规模的流量处理场景。 在实践中，Pipy 独</description></item><item><title>Open Policy Agent: Top 5 Kubernetes 准入控制策略</title><link>https://atbug.com/open-policy-agent-top-5-kubernetes-admission-control/</link><pubDate>Mon, 12 Jul 2021 08:20:40 +0800</pubDate><guid>https://atbug.com/open-policy-agent-top-5-kubernetes-admission-control/</guid><description>
如何使用 Open Policy Agent 实现准入策略控制，可以参考这里 本文翻译自 Open Policy Agent: The Top 5 Kubernetes Admission Control Policies Kubernetes 开发人员和平台工程师通常承受着非常大的压力，以保持应用程序部署的快速进行，并且总是为了速度和进度而做出妥协。平台团队越来越有责任确保这些妥协（例如管理 Ingress）不会导致客户数据暴露在整个互联网上等后果。 幸运的是，Kubernetes 提供了设置策略的能力，通过检查并防止部署错误将其投入生产，从而避免这些后果。为了确保团队的应用程序不会比信心更重要，以下是现在应该在集群中运行的前五个 Kubernetes 准入控制策略。 1. 可信镜像仓库 此策略很简单，但功能强大：仅允许从受信任的镜像仓库中拉取的容器映像，并且可以选择仅拉取与允许的仓库镜像地址</description></item><item><title>Kubernetes 的魔力在于企业标准化，而不是应用程序的可移植性</title><link>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</link><pubDate>Sun, 11 Jul 2021 08:05:42 +0800</pubDate><guid>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</guid><description>
笔者：Kubernetes 抽象了资源和工作负载的操作模式，统一了工具集，实现人机接口的标准化。正如类 Docker 工具提供了应用运行时的操作模式；Spring Framework 提供了 Java 应用的开发模式。 Kubernetes 是关于跨云的技能、工具和实践的可移植性。不是工作负载的可移植性。 &amp;ndash; Bilgin Lbryam @bibryam 本文翻译自 Kubernetes magic is in enterprise standardization, not app portability Kubernetes 不会神奇地使你的应用程序具有可移植性，但它可能会给你带来更好的东西。 云为企业提供了看似无限的选择。然而，根据 Canonical-sponsored 的一项调查，这并不是大多数企业采用 Kubernetes 等云友好技术的原因。相反，Kubernetes 的主要目标是标准化——外观和操作与其他人一样。 可移植性不是目标 我之前已经讨论过这个问题，参考了 Gartner 关于 Kubernetes 和可移植性的指南。许多人认为 K</description></item><item><title>使用 Open Policy Agent 实现可信镜像仓库检查</title><link>https://atbug.com/image-trusted-repository-with-open-policy-agent/</link><pubDate>Sat, 10 Jul 2021 07:14:47 +0800</pubDate><guid>https://atbug.com/image-trusted-repository-with-open-policy-agent/</guid><description>
从互联网（或可信镜像仓库库以外的任何地方）拉取未知镜像会带来风险——例如恶意软件。但是还有其他很好的理由来维护单一的可信来源，例如在企业中实现可支持性。通过确保镜像仅来自受信任的镜像仓库，可以密切控制镜像库存，降低软件熵和蔓延的风险，并提高集群的整体安全性。除此以外，有时还会需要检查镜像的 tag，比如禁止使用 latest 镜像。 这今天我们尝试用“策略即代码”的实现 OPA 来实现功能。 还没开始之前可能有人会问：明明可以实现个 Admission Webhook 就行，为什么还要加上 OPA？ 确实可以，但是这样策略和逻辑都会耦合在一起，当策略需要调整的时候需要修改代码重新发布。而 OPA 就是用来做解耦的，其更像是一个策略的执行引擎。 什么是 OPA Open Policy Agent（</description></item><item><title>Kubernetes CKA 证书备考笔记</title><link>https://atbug.com/notes-for-cka-preparation/</link><pubDate>Fri, 02 Jul 2021 08:02:15 +0800</pubDate><guid>https://atbug.com/notes-for-cka-preparation/</guid><description>
Kubernetes 使用有好几年了，但在今年 5 月才完成 CKA 的考试。虽说用了几年，还是提前刷了部分题熟悉下。 绝大部分题都是有在 minikube 的环境上操作过，只有部分比如升级集群受限于环境问题没有实地操作。 写在最前 保存常用文档进书签，如果有 Alfred 启用浏览器书签 workflow。效果见下图 kubectl 自动补全 echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc; source ~/.bashrc 每道题开始前要切换 context 和 namespace，直接复制题目里的命令即可 必要的 alias 善用 --dry-run=client -o yaml 避免手动敲太多 善用 kubectl explain [resource[.field]] 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 书签地址：K8s-CKA-CAKD-Bookmarks</description></item><item><title>可编程网关 Pipy 第三弹：事件模型设计</title><link>https://atbug.com/pipy-event-handling-design/</link><pubDate>Sun, 27 Jun 2021 09:38:18 +0800</pubDate><guid>https://atbug.com/pipy-event-handling-design/</guid><description>
自从参加了 Flomesh 的 workshop，了解了可编程网关 Pipy。对这个“小东西”充满了好奇，前后写了两篇文章，看了部分源码解开了其部分面纱。但始终未见其全貌，没有触及其核心设计。 不是有句话，“好奇害死猫”。其实应该还有后半句，“满足了就没事”（见维基百科）。 所有就有了今天的这一篇，对前两篇感兴趣的可以跳转翻看。 初探可编程网关 Pipy 可编程网关 Pipy 第二弹：编程实现 Metrics 及源码解读 言归正传。 事件模型 上篇写了 Pipy 基于事件的信息流转，其实还未深入触及其核心的事件模型。既然是事件模型，先看事件。 src/event.hpp:41 中定义了 Pipy 的四种事件： Data MessageStart MessageEnd SessionEnd 翻看源码可知（必须吐槽文档太少）这几种事件其实是有顺序的：MessageStart -&amp;gt; Data -&amp;gt; MessageEnd -&amp;gt; Ses</description></item><item><title>常用的几款工具让 Kubernetes 集群上的工作更容易</title><link>https://atbug.com/tools-accelerate-work-on-kubernetes-cluster/</link><pubDate>Sat, 26 Jun 2021 12:16:28 +0800</pubDate><guid>https://atbug.com/tools-accelerate-work-on-kubernetes-cluster/</guid><description>
之前写过一篇 介绍了工具加速云原生 Java 开发。 其实日常工作中在集群上的操作也非常多，今天就来介绍我所使用的工具。 kubectl-alias 使用频率最高的工具，我自己稍微修改了一下，加入了 StatefulSet 的支持。 这个是我的 https://github.com/addozhang/kubectl-aliases，基于 https://github.com/ahmetb/kubectl-aliases。 比如输出某个 pod 的 json，kgpoojson xxx 等同于 kubectl get pod xxx -o json。 结合 jq 使用效果更好 😂。 语法解读 k=kubectl sys=--namespace kube-system commands: g=get d=describe rm=delete a:apply -f ak:apply -k k:kustomize ex: exec -i -t lo: logs -f resources: po=pod, dep=deployment, ing=ingress, svc=service, cm=configmap, sec=secret,ns=namespace, no=node flags: output format: oyaml, ojson, owide all: --all or --all-namespaces depending on the command sl: --show-labels w=-w/--watch value flags (should be at the end): n=-n/--namespace f=-f/--filename l=-l/--selector kubectx + kubens 安装看这里 kubectx 用于在不同的集群间</description></item><item><title>Jenkins 如何与 Kubernetes 集群的 Tekton Pipeline 交互？</title><link>https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/</link><pubDate>Wed, 23 Jun 2021 07:58:45 +0800</pubDate><guid>https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/</guid><description>
本文详细介绍了 Jenkins 如何通过 tekton-client-plugin 实现与 Kubernetes 上的 Tekton Pipeline 交互，包括 Kubernetes 上安装 Jenkins、Tekton Pipelines 等。 关于如何使用 Tekton Pipeline 实现 CICD 可以看这篇文章 云原生 CICD: Tekton Pipeline 实战 本文用于构建的项目以及所有 manifest yaml 都在可以在这里下载。 TL;DR 惯例，先上总结。tekton-client-plugin 虽然还是处于初期阶段，但是 其价值非常明显，尤其是对先用使用 Jenkins 作为 CICD 实现的用户来说。从 Jenkins 迁移到云原生的 Tekton 时，可以省掉用户界面的开发成本，而且尽可能少的改变用户习惯 ，依靠版本管理可以控制迁移的节奏。 tekton-client-plugin 在今年 5 月 7 日发布的 1.0.0 版本，目前为 1.0.02。目前还处于初期阶段，我个人感觉目前仅仅算是打通 Jenkins 与 Tekton 交互这条路，扩展性还不够好。 比如目前仅仅支持如下几</description></item><item><title>云原生 CICD: Tekton Pipeline 实战</title><link>https://atbug.com/tekton-pipeline-practice/</link><pubDate>Tue, 22 Jun 2021 07:19:33 +0800</pubDate><guid>https://atbug.com/tekton-pipeline-practice/</guid><description>
更新历史： v1：2020.1.21 基于 Tekton Pipline v0.9.0 v2（当前）：2021.6.22 基于 Tekton Pipeline v0.25.0 Tekton 是 Google 开源的 Kubernetes 原生CI/CD 系统, 功能强大扩展性强. 前身是 Knavite 里的 build-pipeline 项目, 后期孵化成独立的项目. 并成为 CDF 下的四个项目之一, 其他三个分别是 Jenkins, Jenkins X, Spinnaker. 为什么说 Tekton 是 Kubernetes 原生的, 以内其基于 Kubernetes 的 CRD 定义了 Pipeline 流水线. Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/uPic/bquuTV.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/uPic/bquuTV.jpg 使用 Page Bundles: false CRD 及说明: Task: 构建任务, 可以定义一些列的 steps. 每个 step 由一个 container 执行. TaskRun: task 实际的执行, 并提供执行所需的参数. 这个对象创建后, 就会有 pod 被创建. Pipeline: 定义一个或者多个 task 的执行, 以及 PipelineResource 和各种定义参数的集合 PipelineRun: 类似 task 和 taskrun 的关系: 一个定义一个执行. PipelineRun 则是 pipeline 的实际执行. 创建</description></item><item><title>源码解析：一文读懂 Kubelet</title><link>https://atbug.com/kubelet-source-code-analysis/</link><pubDate>Tue, 15 Jun 2021 08:25:25 +0800</pubDate><guid>https://atbug.com/kubelet-source-code-analysis/</guid><description>
本文主要介绍 kubelet 功能、核心组件，以及启动流程的源码分析，总结了 kubelet 的工作原理。 kubelet 简介 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/15/20210613091144.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/15/20210613091144.png 使用 Page Bundles: false 从官方的架构图中很容易就能找到 kubelet 执行 kubelet -h 看到 kubelet 的功能介绍： kubelet 是每个 Node 节点上都运行的主要“节点代理”。使用如下的一个向 apiserver 注册 Node 节点：主机的 hostname；覆盖 host 的参数；或者云提供商指定的逻辑。 kubelet 基于 PodSpec 工作。PodSpec 是用 YAML 或者 JSON 对象来描述 Pod。Kubelet 接受通过各种机制（主要是 apiserver）提供的一组 PodSpec，并确保里面描述的容器良好运行。 除了由 apiserver 提供 PodSpec，还可以通过以下方式提供： 文件 HTTP 端点 HTTP 服务器 kubelet 功能归纳一下</description></item><item><title>可编程网关 Pipy 第二弹：编程实现 Metrics 及源码解读</title><link>https://atbug.com/programming-archive-metrics-with-pipy/</link><pubDate>Fri, 11 Jun 2021 08:27:36 +0800</pubDate><guid>https://atbug.com/programming-archive-metrics-with-pipy/</guid><description>
由于要给团队做一下关于 Flomesh 的分享，准备下材料。 “分享是最好的学习方法。” 上一回初探可编程网关 Pipy，领略了 Pipy 的“风骚”。从 Pipy 的 GUI 交互深入了解了 Pipy 的配置加载流程。 今天看一下 Pipy 如何实现 Metrics 的功能，顺便看下数据如何在多个 Pipeline 中进行流转。 前置 首先，需要对 Pipy 有一定的了解，如果不了解看一下上一篇文章。 其次构建好 Pipy 环境，关于构建还是去看上一篇文章。 Metrics 功能实现 至于 Pipy 实现 Metrics 的方式，源码中就有，位于 test/006-metrics/pipy.js。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/10/20210610202734.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/10/20210610202734.png 使用 Page Bundles: false 代理监听 6080 端口，后端服务在 8080 端口，Metrics 在 9090 端口 共有 5 个 Pipeline：3 个 listen 类型，2 个 Pipeline</description></item><item><title>Kubernetes 的自动伸缩你用对了吗？</title><link>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</link><pubDate>Wed, 09 Jun 2021 00:34:25 +0800</pubDate><guid>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</guid><description>
本文翻译自 learnk8s 的 Architecting Kubernetes clusters — choosing the best autoscaling strategy，有增删部分内容。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2159402x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2159402x.png 使用 Page Bundles: false TL;DR: 在默认设置下，扩展 Kubernetes 集群中的 pod 和节点可能需要几分钟时间。了解如何调整集群节点的大小、配置水平和集群自动缩放器以及过度配置集群以加快扩展速度。 自动扩展器 在 Kubernetes 中，常说的“自用扩展”有： HPA：Pod 水平缩放器 VPA：Pod 垂直缩放器 CA：集群自动缩放器 不同类型的自动缩放器，使用的场景不一样。 HPA HPA 定期检查内存和 CPU 等指标，自动调整 Deployment 中的副本数，比如流量变化： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2206552x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2206552x.png 使用 Page Bundles: false Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网</description></item><item><title>初探可编程网关 Pipy</title><link>https://atbug.com/glance-at-programmable-gateway-pipy/</link><pubDate>Mon, 31 May 2021 00:45:08 +0800</pubDate><guid>https://atbug.com/glance-at-programmable-gateway-pipy/</guid><description>
有幸参加了 Flomesh 组织的workshop，了解了他们的 Pipy 网络代理，以及围绕 Pipy 构建起来的生态。Pipy 在生态中，不止是代理的角色，还是 Flomesh 服务网格​中的数据平面。 整理一下，做个记录，顺便瞄一下 Pipy 的部分源码。 介绍 下面是摘自 Github 上关于 Pipy 的介绍： Pipy 是一个轻量级、高性能、高稳定、可编程的网络代理。Pipy 核心框架使用 C++ 开发，网络 IO 采用 ASIO 库。 Pipy 的可执行文件仅有 5M 左右，运行期的内存占用 10M 左右，因此 Pipy 非常适合做 Sidecar proxy。 Pipy 内置了自研的 pjs 作为脚本扩展，使得Pipy 可以用 JS 脚本根据特定需求快速定制逻辑与功能。 Pipy 采用了模块化、链式的处理架构，用顺序执行的模块来对网络数据块进行处理。这种简单的架构使得 Pipy 底层简单可靠，同时具</description></item><item><title>使用 Quarkus 和 MicroProfile 实现微服务特性</title><link>https://atbug.com/microservicilities-quarkus/</link><pubDate>Wed, 26 May 2021 07:37:04 +0800</pubDate><guid>https://atbug.com/microservicilities-quarkus/</guid><description>
Quarkus 的文章之前写过三篇了，讲过了 Quarkus 的小而快。 Hello, Quarkus 应&amp;quot;云&amp;quot;而生的 Java 框架 Quarkus：构建本机可执行文件 谁说 Java 不能用来跑 Serverless？ 一直在酝酿写一篇 Quarkus 生态相关的，因为最近一直在忙 Meetup 的事情而搁浅。正好看到了这篇文章，就拿来翻译一下，补全云原生中的“微服务”这一块。 本文译自《Implementing Microservicilities with Quarkus and MicroProfile》 。 为什么要使用微服务特性？ 在微服务架构中，一个应用程序是由几个相互连接的服务组成的，这些服务一起工作来实现所需的业务功能。 因此，典型的企业微服务架构如下所示： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/05/26/16219855266680.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/05/26/16219855266680.jpg 使用 Page Bundles: false 刚开始，使用微</description></item><item><title>开箱即用的 Prometheus 告警规则集</title><link>https://atbug.com/introduction-awesome-prometheus-alerts/</link><pubDate>Thu, 13 May 2021 08:01:00 +0800</pubDate><guid>https://atbug.com/introduction-awesome-prometheus-alerts/</guid><description>
在配置系统监控的时候，是不是即使绞尽脑汁监控的也还是不够全面，或者不知如何获取想要的指标。 Awesome Prometheus alerts 维护了一套开箱即用的 Prometheus 告警规则集合，有 300 多个告警规则。同时，还是说明如何获取对应的指标。这些规则，对每个 Prometheus 都是通用的。 涉及如主机、硬件、容器等基础资源，到数据库、消息代理、运行时、反向代理、负责均衡器，运行时、服务编排，甚至是网络层面和 Prometheus 自身和集群。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/05/13/alertrulescover.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/05/13/alertrulescover.png 使用 Page Bundles: false Prometheus 的安装和配置不做赘述，配置可以看这里。下面简单看下几个常用规则 主机和硬件资源 主机和硬件资源的告警依赖 node-exporter 输出的指标。例如： 内存不足 可用内存低于阈值 10% 就会触发告警。 - alert:HostOutOfMemoryexpr:node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 &amp;lt; 10for:2mlabels:severity:warningannotations:summary:Host out of</description></item><item><title>Kubernetes 上如何控制容器的启动顺序？</title><link>https://atbug.com/k8s-1.18-container-start-sequence-control/</link><pubDate>Fri, 30 Apr 2021 07:43:54 +0800</pubDate><guid>https://atbug.com/k8s-1.18-container-start-sequence-control/</guid><description>
去年写过一篇博客：控制 Pod 内容器的启动顺序，分析了 TektonCD 的容器启动控制的原理。 为什么要做容器启动顺序控制？我们都知道 Pod 中除了 init-container 之外，是允许添加多个容器的。类似 TektonCD 中 task 和 step 的概念就分别与 pod 和 container 对应，而 step 是按照顺序执行的。此外还有服务网格的场景，sidecar 容器需要在服务容器启动之前完成配置的加载，也需要对容器的启动顺序加以控制。否则，服务容器先启动，而 sidecar 还无法提供网络上的支持。 现实 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle1.gif 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle1.gif 使用 Page Bundles: false 期望 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle2.gif 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/sidecarlifecycle2.gif 使用 Page Bundles: false 到了这里肯定有同学会问，spec.containers[] 是一个数组，数组是</description></item><item><title>云上细粒度访问管理的参考架构</title><link>https://atbug.com/translation-access-management-reference-architecture/</link><pubDate>Wed, 28 Apr 2021 08:02:11 +0800</pubDate><guid>https://atbug.com/translation-access-management-reference-architecture/</guid><description>
本文由 Addo Zhang 翻译自 A Reference Architecture for Fine-Grained Access Management on the Cloud 什么是访问管理？ 访问管理是识别用户或一组用户是否应该能够访问给定资源（例如主机、服务或数据库）的过程。例如，对于开发人员来说是否可以使用 SSH 登录生产应用程序服务器，如果可以，那么可以登录多长时间？如果 SRE 在非支持时间尝试访问数据库，他们这样做？如果数据工程师已转移到其他团队，他们是否应该继续访问 ETL 管道的 S3 存储桶？ 现在如何进行访问管理？ 在云上各种基础设施和数据服务激增之前，访问管理是 DevOps 和 Security 团队要解决的相对简单的问题。VPN 和堡垒主机是（现在仍然是）在网络级别封锁所有关键资源的首选机制。用户必须先通过 VPN 服务器进行身份验证，或者登录到堡垒主机，然后才能访问专用网络上的</description></item><item><title>Quarkus：谁说 Java 不能用来跑 Serverless？</title><link>https://atbug.com/quarkus-enable-java-running-in-serverless/</link><pubDate>Sat, 24 Apr 2021 09:16:05 +0800</pubDate><guid>https://atbug.com/quarkus-enable-java-running-in-serverless/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/24/whynotjava.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/24/whynotjava.png 使用 Page Bundles: false 想到这个标题的时候，我第一时间想到的就是星爷的《唐伯虎点秋香》的这一幕。 当讨论起世界上最好的开发语言是什么的时候，Java 的粉丝们总会遇到这种场景： 吹：“Java 语法简单，容易上手！” 黑：“Java 启动慢，性能差，耗资源！” 吹：“Java 有世界上最多的程序员！” 黑：“Java 启动慢，性能差，耗资源！” 吹：“Java 生态好！” 黑：“Java 启动慢，性能差，耗资源！” 吹：“滚！” 今天我们继续说说 Quarkus，应“云”而生的 Java 框架。今天算是第三篇了，没看过的同学可以回顾一下： Hello, Quarkus 应&amp;quot;云&amp;quot;而生的 Java 框架 Quar</description></item><item><title>服务网格平稳落地：Istio 中精准控制 Sidecar 的注入</title><link>https://atbug.com/how-to-control-istio-sidecar-injection/</link><pubDate>Wed, 21 Apr 2021 08:13:04 +0800</pubDate><guid>https://atbug.com/how-to-control-istio-sidecar-injection/</guid><description>
为什么 说起服务网格，这幅图大家肯定不会陌生。这就是服务网格的网络，也是网格架构的终极形态。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2314142x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2314142x.png 使用 Page Bundles: false 那在迁移到网格架构之前，我们的系统是什么样的？ Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2316432x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2316432x.png 使用 Page Bundles: false 我们的系统在演进的过程中，不可避免的会遇到各种 0 到 1 过程中的中间态。比如下面这种，可以比较直观的看出 Istio 或者网格是部分覆盖的。这个过程中，我们需要平滑、可控的推进，才能在保障系统可用性的前提下进行架构的演进。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2318242x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2318242x.png 使用 Page Bundles: false 怎么做 Sidecar 的注入分两种：手动和自动。 手动 手动</description></item><item><title>应“云”而生的 Java 框架：构建本机可执行文件</title><link>https://atbug.com/quarkus-build-native-executable-file/</link><pubDate>Sat, 17 Apr 2021 09:08:40 +0800</pubDate><guid>https://atbug.com/quarkus-build-native-executable-file/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/16186233244243.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/16186233244243.jpg 使用 Page Bundles: false 电影《功夫》中，火云邪神有句话：“天下武功无坚不摧，唯快不破。” 在 上一篇文章 中，我们写了第一个 Quarkus 应用，并尝试着构建了 legacy-jar 和 fast-jar。 今天来看一下 Quarkus 构建出来的本机可执行文件到底比 Spring 应用能快多少，生态的成熟度不在这里讨论。 TLDR 先上结论， 与只有一个 Controller 的Spring Web 应用做下对比。 应用启动时间：0.012s vs 2.294s Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0900292x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0900292x.png 使用 Page Bundles: false Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0915282x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/17/cleanshot-20210417-at-0915282x.png 使用 Page Bundles: false 镜像大小：49MB vs 237 MB Spring 应用镜像使用 openjdk:11.0-jre-slim 作为 base 镜像，大小为 220MB。 docker images REPOSITORY</description></item><item><title>应“云”而生的 Java 框架：Hello, Quarkus</title><link>https://atbug.com/hello-quarkus/</link><pubDate>Mon, 05 Apr 2021 21:08:40 +0800</pubDate><guid>https://atbug.com/hello-quarkus/</guid><description>
Wikipedia上有关 Quarkus 的信息还很少，只有一句简单的介绍： Quarkus 是专为 OpenJDK HotSpot 和 GraalVM 定制的全栈 Kubernetes 原生 Java 应用程序框架。与如 Spring 之类的其他框架相比，它提供了较小的内存占用并缩短了启动时间。它允许结合命令式和非阻塞响应式编程。 从 Quarkus 的官网，可以看到其有几个特性： 容器优先 统一了命令式和响应式编程 开发者友好 最佳品种的库及标准 更多 Quarkus 可以参考官网的介绍及文档。今天主要就是跑一下 Quarkus 的 Hello world。 放一张官网的图： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/05/16176279246527.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/05/16176279246527.jpg 使用 Page Bundles: false 环境准备 基于 Java 11 的 GraalVM Maven 3.6.2+ 笔者使用的是 macos 10.15.4，GraalVM 和 Maven 建议通过 sdkman 进行安装。 $ sdk install java 21.0.0.2.r11-grl #如果已使用其他 java 版本，可以使用命令 sdk</description></item><item><title>分布式系统在 Kubernetes 上的进化</title><link>https://atbug.com/translation-distributed-systems-kubernetes/</link><pubDate>Mon, 29 Mar 2021 23:11:25 +0800</pubDate><guid>https://atbug.com/translation-distributed-systems-kubernetes/</guid><description>
本文译自 The Evolution of Distributed Systems on Kubernetes 在 3 月份的 QCon 上，我做了一个关于 Kubernetes 的分布式系统进化的演讲。首先，我想先问一个问题，微服务之后是什么？我相信大家都有各自的答案，我也有我的答案。你会在最后发现我的想法是什么。为了达到这个目的，我建议大家看看分布式系统的需求是什么？以及这些需求在过去是如何发展的，从单体应用开始到 Kubernetes，再到最近的 Dapr、Istio、Knative 等项目，它们是如何改变我们做分布式系统的方式。我们将尝试对未来做一些预测。 现代分布式应用 为了给这个话题提供更多的背景信息，我认为的分布式系统是由数百个组件组成的系统。这些组件可以是有状态的、无状态的或者无服务器的。此外，这些组件可以用不</description></item><item><title>【译】2021 年及未来的云原生预测</title><link>https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/</link><pubDate>Tue, 09 Feb 2021 06:43:54 +0800</pubDate><guid>https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/</guid><description>
本文译自 Cloud Native Predictions for 2021 and Beyond 原文发布在 Chris Aniszczyk 的个人博客 我希望每个人都有一个美好的假期，因为 2021 年 1 月的前几周一直非常疯狂，从叛乱到新的 COVID 菌株。在云原生国度，CNCF 最近发布了关于我们去年完成的所有工作的年度报告。我建议大家找个机会去看一下这份报告，在疫情大流行的这一年，我们收获颇丰。https://twitter.com/CloudNativeFdn/status/1343914259177222145 作为我工作的一部分，我对云原生趋势有一个独特的观点，送给所有与我合作的会员公司和开发人员，所以我想我会分享我对 2021 年及以后云原生发展的想法。 云原生的 IDE 作为一个在 Eclipse 基金会内部从事开发者工具工作的人，我对最近的</description></item><item><title>【译】应用架构：为什么要随着市场演进</title><link>https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/</link><pubDate>Sun, 17 Jan 2021 21:37:23 +0800</pubDate><guid>https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/</guid><description>
本文译自 Application architecture: why it should evolve with the market 最初由Mia Platform团队发布在Mia Platform的博客上 如今，IT 挑战在于通过有效选择应用架构来适应市场和业务需求的发展。为了满足业务和客户的需求，IT 部门应能够对技术和方法采取行动以确保软件具有灵活性，并实现产品和服务的持续创新流程，从而做出更快的反应 。 当然，过去的单体应用程序和刚性基础设施无法做到这一点。相反，它可以通过为演化而设计的架构来实现，该架构在需要时易于更新和重构。容器化实践的广泛应用（根据 Gartner，到2022年，大公司的就业人数将从目前的 30％ 增长到 75％），这种情况下采用云原生方法重新设计微服务应用是成功的关键。 如何构建不断发展的应</description></item><item><title>Envoy listener filter times out 问题</title><link>https://atbug.com/envoy-listener-filter-times-out/</link><pubDate>Wed, 09 Dec 2020 20:00:00 +0800</pubDate><guid>https://atbug.com/envoy-listener-filter-times-out/</guid><description>
最近在看 openservicemesh 相关内容，这周更新了 main 分支的代码之后。发现原本 v0.5.0 时可以正常代理的 mysql 流量，在新的 commit 中无法代理了。 开启 envoy 的 filter debug 日志后发现出现了超时。 [2020-12-09 08:54:42.285][15][debug][filter] [source/extensions/filters/listener/original_dst/original_dst.cc:18] original_dst: New connection accepted [2020-12-09 08:54:42.285][15][debug][filter] [source/extensions/filters/listener/http_inspector/http_inspector.cc:38] http inspector: new connection accepted [2020-12-09 08:54:42.285][15][trace][filter] [source/extensions/filters/listener/http_inspector/http_inspector.cc:105] http inspector: recv: 0 [2020-12-09 08:54:57.286][15][debug][conn_handler] [source/server/connection_handler_impl.cc:273] listener filter times out after 15000 ms 贴一下 outbound listener 的配置片段: { &amp;#34;@type&amp;#34;: &amp;#34;type.googleapis.com/envoy.admin.v3.ListenersConfigDump&amp;#34;, &amp;#34;version_info&amp;#34;: &amp;#34;1&amp;#34;, &amp;#34;dynamic_listeners&amp;#34;: [{ &amp;#34;name&amp;#34;: &amp;#34;outbound_listener&amp;#34;, &amp;#34;active_state&amp;#34;: { &amp;#34;version_info&amp;#34;: &amp;#34;1&amp;#34;, &amp;#34;listener&amp;#34;: { &amp;#34;@type&amp;#34;: &amp;#34;type.googleapis.com/envoy.config.listener.v3.Listener&amp;#34;, &amp;#34;name&amp;#34;: &amp;#34;outbound_listener&amp;#34;, &amp;#34;address&amp;#34;: { &amp;#34;socket_address&amp;#34;: { &amp;#34;address&amp;#34;: &amp;#34;0.0.0.0&amp;#34;, &amp;#34;port_value&amp;#34;: 15001 } }, &amp;#34;filter_chains&amp;#34;: [ //省略其他 filter chain { &amp;#34;filters&amp;#34;: [{ &amp;#34;name&amp;#34;: &amp;#34;envoy.filters.network.tcp_proxy&amp;#34;, &amp;#34;typed_config&amp;#34;: { &amp;#34;@type&amp;#34;: &amp;#34;type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy&amp;#34;, &amp;#34;stat_prefix&amp;#34;: &amp;#34;passthrough-outbound&amp;#34;, &amp;#34;cluster&amp;#34;: &amp;#34;passthrough-outbound&amp;#34; } }], &amp;#34;name&amp;#34;: &amp;#34;outbound-egress-filter-chain&amp;#34; }], &amp;#34;listener_filters&amp;#34;: [{ &amp;#34;name&amp;#34;: &amp;#34;envoy.filters.listener.original_dst&amp;#34; }, { &amp;#34;name&amp;#34;: &amp;#34;envoy.filters.listener.http_inspector&amp;#34; } ], &amp;#34;traffic_direction&amp;#34;: &amp;#34;OUTBOUND&amp;#34; }, &amp;#34;last_updated&amp;#34;: &amp;#34;2020-12-08T10:07:41.191Z&amp;#34; } }] } 分析 Envoy 那边看到了有个 issue，为此增加了 timeout 的设置，默认 15s。 listener_filters_timeout：默认 15s。等待 listener filter 完成的超时时间，如果设置为 0，则不超时。如果没有设置下面的配置为true，则会直接关闭连接。</description></item><item><title>使用 cLion 阅读 envoy 源码</title><link>https://atbug.com/read-envoy-source-code-in-clion/</link><pubDate>Tue, 08 Dec 2020 20:53:39 +0800</pubDate><guid>https://atbug.com/read-envoy-source-code-in-clion/</guid><description>
虽然不写 C++，但是看点代码还是能看懂。Envoy 的功能配置复杂，有时候处理问题还是需要看下源码的。 Vim 或者 Code 就算了，我只是阅读源码需要关联跳转就行。在 Clion 中，代码的关联跳转需要一个CMakeLists.txt 文件。 我将生成的内容挂在了 gist 上了，不想花费时间生成的可以直接复制。 准备环境 1. 安装依赖的工具 brew install coreutils wget cmake libtool go automake ninja clang-format 2. bazel 使用 homebrew 进行安装： brew install bazel 即可 安装问运行编译测试下：bazel build //source/exe:envoy-static，提示版本太高。 现在 homebrew 安装的版本是3.7.1，而 envoy 需要3.4.1。 按照提示下载3.4.1的版本：cd &amp;quot;/usr/local/Cellar/bazel/3.7.1/libexec/bin&amp;quot; &amp;amp;&amp;amp; curl -fLO https://releases.bazel.build/3.4.1/release/bazel-3.4.1-darwin-x86_64 &amp;amp;&amp;amp; chmod +x bazel-3.4.1-darwin-x86_64，然</description></item><item><title>Kubernetes 源码解析 - Informer</title><link>https://atbug.com/kubernetes-source-code-how-informer-work/</link><pubDate>Sun, 16 Aug 2020 23:32:38 +0800</pubDate><guid>https://atbug.com/kubernetes-source-code-how-informer-work/</guid><description>
上篇扒了 HPA 的源码，但是没深入细节，今天往细节深入。 开局先祭出一张图： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/08/16/15975802542217.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/08/16/15975802542217.png 使用 Page Bundles: false 为什么要有 Informer？ Kubernetes 中的持久化数据保存在 etcd中，各个组件并不会直接访问 etcd，而是通过 api-server暴露的 RESTful 接口对集群进行访问和控制。 资源的控制器（图中右侧灰色的部分）读取数据也并不会直接从 api-server 中获取资源信息（这样会增加 api-server 的压力），而是从其“本地缓存”中读取。这个“本地缓存”只是表象的存在，加上缓存的同步逻辑就是今天要是说的Informer（灰色区域中的第一个蓝色块）所提供的功能。 从图中可以看到 Informer 的几个组件： Reflector：与 api</description></item><item><title>Kubernetes 源码解析 - HPA 水平自动伸缩如何工作</title><link>https://atbug.com/kubernetes-source-code-how-hpa-work/</link><pubDate>Sat, 15 Aug 2020 02:09:37 +0800</pubDate><guid>https://atbug.com/kubernetes-source-code-how-hpa-work/</guid><description>
HPA - Horizontal Pod Autoscaler 的缩写，Pod 水平自动伸缩。通过对 Pod 负载的监控，来自动增加或者减少 Pod 的副本数量。 从字面意思来看，其主要包含了两部分： 监控 Pod 的负载 控制 Pod 的副本数量 那具体是如何实现的呢？以下基于1.17 源码，来分析下 HPA 如何工作。 注意：文章中的代码在源码的基础上进行了精简：删掉了注释、序列化等信息，或保留了部分核心代码，加上新的注释。 资源 HPA 的资源是HorizontalPodAutoscaler，在v1版本中，只支持基于 CPU 指标的计算；在v2beta2版本中加入了基于内存和自定义指标的计算。 v1 //staging/src/k8s.io/api/autoscaling/v1/types.go type HorizontalPodAutoscaler struct { metav1.TypeMeta metav1.ObjectMeta Spec HorizontalPodAutoscalerSpec Status HorizontalPodAutoscalerStatus } type HorizontalPodAutoscalerSpec struct { ScaleTargetRef CrossVersionObjectReference //监控的目标资源 MinReplicas *int32 //最小副本数 MaxReplicas int32 //最大副本数 TargetCPUUtilizationPercentage *int32 //触发调整的CPU 使用</description></item><item><title>带你了解 Ribbon 负载均衡器的实现</title><link>https://atbug.com/how-loadbalancer-works-in-ribbon/</link><pubDate>Tue, 09 Jun 2020 19:35:53 +0800</pubDate><guid>https://atbug.com/how-loadbalancer-works-in-ribbon/</guid><description>
Spring Cloud 中 Ribbon有在 Zuul 和 Feign 中使用，当然也可以通过在RestTemplate的 bean 定义上添加@LoadBalanced注解方式获得一个带有负载均衡更能的RestTemplate。 不过实现的方法都大同小异：对HttpClient进行封装，加上实例的”选择“（这个选择的逻辑就是我们所说的负载均衡）。 要学习某个框架的时候，最简单的方案就是：Running+Debugging。 跑就是了。 debug 不一定是为了 bug debug 出真知 Debugging = Learning 选用 Ali Spittel 的一条推文： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-165236.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-165236.png 使用 Page Bundles: false 以 Zuul 路由的线程栈为例 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-151421.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-151421.png 使用 Page Bundles: false 调整下</description></item><item><title>Eureka 实例注册状态保持 STARTING 的问题排查</title><link>https://atbug.com/troubleshooting-on-eureka-instance-keep-starting/</link><pubDate>Thu, 28 May 2020 22:04:02 +0800</pubDate><guid>https://atbug.com/troubleshooting-on-eureka-instance-keep-starting/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/28/maninblackshirtandgraydenimpantssittingongray11342.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/28/maninblackshirtandgraydenimpantssittingongray11342.jpg 使用 Page Bundles: false 这是真实发生在生产环境的 case，实例启动后正常运行，而在注册中心的状态一直保持STARTING，而本地的状态为UP。导致服务的消费方无法发现可用实例。 这种情况的出现概率非常低，运行一年多未发现两个实例同时出现问题的情况，因此多实例运行可以避免。文末有问题的解决方案，不想花时间看分析过程可直接跳到最后。 环境说明： eureka-client: 1.7.2 spring-boot: 1.5.12.RELEASE spring-cloud: Edgware.SR3 问题重现 借助Btrace重现, java -noverify -cp .:btrace-boot.jar -javaagent:btrace-agent.jar=script=&amp;lt;pre-compiled-btrace-script&amp;gt; &amp;lt;MainClass&amp;gt; &amp;lt;AppArguments&amp;gt; 思路 主线程更新实例本地状态(STARTING-&amp;gt;UP)前, 等待心跳线程完成第一次心跳并尝试注册实例, 获取到当前的状态STARTING. 主线程更新状态后触</description></item><item><title>Tekton 的工作原理</title><link>https://atbug.com/how-tekton-works/</link><pubDate>Sat, 23 May 2020 22:47:14 +0800</pubDate><guid>https://atbug.com/how-tekton-works/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/24/tekton.jpeg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/24/tekton.jpeg 使用 Page Bundles: false 这篇文章是基于 Tekton Pipeline 的最新版本v0.12.1版本。 快速入门请参考：云原生 CICD: Tekton Pipeline 实战 ，实战是基于版本 v0.10.x。 Pipeline CRD 与核心资源的关系 $ k api-resources --api-group=tekton.dev NAME SHORTNAMES APIGROUP NAMESPACED KIND clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task Tekton Pipelines提供了上面的CRD，其中部分CRD与k8s core中资源相对应 Task =&amp;gt; Pod Task.Step =&amp;gt; Container Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902164552270.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902164552270.jpg 使用 Page Bundles: false 工作原理 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902280074872.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902280074872.jpg 使用 Page Bundles: false (图片来自) Tekton Pipeline 是基于 Knative 的实现，pod tekton-pipelines-controller 中有两个 Knative Controller的</description></item><item><title>Java 中的 Mysql 时区问题</title><link>https://atbug.com/mysql-timezone-in-java/</link><pubDate>Thu, 14 May 2020 11:34:24 +0800</pubDate><guid>https://atbug.com/mysql-timezone-in-java/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/14/anonymouspersonwithminiatureairplaneonchalkboard37.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/14/anonymouspersonwithminiatureairplaneonchalkboard37.jpg 使用 Page Bundles: false (Photo by Andrea Piacquadio from Pexels) 话说工作十多年，mysql 还真没用几年。起初是外企银行，无法直接接触到 DB；后来一直从事架构方面，也多是解决问题为主。 这次搭建海外机房，围绕时区大家做了一番讨论。不说最终的结果是什么，期间有同事认为 DB 返回的是 UTC 时间。 这里简单做个验证，顺便看下时区的问题到底是如何处理。 环境 openjdk version &amp;ldquo;1.8.0_242&amp;rdquo; mysql-connector-java &amp;ldquo;8.0.20&amp;rdquo; mysql &amp;ldquo;5.7&amp;rdquo; 时区 TZ=Europe/London 本地时区 GMT+8 创建个简单的库test及表user， 表结构如下： CREATE TABLE `user` ( `name` varchar(50) NOT NULL, `birth_date` timestamp NULL DEFAULT CURRENT_TIMESTAMP ) ENGINE=InnoDB DEFAULT CHARSET=latin1 插入一条测试数据： mysql&amp;gt; insert into `user` -&amp;gt; values (&amp;#39;Tom&amp;#39;, time(&amp;#39;2020-05-15 08:00:00&amp;#39;)); Query OK, 1 row affected (0.01 sec) mysql&amp;gt; select * from user; +------+---------------------+ | name | birth_date | +------+---------------------+ | Tom | 2020-05-14 08:00:00 | +------+---------------------+ 1 row in set (0.00 sec) 测试代码： Connection conn = DriverManager.getConnection(&amp;#34;jdbc:mysql://localhost:3306/test?useSSL=false&amp;#34;, &amp;#34;root&amp;#34;, &amp;#34;root&amp;#34;); Statement stmt = conn.createStatement(); stmt.execute(&amp;#34;select</description></item><item><title>翻译：多运行时微服务架构</title><link>https://atbug.com/translation-multi-runtime-microservices-architecture/</link><pubDate>Wed, 01 Apr 2020 23:18:00 +0800</pubDate><guid>https://atbug.com/translation-multi-runtime-microservices-architecture/</guid><description>
这样文章通过Google翻译和人工逐字修改的方式完成的，某些位置也加上自己的理解。如有错误，请指出。 翻译这篇文章的目的其实是为了自己加深对微服务、分布式架构以及多运行时架构的理解。整篇文章从”战略“上分析了微服务”从古至今“解决的问题，以及带来的新问题；进而在“战术”层面，给出了解决这些新问题的手段。 个人见解：架构从来都是解决问题并带来问题， 取舍之道 。 背景知识 微服务的 12 要素： 基准代码：一份基准代码，多份部署 依赖：显式声明依赖关系 配置：在环境中存储配置 后端服务：把后端服务当做附加资源 构建、发布、运行：严格分离构建和运行 进程：以一个或多个无状态进程运行应用 端口绑定：通过端口绑定提供服务 并发：通过进</description></item><item><title>控制 Pod 内容器的启动顺序</title><link>https://atbug.com/control-process-order-of-pod-containers/</link><pubDate>Thu, 12 Mar 2020 22:05:16 +0800</pubDate><guid>https://atbug.com/control-process-order-of-pod-containers/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/screenshot-20210430-at-092623.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/screenshot-20210430-at-092623.png 使用 Page Bundles: false 2021.4.30 更新： 最新的方案，请跳转新篇 Kubernetes 上如何控制容器的启动顺序。 背景 众所周知, Kubernetes Pod 内有两种容器: 初始化容器(init container)和应用容器(app container). 其中初始化容器的执行先于应用容器, 并且初始化容器和应用容器的个数分别为 0~n 和 1~n. 初始化容器会按照顺序执行, 顺序执行的前提是初始化容器始终会运行到完成(completed)状态. 而应用容器恰好相反: 启动顺序随机, 并始终保持运行(running)状态. 问题 工作中有个架构的方案使用到了 sidecar 容器: 将基础组件功能从容器转移到 sidecar 容器中, 其中有个功能是从远程配置中心获取配置并保持实时更新. 保证实</description></item><item><title>Go Docker 镜像进阶: 精简镜像</title><link>https://atbug.com/build-minimal-docker-image-for-go-app/</link><pubDate>Wed, 11 Mar 2020 23:00:27 +0800</pubDate><guid>https://atbug.com/build-minimal-docker-image-for-go-app/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839387687383.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839387687383.jpg 使用 Page Bundles: false ​[图片来自 https://www.facebook.com/sequenceprocess/] 问题: 入门到生产级的差距 昨天的文章《为 Go 应用创建 Docker 镜像》, 算是入门级的, 并不适用于生产级. 为什么? $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE addozhang/golang-hello-world latest 4cce1292a87a 4 seconds ago 813MB 整个镜像的大小有 813MB, 这还只有一个简单的 Hello world. 因为其中包含了 Golang 的编译和运行环境. 但是实际生产环境中, 我们并不需要这么多. 先看结果 精简之后只有 2.07MB, 而且并不影响运行. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE addozhang/golang-hello-world latest 4cce1292a87a 3 minutes ago 813MB addozhang/golang-hello-world2 latest 1da5bb994074 7 minutes ago 2.07MB $ docker run --rm addozhang/golang-hello-world2 Hello world 解决方案 如果做到的? 首先从基础镜像开始, 换成scratch1. 构建时将编译好的文件复制到镜像中 FROM scratch ADD golang-hello-world / CMD [&amp;#34;/golang-hello-world&amp;#34;] 假如你是使用go build来编译, 在 Macos 上会遇到如下问题: $ docker run</description></item><item><title>为 Go 应用创建 Docker 镜像</title><link>https://atbug.com/build-docker-image-for-go-app/</link><pubDate>Wed, 11 Mar 2020 20:41:58 +0800</pubDate><guid>https://atbug.com/build-docker-image-for-go-app/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839304511808.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839304511808.jpg 使用 Page Bundles: false 嗯嗯, 最近开始用 Golang 了. 今天需要为 Go 应用创建对象, 看了下官方博客. 拿 hello world 做个测试. 使用下面的命令创建个新的项目 $ mkdir -p $GOPATH/src/github.com/addozhang/golang-hello-world &amp;amp;&amp;amp; cd &amp;#34;$_&amp;#34; $ go mod init github.com/addozhang/golang-hello-world go: creating new go.mod: module github.com/addozhang/golang-hello-world $ cat &amp;lt;&amp;lt; EOF &amp;gt; main.go package main import &amp;#34;fmt&amp;#34; func main() { fmt.Println(&amp;#34;Hello world&amp;#34;) } EOF # go fmt 运行检查一次 $ go run main.go Hello world 程序没问题, 下面就是构建镜像了. 创建一个 Dockerfile 文件, 内容如下: FROM golang LABEL Author=&amp;#34;addozhang&amp;#34; ADD . /go/src/github.com/addozhang/golang-hello-world RUN go install github.com/addozhang/golang-hello-world ENTRYPOINT [ &amp;#34;/go/bin/golang-hello-world&amp;#34; ] 构建镜像: $ docker build -t addozhang/golang-hello-world . 运行镜像: $ docker run --rm addozhang/golang-hello-world:latest Hello world 运行没问题, 收工</description></item><item><title>云原生CICD: Tekton Trigger 实战</title><link>https://atbug.com/tekton-trigger-practice/</link><pubDate>Wed, 12 Feb 2020 21:30:03 +0800</pubDate><guid>https://atbug.com/tekton-trigger-practice/</guid><description>
Trigger的介绍看 这里. 接上文 Tekton Pipeline 实战 , 我们为某个项目创建了一个Pipeline, 但是执行时通过 PipelineRun 来完成的. 在 PipelineRun 中我们制定了 Pipepline 以及要使用的 PipelineResource. 但是日常的开发中, 我们更多希望在提交了代码之后开始 Pipeline 的执行. 这时我们就要用到 Tekton Trigger 了. 思路是这样: 代码提交后将Push Event发送给Tekton Trigger EventController(以下简称 Controller), 然后 Controller 基于的TriggerBinding的配置从 payload 中提取信息, 装载在&amp;quot;Params&amp;quot;中作为TriggerTemplate的入参. 最后 Controller 创建PipelineRun. Trigger 相关的资源 TriggerTemplate 回看上回用的PipelineRun Yaml, 参数有revision, url, image</description></item><item><title>Tekton Trigger 介绍</title><link>https://atbug.com/tekton-trigger-glance/</link><pubDate>Wed, 05 Feb 2020 18:03:15 +0800</pubDate><guid>https://atbug.com/tekton-trigger-glance/</guid><description>
背景 Tekton 的介绍请参考Tekton Pipeline 实战. 通常, CI/CD 事件应该包含如下信息: 确定事件的类型(比如 GitHub Push, GitLab Issue, Docker Hub Webhook 等) 可从特定管道访问并映射到特定管道 (从事件负载中获取 SHA 信息, 然后在管道中使用) 准确地触发管道 (基于有效负载值触发管道) Tekton API 的设计分离了配置(比如 PipelineRun VS Pipeline), 保证了 step 可以被重用. 但是没有提供动态封装配置的机制来生成资源(尤其是 PipelineRun 和 PipelineResource). Triggers 通过下面的 CRDs 在架构上对 Tekton 进行了扩展: TriggerTemplate: 创建资源的模板(比如用来创建 PipelineResource 和 PipelineRun) TriggerBinding: 校验事件并提取负载字段 EventListener: 连接TriggerBinding和TriggerTemplate到可寻址的端点(事件接收器). 使用从各个 TriggerBinding中提取的参数来创建TriggerTemp</description></item><item><title>Tekton Dashboard 安装</title><link>https://atbug.com/tekton-dashboard-installation/</link><pubDate>Sat, 01 Feb 2020 12:39:28 +0800</pubDate><guid>https://atbug.com/tekton-dashboard-installation/</guid><description>
Tekton 提供了dashboard方便用户管理和查看 Tekton PipelineRun 和 TaskRun 以及创建, 执行和完成过程中涉及的资源. 它还允许按标签过滤 PipelineRun 和 TaskRun. 安装方法 kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/v0.4.1/dashboard_latest_release.yaml 检查dashboard的运行情况, STATUS为Running的话则说明运行成功. kubectl get pods --namespace tekton-pipelines 访问 访问Tekton的Dashboard有两种方式, 一种是通过port-forward, 另一种是通过ingress来访问. port-forward kubectl port-forward svc/tekton-dashboard 9097 ingress 先检查ingress是否开启. minikube addon list ... - ingress: enabled ... 如果是disabled的话, 通过命令minikube addons enable ingress. 注意: 这里拉取quay.io/kubernetes-ingress-controller/nginx-ingress-controller镜</description></item><item><title>Tekton 0.9.0 更新</title><link>https://atbug.com/tekton-0.9.0-release/</link><pubDate>Sun, 19 Jan 2020 14:33:17 +0800</pubDate><guid>https://atbug.com/tekton-0.9.0-release/</guid><description>
翻译整理自 What’s New in Tekton 0.9 功能及Bug修复 脚本模式 以前如果要在容器里运行个简单的 bash 脚本, 需要这么写: - name:helloimage:ubuntucommand:[&amp;#39;bash&amp;#39;]args:- -c- |set -ex echo &amp;#34;hello&amp;#34;在 0.9 之后, 可以更加简单, 不需要再写command 和讨厌的-c. - name:helloimage:ubuntuscript:|#!/bin/bash echo &amp;#34;hello&amp;#34;性能 通过一系列的工作, 每个 PipelineRun 的运行时间缩短了 5-20 秒 API 变化 为了 beta 版本的发布, API 做了一些调整: 镜像摘要输出路径的标准化 Tekton 目前提供了一个机制用于存储 task 构建出的镜像的摘要. 这个机制早于 PipelineResource子系统, 并要求 Task 编写者镜像这些摘要写到指定的位置/builder/image-outputs. 从现在开始, 有了输出资源的标准路径/workspace/</description></item><item><title>Tekton安装及Hello world</title><link>https://atbug.com/tekton-installation-and-sample/</link><pubDate>Fri, 17 Jan 2020 19:17:14 +0800</pubDate><guid>https://atbug.com/tekton-installation-and-sample/</guid><description>
安装 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 检查安装的tekton相关的CRD: kubectl api-resources | grep tekton clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task tekton的两个pod: kubectl get pods --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-556d8f4494-2qthv 1/1 Running 0 11m tekton-pipelines-webhook-849cff5cf-8m5qq 1/1 Running 0 11m 安装CLI cli: https://github.com/tektoncd/cli#installing-tkn brew install tektoncd-cli Tekton: hello world 创建一个简单的Task, 只有一个step就是打印出&amp;quot;hello world&amp;quot; apiVersion:tekton.dev/v1alpha1kind:Taskmetadata:name:echo-hello-worldspec:steps:- name:echoimage:alpinecommand:- echoargs:- &amp;#34;hello world&amp;#34;创建一个TaskRun执行上面的Task apiVersion:tekton.dev/v1alpha1kind:TaskRunmetadata:name:echo-hello-world-task-runspec:taskRef:name:echo-hello-world运行task: kubectl apply</description></item><item><title>Minikube安装istio</title><link>https://atbug.com/install-istio-on-minikube/</link><pubDate>Fri, 17 Jan 2020 08:02:42 +0800</pubDate><guid>https://atbug.com/install-istio-on-minikube/</guid><description>
准备 注意: istioctl的安装要使用安装里的, 不要是用homebrew里的. github issue curl -L https://istio.io/downloadIstio | sh - cd istio-1.4.2 cp bin/istioctl /usr/local/bin/istioctl 安装前检查 istioctl verify-install 如果检查没问题, 会看到Install Pre-Check passed! The cluster is ready for Istio installation. 安装 istio有5种内建的安装配置1: remote, sds, default, demo, minimal istioctl profile list minimal: 使用istio的流量管理所需组件的最小化安装 default: 根据IstioControlPlane API的默认设置(建议用于生产部署)启用组件. 您可以通过运行命令istioctl profile dump显示默认设置. demo: 几乎安装所有的特性, 包括logging和tracing的比例为100%. 不适合生产环境, 负载太重 default demo minimal sds remote Core components istio-citadel X X X X istio-egressgateway X istio-galley X X X istio-ingressgateway X X X istio-nodeagent X istio-pilot X X X X istio-policy X X X istio-sidecar-injector X X X X istio-telemetry X X X Addons</description></item><item><title>神秘的Eureka自我保护</title><link>https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/</link><pubDate>Sun, 05 Jan 2020 14:14:03 +0800</pubDate><guid>https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/</guid><description>
本文翻译自The Mystery of Eureka Self-Preservation 根据CAP定理, Eureka是一个AP系统, 这就导致了在网络分区期间多个注册表中的信息不一致. 自我保护功能则是为了尽可能降低这种不一致. 自我保护的定义 自我保护(self preservation)是Eureka的一项功能, Eureka注册表在未收到实例的心跳情况超过一定阈值时停止驱逐过期的实例. 从一个健康的系统开始 把下面看成一个健康的系统 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/uPic/n5wZMX.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/uPic/n5wZMX.jpg 使用 Page Bundles: false 假设所有的微服务都处于健康的状态并成功注册到Eureka注册表中. 多个注册表间会同步注册表记录, 所有的微服务实例都处于UP状态. 假设实例2从注册中心发现里实例4, 并调</description></item><item><title>加速云原生的 Java 开发</title><link>https://atbug.com/speed-up-java-development-on-kubernetes/</link><pubDate>Sat, 21 Dec 2019 20:45:22 +0800</pubDate><guid>https://atbug.com/speed-up-java-development-on-kubernetes/</guid><description>
今天来说说日常在Kubernetes开发Java项目遇到的问题. 当我们新建一个项目的时候, 总是面临需要新建manifest, 平时都是copy+paste+modify. 能否以变成的方式来生成? 开发时的步骤也比较繁琐: docker build, docker push, kubectl apple, kubectl delete pod. 对于一个Java应用来说还多了一步编译. 操作一次还ok, 但是一天十几次总会有想吐的感觉. 这些步骤能否简化成一个命令, 甚至修改了代码自动就完成上面一系列的操作? 实现这些我们需要几个工具: dekorate, Jib, Skaffold. 其中Jib也在上一篇文章使用Jib为Java应用构建镜像中介绍过. dekorate Dekorate is a collection of Java compile-time generators and decorators for Kubernetes/OpenShift manifests. Dekorate是Java编译时生成和装饰Kubernetes/OpenShift</description></item><item><title>使用 Jib 为 Java 应用构建镜像</title><link>https://atbug.com/build-docker-or-oci-image-with-jib-for-java/</link><pubDate>Mon, 09 Dec 2019 10:05:30 +0800</pubDate><guid>https://atbug.com/build-docker-or-oci-image-with-jib-for-java/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://github.com/GoogleContainerTools/jib/raw/master/logo/jib-build-docker-java-container-image.png 链接到文件: /static/https://github.com/GoogleContainerTools/jib/raw/master/logo/jib-build-docker-java-container-image.png 使用 Page Bundles: false Jib是Google Container Tools中的一个工具。 Jib builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library. Jib无需Docker守护程序即可为Java应用程序构建优化的Docker和OCI映像-无需深入了解Docker最佳实践. 它可以作为Maven和Gradle的插件以及Java库使用. 与Docker构建流程比较 Docker镜像构建流程: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://4.bp.blogspot.com/-SXeItzMS_oo/WzVemqaj7CI/AAAAAAAAF_w/t5Lau7EOC84Kywct_OPiDGIomCiFTywgwCLcBGAs/s1600/docker_build_flow.png 链接到文件: /static/https://4.bp.blogspot.com/-SXeItzMS_oo/WzVemqaj7CI/AAAAAAAAF_w/t5Lau7EOC84Kywct_OPiDGIomCiFTywgwCLcBGAs/s1600/docker_build_flow.png 使用 Page Bundles: false Jib构建流程: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://3.bp.blogspot.com/-_qNyJdVno8E/WzVeqmuC5PI/AAAAAAAAF_0/AHaZ1_ZnJmg8eaUnTlUGyUVe06KRmvlYQCLcBGAs/s1600/jib_build_flow.png 链接到文件: /static/https://3.bp.blogspot.com/-_qNyJdVno8E/WzVeqmuC5PI/AAAAAAAAF_0/AHaZ1_ZnJmg8eaUnTlUGyUVe06KRmvlYQCLcBGAs/s1600/jib_build_flow.png 使用 Page Bundles: false (pic from Google Cloud Platform Blog) 快</description></item><item><title>Spring Cloud Hoxton发布</title><link>https://atbug.com/spring-cloud-hoxton-release/</link><pubDate>Wed, 04 Dec 2019 11:09:07 +0800</pubDate><guid>https://atbug.com/spring-cloud-hoxton-release/</guid><description>
原文 Spring Cloud Hoxton.RELEASE基于Spring Boot 2.2.1.RELEASE 文档变化 Hoxton.RELEASE使用了新的首页, 新的样式以及单页面, 多页面和PDF版本. 新的负载均衡器实现 Hoxton.RELEASE是第一个包含阻塞和非阻塞客户端负载均衡器实现的版本, 替代进入维护状态的Netflix Ribbon. 搭配BlockingLoadBalancerClient使用RestTemplate, 需要在classpath中引入org.springframework.cloud:spring-cloud-loadbalancer. 这个依赖同样用于使用了@LoadBalanced WebClient.Builder的响应式应用中. 唯</description></item><item><title>Bose QC35 固件降级</title><link>https://atbug.com/bose-qc35-downgrade/</link><pubDate>Sun, 10 Nov 2019 19:50:27 +0800</pubDate><guid>https://atbug.com/bose-qc35-downgrade/</guid><description>
为什么要降级? 既然已经搜到了这里, 相信个中原因也都清楚. 我的QC35一代, 之前升级到了3.0.3固件. 目前已成功降级到了1.0.6, 下面的操作步骤Mac OSX的, 作者也提供了Windows上的操作步骤. Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/LaTKaT.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/LaTKaT.jpg 使用 Page Bundles: false 操作步骤 确认已经卸载Bose Updater GitHub上下载作者已经改好的6.0.0.4388版本的Bose Updater 解压后复制到/Applications下 运行Bose Updater确认能否正常运行 退出 (右键&amp;gt;Exit) 检查是否还有Bose Updater的进程 打开https://btu.bose.com/ 页面上选择Launch Bose Updater 看到下</description></item><item><title>Docker Engine API on Mac Osx</title><link>https://atbug.com/docker-engine-api-on-mac-osx/</link><pubDate>Wed, 06 Nov 2019 20:19:50 +0800</pubDate><guid>https://atbug.com/docker-engine-api-on-mac-osx/</guid><description>
根据官方的文档Docker Desktop on Mac vs. Docker Toolbox, Docker Desktop on Mac只提供了UNIX socket/var/run/docker.sock, 并未提供tcp的监听(默认2375端口). 如果使用linux的配置方式在Docker Desktop中配置host, Docker Desktop将无法启动. 需要去~/.docker/daemon.json中删除hosts配置才能正常启动. 通过下面的方式暴露出2375的tcp docker run --rm -d -v /var/run/docker.sock:/var/run/docker.sock -p 127.0.0.1:2375:2375 bobrik/socat TCP-LISTEN:2375,fork UNIX-CONNECT:/var/run/docker.sock 然后通过docker version查看当前的docker engine的版本, 比如1.40. 查看官方的Engine API文档: https://docs.docker.com/engine/api/v1.40 搜索个镜像测试一下: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/15729240620185.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/15729240620185.jpg 使用 Page Bundles: false</description></item><item><title>Spring Boot 2.2.0 发布</title><link>https://atbug.com/spring-boot-2-2-0-release/</link><pubDate>Tue, 22 Oct 2019 09:27:03 +0800</pubDate><guid>https://atbug.com/spring-boot-2-2-0-release/</guid><description>
译自: https://spring.io/blog/2019/10/16/spring-boot-2-2-0 组件升级 Spring AMQP 2.2 Spring Batch 4.2 Spring Data Moore Spring Framework 5.2 Spring HATEOAS 1.0 Spring Integration 5.2 Spring Kafka 2.3 Spring Security 5.2 Spring Session Corn 第三方库升级 Elasticsearch 6.7 Flyway 6.0 Jackson 2.10 JUnit 5.5 Micrometer 1.3 Reactor Dysprosium Solr 8.0 性能提升 延迟初始化(Lazy initialization) 支持开启全局延迟加载spring.main.lazy-initialization. 代价: 初次处理HTTP请求耗时长 本应在启动初始化时出现的问题, 延后出现 更多参考: https://spring.io/blog/2019/03/14/lazy-initialization-in-spring-boot-2-2 Java 13支持 跟随Spring Framework5.2对Java 13的支持, Spring Boot 2.2现在也支持了Java13. 同时兼容Java 11和8. 不可变的@ConfigurationProperties绑定 现在加入了基于构造器的绑定, 允许@ConfigurationProperties标注的类不可变(属性</description></item><item><title>Zipkin dependencies的坑之二: 心跳超时和Executor OOM</title><link>https://atbug.com/zipkin-dependencies-bug-two-timeout-and-oom/</link><pubDate>Sun, 22 Sep 2019 18:27:37 +0800</pubDate><guid>https://atbug.com/zipkin-dependencies-bug-two-timeout-and-oom/</guid><description>
上回说为了解决吞吐问题, 将zipkin-dependencies的版本升级到了2.3.0. 好景不长, 从某一天开始作业运行报错: Issue communicating with driver in heartbeater org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval ... 19/09/18 08:33:20 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 4) java.lang.OutOfMemoryError: Java heap space ... 解决方案 最新版本(2.3.0)目前不支持额外的spark和elasticsearch-spark的配置, 已经提交了PR 超时的解决方案: 为spark指定配置 spark.executor.heartbeatInterval=600000 spark.network.timeout=600000 OOM解决方案: 根据实际情况通过es.input.max.docs.per.partition配置executor的数量. 调整运行内存及spark.executor.memory</description></item><item><title>Zipkin dependencies的坑之一: 耗时越来越长</title><link>https://atbug.com/zipkin-dependencies-bug-one/</link><pubDate>Sun, 22 Sep 2019 17:59:56 +0800</pubDate><guid>https://atbug.com/zipkin-dependencies-bug-one/</guid><description>
zipkin-dependencies是zipkin调用链的依赖分析工具. 系统上线时使用了当时的最新版本2.0.1, 运行一年之后随着服务的增多, 分析一天的数据耗时越来越多. 从最初的几分钟, 到最慢的几十小时(数据量18m). 最终返现是版本的问题, 升级到&amp;gt;=2.3.0的版本之后吞吐迅速上升. 所以便有了issue: Reminder: do NOT use the version before 2.3.0 但这也引来了另一个坑: 心跳超时和Executor OOM TL;DR 简单浏览了下zipkin-dependencies的源码, 2.0.1和2.3.2的比较大的差距是依赖的elasticsearch-spark的版本. 前者用的是6.3.2, 后者是7.3.0. 尝试在zipkin-depe</description></item><item><title>如何选择Kafka Topic的分区数</title><link>https://atbug.com/how-to-choose-topic-partition-count-number-kafka/</link><pubDate>Fri, 30 Aug 2019 11:10:46 +0800</pubDate><guid>https://atbug.com/how-to-choose-topic-partition-count-number-kafka/</guid><description>
在kafka中, topic的分区是并行计算的单元. 在producer端和broker端, 可以同时并发的写数据到不同的分区中. 在consumer端, Kafka总是将某个分区分配个一个consumer线程. 因此同一个消费组内的并行度与分区数息息相关. Partition分区数的大小, 更多直接影响到消费端的吞吐(一个分区只能同一消费组的一个消费者消费). 分区数小, 消费端的吞吐就低. 但是太大也会有其他的影响 原则: 更多的分区可提高吞吐量 分区数越多打开的文件句柄越多 分区数越多降低可用性 更多的分区增加端到端的延迟 客户端需要更多的内存 归根结底还是得有个度. 如何找出这个度? 有个粗略的计算公式: max(t/p, t/c). t就是所预期吞吐</description></item><item><title>博客最近半年没什么产出</title><link>https://atbug.com/no-output-in-past-half-year/</link><pubDate>Tue, 27 Aug 2019 14:29:12 +0000</pubDate><guid>https://atbug.com/no-output-in-past-half-year/</guid><description>
上一篇日志更新还是在去年的12月, 至今有差不多10个月没有更新了. 不是说没有东西可写, 而且想写的东西很多. 工作太忙, 不忙的时候又太懒, 归根结底还是太懒. 过去一年多都是在做基础架构方面的工作, 围绕技术中台展开的. 有很多技术需要去学习, 也有很多问题要处理. 过程中一直有记笔记的习惯, 所以可以写的东西很多. 不过有些属于公司的部分还是不能写的, 必要的职业道德还是要有的. 笔记记录一直在用MWeb, 并使用iCloud同步, 最近几个月也在结合幕布整理思路和工作安排. 好用的软件我也比较喜欢分享, 记得最早在Workpress上的博客就分享了很多自己常用的软件. (有点扯远了~~~) MWeb没有统计功能, 还有使用的是</description></item><item><title>Spring Boot源码分析 - Configuration注解</title><link>https://atbug.com/spring-boot-configuration-annotation/</link><pubDate>Mon, 10 Dec 2018 16:24:33 +0000</pubDate><guid>https://atbug.com/spring-boot-configuration-annotation/</guid><description>
@Configuration注解 @Configuration注解指示一个类声明一个或多个@Bean方法, 并且可以由Spring容器处理, 以在运行时为这些bean生成bean定义和服务请求. 使用ConfigurationClassParser来对@Configuration标注的类进行解析, 封装成ConfigurationClass实例. 具体的实现通过ConfigurationClassPostProcessor来实现的. ConfigurationClassPostProcessor 实现了BeanDefinitionRegistryPostProcessor接口, 间接实现了BeanFactorPostProcessor接口. #postProcessBeanDefinitionRegistry(): 注册所有Configurat</description></item><item><title>Nginx实现Elasticsearch的HTTP基本认证</title><link>https://atbug.com/elasticsearch-http-basic-authentication-via-nginx/</link><pubDate>Tue, 06 Nov 2018 09:24:24 +0000</pubDate><guid>https://atbug.com/elasticsearch-http-basic-authentication-via-nginx/</guid><description>
Elasticssearch的HTTP基本认证实现有两种方案: x-pack和nginx反向代理. 前者收费, 后者不太适合生产使用. 如果仅仅是开发测试, 第二种完全足够. 创建密码 htpasswd -bc ./passwd [username] [password] Docker compose version:&amp;#39;3&amp;#39;services:elasticsearch:image:elasticsearch:5.5.2container_name:elasticsearchrestart:unless-stoppedvolumes:- /tmp/elasticsearch:/usr/share/elasticsearch/datanginx:image:nginx:latestcontainer_name:elasticsearch-proxyports:- 9200:9200links:- elasticsearchvolumes:- ./passwd:/etc/nginx/.passwd- ./default.conf:/etc/nginx/conf.d/default.confnginx配置文件 upstream es { server elasticsearch:9200; keepalive 15; } server { listen 9200; server_name localhost; access_log /dev/stdout; error_log /dev/stdout; location / { auth_basic &amp;#34;Administrator’s Area&amp;#34;; auth_basic_user_file /etc/nginx/.passwd; proxy_http_version 1.1; proxy_set_header Connection &amp;#34;Keep-Alive&amp;#34;; proxy_set_header Proxy-Connection &amp;#34;Keep-Alive&amp;#34;; proxy_pass http://es; } location /health { access_log off; return 200 &amp;#34;healthy\n&amp;#34;; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to</description></item><item><title>Alpine容器安装Docker和OpenShift Client Tools</title><link>https://atbug.com/install-docker-and-openshift-client-tools-in-alpine-container/</link><pubDate>Tue, 28 Aug 2018 09:14:12 +0000</pubDate><guid>https://atbug.com/install-docker-and-openshift-client-tools-in-alpine-container/</guid><description>
安装Docker echo &amp;#34;http://dl-2.alpinelinux.org/alpine/edge/main&amp;#34; &amp;gt; /etc/apk/repositories echo &amp;#34;http://dl-2.alpinelinux.org/alpine/edge/community&amp;#34; &amp;gt;&amp;gt; /etc/apk/repositories echo &amp;#34;http://dl-2.alpinelinux.org/alpine/edge/testing&amp;#34; &amp;gt;&amp;gt; /etc/apk/repositories apk -U --no-cache \ --allow-untrusted add \ shadow \ docker \ py-pip \ openrc \ &amp;amp;&amp;amp; pip install docker-compose rc-update add docker boot 安装OpenShift Client Tools 需要先安装glibc apk --no-cache add ca-certificates wget wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.28-r0/glibc-2.28-r0.apk apk add glibc-2.28-r0.apk curl --retry 7 -Lo /tmp/client-tools.tar.gz &amp;quot;https://mirror.openshift.com/pub/openshift-v3/clients/3.9.1/linux/oc.tar.gz&amp;quot; curl --retry 7 -Lo /tmp/client-tools.tar.gz &amp;#34;https://mirror.openshift.com/pub/openshift-v3/clients/3.9.1/linux/oc.tar.gz&amp;#34; tar zxf /tmp/client-tools.tar.gz -C /usr/local/bin oc \ &amp;amp;&amp;amp; rm /tmp/client-tools.tar.gz \ &amp;amp;&amp;amp; apk del .build-deps # ADDED: Resolve issue x509 oc login issue apk add --update ca-certificates 参考: github issue</description></item><item><title>Zuul网关Ribbon重试</title><link>https://atbug.com/ribbon-retry-in-zuul/</link><pubDate>Thu, 02 Aug 2018 08:55:43 +0000</pubDate><guid>https://atbug.com/ribbon-retry-in-zuul/</guid><description>
相关配置 #如果路由转发请求发生超时(连接超时或处理超时), 只要超时时间的设置小于Hystrix的命令超时时间,那么它就会自动发起重试. 默认为false. 或者对指定响应状态码进行重试 zuul.retryable = true zuul.routes.&amp;lt;route&amp;gt;.retryable = false #同一实例上的最大重试次数, 默认值为0. 不包括首次调用 ribbon.MaxAutoRetries=0 #重试其他实例的最大重试次数, 不包括第一次选的实例. 默认为1 ribbon.MaxAutoRetriesNextServer=1 #是否所有操作执行重试, 默认值为false, 只重试`GET`请求 ribbon.OkToRetryOnAllOperations=false #连接超时, 默认2000 ribbon.ConnectTimeout=15000 #响应超时, 默认5000 ribbon.ReadTimeout=15000 #每个host的最大连接数 ribbon.MaxHttpConnectionsPerHost=50 #最大连接数 ribbon.MaxTotalHttpConnections=200 #何种响应状态码才进行重试 ribbon.retryableStatusCodes=404,502 实现 SimpleRouteLocator#getRoute返回的route对象中会带上retryabl</description></item><item><title>Hystrix工作原理三</title><link>https://atbug.com/hystrix-exception-handling/</link><pubDate>Sun, 24 Jun 2018 16:20:16 +0000</pubDate><guid>https://atbug.com/hystrix-exception-handling/</guid><description>
异常处理 Hystrix异常类型 HystrixRuntimeException HystrixBadRequestException HystrixTimeoutException RejectedExecutionException HystrixRuntimeException HystrixCommand失败时抛出, 不会触发fallback. HystrixBadRequestException 用提供的参数或状态表示错误的异常, 而不是执行失败. 与其他HystrixCommand抛出的异常不同, 这个异常不会触发fallback, 也不会记录进failure的指标, 因而也不会触发断路器, 应该在用户输入引起的错误是抛出, 否则会它与容错和后退行为的目的相悖. 不会触发fallback, 也不会记录到错误的指标中, 也不会触发断路器. RejectedExecutionException 线程池发生reject时抛出 HystrixTimeoutException 在HystrixCommand.run()或者HystrixObservableCommand.construct()时抛出, 会记录</description></item><item><title>Hystrix工作原理二</title><link>https://atbug.com/hystrix-isolation/</link><pubDate>Sun, 24 Jun 2018 16:18:52 +0000</pubDate><guid>https://atbug.com/hystrix-isolation/</guid><description>
隔离策略 线程和线程池 客户端(库, 网络调用等)在各自的线程上运行. 这种做法将他们与调用线程隔开, 因此调用者可以从一个耗时的依赖调用&amp;quot;离开(walk away)&amp;quot; Hystrix使用单独的, 每个依赖的线程池作为约束任何给定依赖的一种方式, 因此潜在执行的延迟将仅在该池中使可用线程饱和. Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15280741661560.png 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15280741661560.png 使用 Page Bundles: false 如果不试用线程池可以保护你免受故障的影响, 但是这需要客户端可信任地快速失败(网络连接/读取超时, 重试的配置)并始终表现良好. 在Hystrix的设计中, Netflix选择试用线程和线程池来达到隔离的目的, 原因有: 很多应用程序调用了由很多不同的团队开发的许</description></item><item><title>Hystrix工作原理一</title><link>https://atbug.com/how-hystrix-works/</link><pubDate>Mon, 04 Jun 2018 08:47:40 +0000</pubDate><guid>https://atbug.com/how-hystrix-works/</guid><description>
运行时的流程图 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15273755001891.png 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15273755001891.png 使用 Page Bundles: false 构建HystrixCommand或者HystrixObservableCommand对象 第一步是构建一个HystrixCommand或HystrixObservableCommand对象来代表对依赖服务所做的请求。 将在请求发生时将需要的任何参数传递给构造函数。 如果依赖的服务预期会返回单一的响应, 构造一个HystrixCommand对象, 例如: HystrixCommand command = new HystrixCommand(arg1, arg2); 如果依赖的服务预期会返回一个发出响应的Observable对象, 则构造一个HystrixObservableCommand对象, 例如: HystrixObservableCommand command = new HystrixObservableCommand(arg1, arg2); 执行Comm</description></item><item><title>解决rsyslogd资源占用率高问题</title><link>https://atbug.com/rsyslogd-high-cpu-trouble-shooting/</link><pubDate>Fri, 01 Jun 2018 09:32:28 +0000</pubDate><guid>https://atbug.com/rsyslogd-high-cpu-trouble-shooting/</guid><description>
rsyslogd资源占用高问题记录 问题: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15277280296373.jpg 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15277280296373.jpg 使用 Page Bundles: false openshift集群安装在esxi的虚拟机上. 各个节点出现问题, 集群响应很慢. kswapd0进程cpu 90%多. rsyslogd进程内存 90%多. **先上总结: ** system-journal服务监听/dev/logsocket获取日志, 保存在内存中, 并间歇性的写入/var/log/journal目录中. rsyslog服务启动后监听/run/systemd/journal/syslogsocket获取syslog类型日志, 并写入/var/log/messages文件中. 获取日志时需要</description></item><item><title>Kubernetes中的Nginx动态解析</title><link>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</link><pubDate>Wed, 30 May 2018 12:10:32 +0000</pubDate><guid>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</guid><description>
背景 Nginx运行在kubernets中, 反向代理service提供服务. kubernetes版本v1.9.1+a0ce1bc657. 问题: 配置如下: location ^~/info { proxy_pass: http://serviceName:port; } 删除并重建Service的时候, nginx会出现下面的问题: connect() failed (113: No route to host) &amp;hellip; upstream: &amp;ldquo;xxxxx&amp;rdquo; 分析 通过google发现, 是nginx的dns解析方案的问题. nginx官方的说明: If the domain name can’t be resolved, NGINX fails to start or reload its configuration. NGINX caches the DNS records until the next restart or configuration reload, ignoring the records’ TTL values. We can’t specify another load‑balancing algorithm, nor can we configure passive health checks or other features defined by parameters to the server directive, which we’ll describe in the next section. 意思是说, nginx在启动的时候就会解析proxy_pass后的域名, 并把ip缓存下来</description></item><item><title>Spring Cloud Ribbon 详解</title><link>https://atbug.com/spring-cloud-ribbon-breakdown-1/</link><pubDate>Sat, 05 May 2018 11:18:05 +0000</pubDate><guid>https://atbug.com/spring-cloud-ribbon-breakdown-1/</guid><description>
&lt;p>客户端负载均衡, Ribbon的核心概念是命名的客户端.&lt;/p>
&lt;h2 id="使用">使用&lt;/h2>
&lt;h3 id="引入ribbon依赖和配置">引入Ribbon依赖和配置&lt;/h3>
&lt;p>加入&lt;code>spring-cloud-starter-netflix-ribbon&lt;/code>依赖&lt;/p>
&lt;h3 id="代码中使用ribbonclient注解">代码中使用RibbonClient注解&lt;/h3>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span>
&lt;span class="nd">@RibbonClient&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;foo&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">configuration&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">FooConfiguration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">TestConfiguration&lt;/span> &lt;span class="o">{}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span> &lt;span class="kd">protected&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">FooConfiguration&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">ZonePreferenceServerListFilter&lt;/span> &lt;span class="nf">serverListFilter&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">ZonePreferenceServerListFilter&lt;/span> &lt;span class="n">filter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ZonePreferenceServerListFilter&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">filter&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">setZone&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;myTestZone&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">filter&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">IPing&lt;/span> &lt;span class="nf">ribbonPing&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">PingUrl&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ribbon客户端的配置, 如果不指定会使用默认的实现:&lt;/p>
&lt;ul>
&lt;li>IClientConfig 客户端相关配置&lt;/li>
&lt;li>IRule 定义负载均衡策略&lt;/li>
&lt;li>IPing 定义如何ping目标服务实例来判断是否存活, ribbon使用单独的线程每隔一段时间(默认10s)对本地缓存的ServerList做一次检查&lt;/li>
&lt;li>ServerList&lt;!-- raw HTML omitted --> 定义如何获取服务实例列表. 两种实现基于配置的&lt;code>ConfigurationBasedServerList&lt;/code>和基于Eureka服务发现的&lt;code>DiscoveryEnabledNIWSServerList&lt;/code>&lt;/li>
&lt;li>ServerListFilter&lt;!-- raw HTML omitted --> 用来使用期望的特征过滤静态配置动态获得的候选服务实例列表. 若未提供, 默认使用&lt;code>ZoneAffinityServerListFilter&lt;/code>&lt;/li>
&lt;li>ILoadBalancer 定义了软负载均衡器的操作的接口. 一个典型的负载均衡器至少需要一组用来做负载均衡的服务实例, 一个标记某个服务实例不在旋转中的方法, 和对应的方法调用从实例列表中选出某一个服务实例.&lt;/li>
&lt;li>ServerListUpdater DynamicServerListLoadBalancer用来更新实例列表的策略(推&lt;code>EurekaNotificationServerListUpdater&lt;/code>/拉&lt;code>PollingServerListUpdater&lt;/code>, 默认是拉)&lt;/li>
&lt;/ul></description></item><item><title>Jenkins CI/CD (一) 基于角色的授权策略</title><link>https://atbug.com/using-role-based-authorization-strategy-in-jenkins/</link><pubDate>Fri, 20 Apr 2018 12:18:46 +0000</pubDate><guid>https://atbug.com/using-role-based-authorization-strategy-in-jenkins/</guid><description>
&lt;p>最近开始客串运维做CI/CD的规划设计, 主要是基于&amp;rsquo;Pipeline as Code in Jenkins'. 整理了下思路和技术点, 慢慢的写.&lt;/p>
&lt;p>这一篇是关于基于角色的授权策略, 用的是&lt;code>Role-Based Authorization Strategy Plugin&lt;/code>.&lt;/p>
&lt;p>授权在CI/CD流程中比较常见, 比如我们只让某些特定用户才可以构建Pre-Release的Job. 而更高级的Release发布, 又会需要某些用户的审批才可以进行. 需要授权时, 可能还需要发邮件提醒用户.&lt;/p>
&lt;p>UI上如何使用就不提了, 这里只说Pipeline as Code. 后面的几篇也会是这个背景.&lt;/p>
&lt;p>参考的这篇&lt;a href="https://www.avioconsulting.com/blog/using-role-based-authorization-strategy-jenkins">文章&lt;/a>, 文章里的代码运行失败, 做了修复.&lt;/p>
&lt;h2 id="配置">配置&lt;/h2>
&lt;p>安装完插件, 需要开始&lt;code>基于角色的授权策略&lt;/code>. 同时添加角色和为用户分配角色.&lt;/p>
&lt;h3 id="使用role-based-strategy作为验证方式">使用&lt;code>Role-Based Strategy&lt;/code>作为验证方式&lt;/h3>
&lt;p>&lt;code>Manage Jenkins / Configure Global Security / Configure Global Security&lt;/code>&lt;/p>
&lt;p>
&lt;div class="notices warning image-warning">
&lt;div class="label">Image not found&lt;/div>
&lt;style>
a.warning-link {
color: inherit !important;
font-weight: inherit !important;
text-decoration: underline !important;
border-bottom: none !important; }
&lt;/style>
&lt;p>网站链接: &lt;a href='http://7xvxng.com1.z0.glb.clouddn.com/15241955282214.jpg' title='在新选项卡中打开此网站链接' target='_blank' class='warning-link'>http://7xvxng.com1.z0.glb.clouddn.com/15241955282214.jpg&lt;/a>&lt;/p>
&lt;p>链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15241955282214.jpg&lt;/p>
&lt;p>使用 &lt;a href='https://gohugo.io/content-management/page-bundles/' title='相关信息 Hugo Page Bundles' target='_blank' class='warning-link'>Page Bundles&lt;/a>: false&lt;/p>
&lt;/div>
&lt;/p></description></item><item><title>KVM安装手册</title><link>https://atbug.com/kvm-installation-note/</link><pubDate>Thu, 12 Apr 2018 12:45:15 +0000</pubDate><guid>https://atbug.com/kvm-installation-note/</guid><description>
&lt;h2 id="添加虚拟机流程">添加虚拟机流程：&lt;/h2>
&lt;pre>&lt;code>1. 配置网络
2. 配置存储池
3. 上传镜像
4. 安装虚拟机，指定配置
&lt;/code>&lt;/pre>
&lt;h3 id="安装kvm虚拟机">安装KVM虚拟机&lt;/h3>
&lt;h4 id="1-关闭防火墙selinux">1. 关闭防火墙，selinux&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># service iptables stop&lt;/span>
&lt;span class="c1"># setenforce 0 临时关闭&lt;/span>
&lt;span class="c1"># chkconfig NetworkManager off&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="2-安装kvm虚拟机">2. 安装kvm虚拟机&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># yum install kvm libvirt libvirt-devel python-virtinst python-virtinst qemu-kvm virt-viewer bridge-utils virt-top libguestfs-tools ca-certificates audit-libs-python device-mapper-libs virt-install&lt;/span>
&lt;span class="c1"># 启动服务&lt;/span>
&lt;span class="c1"># service libvirtd restart&lt;/span>
下载virtio-win-1.5.2-1.el6.noarch.rpm 如果不安装window虚拟机或者使用带virtio驱动的镜像可以不用安装
&lt;span class="c1"># rpm -ivh virtio-win-1.5.2-1.el6.noarch.rpm&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3-libvirt在管理本地或远程hypervisor时的表现形式如下">3. Libvirt在管理本地或远程Hypervisor时的表现形式如下。&lt;/h4>
&lt;p>在libvirt内部管理了五部分：&lt;/p>
&lt;ul>
&lt;li>节点：所谓的节点就是我们的物理服务器，一个服务器代表一个节点，上边存放着Hyper和Domain&lt;/li>
&lt;li>Hypervisor：即VMM，指虚拟机的监控程序，在KVM中是一个加载了kvm.ko的标准Linux系统。&lt;/li>
&lt;li>域（Domain）：指虚拟机，一个域代表一个虚拟机（估计思路来源于Xen的Domain0）&lt;/li>
&lt;li>存储池（Storage Pool）：存储空间，支持多种协议和网络存储。作为虚拟机磁盘的存储源。&lt;/li>
&lt;li>卷组（Volume）：虚拟机磁盘在Host上的表现形式。
上边的五部分，我们必须使用的是前三个，因为很多时候根据业务规则或应用的灵活性并没有使用卷组（其实就是有了编制的虚拟磁盘文件），也就没有必要使用存储池。&lt;/li>
&lt;/ul></description></item><item><title>启用Jenkins CLI</title><link>https://atbug.com/jenkins-cli-enable/</link><pubDate>Mon, 09 Apr 2018 11:16:38 +0000</pubDate><guid>https://atbug.com/jenkins-cli-enable/</guid><description>
Jenkins CLI提供了SSH和Client模式. Docker运行Jenkins version:&amp;#39;3&amp;#39;services:jenkins:image:jenkins/jenkins:alpineports:- 8080:8080- 50000:50000- 46059:46059volumes:- &amp;#34;/Users/addo/DevApps/Docker/data/jenkins:/var/jenkins_home&amp;#34;note: 以为是docker运行, ssh端口设置选用了固定端口. Client 从http://JENKINS_URL/cli页面下载client jar 使用方法: java -jar jenkins-cli.jar -s http://localhost:8080/ help 构建: java -jar jenkins-cli.jar -s http://localhost:8080/ build JOB [-c] [-f] [-p] [-r N] [-s] [-v] [-w] Starts a build, and optionally waits for a completion. Aside from general scripting use, this command can be used to invoke another job from within a build of one job. With the -s option, this command changes the exit code based on the outcome of the build (exit code 0 indicates a success) and interrupting the command will interrupt the job. With the -f option, this command changes the exit code based on the outcome of the build (exit code 0 indicates a success) however, unlike -s, interrupting the command will not interrupt the job (exit code 125 indicates the command was interrupted). With the -c option, a build will only run if there has been an SCM change. JOB : Name of the job to build -c : Check for SCM changes before starting the build, and if there&amp;#39;s no change, exit without doing a build -f : Follow the build progress. Like -s only interrupts are not passed through to the build. -p : Specify the build</description></item><item><title>Jenkins - 解决execute shell中启动的进程被在Job退出时被杀死问题</title><link>https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/</link><pubDate>Thu, 15 Mar 2018 17:00:25 +0000</pubDate><guid>https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/</guid><description>
因为ProcessTreeKiller的存在, 构建过程中使用shell启动的进程在Job完成时都会被kill掉. 各种搜索以及ProcessTreeKiller提供的解决方式是修改BUILD_ID和添加 -Dhudson.util.ProcessTree.disable=true都无法解决. 最后参考StackOverflow和Jenkins JIRA, 修改JENKINS_NODE_COOKIE为任何值, 如dontKillMe. 这种方法可以解决, 记录一下. (搜索排名靠前的结果都不对).</description></item><item><title>MacOS安装minishift</title><link>https://atbug.com/install-minishift-on-mac/</link><pubDate>Fri, 23 Feb 2018 15:32:26 +0000</pubDate><guid>https://atbug.com/install-minishift-on-mac/</guid><description>
MacOS环境安装minishift 安装minishift cli brew cask install minishift 使用virtualbox安装 安装的时候可以指定HTTP代理, 拉取墙外镜像时需要; 还可以指定insecure的镜像库. minishift start --docker-env HTTP_PROXY=&amp;#34;192.168.99.1:1087&amp;#34; --docker-env HTTPS_PROXY=&amp;#34;192.168.99.1:1087&amp;#34; --docker-env NO_PROXY=&amp;#34;192.168.0.0/16,172.30.0.0/16&amp;#34; --insecure-registry=&amp;#34;192.168.1.34&amp;#34; --vm-driver=virtualbox 启动 minishift start --vm-driver=virtualbox 删除 minishift delete 打开Openshift控制面板 minishift dashboard 获取集群ip地址 minishift ip 安装Openshift Cli brew install openshift-cli 可以使用openshift cli进行操作. minishift安装完成后会将配置信息写入到主机的用户目录下, $HOME/.kube目录下除了config信息, 还有openshift的集群信息及支持的api. oc login -u system:admin oc get pods --all-namespaces</description></item><item><title>Spring Cloud Zuul详解</title><link>https://atbug.com/spring-cloud-zuul-breakdown/</link><pubDate>Thu, 22 Feb 2018 17:02:26 +0000</pubDate><guid>https://atbug.com/spring-cloud-zuul-breakdown/</guid><description>
&lt;p>Spring Cloud对Netflix Zuul做了封装集成, 使得在Spring Cloud环境中使用Zuul更方便. Netflix Zuul相关分析请看&lt;a href="http://atbug.com/learn-netflix-zuul/">上一篇&lt;/a>.&lt;/p>
&lt;h2 id="实现">实现&lt;/h2>
&lt;p>@EnableZuulProxy 与 @EnableZuulServer
二者的区别在于前者使用了服务发现作为路由寻址, 并使用Ribbon做客户端的负载均衡; 后者没有使用.
Zuul server的路由都通过&lt;code>ZuulProperties&lt;/code>进行配置.&lt;/p>
&lt;h3 id="具体实现">具体实现:&lt;/h3>
&lt;ol>
&lt;li>使用&lt;code>ZuulController&lt;/code>(&lt;code>ServletWrappingController&lt;/code>的子类)封装&lt;code>ZuulServlet&lt;/code>实例, 处理从&lt;code>DispatcherServlet&lt;/code>进来的请求.&lt;/li>
&lt;li>&lt;code>ZuulHandlerMapping&lt;/code>负责注册handler mapping, 将&lt;code>Route&lt;/code>的&lt;code>fullPath&lt;/code>的请求交由&lt;code>ZuulController&lt;/code>处理.&lt;/li>
&lt;li>同时使用&lt;code>ServletRegistrationBean&lt;/code>注册&lt;code>ZuulServlet&lt;/code>, 默认使用&lt;code>/zuul&lt;/code>作为urlMapping. 所有来自以&lt;code>/zuul&lt;/code>开头的path的请求都会直接进入&lt;code>ZuulServlet&lt;/code>, 不会进入&lt;code>DispatcherServlet&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h4 id="使用注解">使用注解&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>&lt;code>@EnableZuulProxy&lt;/code>引入了&lt;code>ZuulProxyMarkerConfiguration&lt;/code>, &lt;code>ZuulProxyMarkerConfiguration&lt;/code>只做了一件事, 实例化了内部类&lt;code>Marker&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">ZuulProxyMarkerConfiguration&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">Marker&lt;/span> &lt;span class="nf">zuulProxyMarkerBean&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Marker&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="kd">class&lt;/span> &lt;span class="nc">Marker&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;code>@EnableZuulServer&lt;/code>引入了&lt;code>ZuulServerMarkerConfiguration&lt;/code>, &lt;code>ZuulServerMarkerConfiguration&lt;/code>也只做了一件事: 实例化了内部类&lt;code>Marker&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">ZuulServerMarkerConfiguration&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">Marker&lt;/span> &lt;span class="nf">zuulServerMarkerBean&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Marker&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="kd">class&lt;/span> &lt;span class="nc">Marker&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul></description></item><item><title>Spring Cloud - Eureka服务注册</title><link>https://atbug.com/spring-cloud-service-registry-via-eureka/</link><pubDate>Wed, 14 Feb 2018 07:32:43 +0000</pubDate><guid>https://atbug.com/spring-cloud-service-registry-via-eureka/</guid><description>
&lt;p>之前分析过&lt;a href="http://atbug.com/spring-cloud-eureka-client-source-code-analysis/">Spring Cloud的Eureka服务发现&lt;/a>, 今天分析一下服务注册.&lt;/p>
&lt;h2 id="配置">配置&lt;/h2>
&lt;h3 id="bootstrapconfiguration">BootstrapConfiguration&lt;/h3>
&lt;h4 id="eurekadiscoveryclientconfigservicebootstrapconfiguration">EurekaDiscoveryClientConfigServiceBootstrapConfiguration&lt;/h4>
&lt;p>spring-cloud-config环境中使用的配置&lt;/p>
&lt;p>引入&lt;code>EurekaDiscoveryClientConfiguration&lt;/code>和&lt;code>EurekaClientAutoConfiguration&lt;/code>&lt;/p>
&lt;h5 id="eurekadiscoveryclientconfiguration">EurekaDiscoveryClientConfiguration&lt;/h5>
&lt;ol>
&lt;li>在spring-cloud中(通过是否存在RefreshScopeRefreshedEvent.class判断), 添加&lt;code>RefreshScopeRefreshedEvent&lt;/code>的listener. 收到事件后重新注册实例.&lt;/li>
&lt;li>在&lt;code>eureka.client.healthcheck.enabled&lt;/code>设置为true时, 注册&lt;code>EurekaHealthCheckHandler&lt;/code>bean. &lt;code>EurekaHealthCheckHandler&lt;/code>负责将应用状态映射为实例状态&lt;code>InstanceStatus&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h5 id="eurekaclientautoconfiguration">EurekaClientAutoConfiguration&lt;/h5>
&lt;p>支持spring-cloud和非spring-cloud环境, 在spring-cloud环境中, 下面两个bean要使用&lt;code>@RefreshScope&lt;/code>标注&lt;/p>
&lt;ol>
&lt;li>实例化&lt;code>EurekaClient&lt;/code>bean, 在spring-cloud中使用实现类&lt;code>CloudEurekaClient&lt;/code>.&lt;/li>
&lt;li>使用&lt;code>EurekaInstanceConfig&lt;/code>实例, 实例化&lt;code>ApplicationInfoManager&lt;/code>bean&lt;/li>
&lt;/ol></description></item><item><title>初识Netflix Zuul</title><link>https://atbug.com/learn-netflix-zuul/</link><pubDate>Sun, 11 Feb 2018 10:07:18 +0000</pubDate><guid>https://atbug.com/learn-netflix-zuul/</guid><description>
&lt;p>嵌入式的zuul代理&lt;/p>
&lt;p>使用了Netfilx OSS的其他组件:&lt;/p>
&lt;ul>
&lt;li>Hystrix 熔断&lt;/li>
&lt;li>Ribbon 负责发送外出请求的客户端, 提供软件负载均衡功能&lt;/li>
&lt;li>Trubine 实时地聚合细粒度的metrics数据&lt;/li>
&lt;li>Archaius 动态配置&lt;/li>
&lt;/ul>
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>由于2.0停止开发且会有bug, 故下面的分析基于1.x版本.&lt;/p>
&lt;h3 id="特性">特性&lt;/h3>
&lt;ul>
&lt;li>Authentication 认证&lt;/li>
&lt;li>Insights 洞察&lt;/li>
&lt;li>Stress Testing 压力测试&lt;/li>
&lt;li>Canary Testing 金丝雀测试&lt;/li>
&lt;li>Dynamic Routing 动态路由&lt;/li>
&lt;li>Multi-Region Resiliency 多区域弹性&lt;/li>
&lt;li>Load Shedding 负载脱落&lt;/li>
&lt;li>Security 安全&lt;/li>
&lt;li>Static Response handling 静态响应处理&lt;/li>
&lt;li>Multi-Region Resiliency 主动/主动流量管理&lt;/li>
&lt;/ul></description></item><item><title>ConfigurationProperties到底需不需要getter</title><link>https://atbug.com/configurationproperties-requires-getter-or-not/</link><pubDate>Wed, 07 Feb 2018 15:53:21 +0000</pubDate><guid>https://atbug.com/configurationproperties-requires-getter-or-not/</guid><description>
为什么要讨论这个问题, 工作中一个同事写的类使用了ConfigurationProperties, 只提供了标准的setter方法. 属性的访问, 提供了定制的方法. 可以参考EurekaClientConfigBean. 他使用的是spring boot 2.0.0.M5版本, 可以正常获取配置文件中的属性值, 但是在1.5.8.RELEASE获取不到. 看下文档和源码: Annotation for externalized configuration. Add this to a class definition or a @Bean method in a @Configuration class if you want to bind and validate some external Properties (e.g. from a .properties file). 外置配置的注解. 当需要绑定外置配置(如properties或者yaml配置)的时候, 将其加到使用了@Configuration注解的类声明处或者@Bean标注的方法上. 值的绑定是通过Co</description></item><item><title>Go In Action 读书笔记 四</title><link>https://atbug.com/go-in-action-four/</link><pubDate>Mon, 01 Jan 2018 12:30:55 +0000</pubDate><guid>https://atbug.com/go-in-action-four/</guid><description>
&lt;p>
&lt;div class="notices warning image-warning">
&lt;div class="label">Image not found&lt;/div>
&lt;style>
a.warning-link {
color: inherit !important;
font-weight: inherit !important;
text-decoration: underline !important;
border-bottom: none !important; }
&lt;/style>
&lt;p>网站链接: &lt;a href='https://talks.golang.org/2013/go4python/img/fib-go.png' title='在新选项卡中打开此网站链接' target='_blank' class='warning-link'>https://talks.golang.org/2013/go4python/img/fib-go.png&lt;/a>&lt;/p>
&lt;p>链接到文件: /static/https://talks.golang.org/2013/go4python/img/fib-go.png&lt;/p>
&lt;p>使用 &lt;a href='https://gohugo.io/content-management/page-bundles/' title='相关信息 Hugo Page Bundles' target='_blank' class='warning-link'>Page Bundles&lt;/a>: false&lt;/p>
&lt;/div>
&lt;/p>
&lt;h2 id="并发模式">并发模式&lt;/h2>
&lt;h3 id="runner">runner&lt;/h3>
&lt;p>runner展示了如何使用通道来监视程序的执行时间, 如果程序执行时间太长, 也可以用终止程序.
这个程序可用作corn作业执行&lt;/p></description></item><item><title>Go In Action 读书笔记 三</title><link>https://atbug.com/go-in-action-three/</link><pubDate>Mon, 01 Jan 2018 12:30:31 +0000</pubDate><guid>https://atbug.com/go-in-action-three/</guid><description>
&lt;h2 id="并发">并发&lt;/h2>
&lt;p>Go语言里的并发是指让某个函数可以独立于其他函数运行的能力. 当一个函数创建为goroutine时, Go会将其视为一个独立的工作单元. 这个工作单元会被调度到可用的&lt;strong>逻辑处理器&lt;/strong>上执行.&lt;/p>
&lt;p>Go的运行时调度器可以管理所有创建的goroutine, 并为其分配执行时间.
这个调度器在操作系统之上, 将操作系统的线程与逻辑处理器绑定, 并在逻辑处理器执行goroutine. &lt;strong>调度器可以在任何给定的时间, 全面控制哪个goroutine在哪个逻辑处理器上运行&lt;/strong>.&lt;/p>
&lt;p>Go的并发同步模型来自一个叫做通信顺序进程(Communicating Sequential Processes, &lt;a href="http://www.usingcsp.com">CSP&lt;/a>). CSP是一个消息传递模型, 通过在goroutine之前传递数据来传递消息, 不需要通过加锁实现同步访问. 用于在goroutine间传递消息的数据结构叫做通道(channel).&lt;/p>
&lt;h3 id="并发与并行">并发与并行&lt;/h3>
&lt;p>操作系统的线程(thread)和进程(process).&lt;/p>
&lt;p>进程类似应用程序在运行中需要用到和维护的各种资源的容器.
资源包括但不限于: 内存(来自文件系统的代码和数据), 句柄(文件, 设备, 操作系统), 线程.&lt;/p></description></item><item><title>Go In Action 读书笔记 二</title><link>https://atbug.com/go-in-action-two/</link><pubDate>Mon, 01 Jan 2018 12:28:04 +0000</pubDate><guid>https://atbug.com/go-in-action-two/</guid><description>
&lt;h2 id="go语言的类型系统">Go语言的类型系统&lt;/h2>
&lt;p>Go语言是静态类型的变成语言. 编译的时候需要确定类型.&lt;/p>
&lt;h3 id="用户定义的类型">用户定义的类型&lt;/h3>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-go" data-lang="go">&lt;span class="kd">type&lt;/span> &lt;span class="nx">user&lt;/span> &lt;span class="kd">struct&lt;/span> &lt;span class="p">{&lt;/span>
&lt;span class="nx">name&lt;/span> &lt;span class="kt">string&lt;/span>
&lt;span class="nx">email&lt;/span> &lt;span class="kt">string&lt;/span>
&lt;span class="nx">ext&lt;/span> &lt;span class="kt">int&lt;/span>
&lt;span class="nx">privileged&lt;/span> &lt;span class="kt">bool&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>使用&lt;/strong>
零值和&lt;strong>结构字面量&lt;/strong>初始化&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-go" data-lang="go">&lt;span class="c1">//引用类型, 各个字段初始化为对应的零值
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="kd">var&lt;/span> &lt;span class="nx">bill&lt;/span> &lt;span class="nx">user&lt;/span> &lt;span class="err">#&lt;/span>&lt;span class="p">{&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">}&lt;/span>
&lt;span class="c1">//创建并初始化, 使用结构字面量
&lt;/span>&lt;span class="c1">&lt;/span>&lt;span class="nx">lisa&lt;/span> &lt;span class="o">:=&lt;/span> &lt;span class="nx">user&lt;/span>&lt;span class="p">{&lt;/span> &lt;span class="c1">//{Lisa lisa@email.com 123 true}
&lt;/span>&lt;span class="c1">&lt;/span> &lt;span class="nx">name&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s">&amp;#34;Lisa&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nx">email&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s">&amp;#34;lisa@email.com&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nx">ext&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">123&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="nx">privileged&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">true&lt;/span>&lt;span class="p">,&lt;/span>
&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>结构字面量的赋值方式:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>不同行声明每一个字段和对应的值, 字段名和字段以&lt;code>:&lt;/code>分隔, 末尾以&lt;code>,&lt;/code>结尾&lt;/li>
&lt;li>不适用字段名, 只声明对应的值. 写在一行里, 以&lt;code>,&lt;/code>分隔, 结尾不需要&lt;code>,&lt;/code>. &lt;strong>要保证顺序&lt;/strong>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-go" data-lang="go">&lt;span class="nx">lisa&lt;/span> &lt;span class="o">:=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s">&amp;#34;Lisa&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s">&amp;#34;lisa@email.com&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">123&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">true&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Go In Action 读书笔记 一</title><link>https://atbug.com/go-in-action-one/</link><pubDate>Mon, 01 Jan 2018 12:27:10 +0000</pubDate><guid>https://atbug.com/go-in-action-one/</guid><description>
&lt;p>
&lt;div class="notices warning image-warning">
&lt;div class="label">Image not found&lt;/div>
&lt;style>
a.warning-link {
color: inherit !important;
font-weight: inherit !important;
text-decoration: underline !important;
border-bottom: none !important; }
&lt;/style>
&lt;p>网站链接: &lt;a href='http://7xvxng.com1.z0.glb.clouddn.com/15142714785285.jpg' title='在新选项卡中打开此网站链接' target='_blank' class='warning-link'>http://7xvxng.com1.z0.glb.clouddn.com/15142714785285.jpg&lt;/a>&lt;/p>
&lt;p>链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15142714785285.jpg&lt;/p>
&lt;p>使用 &lt;a href='https://gohugo.io/content-management/page-bundles/' title='相关信息 Hugo Page Bundles' target='_blank' class='warning-link'>Page Bundles&lt;/a>: false&lt;/p>
&lt;/div>
&lt;/p>
&lt;h2 id="关键字">关键字&lt;/h2>
&lt;h3 id="var">var&lt;/h3>
&lt;p>变量使用&lt;code>var&lt;/code>声明, 如果变量不是定义在任何一个函数作用域内, 这个变量就是包级变量.&lt;/p>
&lt;blockquote>
&lt;p>Go语言中, 所有变量都被初始化为其&lt;strong>零值&lt;/strong>. 对于数值类型, 其零值是&lt;strong>0&lt;/strong>; 对于字符串类型, 其零值是&lt;strong>空字符串&amp;quot;&amp;quot;&lt;/strong>; 对于布尔类型, 其零值是&lt;strong>false&lt;/strong>. 对于引用类型来说, 底层数据结构会被初始化对应的零值. 但是被生命被起零值的引用类型的变量, 会返回&lt;strong>nil&lt;/strong>作为其值.&lt;/p>
&lt;/blockquote>
&lt;h3 id="const">const&lt;/h3>
&lt;p>定义常量&lt;/p>
&lt;h3 id="interface">interface&lt;/h3>
&lt;p>声明接口&lt;/p>
&lt;h3 id="func">func&lt;/h3>
&lt;p>声明函数&lt;/p>
&lt;h3 id="defer">defer&lt;/h3>
&lt;p>安排后面的函数调用在当前函数返回时才执行.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-go" data-lang="go">&lt;span class="nx">file&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">err&lt;/span> &lt;span class="p">=&lt;/span> &lt;span class="nx">os&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nf">open&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;filePath&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="k">if&lt;/span> &lt;span class="nx">err&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="kc">nil&lt;/span>
&lt;span class="k">return&lt;/span>
&lt;span class="k">defer&lt;/span> &lt;span class="nx">file&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nb">close&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="err">#&lt;/span> &lt;span class="nx">more&lt;/span> &lt;span class="nx">file&lt;/span> &lt;span class="nx">operation&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>自定义GOPATH下安装godep失败</title><link>https://atbug.com/install-godep-issue-in-custom-gopath/</link><pubDate>Fri, 22 Dec 2017 13:02:38 +0000</pubDate><guid>https://atbug.com/install-godep-issue-in-custom-gopath/</guid><description>
我的环境变量是这样的: export GOROOT=/usr/local/go export GOPATH=/Users/addo/Workspaces/go_w export GOBIN=$GOROOT/bin export PATH=$PATH:$GOBIN 使用下面的命令安装报错: go get -v github.com/tools/godep github.com/tools/godep (download) github.com/tools/godep/vendor/github.com/pmezard/go-difflib/difflib github.com/tools/godep/vendor/github.com/kr/fs github.com/tools/godep/vendor/github.com/kr/text github.com/tools/godep/vendor/golang.org/x/tools/go/vcs github.com/tools/godep/vendor/github.com/kr/pretty github.com/tools/godep go install github.com/tools/godep: open /usr/local/go/bin/godep: permission denied 默认是安装到$GOBIN目录下, 权限不够. 使用: sudo go get -v github.com/tools/godep sudo go get -v github.com/tools/godep github.com/tools/godep (download) created GOPATH=/Users/addo/go; see &amp;lsquo;go help gopath&amp;rsquo; github.com/tools/godep/vendor/github.com/kr/fs github.com/tools/godep/vendor/github.com/kr/text github.com/tools/godep/vendor/github.com/pmezard/go-difflib/difflib github.com/tools/godep/vendor/golang.org/x/tools/go/vcs github.com/tools/godep/vendor/github.com/kr/pretty github.com/tools/godep $GOBIN并没有找到godef. 输出提示created GOPATH=/Users/addo/go; . 因为sudo的时候找不到GOPATH变量, 便重新创建了目录. 解决方案一: 临时修改GOBIN: export GOBIN=$GOPATH/bin 运行go get github.com/tools/godep 将生成的godef复制到GOROOT/bin下 回滚修改export GOBIN=$GOROOT/bin; export PATH=$PATH:$GOBIN 解决方案二: 修改GOROOT/bin的属组属主, 安全性问题, 不推荐.</description></item><item><title>SpringBoot源码 - 启动</title><link>https://atbug.com/glance-over-spring-boot-source/</link><pubDate>Fri, 08 Dec 2017 17:48:43 +0000</pubDate><guid>https://atbug.com/glance-over-spring-boot-source/</guid><description>
SpringBoot Application启动部分的源码阅读. SpringApplication 常用的SpringApplication.run(Class, Args)启动Spring应用, 创建或者更新ApplicationContext 静态方法run 使用source类实例化一个SpringApplication实例, 并调用实例方法run. public static ConfigurableApplicationContext run(Object[] sources, String[] args) { return new SpringApplication(sources).run(args); } 初始化initialize 实例化的时候首先通过尝试加载javax.servlet.Servlet和org.springframework.web.context.ConfigurableWebApplicationContext推断当前是否是web环境. 然后从spring.facto</description></item><item><title>Java序列化工具性能对比</title><link>https://atbug.com/java-serval-serializer-benchmark/</link><pubDate>Sat, 02 Dec 2017 07:35:43 +0000</pubDate><guid>https://atbug.com/java-serval-serializer-benchmark/</guid><description>
最近在调整系统的性能, 系统中正使用Jackson作为序列化工具. 做了下与fastJson, Avro, ProtoStuff的序列化吞吐对比. 由于只是做横向对比, 没有优化系统或者JVM任何参数. 服务器一般都用Linux, 在Docker里做了Linux系统的测试. Mac: Benchmark Mode Cnt Score Error Units JMHTest.avroSerializer thrpt 2 3124799.325 ops/s JMHTest.fastJsonSerializer thrpt 2 3122720.917 ops/s JMHTest.jacksonSerializer thrpt 2 2373347.208 ops/s JMHTest.protostuffSerializer thrpt 2 4196009.673 ops/s Docker: Benchmark Mode Cnt Score Error Units JMHTest.avroSerializer thrpt 2 3293260.676 ops/s JMHTest.fastJsonSerializer thrpt 2 2996908.084 ops/s JMHTest.jacksonSerializer thrpt 2 2189518.443 ops/s JMHTest.protostuffSerializer thrpt 2 3998265.173 ops/s</description></item><item><title>Kafka的消息可靠传递</title><link>https://atbug.com/kafka-reliable-data-delivery/</link><pubDate>Sat, 18 Nov 2017 14:01:46 +0000</pubDate><guid>https://atbug.com/kafka-reliable-data-delivery/</guid><description>
Kafka提供的基础保障可以用来构建可靠的系统, 却无法保证完全可靠. 需要在可靠性和吞吐之间做取舍. Kafka在分区上提供了消息的顺序保证. 生产的消息在写入到所有的同步分区上后被认为是已提交 (不需要刷到硬盘). 生产者可以选择在消息提交完成后接收broker的确认, 是写入leader之后, 或者所有的副本 只要有一个副本存在, 提交的消息就不会丢失 消费者只能读取到已提交的消息 复制 Kafka的复制机制保证每个分区有多个副本, 每个副本可以作为leader或者follower的角色存在. 为了保证副本的同步, 需要做到: 保持到zk的连接会话: 每隔6s向zk发送心跳, 时间可配置 每隔10s向leader拉取消息, 时间</description></item><item><title>Spring Cloud - Eureka Client源码分析</title><link>https://atbug.com/spring-cloud-eureka-client-source-code-analysis/</link><pubDate>Sat, 14 Oct 2017 22:04:59 +0000</pubDate><guid>https://atbug.com/spring-cloud-eureka-client-source-code-analysis/</guid><description>
准备做个Spring Cloud源码分析系列, 作为Spring Cloud的源码分析笔记. 这一篇是Eureka的客户端. 客户端 两种方式, 最终的实现基本一样. 显示指定服务发现的实现类型 使用@EnableEurekaClient注解显示的指定使用Eureka作为服务发现的实现, 并实例化EurekaClient实例. 实际上使用的是@EnableDiscoveryClient注解. @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @EnableDiscoveryClient public @interface EnableEurekaClient { } 动态配置实现 使用@EnableDiscoveryClient注解来配置服务发现的实现. 源码分析 EnableDiscoveryClient @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Import(EnableDiscoveryClientImportSelector.class) public @interface EnableDiscoveryClient { } EnableDiscoveryClient注解的作用主要是用来引入EnableDiscove</description></item><item><title>Raft算法学习</title><link>https://atbug.com/learning-raft/</link><pubDate>Sat, 14 Oct 2017 05:57:34 +0000</pubDate><guid>https://atbug.com/learning-raft/</guid><description>
Raft 强一致性算法 名词 复制状态机 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg 链接到文件: /static/http://wx4.sinaimg.cn/mw690/4858d6a8ly1fbxcex0w1fj20gt08vwfr.jpg 使用 Page Bundles: false 复制状态机是通过复制日志来实现的, 按照日志中的命令的顺序来执行这些命令. 相同的状态机执行相同的日志命令, 获得相同的执行结果. 任期号 (currentTerm) 每个成员都会保存一个任期号, 称为服务器最后知道的任期号. 投票的候选人id (votedFor) 当前任期内, 投票的候选人id, 即响应投票请求(见下文)返回true时的候选人id. 已被提交的最大日志条目的索引值 (commitIndex) 每个成员都会持有已被提交的最大日志条目的索引值 被状态机执行的最⼤日志条⽬的索引值 (lastApplied) 每个成员都会持有被状态机执行的最⼤日志条⽬的索引值 请求 日志复制请求 (AppendEntries RPC) 由领导人发送给其他服务器, 也</description></item><item><title>Kafka发送不同确认方式的性能差异</title><link>https://atbug.com/kafka-producer-acknowledge-benchmark/</link><pubDate>Tue, 10 Oct 2017 11:49:58 +0000</pubDate><guid>https://atbug.com/kafka-producer-acknowledge-benchmark/</guid><description>
背景 Kafka的性能众所周知，Producer支持acknowledge模式。即Kafka会想Producer返回消息发送的结果。但是在Java Client中，acknowledge的确认有两种：同步和异步。 同步是通过调用future.get()实现的；异步则是通过提供callback方法来实现。写了个简单的程序测试一下单线程中吞吐差异能有多大。注意这里只考虑横向对比。 发送端单线程 Kafka为单集群节点 topic的分区数为1 key长度1 payload长度100 测试工具 JMeter Kafka Meter future.get() + batch size =1 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15076056852541.jpg 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15076056852541.jpg 使用 Page Bundles: false future.get() + batch size = 16K Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important;</description></item><item><title>Kafka消息消费一致性</title><link>https://atbug.com/kafka-consumer-consistency/</link><pubDate>Tue, 26 Sep 2017 19:13:48 +0000</pubDate><guid>https://atbug.com/kafka-consumer-consistency/</guid><description>
Kafka消费端的offset主要由consumer来控制, Kafka降每个consumer所监听的tocpic的partition的offset保存在__consumer_offsets主题中. consumer需要将处理完成的消息的offset提交到服务端, 主要有ConsumerCoordinator完成的. 每次从kafka拉取数据之前, 假如是异步提交offset, 会先调用已经完成的offset commit的callBack, 然后检查ConsumerCoordinator的连接状态. 如果设置了自动提交offset, 会继续上次从服务端获取的数据的offset异步提交到服务端. 这里需要注意的是会</description></item><item><title>Kafka 恰好一次发送和事务消费示例</title><link>https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging-example/</link><pubDate>Fri, 22 Sep 2017 18:03:43 +0000</pubDate><guid>https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging-example/</guid><description>
核心思想 生产端一致性: 开启幂等和事务, 包含重试, 发送确认, 同一个连接的最大未确认请求数. 消费端一致性: 通过设置读已提交的数据和同时处理完成每一条消息之后手动提交offset. 生产端 public class ProducerTest { public static void main(String[] args) throws InterruptedException, ExecutionException { Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;#34;192.168.31.186:9092&amp;#34;); props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, &amp;#34;my-transactional-id&amp;#34;); props.put(ProducerConfig.ACKS_CONFIG, &amp;#34;all&amp;#34;); props.put(ProducerConfig.RETRIES_CONFIG, &amp;#34;3&amp;#34;); props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, &amp;#34;1&amp;#34;); Producer&amp;lt;String, String&amp;gt; producer = new KafkaProducer&amp;lt;&amp;gt;(props, new StringSerializer(), new StringSerializer()); producer.initTransactions(); try { producer.beginTransaction(); for (int i = 0; i &amp;lt; 5; i++) { Future&amp;lt;RecordMetadata&amp;gt; send = producer .send(new ProducerRecord&amp;lt;&amp;gt;(&amp;#34;my-topic&amp;#34;, Integer.toString(i), Integer.toString(i))); System.out.println(send.get().offset()); TimeUnit.MILLISECONDS.sleep(1000L); } producer.commitTransaction(); } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) { // We can&amp;#39;t recover from these exceptions, so our only option is to close the producer and exit. producer.close(); } catch (KafkaException e) { // For all other exceptions, just abort the transaction and try again. producer.abortTransaction(); } producer.close(); } } 消费端 public class ConsumerTest { public static void main(String[] args) throws InterruptedException { Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &amp;#34;192.168.31.186:9092&amp;#34;); props.put(ConsumerConfig.GROUP_ID_CONFIG, &amp;#34;test&amp;#34;); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OffsetResetStrategy.NONE.toString().toLowerCase(Locale.ROOT)); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &amp;#34;false&amp;#34;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &amp;#34;org.apache.kafka.common.serialization.StringDeserializer&amp;#34;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, &amp;#34;org.apache.kafka.common.serialization.StringDeserializer&amp;#34;); props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT)); KafkaConsumer&amp;lt;String, String&amp;gt; consumer = new KafkaConsumer&amp;lt;&amp;gt;(props); consumer.subscribe(Arrays.asList(&amp;#34;my-topic&amp;#34;)); while (true) { ConsumerRecords&amp;lt;String, String&amp;gt; records = consumer.poll(100); if (!records.isEmpty()) { for (ConsumerRecord&amp;lt;String, String&amp;gt; record : records) { System.out.printf(&amp;#34;offset = %d, key = %s, value = %s%n&amp;#34;, record.offset(), record.key(), record.value()); //Manually commit each record consumer.commitSync(Collections.singletonMap(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1))); } } } } }</description></item><item><title>恰好一次发送和事务消息(译)</title><link>https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging/</link><pubDate>Tue, 19 Sep 2017 19:13:26 +0000</pubDate><guid>https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging/</guid><description>
Kafka提供“至少一次”交付语义, 这意味着发送的消息可以传送一次或多次. 人们真正想要的是“一次”语义,因为重复的消息没有被传递。 普遍地发声重复消息的情况有两种: 如果客户端尝试向集群发送消息并获取网络错误, 则重试可能会导致重复. 如果在发送消息之前发生网络错误, 则不会发生重复. 但是, 如果在将消息附加到日志之后发生网络错误, 但在将响应发送给发件人之前, 发件人将不知道发生了什么. 唯一的选择是重试和冒险重复或放弃并声明消息丢失。 如果客户端尝试向集群发送消息并获取网络错误, 则重试可能会导致重复. 如果在发送消息之前发生网络错误, 则不会发生重复. 但是, 如果在将消息附加到日志之后发生网络错误, 但在将响应发送给</description></item><item><title>Kafka Producer配置解读</title><link>https://atbug.com/kafka-producer-config/</link><pubDate>Tue, 19 Sep 2017 15:38:03 +0000</pubDate><guid>https://atbug.com/kafka-producer-config/</guid><description>
按照重要性分类, 基于版本0.11.0.0 高 bootstrap.servers 一组host和port用于初始化连接. 不管这里配置了多少台server, 都只是用作发现整个集群全部server信息. 这个配置不需要包含集群所有的机器信息. 但是最好多于一个, 以防服务器挂掉. key.serializer 用来序列化key的Serializer接口的实现类. value.serializer 用来序列化value的Serializer接口的实现类 acks producer希望leader返回的用于确认请求完成的确认数量. 可选值 all, -1, 0 1. 默认值为1 acks=0 不需要等待服务器的确认. 这是retries设置无效. 响应里来自服务端的offset总是-1. producer只管发不管发送成功与否。延迟低，容易丢失数据。 acks=1 表示le</description></item><item><title>JSON Patch</title><link>https://atbug.com/json-patch/</link><pubDate>Sun, 27 Aug 2017 14:41:44 +0000</pubDate><guid>https://atbug.com/json-patch/</guid><description>
JSON Path是在使用Kubernetes API的过程中首次使用的. 使用API做扩缩容的时候, 发送整个Deployment的全文不是个明智的做法, 虽然可行. 因此便使用了JSON Patch. JsonObject item = new JsonObject(); item.add(&amp;#34;op&amp;#34;, new JsonPrimitive(&amp;#34;replace&amp;#34;)); item.add(&amp;#34;path&amp;#34;, new JsonPrimitive(&amp;#34;/spec/replicas&amp;#34;)); item.add(&amp;#34;value&amp;#34;, new JsonPrimitive(instances)); JsonArray body = new JsonArray(); body.add(item); appsV1beta1Api.patchNamespacedScaleScale(id, namespace, body, null); fabric8s提供的kubernetes-client中使用的zjsonpatch则封装了JSON Patch操作. 例如在做扩缩容的时候或者当前的deployment, 修改replicas的值. 然后比较对象的不同(JsonDiff.asJson(sourceJsonNode, targetJsonNode)). 下面的内容部分翻译自JSON PATH, 有兴趣的可以跳转看原文. 什么是JSON Patch JSON Path是一直描述JSON文</description></item><item><title>如何在Openshift中使用hostPath</title><link>https://atbug.com/how-to-use-hostpath-in-openshift/</link><pubDate>Wed, 23 Aug 2017 19:29:51 +0000</pubDate><guid>https://atbug.com/how-to-use-hostpath-in-openshift/</guid><description>
使用openshift搭建的k8s的api创建Deployment，在启动的时候报下面的错误： Invalid value: &amp;ldquo;hostPath&amp;rdquo;: hostPath volumes are not allowed to be used] 解决方案： 一个方案是将user加入privileged scc中，另一个方案就是： oc edit scc restricted #添加下面这行 allowHostDirVolumePlugin: true</description></item><item><title>Kubernetes — 持久卷</title><link>https://atbug.com/kubernetes-persistent-volumes/</link><pubDate>Sun, 20 Aug 2017 22:25:40 +0000</pubDate><guid>https://atbug.com/kubernetes-persistent-volumes/</guid><description>
Persistent Volume 译自Persistent Volumes 介绍 管理存储是管理计算的独特问题。 PersistentVolume子系统为用户和管理员提供了一个API，其中提供了如何从如何使用存储提供存储的详细信息。为此，我们介绍两种新的API资源：PersistentVolume和PersistentVolumeClaim。 PersistentVolume（PV）是由管理员配置的集群中的一段存储。它是集群中的一种资源就像一个节点是一个集群的资源。 PV是类似Volumes的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。 Persistent</description></item><item><title>Kubernetes学习 — Macos安装Kubernetes</title><link>https://atbug.com/install-kubernetes-on-macos/</link><pubDate>Thu, 17 Aug 2017 09:44:17 +0000</pubDate><guid>https://atbug.com/install-kubernetes-on-macos/</guid><description>
Kubernetes 安装 macos 检查环境 sysctl -a | grep machdep.cpu.features | grep VMX 安装VirtualBox http://download.virtualbox.org/virtualbox/5.1.26/Oracle_VM_VirtualBox_Extension_Pack-5.1.26-117224.vbox-extpack 安装minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.21.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ 创建集群 默认使用virtualbox。 主机的ip是192.168.31.186， 1087是proxy的端口。需要将ss的http代理监听地址从127.0.0.1改为主机的ip。 #启动 minikube start #使用私有库 minikube start --insecure-registry=&amp;#34;192.168.31.34&amp;#34; #使用proxy，用于获取镜像 minikube start --docker-env HTTP_PROXY=&amp;#34;192.168.31.186:1087&amp;#34; --docker-env HTTPS_PROXY=&amp;#34;192.168.31.186:1087&amp;#34; --docker-env NO_PROXY=192.168.99.0/24 安装kubectl curl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.7.3/bin/darwin/amd64/kubectl &amp;amp;&amp;amp; chmod +x kubectl &amp;amp;&amp;amp; sudo mv kubectl /usr/local/bin/ oh-my-zsh tab completion vi ~/.zshrc 添加到plugin部分 plugins=(git zsh-completions kubectl) 使用 minikube 检查版本 minikube version #minikube version: v0.21.0 kubectl version #Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;3&amp;#34;, GitVersion:&amp;#34;v1.3.0&amp;#34;, GitCommit:&amp;#34;283137936a498aed572ee22af6774b6fb6e9fd94&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2016-07-01T19:26:38Z&amp;#34;, GoVersion:&amp;#34;go1.6.2&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;} #Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;7&amp;#34;, GitVersion:&amp;#34;v1.7.0&amp;#34;, GitCommit:&amp;#34;d3ada0119e776222f11ec7945e6d860061339aad&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2017-07-26T00:12:31Z&amp;#34;, GoVersion:&amp;#34;go1.8.3&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} 获取集群地址 minikube ip 192.168.99.100 获取服务列表 minikube service list 打开dashboard minikube dashboard kubectl 部署Dashboard UI 默认</description></item><item><title>暴力停止ExecutorService的线程</title><link>https://atbug.com/stop-a-thread-of-executor-service/</link><pubDate>Wed, 19 Jul 2017 22:25:19 +0000</pubDate><guid>https://atbug.com/stop-a-thread-of-executor-service/</guid><description>
停止，stop，这里说的是真的停止。如何优雅的结束，这里就不提了。 这里要用Thread.stop()。众所周知，stop()方法在JDK中是废弃的。 该方法天生是不安全的。使用thread.stop()停止一个线程，导致释放（解锁）所有该线程已经锁定的监视器（因沿堆栈向上传播的未检查异常ThreadDeath而解锁）。如果之前受这些监视器保护的任何对象处于不一致状态，则不一致状态的对象（受损对象）将对其他线程可见，这可能导致任意的行为。 有时候我们会有这种需求，不需要考虑线程执行到哪一步。一般这种情况是外部执行stop，比如执行业务的线程因为各种原因假死或者耗时较长，由于设计问题又无法响应优雅的停</description></item><item><title>私有构造函数捕获模式</title><link>https://atbug.com/private-constructor-capture-idiom/</link><pubDate>Wed, 24 May 2017 06:50:44 +0000</pubDate><guid>https://atbug.com/private-constructor-capture-idiom/</guid><description>
《Java并发编程实践》的注解中有提到这一概念。 The private constructor exists to avoid the race condition that would occur if the copy constructor were implemented as this (p.x, p.y); this is an example of the private constructor capture idiom (Bloch and Gafter, 2005). 结合原文代码： @ThreadSafe public class SafePoint{ @GuardedBy(&amp;#34;this&amp;#34;) private int x,y; private SafePoint (int [] a) { this (a[0], a[1]); } public SafePoint(SafePoint p) { this (p.get()); } public SafePoint(int x, int y){ this.x = x; this.y = y; } public synchronized int[] get(){ return new int[] {x,y}; } public synchronized void set(int x, int y){ this.x = x; this.y = y; } } 这里的构造器public SafePoint(SafePoint p) { this (p.get()); }是为了捕获另一个实例的状态。get()方法是一个同步方法，为了避免竞态没有分别提供x、y的公有getter方法。 为了保证SafePoint的多线程安全性，在使用另一个实例构造新的实例时，使用了一个私有的构造器。 首先为什么不用下面这种，还是为了避免竞态（p.x和p.y调用不是原子操作）。 public SafePoint(SafePoint p) { this(p.x, p.y) } 同理，这</description></item><item><title>Docker快速构建Cassandra和Java操作</title><link>https://atbug.com/java-operate-cassandra-deployed-in-docker/</link><pubDate>Thu, 18 May 2017 23:33:24 +0000</pubDate><guid>https://atbug.com/java-operate-cassandra-deployed-in-docker/</guid><description>
搭建Cassandra 使用docker创建Cassandra，方便快捷 docker pull cassandra:latest docker run -d --name cassandra -p 9042:9042 cassandra docker exec -it cassandra bash 创建keyspace、table #cqlsh&amp;gt;#createkeyspaceCREATEKEYSPACEcontactsWITHREPLICATION={&amp;#39;class&amp;#39;:&amp;#39;SimpleStrategy&amp;#39;,&amp;#39;replication_factor&amp;#39;:1};#useUSEcontacts;#createtableCREATETABLEcontact(idUUID,emailTEXTPRIMARYKEY);查看表数据 cqlsh:contacts&amp;gt; SELECT * FROM contact; email | id -------+---- (0 rows) Java客</description></item><item><title>从零开始用docker运行spring boot应用</title><link>https://atbug.com/run-spring-boot-app-in-docker/</link><pubDate>Thu, 20 Apr 2017 21:58:42 +0000</pubDate><guid>https://atbug.com/run-spring-boot-app-in-docker/</guid><description>
假设已经安装好Docker Springboot应用 pom添加依赖和构建插件 &amp;lt;parent&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-parent&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.5.3.RELEASE&amp;lt;/version&amp;gt; &amp;lt;/parent&amp;gt; &amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; &amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-maven-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt; &amp;lt;/build&amp;gt; 应用代码 package com.atbug.spring.boot.test; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.context.annotation.Bean; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; /** * Created by addo on 2017/5/15. */ @SpringBootApplication @RestController public class Application { @RequestMapping(&amp;#34;/&amp;#34;) public String home(){ return &amp;#34;Hello world!&amp;#34;; } public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 应用构建 mvn clean package Centos 7 with Java8 获取Centos7 镜像 docker pull centos:7 准备centos-java8的dockerfile FROM centos:7 MAINTAINER Addo Zhang &amp;#34;duwasai@gmail.com&amp;#34; # Set correct environment variables. ENV HOME /root ENV LANG en_US.UTF-8 ENV LC_ALL en_US.UTF-8 RUN yum install -y curl; yum upgrade -y; yum update -y; yum clean all ENV JDK_VERSION 8u11 ENV JDK_BUILD_VERSION b12 RUN curl -LO &amp;#34;http://download.oracle.com/otn-pub/java/jdk/$JDK_VERSION-$JDK_BUILD_VERSION/jdk-$JDK_VERSION-linux-x64.rpm&amp;#34; -H &amp;#39;Cookie: oraclelicense=accept-securebackup-cookie&amp;#39; &amp;amp;&amp;amp; rpm -i jdk-$JDK_VERSION-linux-x64.rpm; rm -f jdk-$JDK_VERSION-linux-x64.rpm; yum clean all ENV JAVA_HOME /usr/java/default RUN yum remove curl; yum clean all 创建centos-java8镜像 docker build -t addo/centos-java8 . Docker中运行应用 准备应用镜像的dockerfile FROM addo/centos-java8 ADD target/boot-test-1.0-SNAPSHOT.jar /opt/app.jar EXPOSE 8080 CMD java -jar /opt/app.jar 构建应用镜像 docker build -t temp/spring-boot-app . 运行 docker run --name spring-boot-app</description></item><item><title>Jasig CAS Web and Proxy flow</title><link>https://atbug.com/jasig-cas-web-and-proxy-flow/</link><pubDate>Tue, 18 Apr 2017 10:36:16 +0000</pubDate><guid>https://atbug.com/jasig-cas-web-and-proxy-flow/</guid><description>
最近因为需求在看CAS相关的只是，由于需要后端调用，用到proxy（代理）模式。整理了下web flow和proxy web flow的流程。 Web Flow Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/CAS-Service-Ticket.jpg 链接到文件: /static//media/CAS-Service-Ticket.jpg 使用 Page Bundles: false Proxy Web Flow Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/CAS-Proxy-Ticket.jpg 链接到文件: /static//media/CAS-Proxy-Ticket.jpg 使用 Page Bundles: false</description></item><item><title>MetaspaceSize的坑</title><link>https://atbug.com/java8-metaspace-size-issue/</link><pubDate>Thu, 13 Apr 2017 11:55:14 +0000</pubDate><guid>https://atbug.com/java8-metaspace-size-issue/</guid><description>
这几天生产上有台机器的Metaspace一直在告警，Metaspace使用达到了97%。使用-XX:MetaspaceSize=512m，告警也还在在持续，查看MC只有81536.0，显然这个参数没起作用。 也有人遇到类似的问题，并在openjdk上提过类似的bug，其实是一个注释的bug，最终在JDK-8151845中修复了。 Class metadata is deallocated when the corresponding Java class is unloaded. Java classes are unloaded as a result of garbage collection, and garbage collections may be induced in order to unload classes and deallocate class metadata. When the space committed for class metadata reaches a certain level (a high-water mark), a garbage collection is induced. After the garbage collection, the high-water mark may be raised or lowered depending on the amount of space freed from class metadata. The high-water mark would be raised so as not to induce another garbage collection too soon. The high-water mark is initially set to the value of the command-line option MetaspaceSize. It is raised or lowered based on the options MaxMetaspaceFreeRatio and MinMetaspaceFreeRatio. If the committed space available for class metadata as a percentage of the total committed space for</description></item><item><title>一个Tomcat类加载问题</title><link>https://atbug.com/one-tomcat-class-load-issue/</link><pubDate>Wed, 12 Apr 2017 10:40:01 +0000</pubDate><guid>https://atbug.com/one-tomcat-class-load-issue/</guid><description>
背景 一个Tomcat实例中运行了三个应用，其中一个对接了Apereo的CAS系统。现在要求另外两个系统也对接CAS系统，问题就出现了： 应用启动后打开其中两个应用的任何一个，登录完成后系统都没有问题。唯独首选打开第三个，其他两个报错ClassNotFoundException: org.apache.xerces.parsers.SAXParser。 发现这个类来自xerces:xercesImpl:jar:2.6.2，使用mvn dependency:tree发现是被xom:xom:1.1简洁引用。 分析 CAS client jar中使用XMLReaderFactory创建XMLReader，首次创建会从classpa</description></item><item><title>Scala笔记：用函数字面量块调用高阶函数</title><link>https://atbug.com/call-high-order-function-in-function-literal/</link><pubDate>Tue, 11 Apr 2017 10:15:15 +0000</pubDate><guid>https://atbug.com/call-high-order-function-in-function-literal/</guid><description>
这里会用到几个概念高阶函数、函数字面量、参数组 高阶函数 high-order function 函数的一种，简单来说它包含了一个函数类型的参数或者返回值。 所谓的高阶是跟一阶函数相比，深入一下： 一个或多个参数是函数，并返回一个值。 返回一个函数，但没有参数是函数。 上述两者叠加：一个或多个参数是函数，并返回一个函数。 示例： def stringSafeOp(s: String, f: String =&amp;gt; String) = { if ( s != null) f(s) else s } //stringSafeOp: (s: String, f: String =&amp;gt; String)String def reverse(s: String) = s.reverse //reverse: (s: String)String stringSafeOp(&amp;#34;Ready&amp;#34;, reverse) //res86: String = ydaeR 函数字面量 function literal，其他名字：匿名函数、Lambda表达式等。 函数字面量可以存储在函数值和变量中，或者也可以定义为高阶函数调用的一部分。在任何接受函数类型的地方都可以使用函数字面量。 reverse的函数字面量定义： val reverse = (s:String) =&amp;gt; s.reverse (s:String) =&amp;gt; s.</description></item><item><title>GreenPlum JDBC和C3P0数据源</title><link>https://atbug.com/greenplum-jdbc-and-c3p0-datasource/</link><pubDate>Mon, 10 Apr 2017 08:29:00 +0000</pubDate><guid>https://atbug.com/greenplum-jdbc-and-c3p0-datasource/</guid><description>
在网上搜索GreenPlum（GPDB）的数据源配置的时候，发现搜索结果都是用postgresql的配置。 import com.mchange.v2.c3p0.DataSources; import javax.sql.DataSource; import java.sql.*; import java.util.Properties; /** * Created by addo on 2017/4/10. */ public class JDBCTest { private static String POSTGRESQL_URL = &amp;#34;jdbc:postgresql://192.168.56.101:5432/example&amp;#34;; private static String POSTGRESQL_USERNAME = &amp;#34;dbuser&amp;#34;; private static String POSTGRESQL_PASSWORD = &amp;#34;password&amp;#34;; private static String GPDB_URL = &amp;#34;jdbc:pivotal:greenplum://192.168.56.101:5432;DatabaseName=test&amp;#34;; private static String GPDB_USERNAME = &amp;#34;dbuser&amp;#34;; private static String GPDB_PASSWORD = &amp;#34;password&amp;#34;; /** * Postgresql Connection * * @return * @throws ClassNotFoundException * @throws SQLException */ public static Connection postgresqlConnection() throws ClassNotFoundException, SQLException { Class.forName(&amp;#34;org.postgresql.Driver&amp;#34;); return DriverManager.getConnection(POSTGRESQL_URL, POSTGRESQL_USERNAME, POSTGRESQL_PASSWORD); } /** * GreenPlum Connection * * @return * @throws ClassNotFoundException * @throws SQLException */ public static Connection gpdbConnection() throws ClassNotFoundException, SQLException { Class.forName(&amp;#34;com.pivotal.jdbc.GreenplumDriver&amp;#34;); return DriverManager.getConnection(GPDB_URL, GPDB_USERNAME, GPDB_PASSWORD); } /** * GreenPlud C3P0 Datasource Connection * * @return * @throws SQLException */ public static Connection gpdbC3P0Connection() throws SQLException { Properties c3p0Props = new Properties(); c3p0Props.setProperty(&amp;#34;driverClass&amp;#34;, &amp;#34;com.pivotal.jdbc.GreenplumDriver&amp;#34;); c3p0Props.setProperty(&amp;#34;jdbcUrl&amp;#34;, GPDB_URL); c3p0Props.setProperty(&amp;#34;user&amp;#34;, GPDB_USERNAME); c3p0Props.setProperty(&amp;#34;password&amp;#34;, GPDB_PASSWORD); c3p0Props.setProperty(&amp;#34;acquireIncrement&amp;#34;, &amp;#34;5&amp;#34;); c3p0Props.setProperty(&amp;#34;initialPoolSize1&amp;#34;, &amp;#34;1&amp;#34;); c3p0Props.setProperty(&amp;#34;maxIdleTime&amp;#34;, &amp;#34;60&amp;#34;); c3p0Props.setProperty(&amp;#34;maxPoolSize&amp;#34;, &amp;#34;50&amp;#34;); c3p0Props.setProperty(&amp;#34;minPoolSize&amp;#34;, &amp;#34;1&amp;#34;); c3p0Props.setProperty(&amp;#34;idleConnectionTestPeriod&amp;#34;, &amp;#34;60&amp;#34;); return DataSources.unpooledDataSource(GPDB_URL, c3p0Props).getConnection(); } public static void main(String[] args) throws ClassNotFoundException, SQLException { Connection[] connections = new Connection[]{postgresqlConnection(), gpdbConnection(), gpdbC3P0Connection()}; for (Connection connection : connections) { CallableStatement callableStatement = connection.prepareCall(&amp;#34;select * from user&amp;#34;); boolean execute = callableStatement.execute(); ResultSet resultSet = callableStatement.getResultSet(); while (resultSet.next()) { System.out.println(resultSet.getString(&amp;#34;current_user&amp;#34;)); } callableStatement.close(); connection.close(); } } } 源代码</description></item><item><title>Scala笔记：def VS val</title><link>https://atbug.com/def-vs-val-in-scala/</link><pubDate>Sun, 09 Apr 2017 08:24:40 +0000</pubDate><guid>https://atbug.com/def-vs-val-in-scala/</guid><description>
先说原理： val修饰的在定义的时候执行 def修饰的在调用的时候执行 直观的例子： //注释的行为REPL输出 def test: () =&amp;gt; Int = { println(&amp;#34;def called&amp;#34;) val r = util.Random.nextInt () =&amp;gt; r } //test: () =&amp;gt; Int test() //def called //res82: Int = -950077410 test() //def called //res83: Int = 1027028032 val test: () =&amp;gt; Int = { println(&amp;#34;def called&amp;#34;) val r = util.Random.nextInt () =&amp;gt; r } //def called //test: () =&amp;gt; Int = $$Lambda$1382/338526071@42f2515d test() //res84: Int = 300588352 test() //res84: Int = 300588352 def在方法定义的时候除了新的方法没有任何输出；之后每次调用的时候都会执行一次，而且是每次调用都获得一个新的方法（random值不同） val在方法定义的时候除了新的方法，还会执行并获得一个方法；之后每次调用都只是执行了定义的时候获得的方法（() =&amp;gt; r，r值固定） 进阶 def timer[A](f: =&amp;gt; A) = { def now = System.currentTimeMillis val start = now; val a = f; val end = now println(s&amp;#34;Executed int ${end - start}ms&amp;#34;) a } val veryRandomAmount = timer { util.Random.setSeed(System.currentTimeMillis) for (i &amp;lt;- 1 to 100000) util.Random.nextDouble util.Random.nextDouble } 看过</description></item><item><title>Centos编译安装Redis</title><link>https://atbug.com/install-redis-on-centos/</link><pubDate>Fri, 07 Apr 2017 16:48:46 +0000</pubDate><guid>https://atbug.com/install-redis-on-centos/</guid><description>
版本 Centos7 Redis3.2.8 编译安装 wget http://download.redis.io/releases/redis-3.2.8.tar.gz tar -zxvf redis-3.2.8.tar.gz cd redis-3.2.8 sudo make test sudo make install 启动 redis-server 问题 /bin/sh: cc: command not found **原因：**Centos安装时选择的类型是Infrastructure，没有c++的编译工具。 解决：sudo yum -y install gcc gcc-c++ libstdc++-devel malloc.h:50:31: fatal error: jemalloc/jemalloc.h: No such file or directory **原因：**Redis使用的默认的memory allocator是libc，而Linux系统中默认的是jemalloc，需要制动MALLOC变量。 解决：sudo make MALLOC=libc install</description></item><item><title>Centos上安装Postgresql</title><link>https://atbug.com/install-postgresql-on-centos/</link><pubDate>Thu, 06 Apr 2017 22:54:17 +0000</pubDate><guid>https://atbug.com/install-postgresql-on-centos/</guid><description>
版本 Centos7 Postgresql9.2 Enable ssh service sshd start Open firewall for 22 firewall-cmd —state firewall-cmd —list-all firewall-cmd —permanent —zone=public —add-port=22/tcp firewall-cmd —reload Install Postgresql yum install postgres su postgres postgres —version 默认会创建postgres:postgres用户和组 切换用户 su - postgres 初始化数据库 通过指定数据文件目录初始化db initdb -D /var/lib/pgsql/data 修改端口防火墙 默认端口是5432，需要在防火墙中打开端口 firewall-cmd &amp;ndash;permanent &amp;ndash;zone=public &amp;ndash;add-port=5432/tcp 修改监听的ip 需要外部访问的话，需要修改postgresql.conf中的监听ip，&amp;lsquo;0.0.0.0&amp;rsquo;允许所有ipv4的ip访问，''::&amp;lsquo;&amp;lsquo;允许所有ipv6的ip访问 listen_addresses = &amp;ldquo;0.0.0.0&amp;rdquo; 修改需要重启p</description></item><item><title>Key长度对Redis性能影响</title><link>https://atbug.com/redis-performance-key-length/</link><pubDate>Thu, 16 Mar 2017 10:37:03 +0000</pubDate><guid>https://atbug.com/redis-performance-key-length/</guid><description>
最近Redis的使用中用的到key可能比较长，但是Redis的官方文档没提到key长度对性能的影响，故简单做了个测试。 环境 Redis和测试程序都是运行在本地，不看单次的性能，只看不同的长度堆读写性能的影响。 测试方法 使用长度分别为10, 100, 500, 1000, 2500, 5000, 7500, 10,000, and 20,000的key，value长度1000，读写1000次。 结果 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/14896309668401.jpg 链接到文件: /static//media/14896309668401.jpg 使用 Page Bundles: false Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/14896309585857.jpg 链接到文件: /static//media/14896309585857.jpg 使用 Page Bundles: false 从结果来看随着长度的增加，读写的耗时都随之增加。 长度为10：写平均耗时0.053ms，读0.040ms 长度为20000：写平均耗时0.352</description></item><item><title>遍历Collection时删除元素</title><link>https://atbug.com/remove-element-while-looping-collection/</link><pubDate>Sun, 05 Mar 2017 22:04:58 +0000</pubDate><guid>https://atbug.com/remove-element-while-looping-collection/</guid><description>
其实标题我想用《为什么foreach边循环边移除元素要用Iterator？》可是太长。 不用Iterator，用Collection.remove()，会报ConcurrentModificationException错误。 for(Integer i : list) { list.remove(i); //Throw ConcurrentModificationException } 其实使用foreach的时候，会自动生成一个Iterator来遍历list。不只是remove，使用add、clear等方法一样会出错。 拿ArrayList来说，它有一个私有的Iterator接口的内部类Itr： private class Itr implements Iterator&amp;lt;E&amp;gt; { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; //sevrval methods } 使用Iterator来遍历ArrayList实际上是通过两个指针来遍历Arr</description></item><item><title>Java Volatile关键字</title><link>https://atbug.com/deep-in-java-volatile-keywork/</link><pubDate>Thu, 02 Mar 2017 08:30:29 +0000</pubDate><guid>https://atbug.com/deep-in-java-volatile-keywork/</guid><description>
volatile通过保证对变量的读或写都是直接从内存中读取或直接写入内存中，保证了可见性；但是volatile并不足以保证线程安全，因为无法保证原子性，如count++操作： 将值从内存读入寄存器中 进行加1操作，内存保存到寄存器中 结果从寄存器flush到内存中 借用一张图来看： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://tutorials.jenkov.com/images/java-concurrency/java-volatile-2.png 链接到文件: /static/http://tutorials.jenkov.com/images/java-concurrency/java-volatile-2.png 使用 Page Bundles: false 不是volatile的变量的指令执行顺序是1-&amp;gt;2-&amp;gt;3；而声明为volatile的变量，顺序是1-&amp;gt;23。从这里看，volatile保证了一个线程修改了volatile修饰的变量，变化会马上体现在内存中。线程间看到的值是一样的。 上面说</description></item><item><title>Haproxy虚拟主机SSL</title><link>https://atbug.com/haproxy-multi-host-with-ssl/</link><pubDate>Mon, 27 Feb 2017 19:31:53 +0000</pubDate><guid>https://atbug.com/haproxy-multi-host-with-ssl/</guid><description>
Haproxy为多个域名配置SSL 生成自签名证书 sudo mkdir /etc/ssl/atbug.com sudo openssl genrsa -out /etc/ssl/atbug.com/atbug.com.key 1024 sudo openssl req -new -key /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.csr sudo openssl x509 -req -days 365 -in /etc/ssl/atbug.com/atbug.com.csr -singkey /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.crt sudo openssl x509 -req -days 365 -in /etc/ssl/atbug.com/atbug.com.csr -signkey /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.crt sudo cat /etc/ssl/atbug.com/atbug.com.crt /etc/ssl/atbug.com/atbug.com.key | sudo tee /etc/ssl/atbug.com/atbug.com.pem Haproxy配置 frontend https bind *:443 ssl crt /etc/ssl/atbug.com/atbug.com.pem option tcplog mode http #option forwardfor ###atbug-https acl atbug-https hdr_beg(host) -i test.atbug.com use_backend rome-atbug-https-backend if atbug-https backend rome-atbug-https-backend balance roundrobin mode http option ssl-hello-chk server node-1 ip:port cookie dw2-vm-test-apps003 check inter 2000 rise 3 fall 3 weight 50</description></item><item><title>mybatis报错“Result Maps collection already contains value for ***”</title><link>https://atbug.com/duplicate-resultmap-in-mybatis-mapper/</link><pubDate>Wed, 22 Feb 2017 14:12:18 +0000</pubDate><guid>https://atbug.com/duplicate-resultmap-in-mybatis-mapper/</guid><description>
这是工作中遇到的一个问题：测试环境部署出错，报了下面的问题。 Caused by: java.lang.IllegalArgumentException: Result Maps collection already contains value for xxx.xxx.xxxRepository.BaseResultMap at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:802) at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:774) at org.apache.ibatis.session.Configuration.addResultMap(Configuration.java:556) at org.apache.ibatis.builder.MapperBuilderAssistant.addResultMap(MapperBuilderAssistant.java:217) at org.apache.ibatis.builder.ResultMapResolver.resolve(ResultMapResolver.java:47) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:285) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:252) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElements(XMLMapperBuilder.java:244) at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:116) 检查了对应的mapper文件和java文件，已经8个多月没有修改过了。也检查了内容，没有发现重复的BaseResultMap；select中也resultMap的引用也都正确。 其实到最后发现跟代码一丁点关系都没有，是部署的时候没有删除旧版本的代码导致两个不同版本的jar同时存在，相应的mapper文件也有两个。 看了下源码，mybatis在创建SessionFactoryBean解析xml时候，会把xml中的resultMap放入到一个HashMap的子类StrictMap中，k</description></item><item><title>消费时offset被重置导致重复消费</title><link>https://atbug.com/offset-be-reset-when-consuming/</link><pubDate>Mon, 20 Feb 2017 13:23:49 +0000</pubDate><guid>https://atbug.com/offset-be-reset-when-consuming/</guid><description>
这是实际使用时遇到的问题：kafka api的版本是0.10，发现有重复消费问题；检查log后发现在commit offset的时候发生超时。 Auto offset commit failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records. 15:12:12.364 [main] WARN o.a.k.c.c.i.ConsumerCoordinator - Auto offset commit failed for group test: Commit offsets failed with retriable exception. You should retry committing offsets. 看了Kafka的API文档，发现0.10中提供了新的配置max.poll.records： The maximum number of records returned in a single call to poll(). type: int default: 2147483647 如果生产端写入很快，消费端处理耗时。一个batch的处理时间大于session.timeout.ms，会导致session time out，引起of</description></item><item><title>TheadPoolExecutor源码分析</title><link>https://atbug.com/threadpoolexecutor-sourcecode-analysis/</link><pubDate>Mon, 20 Feb 2017 09:56:07 +0000</pubDate><guid>https://atbug.com/threadpoolexecutor-sourcecode-analysis/</guid><description>
TheadPoolExecutor源码分析 ThreadPoolExecutor是多线程中经常用到的类，其使用一个线程池执行提交的任务。 实现 没有特殊需求的情况下，通常都是用Executors类的静态方法如newCachedThreadPoll来初始化ThreadPoolExecutor实例： public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&amp;lt;Runnable&amp;gt;()); } 从Executors的方法实现中看出，BlockingQueue使用的SynchronousQueue，底层使用了栈的实现。值得注意的是，这个SynchronousQueue是没有容量限制的，Executors也将maximumPoolSize设为Integer.MAX_VALUE</description></item><item><title>Kafka Java生产者模型</title><link>https://atbug.com/kafka-java-producer-model/</link><pubDate>Wed, 04 Jan 2017 16:33:02 +0000</pubDate><guid>https://atbug.com/kafka-java-producer-model/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.com/media/14835174309242.jpg 链接到文件: /static//media/14835174309242.jpg 使用 Page Bundles: false Producer初始化 初始化KafkaProducer实例，同时通过Config数据初始化MetaData、NetWorkClient、Accumulator和Sender线程。启动Sender线程。 MetaData信息 记录Cluster的相关信息，第一次链接使用Config设置，之后会从远端poll信息回来，比如host.name等信息。 Accumulator实例 Accumulator持有一个Map实例，key为TopicPartition（封装了topic和partition信息）对象，Value为RecordBatc</description></item><item><title>Redis清理缓存</title><link>https://atbug.com/clean-speicified-keys-in-redis/</link><pubDate>Tue, 13 Dec 2016 16:54:41 +0000</pubDate><guid>https://atbug.com/clean-speicified-keys-in-redis/</guid><description>
最近有个需求需要主动的去清理部分缓存，考虑的原子性的问题，用Lua脚本进行实现。 Lua脚本 local count = 0 for _,k in ipairs(redis.call(&amp;#39;KEYS&amp;#39;, ARGV[1])) do redis.call(&amp;#39;DEL&amp;#39;, k) count = count + 1 end return count shell运行 redis-cli --eval file.lua ,[KEY PATTERN] #sample: 清理所有key以Test开头的记录 redis-cli --eval clear.lua , Test* Java Jedis jedis = new Jedis(&amp;#34;127.0.0.1&amp;#34;, 6379); URL resource = Resources.getResource(&amp;#34;META-INF/scripts/clear.lua&amp;#34;); String lua = Resources.toString(resource, Charsets.UTF_8); Object eval = jedis.eval(lua, 0, &amp;#34;Name*&amp;#34;); System.out.println(eval);</description></item><item><title>Flume - FileChannel （一）</title><link>https://atbug.com/flume-filechannel-overview/</link><pubDate>Wed, 23 Nov 2016 09:23:57 +0000</pubDate><guid>https://atbug.com/flume-filechannel-overview/</guid><description>
概述 当使用Flume的时候，每个流程都包含了输入源、通道和输出。一个典型的例子是一个web服务器将事件通过RPC（搬入AvroSource）写入输入源中，输入源将其写入MemoryChannel，最后HDFS Sink消费事件将其写入HDFS中。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://blog.cloudera.com//wp-content/uploads/2012/09/flume1.png 链接到文件: /static/http://blog.cloudera.com//wp-content/uploads/2012/09/flume1.png 使用 Page Bundles: false MemeoryChannel提供了高吞吐量但是在系统崩溃或者断电时会丢失数据。因此需要开发一个可持久话通道。FileChannel是在FLUME-1085里实现的。目标是提供一个高可用高吞吐量的通道。FileChannle保证了在失误提交之后，在崩溃或者断电后不丢失数据。 需要注意的是</description></item><item><title>探索Rabbitmq的Java客户端</title><link>https://atbug.com/deep-in-rabbitmq-java-client/</link><pubDate>Sun, 09 Oct 2016 09:20:07 +0000</pubDate><guid>https://atbug.com/deep-in-rabbitmq-java-client/</guid><description>
AMQPConnection 实例初始化 创建Connection时会通过FrameHandlerFacotry创建一个SocketFrameHandler，SocketFrameHandler对Socket进行了封装。 public AMQConnection(ConnectionParams params, FrameHandler frameHandler) { checkPreconditions(); this.username = params.getUsername(); this.password = params.getPassword(); this._frameHandler = frameHandler; this._virtualHost = params.getVirtualHost(); this._exceptionHandler = params.getExceptionHandler(); this._clientProperties = new HashMap&amp;lt;String, Object&amp;gt;(params.getClientProperties()); this.requestedFrameMax = params.getRequestedFrameMax(); this.requestedChannelMax = params.getRequestedChannelMax(); this.requestedHeartbeat = params.getRequestedHeartbeat(); this.shutdownTimeout = params.getShutdownTimeout(); this.saslConfig = params.getSaslConfig(); this.executor = params.getExecutor(); this.threadFactory = params.getThreadFactory(); this._channelManager = null; this._brokerInitiatedShutdown = false; this._inConnectionNegotiation = true; // we start out waiting for the first protocol response } 启动连接 初始化WorkService和HeartBeatSender。 创建一个channel0的AMQChannel，这个channel不会被ChannelManager管理。 首先channel0会将一个BlockingRpcContinuation作为当前未完成的Rpc请</description></item><item><title>Git回车换行</title><link>https://atbug.com/crlf-in-git/</link><pubDate>Wed, 14 Sep 2016 09:16:10 +0000</pubDate><guid>https://atbug.com/crlf-in-git/</guid><description>
最近又个项目，checkout之后，没做任何改动前git status发现已经有modified了，通过git diff发现有两种改动： - warning: CRLF will be replaced by LF in ** - 删除并添加的同样的行 使用git diff -w却没有改动；使用git diff –ws-error-highlight=new,old发现行尾有**^M** 我本人用的是Linux，其他同事有用Windows，问题就出在平台上。 Windows用CR LF来定义换行，Linux用LF。CR全称是Carriage Return ,或者表示为\r, 意思是回车。 LF全称是Line Feed，它才是真正意义上的换行表示符。 git config中关于CRLF有两个设定：core.autocrlf和c</description></item><item><title>深入剖析HashSet和HashMap实现</title><link>https://atbug.com/deep-in-implementation-of-hashset/</link><pubDate>Mon, 11 Jul 2016 14:57:16 +0000</pubDate><guid>https://atbug.com/deep-in-implementation-of-hashset/</guid><description>
HashSet是一个包含非重复元素的集合，如何实现的，要从底层实现代码看起。 背景 首先非重复元素如何定义，看Set的描述： More formally, sets contain no pair of elements e1 and e2 such that e1.equals(e2), and at most one null element. Set不会找到两个元素，并且两个元素满足e1.equals(e2)为true；并且最多只有一个null元素。 如果没有重写equals方法，查看Object类中equal方法的实现，==比较的其实是两个对象在内存中的地址。 public boolean equals(Object obj) { return (this == obj); } 说起equals方法，就不得不说hashCode方法了。Java中对于hashCode有个常规协定 The general contract of hashCode is: Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent</description></item><item><title>多线程下的单例模式+反汇编</title><link>https://atbug.com/singleton-in-multi-threads-programming/</link><pubDate>Wed, 06 Jul 2016 16:57:09 +0000</pubDate><guid>https://atbug.com/singleton-in-multi-threads-programming/</guid><description>
多线程下的单例模式的实现，顺便做了反汇编。 public class MySingleton { private static MySingleton INSTANCE; private MySingleton() { } public static MySingleton getInstance() { if (INSTANCE == null) { synchronized (MySingleton.class) { INSTANCE = new MySingleton(); } } return INSTANCE; } } Compiled from &amp;#34;MySingleton.java&amp;#34; public class MySingleton { public static MySingleton getInstance(); Code: 0: getstatic #2 // Field INSTANCE:LMySingleton; //+获得类的指定域，并压入栈顶 3: ifnonnull 32 //+不为null时跳转到行号32 6: ldc_w #3 // class MySingleton //+常量值从常量池中推送至栈顶（宽索引），推送的为地址 9: dup //+复制栈顶数值，并且复制值进栈 10: astore_0 //+将栈顶数值（objectref）存入当前 frame的局部变量数组中指定下标(index）处的变量中，栈顶数值出栈。这里存的是MySingleton类定义的地址 11: monitorenter //+获得对象锁即MySingleton地址 12: new #3 // class MySingleton //+创建一个对象，并且其引用进栈 15: dup //+复</description></item><item><title>使用Kryo替换spring amqp的Java序列化</title><link>https://atbug.com/use-kryo-in-spring-amqp-serialization/</link><pubDate>Wed, 29 Jun 2016 05:29:14 +0000</pubDate><guid>https://atbug.com/use-kryo-in-spring-amqp-serialization/</guid><description>
spring amqp的原生并没有对Kryo加以支持，Kryo的优点就不多说了。 git地址：https://github.com/addozhang/spring-kryo-messaeg-converter public class KryoMessageConverter extends AbstractMessageConverter { public static final String CONTENT_TYPE = &amp;#34;application/x-kryo&amp;#34;; public static final String DEFAULT_CHARSET = &amp;#34;UTF-8&amp;#34;; private String defaultCharset = DEFAULT_CHARSET; private KryoFactory kryoFactory = new DefaultKryoFactory(); /** * Crate a message from the payload object and message properties provided. The message id will be added to the * properties if necessary later. * * @param object the payload * @param messageProperties the message properties (headers) * @return a message */ @Override protected Message createMessage(Object object, MessageProperties messageProperties) { byte[] bytes = null; Kryo kryo = kryoFactory.create(); Output output = new ByteBufferOutput(4096, 1024 * 1024); try { kryo.writeClassAndObject(output, object); bytes = output.toBytes(); } finally { output.close(); } messageProperties.setContentType(CONTENT_TYPE); if (messageProperties.getContentEncoding() == null) { messageProperties.setContentEncoding(defaultCharset); } return new Message(bytes, messageProperties); } @Override public Object fromMessage(Message message) throws MessageConversionException { Object content = null; MessageProperties properties = message.getMessageProperties(); if (properties != null) { if (properties.getContentType() != null &amp;amp;amp;&amp;amp;amp; properties.getContentType().contains(&amp;#34;x-kryo&amp;#34;)) { Kryo kryo = kryoFactory.create(); content = kryo.readClassAndObject(new ByteBufferInput(message.getBody())); } else { throw new MessageConversionException(&amp;#34;Converter not applicable to this message&amp;#34;); } } return content; } private class DefaultKryoFactory implements KryoFactory { @Override public Kryo create() { Kryo kryo = new Kryo(); return kryo; } } }</description></item><item><title>Rabbitmq延迟队列实现</title><link>https://atbug.com/rabbitmq-delay-queue-implementation/</link><pubDate>Wed, 30 Mar 2016 14:27:02 +0000</pubDate><guid>https://atbug.com/rabbitmq-delay-queue-implementation/</guid><description>
在requeue=false的情况系，消息被client reject 消息过期 队列长度超过限制</description></item><item><title>关于SLF4J</title><link>https://atbug.com/about-slf4j/</link><pubDate>Sat, 18 Apr 2015 11:16:26 +0000</pubDate><guid>https://atbug.com/about-slf4j/</guid><description>
Spring的功能越来越强大，同时也越来越臃肿。比如想快速搭建一个基于Spring的项目，解决依赖问题非常耗时。Spring的项目模板的出现就解决了这个问题，通过这个描述文件，可以快速的找到你所需要的模板。 第一次认识SLF4J就是在这些项目模板里，它的全称是Simple Logging Facade for Java。从字面上可以看出它只是一个Facade，不提供具体的日志解决方案，只服务于各个日志系统。简单说有了它，我们就可以随意的更换日志系统（如java.util.logging、logback、log4j）。比如在开发的时候使用logback，部署的时候可以切换到log4j；如果关闭所有的log，切换到NOP就可以了。只</description></item><item><title>日常工作中使用的chrome插件</title><link>https://atbug.com/chrome-extensions-for-daily-work/</link><pubDate>Sun, 09 Feb 2014 09:26:50 +0000</pubDate><guid>https://atbug.com/chrome-extensions-for-daily-work/</guid><description>
先说下背景，Java程序猿一枚，兼做hybrid app。所以日常工作中免不了要跟浏览器打交道，一般就只用chrome了，看中它的调试功能和庞大的插件库。下面就列出工作中使用的各种插件，排名不分先后。 google帐号要有，而且两个。一个日常生活娱乐，一个工作使用。可以同步各种数据到云端，如app、插件、设置、自动填写数据、密码、访问历史、打开的标签等等。原谅我把同步也当作插件，简直就是插件中的战斗机。 Developer tool，这个是chrome自带的开发人员工具，集前台设计、代码调试、资源管理等功能。 Adblock Plus 不管工作还是生活用，这个插件必装。有适合国人使用的屏蔽清单（ChinaList+EasyList），轻松搞</description></item><item><title>About</title><link>https://atbug.com/about/</link><pubDate>Tue, 22 Jan 2013 10:02:37 +0800</pubDate><guid>https://atbug.com/about/</guid><description>
基本信息 张晓辉 资深码农，12 年软件开发经验。曾在汇丰软件、唯品会、数人云、小鹏汽车等公司任职。 目前就职 Flomesh，担任高级云原生架构师、布道师。 这里所有的文章都会同步发送到公众号：云原生指北 (微信号：sevenfeet) Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/qrcode.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/qrcode.jpg 使用 Page Bundles: false</description></item></channel></rss>