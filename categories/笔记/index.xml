<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>笔记 on 乱世浮生</title><link>https://atbug.com/categories/%E7%AC%94%E8%AE%B0/</link><description>Recent content in 笔记 on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 18 Jan 2022 12:03:59 +0800</lastBuildDate><atom:link href="https://atbug.com/categories/%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml"/><item><title>Kubernetes HPA 基于 Prometheus 自定义指标的可控弹性伸缩</title><link>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</link><pubDate>Tue, 18 Jan 2022 12:03:59 +0800</pubDate><guid>https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/</guid><description>
在《Kubernetes 的自动伸缩你用对了吗？》 一文中详细说明了如何使用 Kubernetes 的自动伸缩。在 Kubernetes 中弹性伸缩主要有三种：HPA、VPA、CA。本文不再详细说明，有兴趣的可以看那篇文章。这里主要来说下 Pod 水平缩放 HPA。 随着 Kubernetes v1.23 的发布，HPA 的 API 来到了稳定版 autoscaling/v2： 基于自定义指标的伸缩 基于多项指标的伸缩 可配置的伸缩行为 从最初的 v1 版本 HPA 只支持 CPU、内存利用率的伸缩，到后来的自定义指标、聚合层 API 的支持，到了 v1.18 版本又加入了配置伸缩行为的支持，HPA 也越来越好用、可靠。 依靠 CPU 或者内存指标的扩容并非使用所有系统，看起来也没那么可靠。对大部分的 web 后端系统来说，基于 RPS（每秒请求数）的弹性伸缩</description></item><item><title>策略即代码：为了 OpenPolicyAgent 学 Rego？试试 Javascript</title><link>https://atbug.com/policy-as-code-with-pipy/</link><pubDate>Wed, 08 Dec 2021 07:49:37 +0800</pubDate><guid>https://atbug.com/policy-as-code-with-pipy/</guid><description>
距离上个版本 用 Pipy 实现 OPA，已经过去快半年了。当初使用Pipy 实现了可信镜像仓库的检查，那时的版本实现起来会稍微复杂，从策略仓库到证书创建到Admission Webhook 的创建都需要大量的人工操作，配置和逻辑也还是耦合在一起。 这个版本安装和使用起来会更加简单。 当初我用“不务正业”来形容 Pipy 实现准入控制，等看完这篇文章，欢迎留言说说你的看法。 架构 还是继续上次的场景，在 Pod 创建时对 Pod 使用的镜像所在仓库进行检查，以及检查镜像的 tag 是否合法。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/08/untitled.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/08/untitled.jpg 使用 Page Bundles: false 这里借助 Pipy Repo 的能力，将代表策略的脚本和配置交由 Repo 进行管理；Pipy 实例实时从 Pipy Repo 同步策略，并进行动态加载。</description></item><item><title>从 Docker 的信号机制看容器的优雅停止</title><link>https://atbug.com/gracefully-stopping-docker-containers-with-correct-command/</link><pubDate>Mon, 29 Nov 2021 07:30:43 +0800</pubDate><guid>https://atbug.com/gracefully-stopping-docker-containers-with-correct-command/</guid><description>
此文是前段时间笔记的整理，之前自己对这方面的关注不够，因此做下记录。 有太多的文章介绍如何运行容器，然而如何停止容器的文章相对少很多。 根据运行的应用类型，应用的停止过程非常重要。如果应用要写文件，停止前要保证正确刷新数据并关闭文件；如果是 HTTP 服务，要确保停止前处理所有未完成的请求。 信号 信号是 Linux 内核与进程以及进程间通信的一种方式。针对每个信号进程都有个默认的动作，不过进程可以通过定义信号处理程序来覆盖默认的动作，除了 SIGSTOP 和 SIGKILL。二者都不能被捕获或重写，前者用来将进程暂停在当前状态，而后者则是从内核层面立即杀掉进程。 有两个比较重要的进程 SIGTERM 和 SIGKILL。SIGTERM 是优雅地关闭命令，SIG</description></item><item><title>Kubernetes Deployment 的故障排查可视化指南（2021 中文版）</title><link>https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/</link><pubDate>Sat, 20 Nov 2021 18:29:19 +0800</pubDate><guid>https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/</guid><description>
将应用部署到 Kubernetes 时通常会使用 Deployment、Service、Ingress，整个应用从部署到正常运行，经历的流程很长。从 kubectl apply YAML 文件，经过 apiserver、controller manager、scheduler、kubelet、以及 CRI、CNI 等众多组件的协同工作。 漫长的“行程”，Pod 也经历各种正常和不正常的状态变化，即使正常运行也会出现服务无法访问的问题。对于刚开始在 Kubernetes 平台开展工作的同学来说，故障的排查确实棘手。之前工作的时候，经常要协助排查各种问题。去年在 Learnk8s 上看到了关于 Deployment 故障排查的视图，我还参考做了当时整个平台的故障排查视图，包括了从项目源码、CICD 流水线、部署整个流程的故障排查</description></item><item><title>Monterey 12.0.1 上的 bug</title><link>https://atbug.com/bug-with-m1-pro-and-monterey/</link><pubDate>Thu, 18 Nov 2021 08:54:44 +0800</pubDate><guid>https://atbug.com/bug-with-m1-pro-and-monterey/</guid><description>
最近换上了 MacBook Pro 2021，也慢慢将工作转到新的电脑上。结束了一年多的黑白配，之前工作主力机是我的黑苹果，配置以及 OpenCore 的引导放在这里了。 为了稳定性，系统一直停留在了 10.15.4。新的电脑拿到手就是 12.0.1，之前也就在我另一台 2016 款的 macbook pro 上用过几个周的 12.0.0。 新的系统加上新的架构，不免有有些 bug，今天就说下我所遇到的两个比较棘手的。 1. 安全相关 EXC_BAD_ACCESS (SIGKILL (Code Signature Invalid)) 公司的核心产品是用 c++ 开发的，编译之后我都习惯性的放到 /usr/local/bin 目录中。在新系统中，就出现了上面的错误（从控制台获取），运行的时候进程直接被 kill。 网上查了下，说与 kernel cache 有关，重启可解决。 This was caused by kernel caching of previously signed binaries and my replacing those binaries with newly compiled binaries which weren&amp;rsquo;t part of a signed package. By deleting the existing binaries, rebooting</description></item><item><title>Kubernetes 上调试 distroless 容器</title><link>https://atbug.com/debug-distroless-container-on-kubernetes/</link><pubDate>Wed, 03 Nov 2021 07:40:40 +0800</pubDate><guid>https://atbug.com/debug-distroless-container-on-kubernetes/</guid><description>
TL;DR 本文内容： 介绍 distroless 镜像、作用以及简单的使用 如何针对 distroless 容器的进行调试 临时容器(v.1.18+)的使用 Distroless 镜像 Distroless 容器，顾名思义使用 Distroless 镜像作为基础镜像运行的容器。 &amp;ldquo;Distroless&amp;rdquo; 镜像只包含了你的应用程序以及其运行时所需要的依赖。不包含你能在标准 Linxu 发行版里的可以找到的包管理器、shells 或者其他程序。 GoogleContainerTools/distroless 针对不同语言提供了 distroless 镜像： gcr.io/distroless/static-debian11 gcr.io/distroless/base-debian11 gcr.io/distroless/java-debian11 gcr.io/distroless/cc-debian11 gcr.io/distroless/nodejs-debian11 gcr.io/distroless/python3-debian11 Distroless 镜像有什么用？ 那些可能是构建镜像时需要的，但大部分并不是运行时需要的。这也是为什么上篇文章介绍 Buildpacks 时说的一个 builder 的 stack 镜像包含构建时基础镜像和运行时基础镜像，这样可以做到镜像的最小化。 其实控制体积并不是 distroless 镜像的主要作用。将运行时容器中的内容限制为应用程序所需的依赖，此外不应该安装任何</description></item><item><title>无需 Dockerfile 的镜像构建：BuildPack vs Dockerfile</title><link>https://atbug.com/build-docker-image-without-dockerfile/</link><pubDate>Fri, 29 Oct 2021 07:36:43 +0800</pubDate><guid>https://atbug.com/build-docker-image-without-dockerfile/</guid><description>
过去的工作中，我们使用微服务、容器化以及服务编排构建了技术平台。为了提升开发团队的研发效率，我们同时还提供了 CICD 平台，用来将代码快速的部署到 Openshift（企业级的 Kubernetes） 集群。 部署的第一步就是应用程序的容器化，持续集成的交付物从以往的 jar 包、webpack 等变成了容器镜像。容器化将软件代码和所需的所有组件（库、框架、运行环境）打包到一起，进而可以在任何环境任何基础架构上一致地运行，并与其他应用“隔离”。 我们的代码需要从源码到编译到最终可运行的镜像，甚至部署，这一切在 CICD 的流水线中完成。最初，我们在每个代码仓库中都加入了三个文件，也通过项目生成器（类似 Spring Initializer）在新</description></item><item><title>自动替换 Kubernetes 镜像</title><link>https://atbug.com/kubernetes-images-swapper/</link><pubDate>Wed, 06 Oct 2021 08:01:41 +0800</pubDate><guid>https://atbug.com/kubernetes-images-swapper/</guid><description>
前几天有朋友在问如何在某云上拉取 Tekton 的镜像，这种情况其实比较普遍不只是某云。工作中经常要用到过某些靠运气才能拉取到的镜像，这对工作来说真是极度的不友好。 因此也萌生了个想法，维护一个后网络友好的仓库镜像，在 Pod 创建时将镜像仓库切换到自维护的仓库，从自维护的仓库拉取镜像。 前几天体验了极狐Gitlab 的容器镜像库，便是为这个想法做的准备。当然其他的云厂商也有提供针对个人版的免费镜像仓库和企业版仓库。 正好 Pipy 作为策略引擎，非常适合实现这种策略的执行。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005212733.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005212733.png 使用 Page Bundles: false 实现思路 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005210725.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/05/20211005210725.png 使用 Page Bundles: false Admission Webhook Kubernetes 动</description></item><item><title>极狐GitLab SaaS 内测轻度体验</title><link>https://atbug.com/jihu-gitlab-experience/</link><pubDate>Fri, 01 Oct 2021 08:18:09 +0800</pubDate><guid>https://atbug.com/jihu-gitlab-experience/</guid><description>
感谢极狐团队为 GitLab（SaaS）本地化的努力，同时也感谢小马哥提供的内测资格。 最近突然想到了个点子，需要使用一个私有的镜像仓库。极狐GitLab 有提供容器镜像库，正好和 CICD 一起做个轻度体验。 容器镜像库 Container Registry 文档介绍在这里，目前还是英文。（应该本地化的工作量很大，文档还没翻译。） 容器镜像库可以作为独立镜像仓库使用（为什么要这么用，卖个关子下篇文章见），就是使用 docker 命令将构建好的镜像推送到 容器镜像库。 当然也可以同 CICD 流水线结合使用，后文也会介绍。 独立使用 本地登录 Container Registry 有两种验证方式： 使用用户名和密码 开启了双重身份验证，可以使用访问个人访问令牌 其实，不管是否开始双重验证，都建议使用访问令牌。 docker login registry.gitlab.cn #根据</description></item><item><title>ARM64 平台基于 openEuler + iSula 环境部署 Kubernetes</title><link>https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/</link><pubDate>Thu, 02 Sep 2021 20:41:06 +0800</pubDate><guid>https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/</guid><description>
为什么要在 arm64 平台上部署 Kubernetes，而且还是鲲鹏 920 的架构。说来话长 。。。 此处省略5000 字。 介绍下系统信息； 架构：鲲鹏 920(Kunpeng920) OS：openEuler 20.03 (LTS-SP1) CPU：4c 内存：16G 硬盘：若干 整个过程虽然参考了鲲鹏论坛的帖子，不过还是颇费周折。 TL;DR 整个过程中要注意 arm64 平台上安装 Kubernetes 及网络组件，需要使用 arm64 版本的镜像。 环境配置 1.关闭 selinux #临时关闭 setenforce 0 #永久关闭 SELINUX=disabled vim /etc/sysconfig/selinux 2. 关闭swap分区 #临时关闭 swapoff -a #永久关闭 注释 swap 行 vim /etc/fstab 3. 关闭防火墙 systemctl stop firewalld ssystemctl disable firewalld 4. 网络配置 对iptables内部的nf-call需要打开的内生的桥接功能 vim /etc/sysctl.d/k8s.conf 修改如下内容： net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm_swappiness=0 修改完成后执行： modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf 5. 添加 Kubernetes 源 在文件 /etc/yum.repos.d/openEuler.repo 中追加如下内容： [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64/ enabled=1</description></item><item><title>Rego 不好用？用 Pipy 实现 OPA</title><link>https://atbug.com/pipy-implement-kubernetes-admission-control/</link><pubDate>Tue, 13 Jul 2021 08:44:56 +0800</pubDate><guid>https://atbug.com/pipy-implement-kubernetes-admission-control/</guid><description>
还不知道 Pipy 是什么的同学可以看下 GitHub 。 Pipy 是一个轻量级、高性能、高稳定、可编程的网络代理。Pipy 核心框架使用 C++ 开发，网络 IO 采用 ASIO 库。 Pipy 的可执行文件仅有 5M 左右，运行期的内存占用 10M 左右，因此 Pipy 非常适合做 Sidecar proxy。 Pipy 内置了自研的 pjs 作为脚本扩展，使得Pipy 可以用 JS 脚本根据特定需求快速定制逻辑与功能。 Pipy 采用了模块化、链式的处理架构，用顺序执行的模块来对网络数据块进行处理。这种简单的架构使得 Pipy 底层简单可靠，同时具备了动态编排流量的能力，兼顾了简单和灵活。通过使用 REUSE_PORT 的机制（主流 Linux 和 BSD 版本都支持该功能），Pipy 可以以多进程模式运行，使得 Pipy 不仅适用于 Sidecar 模式，也适用于大规模的流量处理场景。 在实践中，Pipy 独</description></item><item><title>Kubernetes CKA 证书备考笔记</title><link>https://atbug.com/notes-for-cka-preparation/</link><pubDate>Fri, 02 Jul 2021 08:02:15 +0800</pubDate><guid>https://atbug.com/notes-for-cka-preparation/</guid><description>
Kubernetes 使用有好几年了，但在今年 5 月才完成 CKA 的考试。虽说用了几年，还是提前刷了部分题熟悉下。 绝大部分题都是有在 minikube 的环境上操作过，只有部分比如升级集群受限于环境问题没有实地操作。 写在最前 保存常用文档进书签，如果有 Alfred 启用浏览器书签 workflow。效果见下图 kubectl 自动补全 echo &amp;quot;source &amp;lt;(kubectl completion bash)&amp;quot; &amp;gt;&amp;gt; ~/.bashrc; source ~/.bashrc 每道题开始前要切换 context 和 namespace，直接复制题目里的命令即可 必要的 alias 善用 --dry-run=client -o yaml 避免手动敲太多 善用 kubectl explain [resource[.field]] 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 书签地址：K8s-CKA-CAKD-Bookmarks</description></item><item><title>云原生 CICD: Tekton Pipeline 实战</title><link>https://atbug.com/tekton-pipeline-practice/</link><pubDate>Tue, 22 Jun 2021 07:19:33 +0800</pubDate><guid>https://atbug.com/tekton-pipeline-practice/</guid><description>
更新历史： v1：2020.1.21 基于 Tekton Pipline v0.9.0 v2（当前）：2021.6.22 基于 Tekton Pipeline v0.25.0 Tekton 是 Google 开源的 Kubernetes 原生CI/CD 系统, 功能强大扩展性强. 前身是 Knavite 里的 build-pipeline 项目, 后期孵化成独立的项目. 并成为 CDF 下的四个项目之一, 其他三个分别是 Jenkins, Jenkins X, Spinnaker. 为什么说 Tekton 是 Kubernetes 原生的, 以内其基于 Kubernetes 的 CRD 定义了 Pipeline 流水线. Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/uPic/bquuTV.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/uPic/bquuTV.jpg 使用 Page Bundles: false CRD 及说明: Task: 构建任务, 可以定义一些列的 steps. 每个 step 由一个 container 执行. TaskRun: task 实际的执行, 并提供执行所需的参数. 这个对象创建后, 就会有 pod 被创建. Pipeline: 定义一个或者多个 task 的执行, 以及 PipelineResource 和各种定义参数的集合 PipelineRun: 类似 task 和 taskrun 的关系: 一个定义一个执行. PipelineRun 则是 pipeline 的实际执行. 创建</description></item><item><title>服务网格平稳落地：Istio 中精准控制 Sidecar 的注入</title><link>https://atbug.com/how-to-control-istio-sidecar-injection/</link><pubDate>Wed, 21 Apr 2021 08:13:04 +0800</pubDate><guid>https://atbug.com/how-to-control-istio-sidecar-injection/</guid><description>
为什么 说起服务网格，这幅图大家肯定不会陌生。这就是服务网格的网络，也是网格架构的终极形态。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2314142x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2314142x.png 使用 Page Bundles: false 那在迁移到网格架构之前，我们的系统是什么样的？ Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2316432x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2316432x.png 使用 Page Bundles: false 我们的系统在演进的过程中，不可避免的会遇到各种 0 到 1 过程中的中间态。比如下面这种，可以比较直观的看出 Istio 或者网格是部分覆盖的。这个过程中，我们需要平滑、可控的推进，才能在保障系统可用性的前提下进行架构的演进。 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2318242x.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/21/cleanshot-20210420-at-2318242x.png 使用 Page Bundles: false 怎么做 Sidecar 的注入分两种：手动和自动。 手动 手动</description></item><item><title>带你了解 Ribbon 负载均衡器的实现</title><link>https://atbug.com/how-loadbalancer-works-in-ribbon/</link><pubDate>Tue, 09 Jun 2020 19:35:53 +0800</pubDate><guid>https://atbug.com/how-loadbalancer-works-in-ribbon/</guid><description>
Spring Cloud 中 Ribbon有在 Zuul 和 Feign 中使用，当然也可以通过在RestTemplate的 bean 定义上添加@LoadBalanced注解方式获得一个带有负载均衡更能的RestTemplate。 不过实现的方法都大同小异：对HttpClient进行封装，加上实例的”选择“（这个选择的逻辑就是我们所说的负载均衡）。 要学习某个框架的时候，最简单的方案就是：Running+Debugging。 跑就是了。 debug 不一定是为了 bug debug 出真知 Debugging = Learning 选用 Ali Spittel 的一条推文： Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-165236.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-165236.png 使用 Page Bundles: false 以 Zuul 路由的线程栈为例 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-151421.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/06/09/screenshot-20200609-at-151421.png 使用 Page Bundles: false 调整下</description></item><item><title>Eureka 实例注册状态保持 STARTING 的问题排查</title><link>https://atbug.com/troubleshooting-on-eureka-instance-keep-starting/</link><pubDate>Thu, 28 May 2020 22:04:02 +0800</pubDate><guid>https://atbug.com/troubleshooting-on-eureka-instance-keep-starting/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/28/maninblackshirtandgraydenimpantssittingongray11342.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/28/maninblackshirtandgraydenimpantssittingongray11342.jpg 使用 Page Bundles: false 这是真实发生在生产环境的 case，实例启动后正常运行，而在注册中心的状态一直保持STARTING，而本地的状态为UP。导致服务的消费方无法发现可用实例。 这种情况的出现概率非常低，运行一年多未发现两个实例同时出现问题的情况，因此多实例运行可以避免。文末有问题的解决方案，不想花时间看分析过程可直接跳到最后。 环境说明： eureka-client: 1.7.2 spring-boot: 1.5.12.RELEASE spring-cloud: Edgware.SR3 问题重现 借助Btrace重现, java -noverify -cp .:btrace-boot.jar -javaagent:btrace-agent.jar=script=&amp;lt;pre-compiled-btrace-script&amp;gt; &amp;lt;MainClass&amp;gt; &amp;lt;AppArguments&amp;gt; 思路 主线程更新实例本地状态(STARTING-&amp;gt;UP)前, 等待心跳线程完成第一次心跳并尝试注册实例, 获取到当前的状态STARTING. 主线程更新状态后触</description></item><item><title>Tekton 的工作原理</title><link>https://atbug.com/how-tekton-works/</link><pubDate>Sat, 23 May 2020 22:47:14 +0800</pubDate><guid>https://atbug.com/how-tekton-works/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/24/tekton.jpeg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/24/tekton.jpeg 使用 Page Bundles: false 这篇文章是基于 Tekton Pipeline 的最新版本v0.12.1版本。 快速入门请参考：云原生 CICD: Tekton Pipeline 实战 ，实战是基于版本 v0.10.x。 Pipeline CRD 与核心资源的关系 $ k api-resources --api-group=tekton.dev NAME SHORTNAMES APIGROUP NAMESPACED KIND clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task Tekton Pipelines提供了上面的CRD，其中部分CRD与k8s core中资源相对应 Task =&amp;gt; Pod Task.Step =&amp;gt; Container Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902164552270.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902164552270.jpg 使用 Page Bundles: false 工作原理 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902280074872.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/23/15902280074872.jpg 使用 Page Bundles: false (图片来自) Tekton Pipeline 是基于 Knative 的实现，pod tekton-pipelines-controller 中有两个 Knative Controller的</description></item><item><title>Java 中的 Mysql 时区问题</title><link>https://atbug.com/mysql-timezone-in-java/</link><pubDate>Thu, 14 May 2020 11:34:24 +0800</pubDate><guid>https://atbug.com/mysql-timezone-in-java/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/14/anonymouspersonwithminiatureairplaneonchalkboard37.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/05/14/anonymouspersonwithminiatureairplaneonchalkboard37.jpg 使用 Page Bundles: false (Photo by Andrea Piacquadio from Pexels) 话说工作十多年，mysql 还真没用几年。起初是外企银行，无法直接接触到 DB；后来一直从事架构方面，也多是解决问题为主。 这次搭建海外机房，围绕时区大家做了一番讨论。不说最终的结果是什么，期间有同事认为 DB 返回的是 UTC 时间。 这里简单做个验证，顺便看下时区的问题到底是如何处理。 环境 openjdk version &amp;ldquo;1.8.0_242&amp;rdquo; mysql-connector-java &amp;ldquo;8.0.20&amp;rdquo; mysql &amp;ldquo;5.7&amp;rdquo; 时区 TZ=Europe/London 本地时区 GMT+8 创建个简单的库test及表user， 表结构如下： CREATE TABLE `user` ( `name` varchar(50) NOT NULL, `birth_date` timestamp NULL DEFAULT CURRENT_TIMESTAMP ) ENGINE=InnoDB DEFAULT CHARSET=latin1 插入一条测试数据： mysql&amp;gt; insert into `user` -&amp;gt; values (&amp;#39;Tom&amp;#39;, time(&amp;#39;2020-05-15 08:00:00&amp;#39;)); Query OK, 1 row affected (0.01 sec) mysql&amp;gt; select * from user; +------+---------------------+ | name | birth_date | +------+---------------------+ | Tom | 2020-05-14 08:00:00 | +------+---------------------+ 1 row in set (0.00 sec) 测试代码： Connection conn = DriverManager.getConnection(&amp;#34;jdbc:mysql://localhost:3306/test?useSSL=false&amp;#34;, &amp;#34;root&amp;#34;, &amp;#34;root&amp;#34;); Statement stmt = conn.createStatement(); stmt.execute(&amp;#34;select</description></item><item><title>控制 Pod 内容器的启动顺序</title><link>https://atbug.com/control-process-order-of-pod-containers/</link><pubDate>Thu, 12 Mar 2020 22:05:16 +0800</pubDate><guid>https://atbug.com/control-process-order-of-pod-containers/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/screenshot-20210430-at-092623.png 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/30/screenshot-20210430-at-092623.png 使用 Page Bundles: false 2021.4.30 更新： 最新的方案，请跳转新篇 Kubernetes 上如何控制容器的启动顺序。 背景 众所周知, Kubernetes Pod 内有两种容器: 初始化容器(init container)和应用容器(app container). 其中初始化容器的执行先于应用容器, 并且初始化容器和应用容器的个数分别为 0~n 和 1~n. 初始化容器会按照顺序执行, 顺序执行的前提是初始化容器始终会运行到完成(completed)状态. 而应用容器恰好相反: 启动顺序随机, 并始终保持运行(running)状态. 问题 工作中有个架构的方案使用到了 sidecar 容器: 将基础组件功能从容器转移到 sidecar 容器中, 其中有个功能是从远程配置中心获取配置并保持实时更新. 保证实</description></item><item><title>Go Docker 镜像进阶: 精简镜像</title><link>https://atbug.com/build-minimal-docker-image-for-go-app/</link><pubDate>Wed, 11 Mar 2020 23:00:27 +0800</pubDate><guid>https://atbug.com/build-minimal-docker-image-for-go-app/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839387687383.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839387687383.jpg 使用 Page Bundles: false ​[图片来自 https://www.facebook.com/sequenceprocess/] 问题: 入门到生产级的差距 昨天的文章《为 Go 应用创建 Docker 镜像》, 算是入门级的, 并不适用于生产级. 为什么? $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE addozhang/golang-hello-world latest 4cce1292a87a 4 seconds ago 813MB 整个镜像的大小有 813MB, 这还只有一个简单的 Hello world. 因为其中包含了 Golang 的编译和运行环境. 但是实际生产环境中, 我们并不需要这么多. 先看结果 精简之后只有 2.07MB, 而且并不影响运行. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE addozhang/golang-hello-world latest 4cce1292a87a 3 minutes ago 813MB addozhang/golang-hello-world2 latest 1da5bb994074 7 minutes ago 2.07MB $ docker run --rm addozhang/golang-hello-world2 Hello world 解决方案 如果做到的? 首先从基础镜像开始, 换成scratch1. 构建时将编译好的文件复制到镜像中 FROM scratch ADD golang-hello-world / CMD [&amp;#34;/golang-hello-world&amp;#34;] 假如你是使用go build来编译, 在 Macos 上会遇到如下问题: $ docker run</description></item><item><title>为 Go 应用创建 Docker 镜像</title><link>https://atbug.com/build-docker-image-for-go-app/</link><pubDate>Wed, 11 Mar 2020 20:41:58 +0800</pubDate><guid>https://atbug.com/build-docker-image-for-go-app/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839304511808.jpg 链接到文件: /static/https://atbug.oss-cn-hangzhou.aliyuncs.com/2020/03/11/15839304511808.jpg 使用 Page Bundles: false 嗯嗯, 最近开始用 Golang 了. 今天需要为 Go 应用创建对象, 看了下官方博客. 拿 hello world 做个测试. 使用下面的命令创建个新的项目 $ mkdir -p $GOPATH/src/github.com/addozhang/golang-hello-world &amp;amp;&amp;amp; cd &amp;#34;$_&amp;#34; $ go mod init github.com/addozhang/golang-hello-world go: creating new go.mod: module github.com/addozhang/golang-hello-world $ cat &amp;lt;&amp;lt; EOF &amp;gt; main.go package main import &amp;#34;fmt&amp;#34; func main() { fmt.Println(&amp;#34;Hello world&amp;#34;) } EOF # go fmt 运行检查一次 $ go run main.go Hello world 程序没问题, 下面就是构建镜像了. 创建一个 Dockerfile 文件, 内容如下: FROM golang LABEL Author=&amp;#34;addozhang&amp;#34; ADD . /go/src/github.com/addozhang/golang-hello-world RUN go install github.com/addozhang/golang-hello-world ENTRYPOINT [ &amp;#34;/go/bin/golang-hello-world&amp;#34; ] 构建镜像: $ docker build -t addozhang/golang-hello-world . 运行镜像: $ docker run --rm addozhang/golang-hello-world:latest Hello world 运行没问题, 收工</description></item><item><title>云原生CICD: Tekton Trigger 实战</title><link>https://atbug.com/tekton-trigger-practice/</link><pubDate>Wed, 12 Feb 2020 21:30:03 +0800</pubDate><guid>https://atbug.com/tekton-trigger-practice/</guid><description>
Trigger的介绍看 这里. 接上文 Tekton Pipeline 实战 , 我们为某个项目创建了一个Pipeline, 但是执行时通过 PipelineRun 来完成的. 在 PipelineRun 中我们制定了 Pipepline 以及要使用的 PipelineResource. 但是日常的开发中, 我们更多希望在提交了代码之后开始 Pipeline 的执行. 这时我们就要用到 Tekton Trigger 了. 思路是这样: 代码提交后将Push Event发送给Tekton Trigger EventController(以下简称 Controller), 然后 Controller 基于的TriggerBinding的配置从 payload 中提取信息, 装载在&amp;quot;Params&amp;quot;中作为TriggerTemplate的入参. 最后 Controller 创建PipelineRun. Trigger 相关的资源 TriggerTemplate 回看上回用的PipelineRun Yaml, 参数有revision, url, image</description></item><item><title>Tekton Trigger 介绍</title><link>https://atbug.com/tekton-trigger-glance/</link><pubDate>Wed, 05 Feb 2020 18:03:15 +0800</pubDate><guid>https://atbug.com/tekton-trigger-glance/</guid><description>
背景 Tekton 的介绍请参考Tekton Pipeline 实战. 通常, CI/CD 事件应该包含如下信息: 确定事件的类型(比如 GitHub Push, GitLab Issue, Docker Hub Webhook 等) 可从特定管道访问并映射到特定管道 (从事件负载中获取 SHA 信息, 然后在管道中使用) 准确地触发管道 (基于有效负载值触发管道) Tekton API 的设计分离了配置(比如 PipelineRun VS Pipeline), 保证了 step 可以被重用. 但是没有提供动态封装配置的机制来生成资源(尤其是 PipelineRun 和 PipelineResource). Triggers 通过下面的 CRDs 在架构上对 Tekton 进行了扩展: TriggerTemplate: 创建资源的模板(比如用来创建 PipelineResource 和 PipelineRun) TriggerBinding: 校验事件并提取负载字段 EventListener: 连接TriggerBinding和TriggerTemplate到可寻址的端点(事件接收器). 使用从各个 TriggerBinding中提取的参数来创建TriggerTemp</description></item><item><title>Tekton Dashboard 安装</title><link>https://atbug.com/tekton-dashboard-installation/</link><pubDate>Sat, 01 Feb 2020 12:39:28 +0800</pubDate><guid>https://atbug.com/tekton-dashboard-installation/</guid><description>
Tekton 提供了dashboard方便用户管理和查看 Tekton PipelineRun 和 TaskRun 以及创建, 执行和完成过程中涉及的资源. 它还允许按标签过滤 PipelineRun 和 TaskRun. 安装方法 kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/v0.4.1/dashboard_latest_release.yaml 检查dashboard的运行情况, STATUS为Running的话则说明运行成功. kubectl get pods --namespace tekton-pipelines 访问 访问Tekton的Dashboard有两种方式, 一种是通过port-forward, 另一种是通过ingress来访问. port-forward kubectl port-forward svc/tekton-dashboard 9097 ingress 先检查ingress是否开启. minikube addon list ... - ingress: enabled ... 如果是disabled的话, 通过命令minikube addons enable ingress. 注意: 这里拉取quay.io/kubernetes-ingress-controller/nginx-ingress-controller镜</description></item><item><title>Tekton安装及Hello world</title><link>https://atbug.com/tekton-installation-and-sample/</link><pubDate>Fri, 17 Jan 2020 19:17:14 +0800</pubDate><guid>https://atbug.com/tekton-installation-and-sample/</guid><description>
安装 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 检查安装的tekton相关的CRD: kubectl api-resources | grep tekton clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task tekton的两个pod: kubectl get pods --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-556d8f4494-2qthv 1/1 Running 0 11m tekton-pipelines-webhook-849cff5cf-8m5qq 1/1 Running 0 11m 安装CLI cli: https://github.com/tektoncd/cli#installing-tkn brew install tektoncd-cli Tekton: hello world 创建一个简单的Task, 只有一个step就是打印出&amp;quot;hello world&amp;quot; apiVersion:tekton.dev/v1alpha1kind:Taskmetadata:name:echo-hello-worldspec:steps:- name:echoimage:alpinecommand:- echoargs:- &amp;#34;hello world&amp;#34;创建一个TaskRun执行上面的Task apiVersion:tekton.dev/v1alpha1kind:TaskRunmetadata:name:echo-hello-world-task-runspec:taskRef:name:echo-hello-world运行task: kubectl apply</description></item><item><title>Minikube安装istio</title><link>https://atbug.com/install-istio-on-minikube/</link><pubDate>Fri, 17 Jan 2020 08:02:42 +0800</pubDate><guid>https://atbug.com/install-istio-on-minikube/</guid><description>
准备 注意: istioctl的安装要使用安装里的, 不要是用homebrew里的. github issue curl -L https://istio.io/downloadIstio | sh - cd istio-1.4.2 cp bin/istioctl /usr/local/bin/istioctl 安装前检查 istioctl verify-install 如果检查没问题, 会看到Install Pre-Check passed! The cluster is ready for Istio installation. 安装 istio有5种内建的安装配置1: remote, sds, default, demo, minimal istioctl profile list minimal: 使用istio的流量管理所需组件的最小化安装 default: 根据IstioControlPlane API的默认设置(建议用于生产部署)启用组件. 您可以通过运行命令istioctl profile dump显示默认设置. demo: 几乎安装所有的特性, 包括logging和tracing的比例为100%. 不适合生产环境, 负载太重 default demo minimal sds remote Core components istio-citadel X X X X istio-egressgateway X istio-galley X X X istio-ingressgateway X X X istio-nodeagent X istio-pilot X X X X istio-policy X X X istio-sidecar-injector X X X X istio-telemetry X X X Addons</description></item><item><title>加速云原生的 Java 开发</title><link>https://atbug.com/speed-up-java-development-on-kubernetes/</link><pubDate>Sat, 21 Dec 2019 20:45:22 +0800</pubDate><guid>https://atbug.com/speed-up-java-development-on-kubernetes/</guid><description>
今天来说说日常在Kubernetes开发Java项目遇到的问题. 当我们新建一个项目的时候, 总是面临需要新建manifest, 平时都是copy+paste+modify. 能否以变成的方式来生成? 开发时的步骤也比较繁琐: docker build, docker push, kubectl apple, kubectl delete pod. 对于一个Java应用来说还多了一步编译. 操作一次还ok, 但是一天十几次总会有想吐的感觉. 这些步骤能否简化成一个命令, 甚至修改了代码自动就完成上面一系列的操作? 实现这些我们需要几个工具: dekorate, Jib, Skaffold. 其中Jib也在上一篇文章使用Jib为Java应用构建镜像中介绍过. dekorate Dekorate is a collection of Java compile-time generators and decorators for Kubernetes/OpenShift manifests. Dekorate是Java编译时生成和装饰Kubernetes/OpenShift</description></item><item><title>使用 Jib 为 Java 应用构建镜像</title><link>https://atbug.com/build-docker-or-oci-image-with-jib-for-java/</link><pubDate>Mon, 09 Dec 2019 10:05:30 +0800</pubDate><guid>https://atbug.com/build-docker-or-oci-image-with-jib-for-java/</guid><description>
Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://github.com/GoogleContainerTools/jib/raw/master/logo/jib-build-docker-java-container-image.png 链接到文件: /static/https://github.com/GoogleContainerTools/jib/raw/master/logo/jib-build-docker-java-container-image.png 使用 Page Bundles: false Jib是Google Container Tools中的一个工具。 Jib builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library. Jib无需Docker守护程序即可为Java应用程序构建优化的Docker和OCI映像-无需深入了解Docker最佳实践. 它可以作为Maven和Gradle的插件以及Java库使用. 与Docker构建流程比较 Docker镜像构建流程: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://4.bp.blogspot.com/-SXeItzMS_oo/WzVemqaj7CI/AAAAAAAAF_w/t5Lau7EOC84Kywct_OPiDGIomCiFTywgwCLcBGAs/s1600/docker_build_flow.png 链接到文件: /static/https://4.bp.blogspot.com/-SXeItzMS_oo/WzVemqaj7CI/AAAAAAAAF_w/t5Lau7EOC84Kywct_OPiDGIomCiFTywgwCLcBGAs/s1600/docker_build_flow.png 使用 Page Bundles: false Jib构建流程: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://3.bp.blogspot.com/-_qNyJdVno8E/WzVeqmuC5PI/AAAAAAAAF_0/AHaZ1_ZnJmg8eaUnTlUGyUVe06KRmvlYQCLcBGAs/s1600/jib_build_flow.png 链接到文件: /static/https://3.bp.blogspot.com/-_qNyJdVno8E/WzVeqmuC5PI/AAAAAAAAF_0/AHaZ1_ZnJmg8eaUnTlUGyUVe06KRmvlYQCLcBGAs/s1600/jib_build_flow.png 使用 Page Bundles: false (pic from Google Cloud Platform Blog) 快</description></item><item><title>Spring Cloud Hoxton发布</title><link>https://atbug.com/spring-cloud-hoxton-release/</link><pubDate>Wed, 04 Dec 2019 11:09:07 +0800</pubDate><guid>https://atbug.com/spring-cloud-hoxton-release/</guid><description>
原文 Spring Cloud Hoxton.RELEASE基于Spring Boot 2.2.1.RELEASE 文档变化 Hoxton.RELEASE使用了新的首页, 新的样式以及单页面, 多页面和PDF版本. 新的负载均衡器实现 Hoxton.RELEASE是第一个包含阻塞和非阻塞客户端负载均衡器实现的版本, 替代进入维护状态的Netflix Ribbon. 搭配BlockingLoadBalancerClient使用RestTemplate, 需要在classpath中引入org.springframework.cloud:spring-cloud-loadbalancer. 这个依赖同样用于使用了@LoadBalanced WebClient.Builder的响应式应用中. 唯</description></item><item><title>Docker Engine API on Mac Osx</title><link>https://atbug.com/docker-engine-api-on-mac-osx/</link><pubDate>Wed, 06 Nov 2019 20:19:50 +0800</pubDate><guid>https://atbug.com/docker-engine-api-on-mac-osx/</guid><description>
根据官方的文档Docker Desktop on Mac vs. Docker Toolbox, Docker Desktop on Mac只提供了UNIX socket/var/run/docker.sock, 并未提供tcp的监听(默认2375端口). 如果使用linux的配置方式在Docker Desktop中配置host, Docker Desktop将无法启动. 需要去~/.docker/daemon.json中删除hosts配置才能正常启动. 通过下面的方式暴露出2375的tcp docker run --rm -d -v /var/run/docker.sock:/var/run/docker.sock -p 127.0.0.1:2375:2375 bobrik/socat TCP-LISTEN:2375,fork UNIX-CONNECT:/var/run/docker.sock 然后通过docker version查看当前的docker engine的版本, 比如1.40. 查看官方的Engine API文档: https://docs.docker.com/engine/api/v1.40 搜索个镜像测试一下: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/15729240620185.jpg 链接到文件: /static/https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/15729240620185.jpg 使用 Page Bundles: false</description></item><item><title>Spring Boot 2.2.0 发布</title><link>https://atbug.com/spring-boot-2-2-0-release/</link><pubDate>Tue, 22 Oct 2019 09:27:03 +0800</pubDate><guid>https://atbug.com/spring-boot-2-2-0-release/</guid><description>
译自: https://spring.io/blog/2019/10/16/spring-boot-2-2-0 组件升级 Spring AMQP 2.2 Spring Batch 4.2 Spring Data Moore Spring Framework 5.2 Spring HATEOAS 1.0 Spring Integration 5.2 Spring Kafka 2.3 Spring Security 5.2 Spring Session Corn 第三方库升级 Elasticsearch 6.7 Flyway 6.0 Jackson 2.10 JUnit 5.5 Micrometer 1.3 Reactor Dysprosium Solr 8.0 性能提升 延迟初始化(Lazy initialization) 支持开启全局延迟加载spring.main.lazy-initialization. 代价: 初次处理HTTP请求耗时长 本应在启动初始化时出现的问题, 延后出现 更多参考: https://spring.io/blog/2019/03/14/lazy-initialization-in-spring-boot-2-2 Java 13支持 跟随Spring Framework5.2对Java 13的支持, Spring Boot 2.2现在也支持了Java13. 同时兼容Java 11和8. 不可变的@ConfigurationProperties绑定 现在加入了基于构造器的绑定, 允许@ConfigurationProperties标注的类不可变(属性</description></item><item><title>Zipkin dependencies的坑之二: 心跳超时和Executor OOM</title><link>https://atbug.com/zipkin-dependencies-bug-two-timeout-and-oom/</link><pubDate>Sun, 22 Sep 2019 18:27:37 +0800</pubDate><guid>https://atbug.com/zipkin-dependencies-bug-two-timeout-and-oom/</guid><description>
上回说为了解决吞吐问题, 将zipkin-dependencies的版本升级到了2.3.0. 好景不长, 从某一天开始作业运行报错: Issue communicating with driver in heartbeater org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval ... 19/09/18 08:33:20 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 4) java.lang.OutOfMemoryError: Java heap space ... 解决方案 最新版本(2.3.0)目前不支持额外的spark和elasticsearch-spark的配置, 已经提交了PR 超时的解决方案: 为spark指定配置 spark.executor.heartbeatInterval=600000 spark.network.timeout=600000 OOM解决方案: 根据实际情况通过es.input.max.docs.per.partition配置executor的数量. 调整运行内存及spark.executor.memory</description></item><item><title>Zipkin dependencies的坑之一: 耗时越来越长</title><link>https://atbug.com/zipkin-dependencies-bug-one/</link><pubDate>Sun, 22 Sep 2019 17:59:56 +0800</pubDate><guid>https://atbug.com/zipkin-dependencies-bug-one/</guid><description>
zipkin-dependencies是zipkin调用链的依赖分析工具. 系统上线时使用了当时的最新版本2.0.1, 运行一年之后随着服务的增多, 分析一天的数据耗时越来越多. 从最初的几分钟, 到最慢的几十小时(数据量18m). 最终返现是版本的问题, 升级到&amp;gt;=2.3.0的版本之后吞吐迅速上升. 所以便有了issue: Reminder: do NOT use the version before 2.3.0 但这也引来了另一个坑: 心跳超时和Executor OOM TL;DR 简单浏览了下zipkin-dependencies的源码, 2.0.1和2.3.2的比较大的差距是依赖的elasticsearch-spark的版本. 前者用的是6.3.2, 后者是7.3.0. 尝试在zipkin-depe</description></item><item><title>如何选择Kafka Topic的分区数</title><link>https://atbug.com/how-to-choose-topic-partition-count-number-kafka/</link><pubDate>Fri, 30 Aug 2019 11:10:46 +0800</pubDate><guid>https://atbug.com/how-to-choose-topic-partition-count-number-kafka/</guid><description>
在kafka中, topic的分区是并行计算的单元. 在producer端和broker端, 可以同时并发的写数据到不同的分区中. 在consumer端, Kafka总是将某个分区分配个一个consumer线程. 因此同一个消费组内的并行度与分区数息息相关. Partition分区数的大小, 更多直接影响到消费端的吞吐(一个分区只能同一消费组的一个消费者消费). 分区数小, 消费端的吞吐就低. 但是太大也会有其他的影响 原则: 更多的分区可提高吞吐量 分区数越多打开的文件句柄越多 分区数越多降低可用性 更多的分区增加端到端的延迟 客户端需要更多的内存 归根结底还是得有个度. 如何找出这个度? 有个粗略的计算公式: max(t/p, t/c). t就是所预期吞吐</description></item><item><title>博客最近半年没什么产出</title><link>https://atbug.com/no-output-in-past-half-year/</link><pubDate>Tue, 27 Aug 2019 14:29:12 +0000</pubDate><guid>https://atbug.com/no-output-in-past-half-year/</guid><description>
上一篇日志更新还是在去年的12月, 至今有差不多10个月没有更新了. 不是说没有东西可写, 而且想写的东西很多. 工作太忙, 不忙的时候又太懒, 归根结底还是太懒. 过去一年多都是在做基础架构方面的工作, 围绕技术中台展开的. 有很多技术需要去学习, 也有很多问题要处理. 过程中一直有记笔记的习惯, 所以可以写的东西很多. 不过有些属于公司的部分还是不能写的, 必要的职业道德还是要有的. 笔记记录一直在用MWeb, 并使用iCloud同步, 最近几个月也在结合幕布整理思路和工作安排. 好用的软件我也比较喜欢分享, 记得最早在Workpress上的博客就分享了很多自己常用的软件. (有点扯远了~~~) MWeb没有统计功能, 还有使用的是</description></item><item><title>Spring Boot源码分析 - Configuration注解</title><link>https://atbug.com/spring-boot-configuration-annotation/</link><pubDate>Mon, 10 Dec 2018 16:24:33 +0000</pubDate><guid>https://atbug.com/spring-boot-configuration-annotation/</guid><description>
@Configuration注解 @Configuration注解指示一个类声明一个或多个@Bean方法, 并且可以由Spring容器处理, 以在运行时为这些bean生成bean定义和服务请求. 使用ConfigurationClassParser来对@Configuration标注的类进行解析, 封装成ConfigurationClass实例. 具体的实现通过ConfigurationClassPostProcessor来实现的. ConfigurationClassPostProcessor 实现了BeanDefinitionRegistryPostProcessor接口, 间接实现了BeanFactorPostProcessor接口. #postProcessBeanDefinitionRegistry(): 注册所有Configurat</description></item><item><title>Alpine容器安装Docker和OpenShift Client Tools</title><link>https://atbug.com/install-docker-and-openshift-client-tools-in-alpine-container/</link><pubDate>Tue, 28 Aug 2018 09:14:12 +0000</pubDate><guid>https://atbug.com/install-docker-and-openshift-client-tools-in-alpine-container/</guid><description>
安装Docker echo &amp;#34;http://dl-2.alpinelinux.org/alpine/edge/main&amp;#34; &amp;gt; /etc/apk/repositories echo &amp;#34;http://dl-2.alpinelinux.org/alpine/edge/community&amp;#34; &amp;gt;&amp;gt; /etc/apk/repositories echo &amp;#34;http://dl-2.alpinelinux.org/alpine/edge/testing&amp;#34; &amp;gt;&amp;gt; /etc/apk/repositories apk -U --no-cache \ --allow-untrusted add \ shadow \ docker \ py-pip \ openrc \ &amp;amp;&amp;amp; pip install docker-compose rc-update add docker boot 安装OpenShift Client Tools 需要先安装glibc apk --no-cache add ca-certificates wget wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.28-r0/glibc-2.28-r0.apk apk add glibc-2.28-r0.apk curl --retry 7 -Lo /tmp/client-tools.tar.gz &amp;quot;https://mirror.openshift.com/pub/openshift-v3/clients/3.9.1/linux/oc.tar.gz&amp;quot; curl --retry 7 -Lo /tmp/client-tools.tar.gz &amp;#34;https://mirror.openshift.com/pub/openshift-v3/clients/3.9.1/linux/oc.tar.gz&amp;#34; tar zxf /tmp/client-tools.tar.gz -C /usr/local/bin oc \ &amp;amp;&amp;amp; rm /tmp/client-tools.tar.gz \ &amp;amp;&amp;amp; apk del .build-deps # ADDED: Resolve issue x509 oc login issue apk add --update ca-certificates 参考: github issue</description></item><item><title>Zuul网关Ribbon重试</title><link>https://atbug.com/ribbon-retry-in-zuul/</link><pubDate>Thu, 02 Aug 2018 08:55:43 +0000</pubDate><guid>https://atbug.com/ribbon-retry-in-zuul/</guid><description>
相关配置 #如果路由转发请求发生超时(连接超时或处理超时), 只要超时时间的设置小于Hystrix的命令超时时间,那么它就会自动发起重试. 默认为false. 或者对指定响应状态码进行重试 zuul.retryable = true zuul.routes.&amp;lt;route&amp;gt;.retryable = false #同一实例上的最大重试次数, 默认值为0. 不包括首次调用 ribbon.MaxAutoRetries=0 #重试其他实例的最大重试次数, 不包括第一次选的实例. 默认为1 ribbon.MaxAutoRetriesNextServer=1 #是否所有操作执行重试, 默认值为false, 只重试`GET`请求 ribbon.OkToRetryOnAllOperations=false #连接超时, 默认2000 ribbon.ConnectTimeout=15000 #响应超时, 默认5000 ribbon.ReadTimeout=15000 #每个host的最大连接数 ribbon.MaxHttpConnectionsPerHost=50 #最大连接数 ribbon.MaxTotalHttpConnections=200 #何种响应状态码才进行重试 ribbon.retryableStatusCodes=404,502 实现 SimpleRouteLocator#getRoute返回的route对象中会带上retryabl</description></item><item><title>Hystrix工作原理三</title><link>https://atbug.com/hystrix-exception-handling/</link><pubDate>Sun, 24 Jun 2018 16:20:16 +0000</pubDate><guid>https://atbug.com/hystrix-exception-handling/</guid><description>
异常处理 Hystrix异常类型 HystrixRuntimeException HystrixBadRequestException HystrixTimeoutException RejectedExecutionException HystrixRuntimeException HystrixCommand失败时抛出, 不会触发fallback. HystrixBadRequestException 用提供的参数或状态表示错误的异常, 而不是执行失败. 与其他HystrixCommand抛出的异常不同, 这个异常不会触发fallback, 也不会记录进failure的指标, 因而也不会触发断路器, 应该在用户输入引起的错误是抛出, 否则会它与容错和后退行为的目的相悖. 不会触发fallback, 也不会记录到错误的指标中, 也不会触发断路器. RejectedExecutionException 线程池发生reject时抛出 HystrixTimeoutException 在HystrixCommand.run()或者HystrixObservableCommand.construct()时抛出, 会记录</description></item><item><title>Hystrix工作原理二</title><link>https://atbug.com/hystrix-isolation/</link><pubDate>Sun, 24 Jun 2018 16:18:52 +0000</pubDate><guid>https://atbug.com/hystrix-isolation/</guid><description>
隔离策略 线程和线程池 客户端(库, 网络调用等)在各自的线程上运行. 这种做法将他们与调用线程隔开, 因此调用者可以从一个耗时的依赖调用&amp;quot;离开(walk away)&amp;quot; Hystrix使用单独的, 每个依赖的线程池作为约束任何给定依赖的一种方式, 因此潜在执行的延迟将仅在该池中使可用线程饱和. Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15280741661560.png 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15280741661560.png 使用 Page Bundles: false 如果不试用线程池可以保护你免受故障的影响, 但是这需要客户端可信任地快速失败(网络连接/读取超时, 重试的配置)并始终表现良好. 在Hystrix的设计中, Netflix选择试用线程和线程池来达到隔离的目的, 原因有: 很多应用程序调用了由很多不同的团队开发的许</description></item><item><title>Hystrix工作原理一</title><link>https://atbug.com/how-hystrix-works/</link><pubDate>Mon, 04 Jun 2018 08:47:40 +0000</pubDate><guid>https://atbug.com/how-hystrix-works/</guid><description>
运行时的流程图 Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15273755001891.png 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15273755001891.png 使用 Page Bundles: false 构建HystrixCommand或者HystrixObservableCommand对象 第一步是构建一个HystrixCommand或HystrixObservableCommand对象来代表对依赖服务所做的请求。 将在请求发生时将需要的任何参数传递给构造函数。 如果依赖的服务预期会返回单一的响应, 构造一个HystrixCommand对象, 例如: HystrixCommand command = new HystrixCommand(arg1, arg2); 如果依赖的服务预期会返回一个发出响应的Observable对象, 则构造一个HystrixObservableCommand对象, 例如: HystrixObservableCommand command = new HystrixObservableCommand(arg1, arg2); 执行Comm</description></item><item><title>解决rsyslogd资源占用率高问题</title><link>https://atbug.com/rsyslogd-high-cpu-trouble-shooting/</link><pubDate>Fri, 01 Jun 2018 09:32:28 +0000</pubDate><guid>https://atbug.com/rsyslogd-high-cpu-trouble-shooting/</guid><description>
rsyslogd资源占用高问题记录 问题: Image not found a.warning-link { color: inherit !important; font-weight: inherit !important; text-decoration: underline !important; border-bottom: none !important; } 网站链接: http://7xvxng.com1.z0.glb.clouddn.com/15277280296373.jpg 链接到文件: /static/http://7xvxng.com1.z0.glb.clouddn.com/15277280296373.jpg 使用 Page Bundles: false openshift集群安装在esxi的虚拟机上. 各个节点出现问题, 集群响应很慢. kswapd0进程cpu 90%多. rsyslogd进程内存 90%多. **先上总结: ** system-journal服务监听/dev/logsocket获取日志, 保存在内存中, 并间歇性的写入/var/log/journal目录中. rsyslog服务启动后监听/run/systemd/journal/syslogsocket获取syslog类型日志, 并写入/var/log/messages文件中. 获取日志时需要</description></item><item><title>Kubernetes中的Nginx动态解析</title><link>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</link><pubDate>Wed, 30 May 2018 12:10:32 +0000</pubDate><guid>https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/</guid><description>
背景 Nginx运行在kubernets中, 反向代理service提供服务. kubernetes版本v1.9.1+a0ce1bc657. 问题: 配置如下: location ^~/info { proxy_pass: http://serviceName:port; } 删除并重建Service的时候, nginx会出现下面的问题: connect() failed (113: No route to host) &amp;hellip; upstream: &amp;ldquo;xxxxx&amp;rdquo; 分析 通过google发现, 是nginx的dns解析方案的问题. nginx官方的说明: If the domain name can’t be resolved, NGINX fails to start or reload its configuration. NGINX caches the DNS records until the next restart or configuration reload, ignoring the records’ TTL values. We can’t specify another load‑balancing algorithm, nor can we configure passive health checks or other features defined by parameters to the server directive, which we’ll describe in the next section. 意思是说, nginx在启动的时候就会解析proxy_pass后的域名, 并把ip缓存下来</description></item><item><title>Spring Cloud Ribbon 详解</title><link>https://atbug.com/spring-cloud-ribbon-breakdown-1/</link><pubDate>Sat, 05 May 2018 11:18:05 +0000</pubDate><guid>https://atbug.com/spring-cloud-ribbon-breakdown-1/</guid><description>
&lt;p>客户端负载均衡, Ribbon的核心概念是命名的客户端.&lt;/p>
&lt;h2 id="使用">使用&lt;/h2>
&lt;h3 id="引入ribbon依赖和配置">引入Ribbon依赖和配置&lt;/h3>
&lt;p>加入&lt;code>spring-cloud-starter-netflix-ribbon&lt;/code>依赖&lt;/p>
&lt;h3 id="代码中使用ribbonclient注解">代码中使用RibbonClient注解&lt;/h3>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span>
&lt;span class="nd">@RibbonClient&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s">&amp;#34;foo&amp;#34;&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">configuration&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">FooConfiguration&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">class&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">TestConfiguration&lt;/span> &lt;span class="o">{}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span> &lt;span class="kd">protected&lt;/span> &lt;span class="kd">static&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">FooConfiguration&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">ZonePreferenceServerListFilter&lt;/span> &lt;span class="nf">serverListFilter&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="n">ZonePreferenceServerListFilter&lt;/span> &lt;span class="n">filter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">ZonePreferenceServerListFilter&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="n">filter&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="na">setZone&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="s">&amp;#34;myTestZone&amp;#34;&lt;/span>&lt;span class="o">);&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="n">filter&lt;/span>&lt;span class="o">;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">IPing&lt;/span> &lt;span class="nf">ribbonPing&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">PingUrl&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ribbon客户端的配置, 如果不指定会使用默认的实现:&lt;/p>
&lt;ul>
&lt;li>IClientConfig 客户端相关配置&lt;/li>
&lt;li>IRule 定义负载均衡策略&lt;/li>
&lt;li>IPing 定义如何ping目标服务实例来判断是否存活, ribbon使用单独的线程每隔一段时间(默认10s)对本地缓存的ServerList做一次检查&lt;/li>
&lt;li>ServerList&lt;!-- raw HTML omitted --> 定义如何获取服务实例列表. 两种实现基于配置的&lt;code>ConfigurationBasedServerList&lt;/code>和基于Eureka服务发现的&lt;code>DiscoveryEnabledNIWSServerList&lt;/code>&lt;/li>
&lt;li>ServerListFilter&lt;!-- raw HTML omitted --> 用来使用期望的特征过滤静态配置动态获得的候选服务实例列表. 若未提供, 默认使用&lt;code>ZoneAffinityServerListFilter&lt;/code>&lt;/li>
&lt;li>ILoadBalancer 定义了软负载均衡器的操作的接口. 一个典型的负载均衡器至少需要一组用来做负载均衡的服务实例, 一个标记某个服务实例不在旋转中的方法, 和对应的方法调用从实例列表中选出某一个服务实例.&lt;/li>
&lt;li>ServerListUpdater DynamicServerListLoadBalancer用来更新实例列表的策略(推&lt;code>EurekaNotificationServerListUpdater&lt;/code>/拉&lt;code>PollingServerListUpdater&lt;/code>, 默认是拉)&lt;/li>
&lt;/ul></description></item><item><title>KVM安装手册</title><link>https://atbug.com/kvm-installation-note/</link><pubDate>Thu, 12 Apr 2018 12:45:15 +0000</pubDate><guid>https://atbug.com/kvm-installation-note/</guid><description>
&lt;h2 id="添加虚拟机流程">添加虚拟机流程：&lt;/h2>
&lt;pre>&lt;code>1. 配置网络
2. 配置存储池
3. 上传镜像
4. 安装虚拟机，指定配置
&lt;/code>&lt;/pre>
&lt;h3 id="安装kvm虚拟机">安装KVM虚拟机&lt;/h3>
&lt;h4 id="1-关闭防火墙selinux">1. 关闭防火墙，selinux&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># service iptables stop&lt;/span>
&lt;span class="c1"># setenforce 0 临时关闭&lt;/span>
&lt;span class="c1"># chkconfig NetworkManager off&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="2-安装kvm虚拟机">2. 安装kvm虚拟机&lt;/h4>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="c1"># yum install kvm libvirt libvirt-devel python-virtinst python-virtinst qemu-kvm virt-viewer bridge-utils virt-top libguestfs-tools ca-certificates audit-libs-python device-mapper-libs virt-install&lt;/span>
&lt;span class="c1"># 启动服务&lt;/span>
&lt;span class="c1"># service libvirtd restart&lt;/span>
下载virtio-win-1.5.2-1.el6.noarch.rpm 如果不安装window虚拟机或者使用带virtio驱动的镜像可以不用安装
&lt;span class="c1"># rpm -ivh virtio-win-1.5.2-1.el6.noarch.rpm&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3-libvirt在管理本地或远程hypervisor时的表现形式如下">3. Libvirt在管理本地或远程Hypervisor时的表现形式如下。&lt;/h4>
&lt;p>在libvirt内部管理了五部分：&lt;/p>
&lt;ul>
&lt;li>节点：所谓的节点就是我们的物理服务器，一个服务器代表一个节点，上边存放着Hyper和Domain&lt;/li>
&lt;li>Hypervisor：即VMM，指虚拟机的监控程序，在KVM中是一个加载了kvm.ko的标准Linux系统。&lt;/li>
&lt;li>域（Domain）：指虚拟机，一个域代表一个虚拟机（估计思路来源于Xen的Domain0）&lt;/li>
&lt;li>存储池（Storage Pool）：存储空间，支持多种协议和网络存储。作为虚拟机磁盘的存储源。&lt;/li>
&lt;li>卷组（Volume）：虚拟机磁盘在Host上的表现形式。
上边的五部分，我们必须使用的是前三个，因为很多时候根据业务规则或应用的灵活性并没有使用卷组（其实就是有了编制的虚拟磁盘文件），也就没有必要使用存储池。&lt;/li>
&lt;/ul></description></item><item><title>启用Jenkins CLI</title><link>https://atbug.com/jenkins-cli-enable/</link><pubDate>Mon, 09 Apr 2018 11:16:38 +0000</pubDate><guid>https://atbug.com/jenkins-cli-enable/</guid><description>
Jenkins CLI提供了SSH和Client模式. Docker运行Jenkins version:&amp;#39;3&amp;#39;services:jenkins:image:jenkins/jenkins:alpineports:- 8080:8080- 50000:50000- 46059:46059volumes:- &amp;#34;/Users/addo/DevApps/Docker/data/jenkins:/var/jenkins_home&amp;#34;note: 以为是docker运行, ssh端口设置选用了固定端口. Client 从http://JENKINS_URL/cli页面下载client jar 使用方法: java -jar jenkins-cli.jar -s http://localhost:8080/ help 构建: java -jar jenkins-cli.jar -s http://localhost:8080/ build JOB [-c] [-f] [-p] [-r N] [-s] [-v] [-w] Starts a build, and optionally waits for a completion. Aside from general scripting use, this command can be used to invoke another job from within a build of one job. With the -s option, this command changes the exit code based on the outcome of the build (exit code 0 indicates a success) and interrupting the command will interrupt the job. With the -f option, this command changes the exit code based on the outcome of the build (exit code 0 indicates a success) however, unlike -s, interrupting the command will not interrupt the job (exit code 125 indicates the command was interrupted). With the -c option, a build will only run if there has been an SCM change. JOB : Name of the job to build -c : Check for SCM changes before starting the build, and if there&amp;#39;s no change, exit without doing a build -f : Follow the build progress. Like -s only interrupts are not passed through to the build. -p : Specify the build</description></item><item><title>Jenkins - 解决execute shell中启动的进程被在Job退出时被杀死问题</title><link>https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/</link><pubDate>Thu, 15 Mar 2018 17:00:25 +0000</pubDate><guid>https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/</guid><description>
因为ProcessTreeKiller的存在, 构建过程中使用shell启动的进程在Job完成时都会被kill掉. 各种搜索以及ProcessTreeKiller提供的解决方式是修改BUILD_ID和添加 -Dhudson.util.ProcessTree.disable=true都无法解决. 最后参考StackOverflow和Jenkins JIRA, 修改JENKINS_NODE_COOKIE为任何值, 如dontKillMe. 这种方法可以解决, 记录一下. (搜索排名靠前的结果都不对).</description></item><item><title>MacOS安装minishift</title><link>https://atbug.com/install-minishift-on-mac/</link><pubDate>Fri, 23 Feb 2018 15:32:26 +0000</pubDate><guid>https://atbug.com/install-minishift-on-mac/</guid><description>
MacOS环境安装minishift 安装minishift cli brew cask install minishift 使用virtualbox安装 安装的时候可以指定HTTP代理, 拉取墙外镜像时需要; 还可以指定insecure的镜像库. minishift start --docker-env HTTP_PROXY=&amp;#34;192.168.99.1:1087&amp;#34; --docker-env HTTPS_PROXY=&amp;#34;192.168.99.1:1087&amp;#34; --docker-env NO_PROXY=&amp;#34;192.168.0.0/16,172.30.0.0/16&amp;#34; --insecure-registry=&amp;#34;192.168.1.34&amp;#34; --vm-driver=virtualbox 启动 minishift start --vm-driver=virtualbox 删除 minishift delete 打开Openshift控制面板 minishift dashboard 获取集群ip地址 minishift ip 安装Openshift Cli brew install openshift-cli 可以使用openshift cli进行操作. minishift安装完成后会将配置信息写入到主机的用户目录下, $HOME/.kube目录下除了config信息, 还有openshift的集群信息及支持的api. oc login -u system:admin oc get pods --all-namespaces</description></item><item><title>Spring Cloud Zuul详解</title><link>https://atbug.com/spring-cloud-zuul-breakdown/</link><pubDate>Thu, 22 Feb 2018 17:02:26 +0000</pubDate><guid>https://atbug.com/spring-cloud-zuul-breakdown/</guid><description>
&lt;p>Spring Cloud对Netflix Zuul做了封装集成, 使得在Spring Cloud环境中使用Zuul更方便. Netflix Zuul相关分析请看&lt;a href="http://atbug.com/learn-netflix-zuul/">上一篇&lt;/a>.&lt;/p>
&lt;h2 id="实现">实现&lt;/h2>
&lt;p>@EnableZuulProxy 与 @EnableZuulServer
二者的区别在于前者使用了服务发现作为路由寻址, 并使用Ribbon做客户端的负载均衡; 后者没有使用.
Zuul server的路由都通过&lt;code>ZuulProperties&lt;/code>进行配置.&lt;/p>
&lt;h3 id="具体实现">具体实现:&lt;/h3>
&lt;ol>
&lt;li>使用&lt;code>ZuulController&lt;/code>(&lt;code>ServletWrappingController&lt;/code>的子类)封装&lt;code>ZuulServlet&lt;/code>实例, 处理从&lt;code>DispatcherServlet&lt;/code>进来的请求.&lt;/li>
&lt;li>&lt;code>ZuulHandlerMapping&lt;/code>负责注册handler mapping, 将&lt;code>Route&lt;/code>的&lt;code>fullPath&lt;/code>的请求交由&lt;code>ZuulController&lt;/code>处理.&lt;/li>
&lt;li>同时使用&lt;code>ServletRegistrationBean&lt;/code>注册&lt;code>ZuulServlet&lt;/code>, 默认使用&lt;code>/zuul&lt;/code>作为urlMapping. 所有来自以&lt;code>/zuul&lt;/code>开头的path的请求都会直接进入&lt;code>ZuulServlet&lt;/code>, 不会进入&lt;code>DispatcherServlet&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h4 id="使用注解">使用注解&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>&lt;code>@EnableZuulProxy&lt;/code>引入了&lt;code>ZuulProxyMarkerConfiguration&lt;/code>, &lt;code>ZuulProxyMarkerConfiguration&lt;/code>只做了一件事, 实例化了内部类&lt;code>Marker&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">ZuulProxyMarkerConfiguration&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">Marker&lt;/span> &lt;span class="nf">zuulProxyMarkerBean&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Marker&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="kd">class&lt;/span> &lt;span class="nc">Marker&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;code>@EnableZuulServer&lt;/code>引入了&lt;code>ZuulServerMarkerConfiguration&lt;/code>, &lt;code>ZuulServerMarkerConfiguration&lt;/code>也只做了一件事: 实例化了内部类&lt;code>Marker&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="nd">@Configuration&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="kd">class&lt;/span> &lt;span class="nc">ZuulServerMarkerConfiguration&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="nd">@Bean&lt;/span>
&lt;span class="kd">public&lt;/span> &lt;span class="n">Marker&lt;/span> &lt;span class="nf">zuulServerMarkerBean&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="k">return&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="n">Marker&lt;/span>&lt;span class="o">();&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="kd">class&lt;/span> &lt;span class="nc">Marker&lt;/span> &lt;span class="o">{&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul></description></item><item><title>Spring Cloud - Eureka服务注册</title><link>https://atbug.com/spring-cloud-service-registry-via-eureka/</link><pubDate>Wed, 14 Feb 2018 07:32:43 +0000</pubDate><guid>https://atbug.com/spring-cloud-service-registry-via-eureka/</guid><description>
&lt;p>之前分析过&lt;a href="http://atbug.com/spring-cloud-eureka-client-source-code-analysis/">Spring Cloud的Eureka服务发现&lt;/a>, 今天分析一下服务注册.&lt;/p>
&lt;h2 id="配置">配置&lt;/h2>
&lt;h3 id="bootstrapconfiguration">BootstrapConfiguration&lt;/h3>
&lt;h4 id="eurekadiscoveryclientconfigservicebootstrapconfiguration">EurekaDiscoveryClientConfigServiceBootstrapConfiguration&lt;/h4>
&lt;p>spring-cloud-config环境中使用的配置&lt;/p>
&lt;p>引入&lt;code>EurekaDiscoveryClientConfiguration&lt;/code>和&lt;code>EurekaClientAutoConfiguration&lt;/code>&lt;/p>
&lt;h5 id="eurekadiscoveryclientconfiguration">EurekaDiscoveryClientConfiguration&lt;/h5>
&lt;ol>
&lt;li>在spring-cloud中(通过是否存在RefreshScopeRefreshedEvent.class判断), 添加&lt;code>RefreshScopeRefreshedEvent&lt;/code>的listener. 收到事件后重新注册实例.&lt;/li>
&lt;li>在&lt;code>eureka.client.healthcheck.enabled&lt;/code>设置为true时, 注册&lt;code>EurekaHealthCheckHandler&lt;/code>bean. &lt;code>EurekaHealthCheckHandler&lt;/code>负责将应用状态映射为实例状态&lt;code>InstanceStatus&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h5 id="eurekaclientautoconfiguration">EurekaClientAutoConfiguration&lt;/h5>
&lt;p>支持spring-cloud和非spring-cloud环境, 在spring-cloud环境中, 下面两个bean要使用&lt;code>@RefreshScope&lt;/code>标注&lt;/p>
&lt;ol>
&lt;li>实例化&lt;code>EurekaClient&lt;/code>bean, 在spring-cloud中使用实现类&lt;code>CloudEurekaClient&lt;/code>.&lt;/li>
&lt;li>使用&lt;code>EurekaInstanceConfig&lt;/code>实例, 实例化&lt;code>ApplicationInfoManager&lt;/code>bean&lt;/li>
&lt;/ol></description></item><item><title>初识Netflix Zuul</title><link>https://atbug.com/learn-netflix-zuul/</link><pubDate>Sun, 11 Feb 2018 10:07:18 +0000</pubDate><guid>https://atbug.com/learn-netflix-zuul/</guid><description>
&lt;p>嵌入式的zuul代理&lt;/p>
&lt;p>使用了Netfilx OSS的其他组件:&lt;/p>
&lt;ul>
&lt;li>Hystrix 熔断&lt;/li>
&lt;li>Ribbon 负责发送外出请求的客户端, 提供软件负载均衡功能&lt;/li>
&lt;li>Trubine 实时地聚合细粒度的metrics数据&lt;/li>
&lt;li>Archaius 动态配置&lt;/li>
&lt;/ul>
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>由于2.0停止开发且会有bug, 故下面的分析基于1.x版本.&lt;/p>
&lt;h3 id="特性">特性&lt;/h3>
&lt;ul>
&lt;li>Authentication 认证&lt;/li>
&lt;li>Insights 洞察&lt;/li>
&lt;li>Stress Testing 压力测试&lt;/li>
&lt;li>Canary Testing 金丝雀测试&lt;/li>
&lt;li>Dynamic Routing 动态路由&lt;/li>
&lt;li>Multi-Region Resiliency 多区域弹性&lt;/li>
&lt;li>Load Shedding 负载脱落&lt;/li>
&lt;li>Security 安全&lt;/li>
&lt;li>Static Response handling 静态响应处理&lt;/li>
&lt;li>Multi-Region Resiliency 主动/主动流量管理&lt;/li>
&lt;/ul></description></item><item><title>ConfigurationProperties到底需不需要getter</title><link>https://atbug.com/configurationproperties-requires-getter-or-not/</link><pubDate>Wed, 07 Feb 2018 15:53:21 +0000</pubDate><guid>https://atbug.com/configurationproperties-requires-getter-or-not/</guid><description>
为什么要讨论这个问题, 工作中一个同事写的类使用了ConfigurationProperties, 只提供了标准的setter方法. 属性的访问, 提供了定制的方法. 可以参考EurekaClientConfigBean. 他使用的是spring boot 2.0.0.M5版本, 可以正常获取配置文件中的属性值, 但是在1.5.8.RELEASE获取不到. 看下文档和源码: Annotation for externalized configuration. Add this to a class definition or a @Bean method in a @Configuration class if you want to bind and validate some external Properties (e.g. from a .properties file). 外置配置的注解. 当需要绑定外置配置(如properties或者yaml配置)的时候, 将其加到使用了@Configuration注解的类声明处或者@Bean标注的方法上. 值的绑定是通过Co</description></item><item><title>自定义GOPATH下安装godep失败</title><link>https://atbug.com/install-godep-issue-in-custom-gopath/</link><pubDate>Fri, 22 Dec 2017 13:02:38 +0000</pubDate><guid>https://atbug.com/install-godep-issue-in-custom-gopath/</guid><description>
我的环境变量是这样的: export GOROOT=/usr/local/go export GOPATH=/Users/addo/Workspaces/go_w export GOBIN=$GOROOT/bin export PATH=$PATH:$GOBIN 使用下面的命令安装报错: go get -v github.com/tools/godep github.com/tools/godep (download) github.com/tools/godep/vendor/github.com/pmezard/go-difflib/difflib github.com/tools/godep/vendor/github.com/kr/fs github.com/tools/godep/vendor/github.com/kr/text github.com/tools/godep/vendor/golang.org/x/tools/go/vcs github.com/tools/godep/vendor/github.com/kr/pretty github.com/tools/godep go install github.com/tools/godep: open /usr/local/go/bin/godep: permission denied 默认是安装到$GOBIN目录下, 权限不够. 使用: sudo go get -v github.com/tools/godep sudo go get -v github.com/tools/godep github.com/tools/godep (download) created GOPATH=/Users/addo/go; see &amp;lsquo;go help gopath&amp;rsquo; github.com/tools/godep/vendor/github.com/kr/fs github.com/tools/godep/vendor/github.com/kr/text github.com/tools/godep/vendor/github.com/pmezard/go-difflib/difflib github.com/tools/godep/vendor/golang.org/x/tools/go/vcs github.com/tools/godep/vendor/github.com/kr/pretty github.com/tools/godep $GOBIN并没有找到godef. 输出提示created GOPATH=/Users/addo/go; . 因为sudo的时候找不到GOPATH变量, 便重新创建了目录. 解决方案一: 临时修改GOBIN: export GOBIN=$GOPATH/bin 运行go get github.com/tools/godep 将生成的godef复制到GOROOT/bin下 回滚修改export GOBIN=$GOROOT/bin; export PATH=$PATH:$GOBIN 解决方案二: 修改GOROOT/bin的属组属主, 安全性问题, 不推荐.</description></item><item><title>Kafka的消息可靠传递</title><link>https://atbug.com/kafka-reliable-data-delivery/</link><pubDate>Sat, 18 Nov 2017 14:01:46 +0000</pubDate><guid>https://atbug.com/kafka-reliable-data-delivery/</guid><description>
Kafka提供的基础保障可以用来构建可靠的系统, 却无法保证完全可靠. 需要在可靠性和吞吐之间做取舍. Kafka在分区上提供了消息的顺序保证. 生产的消息在写入到所有的同步分区上后被认为是已提交 (不需要刷到硬盘). 生产者可以选择在消息提交完成后接收broker的确认, 是写入leader之后, 或者所有的副本 只要有一个副本存在, 提交的消息就不会丢失 消费者只能读取到已提交的消息 复制 Kafka的复制机制保证每个分区有多个副本, 每个副本可以作为leader或者follower的角色存在. 为了保证副本的同步, 需要做到: 保持到zk的连接会话: 每隔6s向zk发送心跳, 时间可配置 每隔10s向leader拉取消息, 时间</description></item><item><title>Spring Cloud - Eureka Client源码分析</title><link>https://atbug.com/spring-cloud-eureka-client-source-code-analysis/</link><pubDate>Sat, 14 Oct 2017 22:04:59 +0000</pubDate><guid>https://atbug.com/spring-cloud-eureka-client-source-code-analysis/</guid><description>
准备做个Spring Cloud源码分析系列, 作为Spring Cloud的源码分析笔记. 这一篇是Eureka的客户端. 客户端 两种方式, 最终的实现基本一样. 显示指定服务发现的实现类型 使用@EnableEurekaClient注解显示的指定使用Eureka作为服务发现的实现, 并实例化EurekaClient实例. 实际上使用的是@EnableDiscoveryClient注解. @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @EnableDiscoveryClient public @interface EnableEurekaClient { } 动态配置实现 使用@EnableDiscoveryClient注解来配置服务发现的实现. 源码分析 EnableDiscoveryClient @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Import(EnableDiscoveryClientImportSelector.class) public @interface EnableDiscoveryClient { } EnableDiscoveryClient注解的作用主要是用来引入EnableDiscove</description></item><item><title>MetaspaceSize的坑</title><link>https://atbug.com/java8-metaspace-size-issue/</link><pubDate>Thu, 13 Apr 2017 11:55:14 +0000</pubDate><guid>https://atbug.com/java8-metaspace-size-issue/</guid><description>
这几天生产上有台机器的Metaspace一直在告警，Metaspace使用达到了97%。使用-XX:MetaspaceSize=512m，告警也还在在持续，查看MC只有81536.0，显然这个参数没起作用。 也有人遇到类似的问题，并在openjdk上提过类似的bug，其实是一个注释的bug，最终在JDK-8151845中修复了。 Class metadata is deallocated when the corresponding Java class is unloaded. Java classes are unloaded as a result of garbage collection, and garbage collections may be induced in order to unload classes and deallocate class metadata. When the space committed for class metadata reaches a certain level (a high-water mark), a garbage collection is induced. After the garbage collection, the high-water mark may be raised or lowered depending on the amount of space freed from class metadata. The high-water mark would be raised so as not to induce another garbage collection too soon. The high-water mark is initially set to the value of the command-line option MetaspaceSize. It is raised or lowered based on the options MaxMetaspaceFreeRatio and MinMetaspaceFreeRatio. If the committed space available for class metadata as a percentage of the total committed space for</description></item><item><title>Redis清理缓存</title><link>https://atbug.com/clean-speicified-keys-in-redis/</link><pubDate>Tue, 13 Dec 2016 16:54:41 +0000</pubDate><guid>https://atbug.com/clean-speicified-keys-in-redis/</guid><description>
最近有个需求需要主动的去清理部分缓存，考虑的原子性的问题，用Lua脚本进行实现。 Lua脚本 local count = 0 for _,k in ipairs(redis.call(&amp;#39;KEYS&amp;#39;, ARGV[1])) do redis.call(&amp;#39;DEL&amp;#39;, k) count = count + 1 end return count shell运行 redis-cli --eval file.lua ,[KEY PATTERN] #sample: 清理所有key以Test开头的记录 redis-cli --eval clear.lua , Test* Java Jedis jedis = new Jedis(&amp;#34;127.0.0.1&amp;#34;, 6379); URL resource = Resources.getResource(&amp;#34;META-INF/scripts/clear.lua&amp;#34;); String lua = Resources.toString(resource, Charsets.UTF_8); Object eval = jedis.eval(lua, 0, &amp;#34;Name*&amp;#34;); System.out.println(eval);</description></item></channel></rss>