<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>翻译 on 乱世浮生</title><link>https://atbug.com/categories/%E7%BF%BB%E8%AF%91/</link><description>Recent content in 翻译 on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 22 Jan 2022 10:15:31 +0800</lastBuildDate><atom:link href="https://atbug.com/categories/%E7%BF%BB%E8%AF%91/index.xml" rel="self" type="application/rss+xml"/><item><title>追踪 Kubernetes 中的网络流量</title><link>https://atbug.com/tracing-path-of-kubernetes-network-packets/</link><pubDate>Sat, 22 Jan 2022 10:15:31 +0800</pubDate><guid>https://atbug.com/tracing-path-of-kubernetes-network-packets/</guid><description>
&lt;p>译者注：&lt;/p>
&lt;p>这篇文章很全面的罗列出了 Kubernetes 中涉及的网络知识，从 Linux 内核的网络内容，到容器、Kubernetes，一一进行了详细的说明。&lt;/p>
&lt;p>​文章篇幅有点长，不得不说，网络是很复杂很麻烦的一层，但恰恰这层多年来变化不大。希望翻译的内容对大家能有所帮助，有误的地方，也欢迎大家指正。&lt;/p>
&lt;p>本文翻译获得 Learnk8s 的授权，原文 &lt;a href="https://learnk8s.io/kubernetes-network-packets">Tracing the path of network traffic in Kubernetes&lt;/a> 作者 Kristijan Mitevski。&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427680911539.jpg" alt="">&lt;/p>
&lt;p>&lt;strong>TL;DR：&lt;/strong> &lt;em>本文将代理了解 Kubernetes 集群内外的数据流转。从最初的 Web 请求开始，一直到托管应用程序的容器。&lt;/em>&lt;/p>
&lt;h2 id="目录">目录&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="#%E7%9B%AE%E5%BD%95">目录&lt;/a>&lt;/li>
&lt;li>&lt;a href="#kubernetes-%E7%BD%91%E7%BB%9C%E8%A6%81%E6%B1%82">Kubernetes 网络要求&lt;/a>&lt;/li>
&lt;li>&lt;a href="#linux-%E7%BD%91%E7%BB%9C%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%E5%A6%82%E6%9E%9C%E5%9C%A8-pod-%E4%B8%AD%E5%B7%A5%E4%BD%9C">Linux 网络命名空间如果在 pod 中工作&lt;/a>&lt;/li>
&lt;li>&lt;a href="#pause-%E5%AE%B9%E5%99%A8%E5%88%9B%E5%BB%BA-pod-%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4">Pause 容器创建 Pod 中的网络命名空间&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E4%B8%BA-pod-%E5%88%86%E9%85%8D%E4%BA%86-ip-%E5%9C%B0%E5%9D%80">为 Pod 分配了 IP 地址&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E6%A3%80%E6%9F%A5%E9%9B%86%E7%BE%A4%E4%B8%AD-pod-%E5%88%B0-pod-%E7%9A%84%E6%B5%81%E9%87%8F">检查集群中 pod 到 pod 的流量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#pod-%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%E8%BF%9E%E6%8E%A5%E5%88%B0%E4%BB%A5%E5%A4%AA%E7%BD%91%E6%A1%A5%E6%8E%A5%E5%99%A8">Pod 命名空间连接到以太网桥接器&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E8%B7%9F%E8%B8%AA%E5%90%8C%E4%B8%80%E8%8A%82%E7%82%B9%E4%B8%8A-pod-%E9%97%B4%E7%9A%84%E6%B5%81%E9%87%8F">跟踪同一节点上 pod 间的流量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E8%B7%9F%E8%B8%AA%E4%B8%8D%E5%90%8C%E8%8A%82%E7%82%B9%E4%B8%8A-pod-%E9%97%B4%E7%9A%84%E9%80%9A%E4%BF%A1">跟踪不同节点上 pod 间的通信&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86">位运算的工作原理&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3---cni">容器网络接口 - CNI&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E6%A3%80%E6%9F%A5-pod-%E5%88%B0%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%B5%81%E9%87%8F">检查 pod 到服务的流量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E4%BD%BF%E7%94%A8-netfilter-%E5%92%8C-iptables-%E6%8B%A6%E6%88%AA%E5%92%8C%E9%87%8D%E5%86%99%E6%B5%81%E9%87%8F">使用 Netfilter 和 Iptables 拦截和重写流量&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E6%A3%80%E6%9F%A5%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%93%8D%E5%BA%94">检查服务的响应&lt;/a>&lt;/li>
&lt;li>&lt;a href="#%E5%9B%9E%E9%A1%BE">回顾&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="kubernetes-网络要求">Kubernetes 网络要求&lt;/h2>
&lt;p>在深入了解 Kubernetes 中的数据流转之前，让我们先澄清下 Kubernetes 网络的要求。&lt;/p>
&lt;p>Kubernetes 网络模型定义了一套基本规则：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>集群中的 pod 应该能够与任何其他 pod 自由通信&lt;/strong>，而无需使用网络地址转换（NAT）。&lt;/li>
&lt;li>在不使用 NAT 的情况下，&lt;strong>集群节点上运行的任意程序都应该能够与同一节点上的任意 pod 通信&lt;/strong>。&lt;/li>
&lt;li>&lt;strong>每个 pod 都有自己的 IP 地址&lt;/strong>（IP-per Pod），其他 pod 都可以使用同一个地址进行访问。&lt;/li>
&lt;/ul>
&lt;p>这些要求不会将实现限制在单一方案上。&lt;/p>
&lt;p>相反，他们概括了集群网络的特性。&lt;/p>
&lt;p>在满足这些限制时，必须解决如下&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/network/networking.md">挑战&lt;/a>：&lt;/p>
&lt;ol>
&lt;li>&lt;em>如何保证同一 pod 中的容器间的访问就像在同一主机上一样？&lt;/em>&lt;/li>
&lt;li>&lt;em>Pod 能否访问集群中的其他 pod？&lt;/em>&lt;/li>
&lt;li>&lt;em>Pod 能否访问服务（service）？以及服务可以负载均衡请求吗？&lt;/em>&lt;/li>
&lt;li>&lt;em>Pod 可以接收来自集群外的流量吗？&lt;/em>&lt;/li>
&lt;/ol>
&lt;p>本文将专注于前三点，从 pod 内部网络或者容器间的通信说起。&lt;/p>
&lt;h2 id="linux-网络命名空间如果在-pod-中工作">Linux 网络命名空间如果在 pod 中工作&lt;/h2>
&lt;p>我们想象下，有一个承载应用程序的主容器和另一个与它一起运行的容器。&lt;/p>
&lt;p>在示例 Pod 中有一个 Nginx 容器和 busybox 容器：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Pod&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">multi-container-pod&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">container-1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">busybox&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">command&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;/bin/sh&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;-c&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;sleep 1d&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">container-2&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在部署时，会出现如下情况：&lt;/p>
&lt;ol>
&lt;li>Pod 在节点上得到&lt;strong>自己的网络命名空间&lt;/strong>。&lt;/li>
&lt;li>Pod &lt;strong>分配到一个 IP 地址&lt;/strong>，两个容器间共享端口。&lt;/li>
&lt;li>&lt;strong>两个容器共享同一个网络命名空间&lt;/strong>，在本地互相可见。&lt;/li>
&lt;/ol>
&lt;p>网络配置在后台很快完成。&lt;/p>
&lt;p>然后，我们退后一步，是这理解&lt;em>为什么&lt;/em>上面是容器运行所必须的。&lt;/p>
&lt;p>&lt;a href="https://blog.scottlowe.org/2013/09/04/introducing-linux-network-namespaces/">在 Linux 中，网络命名空间是独立的、隔离的逻辑空间。&lt;/a>&lt;/p>
&lt;p>可以将网络命名空间堪称将物理网络接口分割成更小的独立部分。&lt;/p>
&lt;p>每部分都可以单独配置，并使用自己的网络规则和资源。&lt;/p>
&lt;p>这些可以包括防火墙规则、接口（虚拟或物理）、路由和其他所有与网络相关的内容。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>物理接口持有根命名空间。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427699864958.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>可以使用 Linux 网络命名空间创建隔离的网络。每个网络都是独立的，除非进行配置否则不会与其他命名空间通信。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427700613518.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>物理接口必须处理最后的所有&lt;em>真实&lt;/em>数据包，因此所有的虚拟接口都是从中创建的。&lt;/p>
&lt;p>网络命名空间可以通过 &lt;a href="https://man7.org/linux/man-pages/man8/ip-netns.8.html">&lt;code>ip-netns&lt;/code> 管理工具&lt;/a> 来管理，可以使用 &lt;code>ip netns list&lt;/code> 列出主机上的命名空间。&lt;/p>
&lt;blockquote>
&lt;p>请注意，创建的网络命名空间将会出现在 &lt;code>/var/run/netns&lt;/code> 目录下，但 &lt;a href="https://www.packetcoders.io/how-to-view-the-network-namespaces-in-kubernetes/">Docker 并没有遵循这一点&lt;/a>。&lt;/p>
&lt;/blockquote>
&lt;p>例如，下面是 Kubernetes 节点的命名空间：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip netns list
cni-0f226515-e28b-df13-9f16-dd79456825ac &lt;span class="o">(&lt;/span>id: 3&lt;span class="o">)&lt;/span>
cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd &lt;span class="o">(&lt;/span>id: 4&lt;span class="o">)&lt;/span>
cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e &lt;span class="o">(&lt;/span>id: 2&lt;span class="o">)&lt;/span>
cni-7619c818-5b66-5d45-91c1-1c516f559291 &lt;span class="o">(&lt;/span>id: 1&lt;span class="o">)&lt;/span>
cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8 &lt;span class="o">(&lt;/span>id: 0&lt;span class="o">)&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>注意 &lt;code>cni-&lt;/code> 前缀意味着命名空间的创建由 CNI 来完成。&lt;/p>
&lt;/blockquote>
&lt;p>当创建 pod 并分配给节点时，&lt;a href="https://github.com/containernetworking/cni#what-is-cni">CNI&lt;/a> 会：&lt;/p>
&lt;ol>
&lt;li>为其创建网络命名空间。&lt;/li>
&lt;li>分配 IP 地址。&lt;/li>
&lt;li>将容器连接到网络。&lt;/li>
&lt;/ol>
&lt;p>如果 pod 像上面的示例一样包含多个容器，则所有容器都被置于同一个命名空间中。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>创建 pod 时，CNI 为容器创建网络命名空间&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427706567489.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>然后分配 IP 地址&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427707025246.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后将容器连接到网络的其余部分&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427710287189.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;em>那么当列出节点上的容器时会看到什么？&lt;/em>&lt;/p>
&lt;p>可以 SSH 到 Kubernetes 节点来查看命名空间：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ lsns -t net
NS TYPE NPROCS PID USER NETNSID NSFS COMMAND
&lt;span class="m">4026531992&lt;/span> net &lt;span class="m">171&lt;/span> &lt;span class="m">1&lt;/span> root unassigned /run/docker/netns/default /sbin/init noembed norestore
&lt;span class="m">4026532286&lt;/span> net &lt;span class="m">2&lt;/span> &lt;span class="m">4808&lt;/span> &lt;span class="m">65535&lt;/span> &lt;span class="m">0&lt;/span> /run/docker/netns/56c020051c3b /pause
&lt;span class="m">4026532414&lt;/span> net &lt;span class="m">5&lt;/span> &lt;span class="m">5489&lt;/span> &lt;span class="m">65535&lt;/span> &lt;span class="m">1&lt;/span> /run/docker/netns/7db647b9b187 /pause
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>lsns&lt;/code> 命令会列出主机上&lt;em>所有&lt;/em>的命名空间。&lt;/p>
&lt;blockquote>
&lt;p>记住 Linux 中有&lt;a href="https://man7.org/linux/man-pages/man7/namespaces.7.html">多种命名空间类型&lt;/a>。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Nginx 容器在哪？&lt;/em>&lt;/p>
&lt;p>&lt;em>那么 &lt;code>pause&lt;/code> 容器又是什么？&lt;/em>&lt;/p>
&lt;h2 id="pause-容器创建-pod-中的网络命名空间">Pause 容器创建 Pod 中的网络命名空间&lt;/h2>
&lt;p>从节点上的所有进程中找出 Nginx 容器：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ lsns
NS TYPE NPROCS PID USER COMMAND
&lt;span class="c1"># truncated output&lt;/span>
&lt;span class="m">4026532414&lt;/span> net &lt;span class="m">5&lt;/span> &lt;span class="m">5489&lt;/span> &lt;span class="m">65535&lt;/span> /pause
&lt;span class="m">4026532513&lt;/span> mnt &lt;span class="m">1&lt;/span> &lt;span class="m">5599&lt;/span> root sleep 1d
&lt;span class="m">4026532514&lt;/span> uts &lt;span class="m">1&lt;/span> &lt;span class="m">5599&lt;/span> root sleep 1d
&lt;span class="m">4026532515&lt;/span> pid &lt;span class="m">1&lt;/span> &lt;span class="m">5599&lt;/span> root sleep 1d
&lt;span class="m">4026532516&lt;/span> mnt &lt;span class="m">3&lt;/span> &lt;span class="m">5777&lt;/span> root nginx: master process nginx -g daemon off&lt;span class="p">;&lt;/span>
&lt;span class="m">4026532517&lt;/span> uts &lt;span class="m">3&lt;/span> &lt;span class="m">5777&lt;/span> root nginx: master process nginx -g daemon off&lt;span class="p">;&lt;/span>
&lt;span class="m">4026532518&lt;/span> pid &lt;span class="m">3&lt;/span> &lt;span class="m">5777&lt;/span> root nginx: master process nginx -g daemon off&lt;span class="p">;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>该容器出现在了挂在（mount &lt;code>mnt&lt;/code>）、Unix 分时系统（Unix time-sharing &lt;code>uts&lt;/code>）和 PID（&lt;code>pid&lt;/code>）命名空间中，但是并不在网络命名空间（&lt;code>net&lt;/code>）中。&lt;/p>
&lt;p>不幸的是，&lt;code>lsns&lt;/code> 只显示了每个进程最低的 PID，不过可以根据进程 ID 进一步过滤。&lt;/p>
&lt;p>可以通过以下内容检索Nginx 容器的所有命名空间：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ sudo lsns -p &lt;span class="m">5777&lt;/span>
NS TYPE NPROCS PID USER COMMAND
&lt;span class="m">4026531835&lt;/span> cgroup &lt;span class="m">178&lt;/span> &lt;span class="m">1&lt;/span> root /sbin/init noembed norestore
&lt;span class="m">4026531837&lt;/span> user &lt;span class="m">178&lt;/span> &lt;span class="m">1&lt;/span> root /sbin/init noembed norestore
&lt;span class="m">4026532411&lt;/span> ipc &lt;span class="m">5&lt;/span> &lt;span class="m">5489&lt;/span> &lt;span class="m">65535&lt;/span> /pause
&lt;span class="m">4026532414&lt;/span> net &lt;span class="m">5&lt;/span> &lt;span class="m">5489&lt;/span> &lt;span class="m">65535&lt;/span> /pause
&lt;span class="m">4026532516&lt;/span> mnt &lt;span class="m">3&lt;/span> &lt;span class="m">5777&lt;/span> root nginx: master process nginx -g daemon off&lt;span class="p">;&lt;/span>
&lt;span class="m">4026532517&lt;/span> uts &lt;span class="m">3&lt;/span> &lt;span class="m">5777&lt;/span> root nginx: master process nginx -g daemon off&lt;span class="p">;&lt;/span>
&lt;span class="m">4026532518&lt;/span> pid &lt;span class="m">3&lt;/span> &lt;span class="m">5777&lt;/span> root nginx: master process nginx -g daemon off&lt;span class="p">;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>pause&lt;/code> 进程再次出现，这次它劫持了网络命名空间。&lt;/p>
&lt;p>&lt;em>那是什么？&lt;/em>&lt;/p>
&lt;p>&lt;strong>集群中的每个 pod 都有一个在后台运行的隐藏容器，被称为 &lt;code>pause&lt;/code>&lt;/strong>。&lt;/p>
&lt;p>列出节点上的所有容器并过滤出 pause 容器：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ docker ps &lt;span class="p">|&lt;/span> grep pause
fa9666c1d9c6 k8s.gcr.io/pause:3.4.1 &lt;span class="s2">&amp;#34;/pause&amp;#34;&lt;/span> k8s_POD_kube-dns-599484b884-sv2js…
44218e010aeb k8s.gcr.io/pause:3.4.1 &lt;span class="s2">&amp;#34;/pause&amp;#34;&lt;/span> k8s_POD_blackbox-exporter-55c457d…
5fb4b5942c66 k8s.gcr.io/pause:3.4.1 &lt;span class="s2">&amp;#34;/pause&amp;#34;&lt;/span> k8s_POD_kube-dns-599484b884-cq99x…
8007db79dcf2 k8s.gcr.io/pause:3.4.1 &lt;span class="s2">&amp;#34;/pause&amp;#34;&lt;/span> k8s_POD_konnectivity-agent-84f87c…
&lt;/code>&lt;/pre>&lt;/div>&lt;p>将看到对于节点分配到的每个 pod，都有一个匹配的 &lt;code>pause&lt;/code> 容器。&lt;/p>
&lt;p>&lt;strong>该 &lt;code>pause&lt;/code> 容器负责创建和维持网络命名空间。&lt;/strong>&lt;/p>
&lt;p>它包含的代码极少，部署后立即进入睡眠状态。&lt;/p>
&lt;p>然而，&lt;a href="https://www.ianlewis.org/en/almighty-pause-container">它在 Kubernetes 生态中的首当其冲，发挥着至关重要的作用。&lt;/a>。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>创建 pod 时，CNI 会创建一个带有&lt;em>睡眠&lt;/em>容器的网络命名空间&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427720335296.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Pod 中的所有容器都会加入到它创建的网络命名空间中&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427721343650.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>此时 CNI 分配 IP 地址并将容器连接到网络&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427710287189.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;em>进入睡眠状态的容器有什么用？&lt;/em>&lt;/p>
&lt;p>要了解它的实用性，我们可以想象下如示例一样有两个容器的 pod，但没有 &lt;code>pause&lt;/code> 容器。&lt;/p>
&lt;p>容器启动，CNI：&lt;/p>
&lt;ol>
&lt;li>为 Nginx 容器创建一个网络命名空间。&lt;/li>
&lt;li>把 busybox 容器加入到前面创建的网络命名空间中。&lt;/li>
&lt;li>为 pod 分配 IP 地址。&lt;/li>
&lt;li>将容器连接到网络。&lt;/li>
&lt;/ol>
&lt;p>&lt;em>假如 Nginx 容器崩溃了会发生什么？&lt;/em>&lt;/p>
&lt;p>CNI 将不得不&lt;em>再次&lt;/em>完成所有流程，两个容器的网络都会中断。&lt;/p>
&lt;p>由于 &lt;code>sleep&lt;/code> 容器不太可能有任何 bug，因此创建网络命名空间通常是一个更保险、更健壮的选择。&lt;/p>
&lt;p>&lt;strong>如果 pod 中的一个容器崩溃，其余的仍可以处理网络请求。&lt;/strong>&lt;/p>
&lt;h2 id="为-pod-分配了-ip-地址">为 Pod 分配了 IP 地址&lt;/h2>
&lt;p>前面提到 pod 和所有容器获得了同样的 IP。&lt;/p>
&lt;p>&lt;code>这是怎么配置的？&lt;/code>&lt;/p>
&lt;p>&lt;em>&lt;em>在 pod 网络命名空间中，创建一个接口并分配 IP 地址&lt;/em>。&lt;/em>&lt;/p>
&lt;p>我们来验证下。&lt;/p>
&lt;p>首先，找到 pod 的 IP 地址：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ kubectl get pod multi-container-pod -o &lt;span class="nv">jsonpath&lt;/span>&lt;span class="o">={&lt;/span>.status.podIP&lt;span class="o">}&lt;/span>
10.244.4.40
&lt;/code>&lt;/pre>&lt;/div>&lt;p>接下来，找到相关的网络命名空间。&lt;/p>
&lt;p>由于网络命名空间是从物理接口创建的，需要访问集群节点。&lt;/p>
&lt;blockquote>
&lt;p>如果你运行的是 minikube，可以通过 &lt;code>minikube ssh&lt;/code> 访问节点。如果在云提供商中运行，应该有某种方法通过 SSH 访问节点。&lt;/p>
&lt;/blockquote>
&lt;p>进入后，可以找到创建的最新的网络命名空间：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ls -lt /var/run/netns
total &lt;span class="m">0&lt;/span>
-r--r--r-- &lt;span class="m">1&lt;/span> root root &lt;span class="m">0&lt;/span> Sep &lt;span class="m">25&lt;/span> 13:34 cni-0f226515-e28b-df13-9f16-dd79456825ac
-r--r--r-- &lt;span class="m">1&lt;/span> root root &lt;span class="m">0&lt;/span> Sep &lt;span class="m">24&lt;/span> 09:39 cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd
-r--r--r-- &lt;span class="m">1&lt;/span> root root &lt;span class="m">0&lt;/span> Sep &lt;span class="m">24&lt;/span> 09:39 cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e
-r--r--r-- &lt;span class="m">1&lt;/span> root root &lt;span class="m">0&lt;/span> Sep &lt;span class="m">24&lt;/span> 09:39 cni-7619c818-5b66-5d45-91c1-1c516f559291
-r--r--r-- &lt;span class="m">1&lt;/span> root root &lt;span class="m">0&lt;/span> Sep &lt;span class="m">24&lt;/span> 09:39 cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8
&lt;/code>&lt;/pre>&lt;/div>&lt;p>本示例中，它是 &lt;code>cni-0f226515-e28b-df13-9f16-dd79456825ac&lt;/code>。此时，可以在该命名空间总执行 &lt;code>exec&lt;/code> 命令：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip netns &lt;span class="nb">exec&lt;/span> cni-0f226515-e28b-df13-9f16-dd79456825ac ip a
&lt;span class="c1"># output truncated&lt;/span>
3: eth0@if12: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span class="m">1450&lt;/span> qdisc noqueue state UP group default
link/ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid &lt;span class="m">0&lt;/span>
inet 10.244.4.40/32 brd 10.244.4.40 scope global eth0
valid_lft forever preferred_lft forever
inet6 fe80::14a4:f8ff:fe4f:5677/64 scope link
valid_lft forever preferred_lft forever
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>&lt;code>10.244.4.40&lt;/code> 就是 pod 的 IP 地址。&lt;/strong>&lt;/p>
&lt;p>通过查找 &lt;code>@if12&lt;/code> 中的 &lt;code>12&lt;/code> 找到网络接口。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip link &lt;span class="p">|&lt;/span> grep -A1 ^12
12: vethweplb3f36a0@if16: mtu &lt;span class="m">1376&lt;/span> qdisc noqueue master weave state UP mode DEFAULT group default
link/ether 72:1c:73:d9:d9:f6 brd ff:ff:ff:ff:ff:ff link-netnsid &lt;span class="m">1&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>还可以验证 Nginx 容器是否从该命名空间中监听 HTTP 流量：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip netns &lt;span class="nb">exec&lt;/span> cni-0f226515-e28b-df13-9f16-dd79456825ac netstat -lnp
Active Internet connections &lt;span class="o">(&lt;/span>only servers&lt;span class="o">)&lt;/span>
Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name
tcp &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> 0.0.0.0:80 0.0.0.0:* LISTEN 692698/nginx: master
tcp6 &lt;span class="m">0&lt;/span> &lt;span class="m">0&lt;/span> :::80 :::* LISTEN 692698/nginx: master
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>如果无法通过 SSH 访问集群的节点，可以试试 &lt;code>kubectl exec&lt;/code> 进入到 busybox 容器，然后使用 &lt;code>ip&lt;/code> 和 &lt;code>netstat&lt;/code> 命令。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>太棒了！&lt;/em>&lt;/p>
&lt;p>现在我们已经介绍了容器间的通信，接下来看看 Pod 与 Pod 直接如何建立通信。&lt;/p>
&lt;h2 id="检查集群中-pod-到-pod-的流量">检查集群中 pod 到 pod 的流量&lt;/h2>
&lt;p>当说起 pod 间通信时，会有两种可能：&lt;/p>
&lt;ol>
&lt;li>Pod 流量流向同一节点上的 pod。&lt;/li>
&lt;li>Pod 流量流量另一个节点上的 pod。&lt;/li>
&lt;/ol>
&lt;p>为了使整个设置正常工作，我们需要之前讨论过的虚拟接口和以太网桥接。&lt;/p>
&lt;p>在继续之前，我们先讨论下他们的功能以及为什么他们时必需的。&lt;/p>
&lt;p>&lt;strong>要完成 pod 与其他 pod 的通信，它必须先访问节点的根命名空间。&lt;/strong>&lt;/p>
&lt;p>这是使用连接 pod 和根命名空间的虚拟以太网对来实现的。&lt;/p>
&lt;p>这些&lt;a href="https://man7.org/linux/man-pages/man4/veth.4.html">虚拟接口设备&lt;/a>（&lt;code>veth&lt;/code> 中的 &lt;code>v&lt;/code>）连接并充当两个命名空间间的隧道。&lt;/p>
&lt;p>使用此 &lt;code>veth&lt;/code> 设备，将一端连接到 pod 的命名空间，另一端连接到根命名空间。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427710287189.jpg" alt="">&lt;/p>
&lt;p>这些 CNI 可以替你做，也可以手动操作：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip link add veth1 netns pod-namespace &lt;span class="nb">type&lt;/span> veth peer veth2 netns root
&lt;/code>&lt;/pre>&lt;/div>&lt;p>现在 pod 的命名空间有了可以访问根命名空间的隧道。&lt;/p>
&lt;p>&lt;strong>节点上每个新建的 pod 都会设置如下所示的 veth 对。&lt;/strong>&lt;/p>
&lt;p>创建接口对时其中一部分。&lt;/p>
&lt;p>其他的就是为以太网设备分配地址，并创建默认路由。&lt;/p>
&lt;p>来看下如何在 pod 的命名空间中设置 &lt;code>veth1&lt;/code> 接口：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip netns &lt;span class="nb">exec&lt;/span> cni-0f226515-e28b-df13-9f16-dd79456825ac ip addr add 10.244.4.40/24 dev veth1
$ ip netns &lt;span class="nb">exec&lt;/span> cni-0f226515-e28b-df13-9f16-dd79456825ac ip link &lt;span class="nb">set&lt;/span> veth1 up
$ ip netns &lt;span class="nb">exec&lt;/span> cni-0f226515-e28b-df13-9f16-dd79456825ac ip route add default via 10.244.4.40
&lt;/code>&lt;/pre>&lt;/div>&lt;p>在节点侧，我们创建另一个 &lt;code>veth2&lt;/code> 对：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip addr add 169.254.132.141/16 dev veth2
$ ip link &lt;span class="nb">set&lt;/span> veth2 up
&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以像以前一样检查现有的 &lt;code>veth&lt;/code> 对。&lt;/p>
&lt;p>在 pod 的命名空间中，检查 &lt;code>eth0&lt;/code> 接口的后缀。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip netns &lt;span class="nb">exec&lt;/span> cni-0f226515-e28b-df13-9f16-dd79456825ac ip link show &lt;span class="nb">type&lt;/span> veth
3: eth0@if12: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span class="m">1450&lt;/span> qdisc noqueue state UP mode DEFAULT group default
link/ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid &lt;span class="m">0&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>这种情况下可以使用 &lt;code>grep -A1 ^12&lt;/code> 进行查找（或者滚动到目标所在）：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ ip link show &lt;span class="nb">type&lt;/span> veth
&lt;span class="c1"># output truncated&lt;/span>
12: cali97e50e215bd@if3: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu &lt;span class="m">1450&lt;/span> qdisc noqueue state UP mode DEFAULT group default
link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-0f226515-e28b-df13-9f16-dd79456825ac
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>也可以使用 &lt;code>ip -n cni-0f226515-e28b-df13-9f16-dd79456825ac link show type veth&lt;/code> 命令。&lt;/p>
&lt;/blockquote>
&lt;p>注意 &lt;code>3: eth0@if12&lt;/code> 和 &lt;code>12: cali97e50e215bd@if3&lt;/code> 接口上的符号。&lt;/p>
&lt;p>在 pod 命名空间中，&lt;code>eth0&lt;/code> 接口连接到根命名空间中编号为 &lt;code>12&lt;/code> 的接口。因此是 &lt;code>@if12&lt;/code>。&lt;/p>
&lt;p>在 &lt;code>veth&lt;/code> 对的另一端，根命名空间连接到 pod 命名空间的 &lt;code>3&lt;/code> 号接口。&lt;/p>
&lt;p>接下来是连接 &lt;code>veth&lt;/code> 对两端的桥接器（bridge）。&lt;/p>
&lt;h2 id="pod-命名空间连接到以太网桥接器">Pod 命名空间连接到以太网桥接器&lt;/h2>
&lt;p>桥接器将位于根命名空间中的虚拟接口的每一端“绑定”。&lt;/p>
&lt;p>&lt;strong>该桥接器将允许流量在虚拟对之间流动，并通过公共根命名空间。&lt;/strong>&lt;/p>
&lt;p>&lt;em>理论时间。&lt;/em>&lt;/p>
&lt;p>以太网桥接器位于&lt;a href="https://en.wikipedia.org/wiki/OSI_model">OSI 网络模型&lt;/a>的第二层。&lt;/p>
&lt;p>&lt;a href="https://ops.tips/blog/using-network-namespaces-and-bridge-to-isolate-servers/">可以将桥接器看作一个虚拟交换机，接受来自不同命名空间和接口的连接。&lt;/a>&lt;/p>
&lt;p>&lt;strong>以太网桥接器允许连接同一个节点上的多个可用网络。&lt;/strong>&lt;/p>
&lt;p>因此，可以使用该设置连接两个接口：从 pod 命名空间的 &lt;code>veth&lt;/code> 连接到同一节点上另一个 pod 的 &lt;code>veth&lt;/code>。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427769020626.jpg" alt="">&lt;/p>
&lt;p>我们继续看下以太网桥接器和 veth 对的作用。&lt;/p>
&lt;h2 id="跟踪同一节点上-pod-间的流量">跟踪同一节点上 pod 间的流量&lt;/h2>
&lt;p>假设同一个节点上有两个 pod，Pod-A 想向 Pod-B 发送消息。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>由于目标不是同命名空间的容器，Pod-A 向其默认接口 &lt;code>eth0&lt;/code> 发送数据包。这个接口与 &lt;code>veth&lt;/code> 对的一端绑定，作为隧道。因此数据包将被转发到节点的根命名空间。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427770131482.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>以太网桥接器作为虚拟交换机，必须以某种方式将目标 pod IP（Pod-B）解析为其 MAC 地址。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427772473425.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>轮到ARP 协议上场了。当帧到达桥接器时，会向所有连接的设备发送 ARP 广播。桥接器喊道&lt;em>谁有 Pod-B 的 IP 地址&lt;/em>。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427774225871.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>收到带有连接 Pod-B 接口的 MAC 地址的回复，然后此信息存储在桥接器 ARP 缓存（查找表）中。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427775056174.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>IP 和 MAC 地址的映射存储完成后，桥接器在表中查找，并将数据包转发到正确的短点。数据包到达根命名空间中 Pod- B 的 &lt;code>veth&lt;/code>，然后从那快速到达 Pod-B 命名空间内的 &lt;code>eth0&lt;/code> 接口。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427775056174.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>有了这个，Pod-A 和 Pod-B 之间的通信取得了成功。&lt;/p>
&lt;h2 id="跟踪不同节点上-pod-间的通信">跟踪不同节点上 pod 间的通信&lt;/h2>
&lt;p>对于需要跨不同节点通信的 pod，需要额外的通信跳转。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>前几个步骤保持不变，直到数据包到达根命名空间并需要发送到 Pod- B。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427777182601.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>当目标地址不在本地网络中，数据包将被转发到本节点的默认网关。节点上退出或默认网关通常位于 &lt;code>eth0&lt;/code> 接口上 &amp;ndash; 将节点连接到网络的物理接口。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16427777968858.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>这次并不会发生 ARP 解析，因为源和目标 IP 在不同网络上。&lt;/strong>&lt;/p>
&lt;p>检查使用位运算（Bitwise）操作完成。&lt;/p>
&lt;p>当目标 IP 不在当前网络上时，数据包将被转发到节点的默认网关。&lt;/p>
&lt;h3 id="位运算的工作原理">位运算的工作原理&lt;/h3>
&lt;p>在确定数据包的转发位置时，源节点必须执行位运算。&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Bitwise_operation#AND">这也被称为与操作。&lt;/a>&lt;/p>
&lt;p>作为复习，位与操作产生如下结果：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">0 AND 0 = 0
0 AND 1 = 0
1 AND 0 = 0
1 AND 1 = 1
&lt;/code>&lt;/pre>&lt;/div>&lt;p>除了 1 与 1 以外的都是 false。&lt;/p>
&lt;p>如果源节点的 IP 为 192.168.1.1，子网掩码为 /24，目标 IP 为 172.16.1.1/16，则按位与操作将确认他们不在同一网络上。&lt;/p>
&lt;p>这意味着目标 IP 与数据包的源在不同的网络上，因此数据包将在默认网关中转发。&lt;/p>
&lt;p>&lt;em>数学时间。&lt;/em>&lt;/p>
&lt;p>我们必须从二进制文件中的 32 位地址执行与操作开始。&lt;/p>
&lt;p>先找出源和目标 IP 的网络。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">| Type | Binary | Converted |
| ---------------- | ----------------------------------- | ------------------ |
| Src. IP Address | 11000000.10101000.00000001.00000001 | 192.168.1.1 |
| Src. Subnet Mask | 11111111.11111111.11111111.00000000 | 255.255.255.0(/24) |
| Src. Network | 11000000.10101000.00000001.00000000 | 192.168.1.0 |
| | | |
| Dst. IP Address | 10101100.00010000.00000001.00000001 | 172.16.1.1 |
| Dst. Subnet Mask | 11111111.11111111.00000000.00000000 | 255.255.0.0(/16) |
| Dst. Network | 10101100.00010000.00000000.00000000 | 172.16.0.0 |
&lt;/code>&lt;/pre>&lt;/div>&lt;p>位运算操作后，需要将目标 IP 与数据包源节点的子网进行比较。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">| Type | Binary | Converted |
| ---------------- | ----------------------------------- | ------------------ |
| Dst. IP Address | 10101100.00010000.00000001.00000001 | 172.16.1.1 |
| Src. Subnet Mask | 11111111.11111111.11111111.00000000 | 255.255.255.0(/24) |
| Network Result | 10101100.00010000.00000001.00000000 | 172.16.1.0
&lt;/code>&lt;/pre>&lt;/div>&lt;p>进行位比较后，ARP 会检查其查询表来查找默认网关的 MAC 地址。&lt;/p>
&lt;p>如果有条目，将立即转发数据包。&lt;/p>
&lt;p>否则，先进行广播以确定网关的 MAC 地址。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>数据包现在路由到另一个节点的默认接口，我们叫它 Node-B。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428091589748.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>以相反的顺序。数据包现在位与 Node-B 的根命名空间，并到达桥接器，这里会进行 ARP 解析。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428095883798.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>收到带有连接 Pod-B 的接口 MAC地址的回复。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428097041936.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这次桥接器通过 Pod-B 的 &lt;code>veth&lt;/code> 设备将帧转发，并到达 Pod-B 自己的命名空间。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428097041936.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>此时应该已经熟悉了 pod 之间的流量如何流转，接下来再探索下 CNI 如何创建上述内容。&lt;/p>
&lt;h2 id="容器网络接口---cni">容器网络接口 - CNI&lt;/h2>
&lt;p>&lt;a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">容器网络接口（CNI）关注当前节点的网络。&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428099347551.jpg" alt="">&lt;/p>
&lt;p>&lt;strong>可以将 CNI 看作网络插件在解决 Kubernetes &lt;em>某些&lt;/em> 需求时要遵循的一套规则。&lt;/strong>&lt;/p>
&lt;p>然而，它不仅仅与 Kubernetes 或者特定网络插件关联。&lt;/p>
&lt;p>可以使用如下 CNI：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.tigera.io/project-calico/">Calico&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://cilium.io/">Cilium&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/flannel-io/flannel">Flannel&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.weave.works/docs/net/latest/overview/">Weave Net&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/containernetworking/cni#3rd-party-plugins">其他网络插件&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>他们都实现相同的 CNI 标准。&lt;/p>
&lt;p>如果没有 CNI，你需要手动完成如下操作：&lt;/p>
&lt;ul>
&lt;li>创建 pod（容器）的网络命名空间&lt;/li>
&lt;li>创建接口&lt;/li>
&lt;li>创建 veth 对&lt;/li>
&lt;li>设置命名空间网络&lt;/li>
&lt;li>设置静态路由&lt;/li>
&lt;li>配置以太网桥接器&lt;/li>
&lt;li>分配 IP 地址&lt;/li>
&lt;li>创建 NAT 规则&lt;/li>
&lt;/ul>
&lt;p>还有太多其他需要手动完成的工作。&lt;/p>
&lt;p>更不用说删除或重新启动 pod 时删除或调整上述所有内容了。&lt;/p>
&lt;p>CNI 必须支持&lt;a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#cni-operations">四个不同的操作&lt;/a>：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ADD&lt;/strong> - 将容器添加到网络&lt;/li>
&lt;li>&lt;strong>DEL&lt;/strong> - 从网络中删除容器&lt;/li>
&lt;li>&lt;strong>CHECK&lt;/strong> - 如果容器的网络出现问题，则返回错误&lt;/li>
&lt;li>&lt;strong>VERSION&lt;/strong> - 显示插件的版本&lt;/li>
&lt;/ul>
&lt;p>&lt;em>让我们在实践中看看它是如何工作的。&lt;/em>&lt;/p>
&lt;p>当 pod 分配到特定节点时，kubelet 本身不会初始化网络。&lt;/p>
&lt;p>相反，它将任务交给了 CNI。&lt;/p>
&lt;p>&lt;strong>然后，它指定了配置，并以 JSON 格式将其发送给 CNI 插件。&lt;/strong>&lt;/p>
&lt;p>可以在节点的 &lt;code>/etc/cni/net.d&lt;/code> 目录中，找到当前 CNI 的配置文件：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ cat 10-calico.conflist
&lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;name&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;k8s-pod-network&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;cniVersion&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;0.3.1&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;plugins&amp;#34;&lt;/span>: &lt;span class="o">[&lt;/span>
&lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;calico&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;datastore_type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;kubernetes&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;mtu&amp;#34;&lt;/span>: 0,
&lt;span class="s2">&amp;#34;nodename_file_optional&amp;#34;&lt;/span>: false,
&lt;span class="s2">&amp;#34;log_level&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;Info&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;log_file_path&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;/var/log/calico/cni/cni.log&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;ipam&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span> &lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;calico-ipam&amp;#34;&lt;/span>, &lt;span class="s2">&amp;#34;assign_ipv4&amp;#34;&lt;/span> : &lt;span class="s2">&amp;#34;true&amp;#34;&lt;/span>, &lt;span class="s2">&amp;#34;assign_ipv6&amp;#34;&lt;/span> : &lt;span class="s2">&amp;#34;false&amp;#34;&lt;/span>&lt;span class="o">}&lt;/span>,
&lt;span class="s2">&amp;#34;container_settings&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;allow_ip_forwarding&amp;#34;&lt;/span>: &lt;span class="nb">false&lt;/span>
&lt;span class="o">}&lt;/span>,
&lt;span class="s2">&amp;#34;policy&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;k8s&amp;#34;&lt;/span>
&lt;span class="o">}&lt;/span>,
&lt;span class="s2">&amp;#34;kubernetes&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;k8s_api_root&amp;#34;&lt;/span>:&lt;span class="s2">&amp;#34;https://10.96.0.1:443&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;kubeconfig&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;/etc/cni/net.d/calico-kubeconfig&amp;#34;&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>,
&lt;span class="o">{&lt;/span>
&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;bandwidth&amp;#34;&lt;/span>,
&lt;span class="s2">&amp;#34;capabilities&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span>&lt;span class="s2">&amp;#34;bandwidth&amp;#34;&lt;/span>: true&lt;span class="o">}&lt;/span>
&lt;span class="o">}&lt;/span>,
&lt;span class="o">{&lt;/span>&lt;span class="s2">&amp;#34;type&amp;#34;&lt;/span>: &lt;span class="s2">&amp;#34;portmap&amp;#34;&lt;/span>, &lt;span class="s2">&amp;#34;snat&amp;#34;&lt;/span>: true, &lt;span class="s2">&amp;#34;capabilities&amp;#34;&lt;/span>: &lt;span class="o">{&lt;/span>&lt;span class="s2">&amp;#34;portMappings&amp;#34;&lt;/span>: true&lt;span class="o">}}&lt;/span>
&lt;span class="o">]&lt;/span>
&lt;span class="o">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>每个插件都使用不同类型的配置来设置网络。&lt;/strong>&lt;/p>
&lt;p>例如，Calico 使用 BGP 路由协议配对的第 3 层网络来连接 pod。&lt;/p>
&lt;p>Cilium 在第 3 到 7 层使用 eBPF 配置覆盖网络。&lt;/p>
&lt;p>与 Calico 一起，Cilium 支持设置网络策略来限制流量。&lt;/p>
&lt;p>&lt;em>那该如何选择呢？&lt;/em>&lt;/p>
&lt;p>这取决于。&lt;/p>
&lt;p>CNI 主要有两组。&lt;/p>
&lt;p>&lt;strong>第一组中，可以找到使用基本网络设置（也称为扁平网络）的CNI&lt;/strong>，并将集群 IP 池 中的IP 地址分配给 pod。&lt;/p>
&lt;p>这可能会因为快速用尽可用的 IP 地址而成为负担。&lt;/p>
&lt;p>&lt;strong>相反，另一种方法是使用覆盖网络。&lt;/strong>&lt;/p>
&lt;p>简而言之，覆盖网络是主（底层）网络之上的辅助网络。&lt;/p>
&lt;p>&lt;strong>覆盖网络的工作原理是封装来自底层网络的所有数据包，这些数据包指向另一个节点上的 pod。&lt;/strong>&lt;/p>
&lt;p>覆盖网络的一项流行技术是 &lt;a href="https://en.wikipedia.org/wiki/Virtual_Extensible_LAN">VXLAN&lt;/a>，它允许在 L3 网络上隧道传输 L2 域。&lt;/p>
&lt;p>&lt;em>那么哪种更好？&lt;/em>&lt;/p>
&lt;p>&lt;strong>没有唯一的答案，通常取决于你的需求。&lt;/strong>&lt;/p>
&lt;p>&lt;em>你是在构建一个拥有数万个节点的大集群吗？&lt;/em>&lt;/p>
&lt;p>可能覆盖网络更好。&lt;/p>
&lt;p>&lt;em>你是否在意更简单的设置和在嵌套网络中不失去检查网络流量的能力。&lt;/em>&lt;/p>
&lt;p>扁平网络更适合你。&lt;/p>
&lt;p>现在已经讨论了 CNI，让我们继续探索 Pod 到服务（service）的通信是如何完成的。&lt;/p>
&lt;h2 id="检查-pod-到服务的流量">检查 pod 到服务的流量&lt;/h2>
&lt;p>由于 Kubernetes 环境下 pod 的动态特性，分配给 pod 的 IP 地址不是静态的。&lt;/p>
&lt;p>&lt;strong>这些 IP 地址是短暂的，每次创建或者删除 pod 时都会发生变化。&lt;/strong>&lt;/p>
&lt;p>服务解决了这个问题，为连接到一组 pod 提供了稳定的机制。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428122010963.jpg" alt="">&lt;/p>
&lt;p>默认情况下，在 Kubernetes 中创建服务时，会&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies">为其预定并分配虚拟 IP &lt;/a>。&lt;/p>
&lt;p>使用选择器将服务于目标 pod 进行管理。&lt;/p>
&lt;p>&lt;em>当删除 pod 并添加新 pod 时会发生什么？&lt;/em>&lt;/p>
&lt;p>&lt;strong>该服务的虚拟 IP 保持不变。&lt;/strong>&lt;/p>
&lt;p>然而，无需敢于，流量将到达新创建的 pod。&lt;/p>
&lt;p>换句话说，Kubernetes 中的服务类似于负载均衡器。&lt;/p>
&lt;p>&lt;em>但他们时如何工作的？&lt;/em>&lt;/p>
&lt;h2 id="使用-netfilter-和-iptables-拦截和重写流量">使用 Netfilter 和 Iptables 拦截和重写流量&lt;/h2>
&lt;p>Kubernetes 中的服务基于两个 Linux 内核组件：&lt;/p>
&lt;ol>
&lt;li>Netfilter&lt;/li>
&lt;li>Iptables&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Netfilter 是一个框架，允许配置数据包过滤、创建 NAT或端口翻译规则，并管理网络中的流量。&lt;/strong>&lt;/p>
&lt;p>此外，它还屏蔽和阻止不请自来的连接访问服务。&lt;/p>
&lt;p>&lt;strong>另一方面，Iptables 是一个用户空间程序，允许你配置 Linux 内核防火墙的 IP 数据包过滤器规则。&lt;/strong>&lt;/p>
&lt;p>iptables 使用不同的 Netfilter 模块实现。&lt;/p>
&lt;p>你可以使用 iptables CLI 实时更改过滤规则，并将其插入 netfilters 的挂点。&lt;/p>
&lt;p>过滤器组织在不同的表中，其中包含处理网络流量数据包的链。&lt;/p>
&lt;p>每个协议都使用不同的内核模块和程序。&lt;/p>
&lt;blockquote>
&lt;p>当提到 iptables 时，通常说的是 IPV4。对于 IPV6 的规则，CLI 是 ip6tables。&lt;/p>
&lt;/blockquote>
&lt;p>Iptables 有五种类型的链，每种链都直接映射到 Netfilter 钩子。&lt;/p>
&lt;p>从 iptables 角度看是：&lt;/p>
&lt;ul>
&lt;li>&lt;code>PRE_ROUTING&lt;/code>&lt;/li>
&lt;li>&lt;code>INPUT&lt;/code>&lt;/li>
&lt;li>&lt;code>FORWARD&lt;/code>&lt;/li>
&lt;li>&lt;code>OUTPUT&lt;/code>&lt;/li>
&lt;li>&lt;code>POST_ROUTING&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>对应映射到 Netfilter 钩子：&lt;/p>
&lt;ul>
&lt;li>&lt;code>NF_IP_PRE_ROUTING&lt;/code>&lt;/li>
&lt;li>&lt;code>NF_IP_LOCAL_IN&lt;/code>&lt;/li>
&lt;li>&lt;code>NF_IP_FORWARD&lt;/code>&lt;/li>
&lt;li>&lt;code>NF_IP_LOCAL_OUT&lt;/code>&lt;/li>
&lt;li>&lt;code>NF_IP_POST_ROUTING&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>当数据包到达时，根据所处的阶段，会“出发” Netfilter 钩子，该钩子应用特定的 iptables 过滤。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428137183275.jpg" alt="">&lt;/p>
&lt;p>&lt;em>哎呀，看起来很复杂！&lt;/em>&lt;/p>
&lt;p>不过不需要担心。&lt;/p>
&lt;p>这就是为什么我们使用 Kubernetes，上面的所有内容都是通过使用服务来抽象的，一个简单的 YAML 定义就可以自动完成这些规则的设置。&lt;/p>
&lt;p>如果对这些 iptables 规则感兴趣，可以登陆到节点并运行：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">iptables-save
&lt;/code>&lt;/pre>&lt;/div>&lt;p>也可以使用&lt;a href="https://github.com/Nudin/iptable_vis">可视化工具&lt;/a>浏览节点上的 iptables 链。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428153624915.jpg" alt="">&lt;/p>
&lt;p>&lt;em>记住，可能会有数百条规则。想象下手动创建的难度。&lt;/em>&lt;/p>
&lt;p>我们已经解释了相同和不同节点上的 pod 间如何通信。&lt;/p>
&lt;p>在 Pod-to-Service 中，通信的前半部分保持不变。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428154678475.jpg" alt="">&lt;/p>
&lt;p>当 Pod-A 发出请求时，希望到达 Pod-B（这种情况下，Pod-B 位与服务之后），转移的过程中会发生其他变化。&lt;/p>
&lt;p>原始请求从 Pod-A 命名空间中的 &lt;code>eth0&lt;/code> 接口出来。&lt;/p>
&lt;p>从那里穿过 &lt;code>veth&lt;/code> 对，到达根命名空间的以太网桥。&lt;/p>
&lt;p>一旦到达桥接器，数据包立即通过默认网关转发。&lt;/p>
&lt;p>与 Pod-to-Pod 部分一样，主机进行位比较，由于服务的 vIP 不是节点 CIDR 的一部分，数据包将立即通过默认网关转发出去。&lt;/p>
&lt;p>如果查找表中尚没有默认网关的 MAC 地址，则会进行相同的 ARP 解析。&lt;/p>
&lt;p>&lt;em>现在魔法发生了。&lt;/em>&lt;/p>
&lt;p>在数据包经过节点的路由处理之前，Netfilter 钩子 &lt;code>NF_IP_PRE_ROUTING&lt;/code> 被触发并应用一条 iptables 规则。规则进行了 DNAT 转换，重写了 POD-A 数据包的目标 IP 地址。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428159345915.jpg" alt="">&lt;/p>
&lt;p>原来服务 vIP 地址被重写称 POD-B 的IP 地址。&lt;/p>
&lt;p>从那里，路由就像 Pod-to-Pod 直接通信一样。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428160707534.jpg" alt="">&lt;/p>
&lt;p>然而，在所有这些通信之间，使用了第三个功能。&lt;/p>
&lt;p>&lt;a href="https://www.linuxtopia.org/Linux_Firewall_iptables/x1298.html">这个功能被称为 conntrack&lt;/a>，或连接跟踪。&lt;/p>
&lt;p>&lt;strong>Conntrack 将数据包与连接关联起来，并在 Pod-B 发送回响应时跟踪其来源。&lt;/strong>&lt;/p>
&lt;p>NAT 严重依赖 contrack 工作。&lt;/p>
&lt;p>如果没有连接跟踪，它将不知道将包含响应的数据包发送回哪里。&lt;/p>
&lt;p>使用 conntrack 时，数据包的返回路径可以轻松设置相同的源或目标 NAT 更改。&lt;/p>
&lt;p>另一半使用相反的顺序执行。&lt;/p>
&lt;p>Pod-B 接收并处理了请求，现在将数据发送回 Pod-A。&lt;/p>
&lt;p>&lt;em>此时会发生什么？&lt;/em>&lt;/p>
&lt;h2 id="检查服务的响应">检查服务的响应&lt;/h2>
&lt;p>现在 Pod-B 发送响应，将其 IP 地址设置为源地址，Pod-A IP 地址设置为目标地址。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>当数据包到达 Pod-A 所在节点的接口时，就会发生另一个 NAT&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428165510048.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>这次，使用 conntrack 更改源 IP 地址，iptables 规则执行 SNAT 将 Pod-B IP 地址替换为原始服务的 VIP 地址。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428166224192.jpg" alt="">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从 Pod-A 来看像是服务发回的响应，而不是 Pod-B。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/22/16428166224192.jpg" alt="">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>其他部分都一样；一旦 SNAT 完成，数据包到达根命名空间中的以太网桥接器，并通过 veth 对转发到 Pod-A。&lt;/p>
&lt;h2 id="回顾">回顾&lt;/h2>
&lt;p>让我们来总结下你在本文中学到的东西：&lt;/p>
&lt;ul>
&lt;li>容器如何在本地或 pod 内通信。&lt;/li>
&lt;li>当 pod 位于相同和不同的节点上时，Pod-to- Pod 如何通信。&lt;/li>
&lt;li>Pod-to-Service - 当 pod 向 Kubernetes 服务背后的 pod 发送流量时。&lt;/li>
&lt;li>Kubernetes 网络工具箱中有效通信所需的命名空间、veth、iptables、链、Netfilter、CNI、覆盖网络以及所有其他内容。&lt;/li>
&lt;/ul></description></item><item><title>eBPF 和 Wasm：探索服务网格数据平面的未来</title><link>https://atbug.com/ebpf-wasm-service-mesh/</link><pubDate>Tue, 11 Jan 2022 10:40:56 +0800</pubDate><guid>https://atbug.com/ebpf-wasm-service-mesh/</guid><description>
&lt;p>本文翻译自 Vivian Hu 的 &lt;a href="https://www.infoq.com/news/2022/01/ebpf-wasm-service-mesh">《eBPF and Wasm: Exploring the Future of the Service Mesh Data Plane》&lt;/a>。&lt;/p>
&lt;hr>
&lt;p>在 2021 年 12 月 2 日，Cilium 项目宣布了 &lt;a href="https://cilium.io/blog/2021/12/01/cilium-service-mesh-beta">Cilium Service Mesh&lt;/a> 项目的测试版。在 2020 年 8 月 Google Cloud 宣布基于 eBPF 的 Google Kubernetes 服务（GKS）的数据平面 V2 的一年后，Cilium Service Mesh 带来了 “无边车服务网格”（sidecarless service mesh）的想法。它扩展了 Cilium eBPF 产品来处理服务网格中的大部分边车代理功能，包括 7 层路由和负载均衡、TLS 终止、访问策略、健康检查、日志和跟踪，以及内置的 Kubernetes Ingress。&lt;/p>
&lt;p>Cilium 的创始人 Isovalent 在一篇名为 “&lt;a href="https://isovalent.com/blog/post/2021-12-08-ebpf-servicemesh">How eBPF will solve Service Mesh - Goodbye Sidecars&lt;/a>” 的文章中解释了 eBPF 如何替代边车代理。&lt;/p>
&lt;blockquote>
&lt;p>它将我们从边车模型中解放处理，并允许我们将现有的代理技术集成到现有的内核命名空间概念中，使其成为日常使用的优雅容器抽象的一部分。&lt;/p>
&lt;/blockquote>
&lt;p>简而言之，eBPF 承诺会解决服务网格中的重要痛点 &amp;ndash; 当有许多细粒度微服务时的性能损耗。然而，使用 eBPF 替换边车代理这种新颖的想法，也存在着争议。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/11/16418644881451.jpg" alt="图片来自 How eBPF will solve Service Mesh - Goodbye Sidecars">&lt;/p>
&lt;p>服务网格中的数据平面是指管理数据流量如何路由和服务之间的流转的基础设施服务。目前，这是通过使用服务代理实现的。这种设计模式也通常被称为边车模式。边车允许其附加的微服务透明地向服务网格中的其他组件发送和接收请求。&lt;/p>
&lt;p>边车通常包含了 7 层代理，比如 &lt;a href="https://envoyproxy.io/">Envoy&lt;/a>、&lt;a href="https://linkerd.io/">Linkerd&lt;/a> 或者 &lt;a href="https://mosn.io/en/">MOSN&lt;/a>。该代理处理流量的路由、负载均衡、健康检查、身份验证、授权、加密、日志、跟踪和统计数据收集。边车还可以包含一个基于 SDK 的应用程序框架（比如 &lt;a href="https://dapr.io/">Dapr&lt;/a>）以提供网络代理以外的应用程序服务。此类服务的示例还包含了服务注册、服务发现、资源绑定、基于名称的服务调用、状态管理、actor 框架 和密钥存储。&lt;/p>
&lt;p>边车代理通常在 Kubernetes Pod 或者容器中运行。微服务应用也同样运行在容器内，并通过网络接口连接到边车。然而，这些容器化应用程序的一个重要问题就是资源消耗。边车服务随着微服务的数量几何级增长。当应用程序有数百个互联和负载均衡的微服务时，开销变得难以接受。服务网格代理商开始了性能上的竞争。正如 &lt;a href="https://www.infoq.com/news/2021/08/linkerd-rust-cloud-native/">Infoq 之前报道的那样&lt;/a>，Linkerd 将其 Go 写的代理用 Rust 重写了，并获得了显著的性能提升。&lt;/p>
&lt;p>并不奇怪，现有的服务网格提供商不相信 eBPF 是解决所有问题的圣杯。Solo.io 的 Idit Levine 等人针对 Cilium 的这篇公告撰写了一篇文章 “&lt;a href="https://www.solo.io/blog/ebpf-for-service-mesh/">eBPF for Service Mesh? Yes, But Envoy Proxy is Here to Stay&lt;/a>”。&lt;/p>
&lt;blockquote>
&lt;p>在 Solo.io，我们认为 eBPF 是优化服务网格的很好的方式，并将 Envoy 代理视为数据平面的基石。&lt;/p>
&lt;/blockquote>
&lt;p>Solo.io 作者提出的观点是：边车代理现在所做的不仅仅是简单的网络流量管理。当今的服务网格部署中有着复杂的需求，远超过 eBPF 支持的有限编程模型，其不符合图灵完备性且受到内核安全的诸多限制。Cilium eBPF 产品可以处理边车代理许多但并不是全部的任务。此外，Solo.io 的作者还指出，eBPF 的节点级代理与传统的 Pod 级边车代理相比缺乏灵活性，反而增加了整体开销。eBPF 缺陷在开发人员必须编写并部署到服务网格代理中的流量路由、负载均衡和授权的特定应用程序逻辑中体现得尤为明显。&lt;/p>
&lt;p>Terate.io 的开发人员在回应名为 &lt;a href="">The Debate in the Community about Istio and Service Mesh&lt;/a> 的 Cilium 公告中也提出了类似的观点。他们指出，当前边车代理的性能是合理的，开源社区也已经有了进一步提高性能的方法。与此同时，开发人员很难在 eBPF 等新颖但图灵不完整的技术中构建应用程序特定的数据平面逻辑。&lt;/p>
&lt;blockquote>
&lt;p>Istio 架构稳定且生产就绪，生态系统正在发展期。&lt;/p>
&lt;/blockquote>
&lt;p>eBPF 的许多问题与其是一种内核技术分不开，必定收到安全限制。有没有一种方法可以在不使用空间技术降低性能的情况下将复杂的应用程序特定的代理逻辑集成到数据平面中？事实证明，WebAssembly（Wasm）可能会是个选择。Wasm 运行时可以以近似原生性能安全地隔离和执行用户空间代码。&lt;/p>
&lt;p>Envoy Proxy 率先使用 Wasm 作为扩展机制对数据平面的编程。开发人员可以用C、C++、Rust、AssemblyScript、Swift 和 TinyGO 等语言编写应用程序特定的代理逻辑，并将模块编译为 Wasm。通过proxy-Wasm 标准，代理可以在例如 &lt;a href="https://github.com/bytecodealliance/wasmtime">Wasmtime&lt;/a> 和 &lt;a href="https://github.com/WasmEdge/WasmEdge">WasmEdge&lt;/a> 的高性能运行时执行这些 Wasm 插件。目前，&lt;a href="https://envoyproxy.io/">Envoy 代理&lt;/a>、&lt;a href="https://github.com/istio/proxy">Istio 代理&lt;/a>、MOSN 和 &lt;a href="http://openresty.org/">OpenResty&lt;/a> 支持 &lt;a href="https://github.com/proxy-wasm">proxy-Wasm&lt;/a>。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/01/11/16418673669621.jpg" alt="容器生态 来自 WasmEdge Book">&lt;/p>
&lt;p>此外，Wasm 可以充当通用应用程序容器。它在服务网格数据平面上的应用不仅限于边车代理。附加到边车的微服务也可以运行在轻量级 Wasm 运行时中。WasmEdge WebAssembly 运行时是一个安全、轻量级、快速、便携式和多语言运行时，Kubernetes 可以直接&lt;a href="https://wasmedge.org/book/en/kubernetes.html">将其作为容器进行管理&lt;/a>。到 2012 年 12 月，WasmEdge 贡献者社区验证了基于 WasEdge 的微服务可以与 &lt;a href="https://github.com/second-state/dapr-wasm">Dapr&lt;/a> 和 &lt;a href="https://github.com/Liquid-Reply/kind-crun-wasm">Linkerd&lt;/a> 边车一同工作，作为带有 guest 操作系统和完整软件对战的重量级 Linux 容器的替代品。与 Linux 容器应用程序相比，WebAssembly 微服务消耗了 1% 的资源，冷启动时间也只用了 1%。&lt;/p>
&lt;p>eBPF 和 Wasm 是服务网格应用的新方向，以便在数据平面上实现高性能。他们仍然是新兴技术，但可能会成为微服务生态系统 Linux 容器的替代品或者补充。&lt;/p>
&lt;p>&lt;strong>关于作者：&lt;/strong>&lt;/p>
&lt;p>Vivian Hu：Vivian 是亚洲的开源爱好者和开发人员倡导者，Second State 的产品经理。她非常关心通过更好的工具、文档和教程来改善开发人员体验和生产力。Vivian 在 WebAssembly.today 上为 WebAssembly、Rust 和无服务器编写每周时事通讯。&lt;/p></description></item><item><title>OpenFaaS - 以自己的方式运行容器化函数</title><link>https://atbug.com/openfaas-case-study-zh/</link><pubDate>Fri, 17 Dec 2021 09:13:59 +0800</pubDate><guid>https://atbug.com/openfaas-case-study-zh/</guid><description>
&lt;p>&lt;strong>译者注：&lt;/strong>
本文篇幅较长，有助于了解 FaaS 和 OpenFaaS。作者分别从开发人员和运维人员的视角来了解 OpenFaaS，对了解新的技术是个很好的方式。&lt;/p>
&lt;p>本文翻译自 &lt;a href="https://twitter.com/iximiuz">Ivan Velichko&lt;/a> 的 &lt;a href="https://iximiuz.com/en/posts/openfaas-case-study/">OpenFaaS - Run Containerized Functions On Your Own Terms&lt;/a>。&lt;/p>
&lt;hr>
&lt;p>长期以来，&lt;em>无服务器（serverless）&lt;/em> 对我来说无非就是 AWS Lambda 的代名词。Lambda 提供了一种方便的途径，可以将任意代码附加到平台事件（云实例的状态变更、DynamoDB 记录的更新或新的 SNS 消息）中。但是，我时不时会想到某个逻辑，但其又没大到足以有自己的服务，同时有不适合任何现有服务的范围。因此，我经常将其放入函数中，以便日后使用 CLI 命令或者 HTTP 调用来调用它。&lt;/p>
&lt;p>几年前，我来开了 AWS，自那以后，我一直怀念部署无服务器功能的便利性。因此，当我得知 &lt;a href="https://www.openfaas.com">OpenFaaS&lt;/a>
项目时惊喜万分。它将在 Kubernetes 集群上部署函数变得简单，甚至仅需要 Containerd 就可以部署到虚拟机上。&lt;/p>
&lt;p>有兴趣？那么继续！&lt;/p>
&lt;h2 id="无服务器与-faas">无服务器与 FaaS&lt;/h2>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Serverless_computing">无服务器&lt;/a> 已成为一个流行词，目前其实际含义扔不够清晰。&lt;/p>
&lt;p>许多现代平台被视为 &lt;em>无服务器&lt;/em> 平台。在 AWS Fargate 或 GCP Cloud Run 上部署容器化服务？无服务器！在 Heroku 上运行应用程序？也可能是无服务器的。&lt;/p>
&lt;p>同时，我更喜欢将 &lt;a href="https://en.wikipedia.org/wiki/Function_as_a_service">FaaS&lt;/a> 视为一种具体的设计模式。按照 FaaS 范式，可以部署代码片段（响应某些外部时间执行的&lt;em>函数&lt;/em>）。这些函数与事件驱动程序中的回调类似，但是是运行在其他人的的服务器上。由于操作的是函数而不是服务器，顾名思义 FaaS 是无服务器的。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/20211217-at-001929.png" alt="">
&lt;em>&lt;a href="https://twitter.com/iximiuz/status/1465273596033609736?ref_src=twsrc%255Etfw%257Ctwcamp%255Etweetembed%257Ctwterm%255E1465273596033609736%257Ctwgr%255E%257Ctwcon%255Es1_&amp;amp;ref_url=https%253A%252F%252Fiximiuz.com%252Fen%252Fposts%252Fopenfaas-case-study%252F">source&lt;/a>&lt;/em>&lt;/p>
&lt;p>&lt;strong>OpenFaaS 项目旨在将 Kubernetes 集群或者独立的虚拟机等低级基础设施转化为管理无服务器函数的高级平台。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>站在开发人员的角度&lt;/strong>，这样一个平台看起来是真的无服务器的 &amp;ndash; 你只需要知道特定的 CLI/UI/API 来处理 &lt;em>函数&lt;/em> 抽象。但&lt;strong>站在运维的角度&lt;/strong>，需要了解 OpenFaaS 如何使用 &lt;em>服务器&lt;/em> 来运行这些函数。&lt;/p>
&lt;p>就我而言，我经常既是开发又是运维，下面我将尝试从二者展开说明。然而，我认为在评估 UX 时，我们应该明确区分它们。&lt;/p>
&lt;h2 id="开发人员眼中的-openfaas">开发人员眼中的 OpenFaaS&lt;/h2>
&lt;p>OpenFaaS 创建于 2016 年，现在网上也有大量的教程。这里不会重复介绍，但可以通过以下链接了解：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.openfaas.com/deployment/">How to deploy OpenFaaS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.openfaas.com/cli/templates/">Create Functions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.openfaas.com/cli/build/">Build Functions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.openfaas.com/cli/templates/#nodejs-12-node12-of-watchdog-template">Writing a Node.js function - step-by-step guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>相反，我将描述我所理解的 OpenFaaS。我希望有助于一些需要评估该技术是否解决其问题的人，以及那些希望更有效地使用该技术的人。&lt;/p>
&lt;h3 id="函数运行时">函数运行时&lt;/h3>
&lt;p>在进入正式编码之前，有必要了解下其未来的执行环境（又名运行时）。或者，简单说：&lt;/p>
&lt;ul>
&lt;li>如何启动函数&lt;/li>
&lt;li>如何组织 I/O 操作&lt;/li>
&lt;li>如何重置 / 终止函数&lt;/li>
&lt;li>如何隔离函数和调用&lt;/li>
&lt;/ul>
&lt;p>OpenFaaS 自带多个运行时模式，这些模式针对不同的场景定制。因此，不同的场景下上述问题的答案会略有不同。&lt;/p>
&lt;p>&lt;strong>OpenFaaS 函数在容器中运行&lt;/strong>，并且每个容器必须遵守&lt;a href="https://docs.openfaas.com/reference/workloads/">简单的约定&lt;/a> ：它作为监听在预设端口（默认为 &lt;em>8080&lt;/em>）上的 HTTP 服务器，临时存储并且是无状态的。&lt;/p>
&lt;p>然而，OpenFaaS 通过 &lt;em>函数 watchdog&lt;/em>（译者注：watchdog 不做翻译）模式避免了用户编写此类服务器。&lt;em>函数 watchdog&lt;/em> 是一种轻量级 HTTP 服务器，可以感知如何执行实际函数业务逻辑。因此，安装在容器中的所有内容加上作为入口点的 watchdog，就构成了函数的运行时环境。&lt;/p>
&lt;h4 id="经典-watchdog">经典 watchdog&lt;/h4>
&lt;p>从&lt;strong>最简单&lt;/strong>的开始，或者又是被称为&lt;a href="https://github.com/openfaas/classic-watchdog">&lt;em>经典&lt;/em> watchdog&lt;/a>：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16396730767429.png" alt="">&lt;/p>
&lt;p>这种模式下，watchdog 启动了监听在 &lt;em>8080&lt;/em> 端口的轻量级 HTTP 服务器，每个进来的请求都会：&lt;/p>
&lt;ul>
&lt;li>读取请求头和请求体&lt;/li>
&lt;li>fork 或者 exec 包含实际函数的可执行文件&lt;/li>
&lt;li>将请求头和请求体写入到函数进程的 &lt;em>stdin&lt;/em>&lt;/li>
&lt;li>等待函数进程的退出（或者超市）&lt;/li>
&lt;li>读取函数进程的 &lt;em>stdout&lt;/em> 和 &lt;em>stderr&lt;/em>&lt;/li>
&lt;li>在 HTTP 响应中将去读的字节发送回调用方&lt;/li>
&lt;/ul>
&lt;p>上述逻辑类似于传统的 &lt;a href="https://en.wikipedia.org/wiki/Common_Gateway_Interface">通用网关接口（CGI）&lt;/a>。一方面，每次函数调用都启动单独的进程看起来不够高效，而另一方面，它确实超级方便，因为 &lt;strong>任何使用 &lt;em>stdio&lt;/em> 流进行 I/O 处理的程序（包括最喜欢的 CLI 工具）都可以部署为 OpenFaaS 函数&lt;/strong>。&lt;/p>
&lt;p>提起&lt;strong>隔离&lt;/strong>，我们有必要区分下&lt;em>函数&lt;/em>和&lt;em>调用&lt;/em>：&lt;/p>
&lt;ul>
&lt;li>OpenFaaS 中的不同函数始终分布在不同的容器中&lt;/li>
&lt;li>一个函数可以有一个或多个容器 —— 取决于缩放选项&lt;/li>
&lt;li>同一函数的独立调用可能会最终进入同一个容器&lt;/li>
&lt;li>同一函数的独立调用将始终使用不同的进程进行&lt;/li>
&lt;/ul>
&lt;h4 id="反向代理-watchdog">反向代理 watchdog&lt;/h4>
&lt;p>&lt;em>注意：使用 OpenFaaS 官方术语，本节讨论在 HTTP 模式下运行的 &lt;a href="https://github.com/openfaas/of-watchdog">of-watchdog&lt;/a>。但我个人认为称之为反向代理 watchdog 更加形象。&lt;/em>&lt;/p>
&lt;p>如果 &lt;strong>经典&lt;/strong> 运行时类似于 CGI，那么这个运行时模式类似于后来的 &lt;a href="https://en.wikipedia.org/wiki/FastCGI">FastCGI&lt;/a>。运行时希望在 watchdog 后面有一个长期运行的 HTTP 服务器，而不是每次函数调用时创建新的进程。这本质上是 &lt;a href="https://github.com/openfaas/of-watchdog/blob/a0289419078824f0a070860f84a6b383eb4f2169/README.md#1-http-modehttp">将 watchdog 组件变成反向代理&lt;/a>：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16396740301372.png" alt="">&lt;/p>
&lt;p>当容器启动时，&lt;strong>反向代理&lt;/strong> watchdog 也会创建一个监听在 &lt;em>8080&lt;/em> 端口的轻量级 HTTP 服务器。然而，与 &lt;strong>经典&lt;/strong> watchdog 不同的是&lt;strong>反向代理&lt;/strong>watchdog 只创建一次函数的进程，并将其当成（长期运行的）上游服务器。然后，函数调用转变成到该上游的 HTTP 请求。&lt;/p>
&lt;p>然而，&lt;strong>反向代理&lt;/strong>模式并不为了取代&lt;strong>经典&lt;/strong>模式。&lt;strong>经典&lt;/strong>模式的强项在于其函数的编写非常简单。这也是没有 HTTP 服务器的代码的唯一选择。比如使用 Cobol、bash 或者 PowerShell 脚本等等编写的函数。&lt;/p>
&lt;p>何时该使用&lt;strong>反向代理&lt;/strong>运行时模式：&lt;/p>
&lt;ul>
&lt;li>函数需要在两次调用之间保持状态：
&lt;ul>
&lt;li>缓存&lt;/li>
&lt;li>持久连接（例如，保持从函数到数据库的连接打开）&lt;/li>
&lt;li>有状态函数 🥴&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>每个函数启动一个进程可能开销很大，为每个调用带来了延迟&lt;/li>
&lt;li>你想运行一个（微）服务作为函数 🤔&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>根据 OpenFaaS 的创建者 &lt;a href="https://twitter.com/alexellisuk">Alex Ellis&lt;/a> 的解释，&lt;em>FaaS&lt;/em>，特别是 OpenFaaS，可以被视为在不依赖服务器抽象的情况下 &lt;a href="https://blog.alexellis.io/introducing-functions-as-a-service/">部署微服务的简化方式&lt;/a>。即 FaaS 是无服务器架构的规范示例。&lt;/p>
&lt;/blockquote>
&lt;p>因此，使用反向代理的方式，函数可以被看作是部署微服务的固执的方式。方便、快速、简单。但使用有状态函数时，要留意由于多个调用可能在同一个进程中结束而导致的警告：&lt;/p>
&lt;ul>
&lt;li>在一个进程中结束的并发调用可能会触发代码中的竞争条件（例如，一个带有全局变量的 Go 函数，而全局变量没有锁的保护）。&lt;/li>
&lt;li>在一个进程中结束的后续调用可能会导致交叉调用数据泄露（当然，就像传统微服务一样）。&lt;/li>
&lt;li>由于该进程在两次调用之间被复用，因此代码中的任何内存泄漏都不会被缓解。&lt;/li>
&lt;/ul>
&lt;h4 id="其他运行时模式">其他运行时模式&lt;/h4>
&lt;p>&lt;strong>经典&lt;strong>运行时模式在将函数结果发送回调用方之前缓冲了函数的整个响应。但如果响应的大小超出了容器的内存怎么办？OpenFaaS 提供了&lt;/strong>响应流&lt;/strong>&lt;a href="https://github.com/openfaas/of-watchdog/blob/a0289419078824f0a070860f84a6b383eb4f2169/README.md#3-streaming-fork-modestreaming---default">另一种运行时模式，该模式仍然为每个调用创建进程，但添加了&lt;/a>。&lt;/p>
&lt;p>另一个又去的场景是从函数中提供静态文件服务。&lt;a href="https://github.com/openfaas/of-watchdog/blob/a0289419078824f0a070860f84a6b383eb4f2169/README.md#4-static-modestatic">OpenFaaS 也有解决方案&lt;/a>。&lt;/p>
&lt;p>这可能是所有的内置运行时模式。但如果仍未满足需求，OpenFaaS 是一个开源项目！看下现有的watchdog（&lt;a href="https://github.com/openfaas/classic-watchdog">1&lt;/a> &amp;amp; &lt;a href="https://github.com/openfaas/of-watchdog">2&lt;/a>），简洁明了。因此，可以随时提交 PR 或者 issue，让整个社区因你的贡献收益。&lt;/p>
&lt;h3 id="编写函数">编写函数&lt;/h3>
&lt;p>此时，我们已经了解函数如何在配备了函数 watchdog 的容器中运行。那么最小的函数是什么样子的？&lt;/p>
&lt;p>下面的示例将简单的 shell 脚本封装到 OpenFaaS 函数中：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="c">########################################################&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># WARNING: Not for Production - No Security Hardening! #&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c">########################################################&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># This FROM is just to get the watchdog executable.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> ghcr.io/openfaas/classic-watchdog:0.2.0 as watchdog&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># FROM this line the actual runtime definion starts.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> alpine:latest&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># Mandatory step - put the watchdog.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> --from&lt;span class="o">=&lt;/span>watchdog /fwatchdog /usr/bin/fwatchdog&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># Optionally - install extra packages, libs, tools, etc.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># Function&amp;#39;s payload - script echoing its STDIN, a bit transformed.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> &lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;#!/bin/sh&amp;#39;&lt;/span> &amp;gt; /echo.sh&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> &lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;cat | rev | tr &amp;#34;[:lower:]&amp;#34; &amp;#34;[:upper:]&amp;#34;&amp;#39;&lt;/span> &amp;gt;&amp;gt; /echo.sh&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> chmod +x /echo.sh&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># Point the watchdog to the actual thingy to run.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">fprocess&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;/echo.sh&amp;#34;&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># Start the watchdog server.&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">CMD&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;fwatchdog&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="err">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>当构建、部署和调用时，上面的函数作为 &lt;em>回显服务器&lt;/em>，倒转并大写其输入。&lt;/p>
&lt;p>稍微高级点的例子：一个 Node.js &lt;em>Hello World&lt;/em> 脚本作为函数：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span class="c">########################################################&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c"># WARNING: Not for Production - No Security Hardening! #&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="c">########################################################&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> ghcr.io/openfaas/classic-watchdog:0.2.0 as watchdog&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="s"> node:17-alpine&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">COPY&lt;/span> --from&lt;span class="o">=&lt;/span>watchdog /fwatchdog /usr/bin/fwatchdog&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">RUN&lt;/span> &lt;span class="nb">echo&lt;/span> &lt;span class="s1">&amp;#39;console.log(&amp;#34;Hello World!&amp;#34;)&amp;#39;&lt;/span> &amp;gt; index.js&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">ENV&lt;/span> &lt;span class="nv">fprocess&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;node index.js&amp;#34;&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">
&lt;/span>&lt;span class="err">&lt;/span>&lt;span class="k">CMD&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;fwatchdog&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="err">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>因此，要编写一个简单的函数，只需要在 Dockerfile 中加入：&lt;/p>
&lt;ul>
&lt;li>实际脚本（或可执行文件）&lt;/li>
&lt;li>它的所有依赖项——软件包、操作系统库等&lt;/li>
&lt;li>首选的 watchdog&lt;/li>
&lt;/ul>
&lt;p>然后将 watchdog 指向该脚本（或可执行文件），并将 watchdog 作为入口。有点酷，因为：&lt;/p>
&lt;ul>
&lt;li>可以完全控制函数未来的运行时&lt;/li>
&lt;li>可以部署任何可以在容器中作为函数运行的东西&lt;/li>
&lt;/ul>
&lt;p>但上述方法有个明显的缺点 &amp;ndash; 一个生产就绪的 Dockerfile 可能有上百行。如果我只想运行一个简单的 Node.js/Python 脚本或者一个小的 Go 程序作为函数，要怎么处理 Dockerfile？就不能有一个占位符来粘贴代码片段？&lt;/p>
&lt;h4 id="功能模板">功能模板&lt;/h4>
&lt;p>OpenFaaS 的美妙之处在于，我们可以两者兼而有之 —— 使用 Dockerfile 进行低级修补或目标语言编写高级脚本！得益于丰富的功能模板库！&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ faas-cli template store list
NAME SOURCE DESCRIPTION
csharp openfaas Classic C# template
dockerfile openfaas Classic Dockerfile template
go openfaas Classic Golang template
java8 openfaas Java &lt;span class="m">8&lt;/span> template
...
node14 openfaas HTTP-based Node &lt;span class="m">14&lt;/span> template
node12 openfaas HTTP-based Node &lt;span class="m">12&lt;/span> template
node openfaas Classic NodeJS &lt;span class="m">8&lt;/span> template
php7 openfaas Classic PHP &lt;span class="m">7&lt;/span> template
python openfaas Classic Python 2.7 template
python3 openfaas Classic Python 3.6 template
...
python3-flask openfaas Python 3.7 Flask template
python3-http openfaas Python 3.7 with Flask and HTTP
...
golang-http openfaas Golang HTTP template
...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>上述功能模板由 OpenFaaS 作者和社区精心只做。典型的模板附带一个复杂的 Dockerfile，指向虚拟处理程序函数。当引导新函数时，通过 &lt;code>faas-cli new&lt;/code> 命令来使用这些模板。例如：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ faas-cli new --lang python my-fn
Folder: my-fn created.
Function created in folder: my-fn
Stack file written: my-fn.yml
$ cat my-fn/handler.py
def handle&lt;span class="o">(&lt;/span>req&lt;span class="o">)&lt;/span>:
&lt;span class="s2">&amp;#34;&amp;#34;&amp;#34; PUT YOUR BUSINESS LOGIC HERE &amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;span class="k">return&lt;/span> req
&lt;/code>&lt;/pre>&lt;/div>&lt;p>因此，对于模板，编写函数的工作可以归结为简单地将业务逻辑放入响应的处理程序文件中。&lt;/p>
&lt;p>使用模板时，了解使用那种 &lt;em>watchdog&lt;/em> 和 &lt;em>模式&lt;/em> 很重要：&lt;/p>
&lt;ul>
&lt;li>使用&lt;strong>经典的&lt;/strong>类似 CGI 的 watchdog，处理程序通常被编写为接受和返回纯字符串的函数（例如：&lt;a href="https://github.com/openfaas/templates/blob/d8893afe3d1072840174911859c6f5db2986e814/template/python3/function/handler.py">python3&lt;/a>、&lt;a href="https://github.com/openfaas/templates/blob/d8893afe3d1072840174911859c6f5db2986e814/template/php7/function/src/Handler.php">php7&lt;/a>）&lt;/li>
&lt;li>在 &lt;strong>HTTP 模式下&lt;/strong>，使用 &lt;strong>of-watchdog&lt;/strong>时，处理程序看起来更像 HTTP 处理程序接受请求并返回响应结构（例如：&lt;a href="https://github.com/openfaas/python-flask-template/blob/12db680950b42c7cfcc7d21ba036bd1397d62eb7/template/python3-http/function/handler.py">python3-http&lt;/a>，&lt;a href="https://github.com/openfaas/templates/blob/d8893afe3d1072840174911859c6f5db2986e814/template/node17/function/handler.js">node17&lt;/a>）。&lt;/li>
&lt;/ul>
&lt;h4 id="函数商店">函数商店&lt;/h4>
&lt;p>你的最佳函数是什么？对，你不需要写。OpenFaaS 接受这种想法，并带来了&lt;a href="https://github.com/openfaas/store">函数商店&lt;/a>（经过社区测试并根据过往经验选择的 OpenFaaS 函数精选索引）。&lt;/p>
&lt;p>该商店包含一些有趣的函数，可以一键部署到现有的 OpenFaaS 中：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-shell" data-lang="shell">$ faas-cli store list
FUNCTION DESCRIPTION
NodeInfo Get info about the machine that you&lt;span class="s1">&amp;#39;r...
&lt;/span>&lt;span class="s1">alpine An Alpine Linux shell, set the &amp;#34;fproc...
&lt;/span>&lt;span class="s1">env Print the environment variables prese...
&lt;/span>&lt;span class="s1">sleep Simulate a 2s duration or pass an X-S...
&lt;/span>&lt;span class="s1">shasum Generate a shasum for the given input
&lt;/span>&lt;span class="s1">Figlet Generate ASCII logos with the figlet CLI
&lt;/span>&lt;span class="s1">curl Use curl for network diagnostics, pas...
&lt;/span>&lt;span class="s1">SentimentAnalysis Python function provides a rating on ...
&lt;/span>&lt;span class="s1">hey HTTP load generator, ApacheBench (ab)...
&lt;/span>&lt;span class="s1">nslookup Query the nameserver for the IP addre...
&lt;/span>&lt;span class="s1">SSL/TLS cert info Returns SSL/TLS certificate informati...
&lt;/span>&lt;span class="s1">Colorization Turn black and white photos to color ...
&lt;/span>&lt;span class="s1">Inception This is a forked version of the work ...
&lt;/span>&lt;span class="s1">Have I Been Pwned The Have I Been Pwned function lets y...
&lt;/span>&lt;span class="s1">Face Detection with Pigo Detect faces in images using the Pigo...
&lt;/span>&lt;span class="s1">Tesseract OCR This function brings OCR - Optical Ch...
&lt;/span>&lt;span class="s1">Dockerhub Stats Golang function gives the count of re...
&lt;/span>&lt;span class="s1">QR Code Generator - Go QR Code generator using Go
&lt;/span>&lt;span class="s1">Nmap Security Scanner Tool for network discovery and securi...
&lt;/span>&lt;span class="s1">ASCII Cows Generate a random ASCII cow
&lt;/span>&lt;span class="s1">YouTube Video Downloader Download YouTube videos as a function
&lt;/span>&lt;span class="s1">OpenFaaS Text-to-Speech Generate an MP3 of text using Google&amp;#39;&lt;/span>...
Docker Image Manifest Query Query an image on the Docker Hub &lt;span class="k">for&lt;/span> ...
face-detect with OpenCV Detect faces in images. Send a URL as...
Face blur by Endre Simo Blur out faces detected in JPEGs. Inv...
Left-Pad left-pad on OpenFaaS
normalisecolor Automatically fix white-balance in ph...
mememachine Turn any image into a meme.
Business Strategy Generator Generates a Business Strategy &lt;span class="o">(&lt;/span>using ...
Image EXIF Reader Reads EXIF information from an URL or...
Open NSFW Model Score images &lt;span class="k">for&lt;/span> NSFW &lt;span class="o">(&lt;/span>nudity&lt;span class="o">)&lt;/span> content.
Identicon Generator Create an identicon from a provided s...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>这些函数实际上是存储在 Docker Hub 或者 Quay 等公共库的容器镜像，可以自由复用。&lt;/p>
&lt;p>场景示例：&lt;/p>
&lt;ul>
&lt;li>使用 &lt;code>env&lt;/code> 函数调试函数接收的HTTP标头&lt;/li>
&lt;li>使用 &lt;code>curl&lt;/code> 函数从 OpenFaaS 部署内部测试连接&lt;/li>
&lt;li>从运行多个副本的函数中使用 &lt;code>hey&lt;/code> 来增加负载&lt;/li>
&lt;/ul>
&lt;h3 id="函数的构建和部署">函数的构建和部署&lt;/h3>
&lt;p>由于函数是在容器中运行的，因此需要有人为这些容器构建镜像。无论你喜不喜欢，这都是开发人员的事情。OpenFaaS 提供了方便的 &lt;code>faas-cli build&lt;/code> 命令，但没有服务器端构建。因此，要么需要（在安装 Docker 的机器上）手动运行 &lt;code>faas-cli build&lt;/code>，要么使用 CI/CD 完成。&lt;/p>
&lt;p>接下来，构建好的镜像需要通过 &lt;code>faas-cli push&lt;/code> 到仓库。显然，这种仓库也应该可以从 OpenFaaS 服务器端访问。否则，使 用&lt;code>faas-cli deploy&lt;/code> 部署函数时会失败。&lt;/p>
&lt;p>开发人员的工作流程如下：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16396975427765.png" alt="">&lt;/p>
&lt;h3 id="调用函数">调用函数&lt;/h3>
&lt;p>函数部署后，可以通过向 &lt;code>$API_HOST:$API_PORT/function/&amp;lt;fn-name&amp;gt;&lt;/code> 端点发送 GET、POST、PUT 或者 DELET HTTP 请求来调用它。常见的调用方式有：&lt;/p>
&lt;ul>
&lt;li>各种钩子（webhook）&lt;/li>
&lt;li>&lt;code>faas-cli invoke&lt;/code>&lt;/li>
&lt;li>&lt;strong>event connectors&lt;/strong>！&lt;/li>
&lt;/ul>
&lt;p>前两个选项相当简单。使用函数作为作为 webhook 处理器（GitHub、IFTTT 等）很方便，每个函数开发人员都已经安装了 &lt;code>faas-cli&lt;/code>，因此可以成为日常脚本编写的组成部分。&lt;/p>
&lt;h4 id="那什么是事件连接器">那什么是事件连接器？&lt;/h4>
&lt;p>在本文开头是我对 AWS Lambda 与 AWS 平台事件紧密集成的温暖回忆。请记住，可以在响应新 SQS/SNS 消息、新的 Kinesis 记录、EC2 实例生命周期等事件时调用 Lambda。OpenFaaS 函数是否存在类似的东西呢？&lt;/p>
&lt;p>显然，OpenFaaS 无法开箱即用地与任何生态系统集成。然而，它提供了一种名为&lt;a href="https://docs.openfaas.com/reference/triggers/#event-connector-pattern">&lt;strong>事件连接器&lt;/strong>&lt;/a>模式的通用解决方案。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16396992441046.png" alt="">&lt;/p>
&lt;p>官方支持的连接器：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/openfaas/cron-connector">Cron connector&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/openfaas/mqtt-connector">MQTT connector&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/openfaas/nats-connector">NATS connector&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.openfaas.com/reference/triggers/#apache-kafka-openfaas-pro">Kafka connector&lt;/a> (需要&lt;strong>专业版&lt;/strong>订阅)&lt;/li>
&lt;/ul>
&lt;p>OpenFaaS 还提供了很小的&lt;a href="https://github.com/openfaas/connector-sdk">&lt;strong>连接器-sdk&lt;/strong>&lt;/a>库来简化连接器的开发。&lt;/p>
&lt;h2 id="运维眼中的-openfaas">运维眼中的 OpenFaaS&lt;/h2>
&lt;p>开发眼中的 OpenFaaS 是个黑盒，提供简单的 API 来部署和调用函数。然而，作为运维可能会从了解 一点 OpenFaaS 内部原理中受益。&lt;/p>
&lt;h3 id="openfaas-通用架构">OpenFaaS 通用架构&lt;/h3>
&lt;p>OpenFaaS 有一个简单但强大的架构，允许使用不同的基础设施作为后端。如果已经有了 Kubernetes 集群，可以通过&lt;a href="https://docs.openfaas.com/deployment/kubernetes/">在上面部署 OpenFaaS&lt;/a> 轻松将其变成 &lt;a href="https://docs.openfaas.com/deployment/kubernetes/">FaaS&lt;/a> 解决方案。但是如果旧的虚拟（或物理）机，仍然可以在上面安装 OpenFaaS，并获得差不多功能的更小的 FaaS 解决方案。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16396997846325.png" alt="">&lt;/p>
&lt;p>上面的架构中唯一面向用户的组件是 &lt;a href="https://github.com/openfaas/faas/tree/fe152ba69c591d584b1f183f9f5f209e29a9b049/gateway">API 网关&lt;/a>。OpenFaaS 的 API 网关：&lt;/p>
&lt;ul>
&lt;li>暴露 API 来管理和调用函数&lt;/li>
&lt;li>提供内置的 UI 来管理函数&lt;/li>
&lt;li>处理函数的自动缩放&lt;/li>
&lt;li>预计后面会有兼容的 OpenFaaS 提供商&lt;/li>
&lt;/ul>
&lt;p>因此，当开发人员运行 &lt;code>faas-cli deploy&lt;/code>、&lt;code>faas-cli list&lt;/code> 或使用 &lt;code>curl $API_URL/function/foobar&lt;/code> 调用函数等内容时，请求将发送到上述的 API 网关。&lt;/p>
&lt;p>上图中的另一个重要组成部分是  &lt;a href="https://github.com/openfaas/faas-provider">faas-provider&lt;/a>。它不是一个具体的组件，而更像是接口。任何实现&lt;a href="https://github.com/openfaas/faas-provider/blob/36474d89ca995ea0a7c064493258d9edec88fe3f/serve.go#L32-L96">（非常简洁）的提供商 API&lt;/a> 的软件都可以成为提供商。OpenFaaS 提供商：&lt;/p>
&lt;ul>
&lt;li>管理功能（部署、列表、缩放、删除）&lt;/li>
&lt;li>调用函数&lt;/li>
&lt;li>暴露一些系统信息&lt;/li>
&lt;/ul>
&lt;p>两个最注明的提供商是 &lt;a href="https://github.com/openfaas/faas-netes">faas-netes&lt;/a>（Kubernetes 上的 OpenFaaS）和 &lt;a href="https://github.com/openfaas/faasd">faasd&lt;/a>（Containerd 上的 OpenFaaS）。下面，将介绍他们的实现。&lt;/p>
&lt;h3 id="kubernetes-上的-openfaasfaas-nets">Kubernetes 上的 OpenFaaS（faas-nets）&lt;/h3>
&lt;p>&lt;a href="https://github.com/openfaas/faas-netes">当部署在 Kubernetes 上时&lt;/a>，OpenFaaS 利用了该平台开箱即用的强大原语。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16397005465805.png" alt="">&lt;/p>
&lt;p>关键要点：&lt;/p>
&lt;ul>
&lt;li>API 网关成为标准（部署+服务）对。因此，可以随心所欲地扩展它。也可以随心所欲地把它暴露出来&lt;/li>
&lt;li>每个函数也成为（部署+服务）对。可能不会直接处理函数，但对于 faas-netes，缩放变得就像调整相应的副本数一样简单&lt;/li>
&lt;li>高可用性和开箱即用的水平缩放 - 同一功能的 pod 可以（而且应该）跨多个集群节点运行。&lt;/li>
&lt;li>Kubernetes 作为一个数据库工作；例如，当运行 &lt;code>faas-cli list&lt;/code> 等命令来获取当前部署的函数列表时，faas-netes 只会将其转换为相应的 Kubernetes API 查询&lt;/li>
&lt;/ul>
&lt;h3 id="containerd-上的-openfaasfaasd">Containerd 上的 OpenFaaS（faasd）&lt;/h3>
&lt;p>对于没有使用 Kubernetes 集群的人来说，OpenFaaS 提供了名为 &lt;a href="https://github.com/openfaas/faasd">faasd&lt;/a> 的替代轻量级提供商。它可以安装在（虚拟或物理）服务器上，，并利用 &lt;a href="https://iximiuz.com/en/posts/containerd-command-line-clients/">containerd&lt;/a> 来管理容器。&lt;a href="https://iximiuz.com/en/posts/journey-from-containerization-to-orchestration-and-beyond/#containerd">正如我之前写的那样&lt;/a>，容器是一个在 Docker 和 Kubernetes 下使用的较低级别的容器管理器。结合 &lt;a href="https://github.com/containernetworking/plugins">CNI 插件&lt;/a>，它成为编写容器调度器的构建组件，OpenFaaS 的 faasd 是个很好的讲究案例：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/12/17/16397012181479.png" alt="">&lt;/p>
&lt;p>关键要点：&lt;/p>
&lt;ul>
&lt;li>被设计为在 IoT 设备上或 VM 中运行&lt;/li>
&lt;li>使用 containerd 的原生 &lt;code>pause&lt;/code> （通过&lt;em>cgroup freezers&lt;/em>）和超快速函数冷启动快速扩展到零&lt;/li>
&lt;li>&lt;a href="https://metal.equinix.com/proximity/?wchannelid=ujj9b20qi5&amp;amp;wmediaid=hkkw4b4o5n">它可以在每台服务器上运行比 faas-netes 多十倍的函数&lt;/a>，并且可以有效地使用更便宜的硬件，包括树莓派&lt;/li>
&lt;li>containerd 和 faasd 作为 systemd 服务进行管理，因此会自动处理日志、重启等&lt;/li>
&lt;li>没有 Kubernetes DNS，但 faasd 确保 DNS 在函数之间共享以简化互操作&lt;/li>
&lt;li>containerd 扮演着数据库的角色（比如 &lt;code>faas-cli list&lt;/code> 变成了类似 &lt;code>ctr container list&lt;/code>的操作 ），所以如果服务器挂了，所有状态就会丢失，每个函数都需要重新部署&lt;/li>
&lt;li>没有开箱即用的高可用性或水平扩展（参见 &lt;a href="https://github.com/openfaas/faasd/issues/225">issue/225&lt;/a>）&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>对你所使用的软件有个好的心智模型是是有益的，它可以提高开发效率，防止单例场景的发生，并简化了故障排查。&lt;/p>
&lt;p>以 OpenFaaS 为例，区分开发人员和运维人员对系统的看法可能是个很好的思路。从&lt;strong>开发人员角度来看&lt;/strong>，这是一个简单而强大的无服务器解决方案，主要关注 FaaS 场景。该解决方案由一个用于管理和调用函数的简洁 API、一个涵盖开发人员工作流的命令行工具以及一个函数模板库组成。无服务器函数以不同的运行时模式（类 CGI、反向代理）在容器中运行，并提供不同的隔离和状态保障。&lt;/p>
&lt;p>从&lt;strong>运维人员的角度来看&lt;/strong>，OpenFaaS 是一个具有灵活架构的模块化系统，可以部署在不同类型的基础设施之上：从树莓派到裸机或虚拟机、以及成熟的 Kubernetes、OpenShift 或 Docker Swarm 集群。当然，每种选择都有其优缺点，需要详细评估取舍。但即使现有选项都不合适，简单的 &lt;em>faas-provider&lt;/em> 抽象允许开发自己的后端来运行无服务器功能。&lt;/p>
&lt;p>上述内容主要集中在 OpenFaaS 基础知识上。但是 OpenFaaS 也有一些高级功能。通过以下链接进一步了解：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.openfaas.com/reference/async/">使用 NATS 消息传递系统的异步函数调用&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.openfaas.com/architecture/autoscaling/">使用 Prometheus 和 AlertManager 自动缩放功能&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/openfaas/faas-middleware/tree/ace4eb24749c0814850a56b23b0015232dd41c2a/concurrency-limiter">函数调用限制&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://openfaas.gumroad.com/l/serverless-for-everyone-else">使用 docker-compose 通过 faasd 运行有状态的工作负载&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="资源">资源&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://docs.openfaas.com/">OpenFaaS 官方文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.alexellis.io/deploy-serverless-faasd-with-cloud-init/">一堆清晰的 OpenFaaS 用例&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.alexellis.io/cli-functions-with-openfaas/">使用 OpenFaaS 将任何 CLI 转换为函数&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.openfaas.com/blog/introducing-faasd/">faasd 介绍、动机、主要用例&lt;/a>&lt;/li>
&lt;li>📖&lt;a href="https://openfaas.gumroad.com/l/serverless-for-everyone-else">面向其他人的无服务器&lt;/a>- 尽管有通用名称，但它是 faasd 的一个非常详细的指南&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>作者介绍：
Ivan Velichko
Software Engineer at heart, SRE at day job, Tech Storyteller at night.&lt;/p>
&lt;/blockquote></description></item><item><title>低复杂度 - 服务网格的下一站</title><link>https://atbug.com/service-mesh-unnecessary-complexity/</link><pubDate>Fri, 15 Oct 2021 07:58:25 +0800</pubDate><guid>https://atbug.com/service-mesh-unnecessary-complexity/</guid><description>
&lt;p>&lt;strong>译者：&lt;/strong>&lt;/p>
&lt;p>作为一个曾经在制造业企业的基础架构团队任职，为支持公司的“互联网基因”和“数字化转型”落地了云原生基础设施平台，并在尝试采用服务网格未成的我来说，看到这篇文章深有感触。尤其是文中所说的“人少，问题多，需要快速输出价值”，直戳到了痛处。有限的人手有限的时间，我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上，要想很好的运行复杂的系统是非常困难的。&lt;/p>
&lt;p>服务网格是一个新的基础设施层，可以承载很多的功能，未来还会有更大的想象空间和光明的未来。&lt;/p>
&lt;p>以上的种种原因，也促使我后来选择进入一家提供服务网格的产品企业，也希望服务网格可以被更简单的使用。&lt;/p>
&lt;p>“道阻且长，行则将至！”&lt;/p>
&lt;p>本文翻译自 Chris Campbell 的 &lt;a href="https://www.infoq.com/articles/service-mesh-unnecessary-complexity">How Unnecessary Complexity Gave the Service Mesh a Bad Name&lt;/a>&lt;/p>
&lt;hr>
&lt;h3 id="关键要点">关键要点&lt;/h3>
&lt;ul>
&lt;li>采用服务网格有巨大的价值，但必须以轻量级的方式进行，以避免不必要的复杂性。&lt;/li>
&lt;li>在实施服务网时，要采取务实的方法，与技术的核心功能保持一致，并小心干扰（译者：注意力的分散）。&lt;/li>
&lt;li>服务网格的一些核心特性包括标准化监控、自动加密和身份识别、智能路由、可靠的重试和网络可扩展性。&lt;/li>
&lt;li>服务网格可以提供强大的功能，但这些功能会分散本应对核心优势的关注，并且这些功能也不是实施服务网格的主要原因。&lt;/li>
&lt;li>在初始实施服务网格时没有必要去关注那些明显会分散注意力的功能，比如复杂的控制平面、多集群支持、Envoy、WASM 和 A/B 测试。&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>服务网格是 Kubernetes 世界中的一个热门话题，但许多潜在的采用者已经有些失望了。服务网格的落地受到压倒性的复杂性和看似无穷无尽的供应商解决方案的限制。在我亲自浏览了这个领域之后，我发现采用服务网格具有巨大的价值，但它必须以轻量级的方式完成，以避免不必要的复杂性。尽管普遍存在幻灭感，但服务网格的未来依然光明。&lt;/p>
&lt;h2 id="在工作中学习">在工作中学习&lt;/h2>
&lt;p>我进入服务网格的世界始于我在一家老牌的财富 500 强技术公司担任云计算架构师的角色。在开始我们的服务网格之旅时，我身边有许多强大的工程师，但大多数人几乎没有云计算开发经验。我们的组织诞生于云计算之前，完全实现云计算的价值需要时间。我们的传统业务线主要集中在技术栈的硬件元素上，云计算的决策最初是由为运送硬件或为该硬件提供固件和驱动程序而开发的流程驱动的。&lt;/p>
&lt;p>随着该组织经历其“数字化转型”，它越来越依赖于提供高质量的软件服务，并逐渐开发出更好的方法。但作为云计算架构师，我仍在为优先考虑硬件的业务流程，以及具有不同技能、流程和信念的工程团队导航。随着时间的推移，我和我的团队在将 .NET 应用程序迁移到 Linux、采用 Docker、迁移到 AWS 以及与之相关的最佳实践（如持续集成、自动化部署、不可变基础设施、基础设施即代码、监控等）方面变得熟练并成功。但挑战依然存在。&lt;/p>
&lt;p>在此期间，我们开始将我们的应用程序拆分为一组微服务。起初，这是一个缓慢的转变，但最终这种方法流行起来，开发人员开始更喜欢构建新的服务而不是添加到现有服务。我们这些基础设施团队的人把这看作是一种成功。唯一的问题是与网络相关的问题数量激增，开发人员正在向我们寻求答案，而我们还没有准备好有效地应对这种冲击。&lt;/p>
&lt;h2 id="服务网格的援救">服务网格的援救&lt;/h2>
&lt;p>我第一次听说服务网格是在 2015 年，当时我正在研究服务发现工具并寻找与 Consul 集成的简单方法。我喜欢将应用程序职责卸载到“sidecar”容器的想法，并找到了一些可以帮助做到这一点的工具。大约在这个时候，Docker 有一个叫做“链接”的功能，让你可以将两个应用程序放在一个共享的网络空间中，这样它们就可以通过 localhost 进行通信。此功能提供了类似于我们现在在 Kubernetes pod 中所拥有的体验：两个独立构建的服务可以在部署时进行组合以实现一些附加功能。&lt;/p>
&lt;p>我总是抓住机会用简单的方案来解决大问题，因此这些新功能的力量立即打动了我。虽然这个工具是为了与 Consul 集成而构建的，但实际上，它可以做任何你想做的事情。这是我们拥有的基础设施层，可以用来一次为所有人解决问题。&lt;/p>
&lt;p>这方面的一个具体例子出现在我们采用过程的早期。当时，我们正致力于跨不同服务的日志标准化输出。通过采用服务网格和这种新的设计模式，我们能够将我们的人的问题——让开发人员标准化他们的日志——换成技术问题——将所有流量传递给可以为他们记录日志的代理。这是我们团队向前迈出的重要一步。&lt;/p>
&lt;p>我们对服务网格的实现非常务实，并且与该技术的核心功能非常吻合。然而，大部分营销炒作都集中在不太需要的边缘案例上，在评估服务网格是否适合你时，能够识别这些干扰是很重要的。&lt;/p>
&lt;h1 id="核心功能">核心功能&lt;/h1>
&lt;p>服务网格可以提供的核心功能分为四个关键责任领域：可观察性、安全性、连接性和可靠性。这些功能包括：&lt;/p>
&lt;h2 id="标准化监控">标准化监控&lt;/h2>
&lt;p>我们取得的最大胜利之一，也是最容易采用的，是标准化监控。它的运营成本非常低，可以适应你使用的任何监控系统。它使组织能够捕获所有 HTTP 或 gRPC 指标，并以标准方式在整个系统中存储它们。这控制了复杂性并减轻了应用程序团队的负担，他们不再需要实现 Prometheus 指标端点或标准化日志格式。它还使用户能够公正地了解其应用程序的&lt;a href="https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals">黄金信号&lt;/a>。&lt;/p>
&lt;h2 id="自动加密和身份识别">自动加密和身份识别&lt;/h2>
&lt;p>证书管理很难做好。如果一个组织还没有在这方面进行投入，他们应该找到一个网格来为他们做这件事。证书管理需要维护具有巨大安全隐患的复杂基础设施代码。相比之下，网格将能够与编排系统集成，以了解工作负载的身份，在需要时可以用来执行策略。这允许提供与 Calico 或 Cilium 等功能强大的 CNI 提供的安全态势相当或更好的安全态势。&lt;/p>
&lt;h2 id="智能路由">智能路由&lt;/h2>
&lt;p>智能路由是另一个特性，它使网格能够在发送请求时“做正确的事”。场景包括：&lt;/p>
&lt;ol>
&lt;li>使用延迟加权算法优化流量&lt;/li>
&lt;li>拓扑感知路由以提高性能并降低成本&lt;/li>
&lt;li>根据请求成功的可能性使请求超时&lt;/li>
&lt;li>与编排系统集成以进行 IP 解析，而不是依赖 DNS&lt;/li>
&lt;li>传输升级，例如 HTTP 到 HTTP/2&lt;/li>
&lt;/ol>
&lt;p>这些功能可能不会让普通人感到兴奋，但随着时间的推移，它们从根本上增加了价值&lt;/p>
&lt;h2 id="可靠的重试">可靠的重试&lt;/h2>
&lt;p>在分布式系统中重试请求可能很麻烦，但是它几乎总是需要实现的。分布式系统通常会将一个客户端请求转换为更多下游请求，这意味着“尾巴”场景的可能性会大大增加，例如发生异常失败的请求。对此最简单的缓解措施是重试失败的请求。&lt;/p>
&lt;p>困难来自于避免“重试风暴”或“重试 DDoS”，即当处于降级状态的系统触发重试、随着重试增加而增加负载并进一步降低性能时。天真的实现不会考虑这种情况，因为它可能需要与缓存或其他通信系统集成以了解是否值得执行重试。服务网格可以通过对整个系统允许的重试总数进行限制来实现这一点。网格还可以在这些重试发生时报告这些重试，可能会在你的用户注意到系统降级之前提醒你。&lt;/p>
&lt;h2 id="网络可扩展性">网络可扩展性&lt;/h2>
&lt;p>也许服务网格的最佳属性是它的可扩展性。它提供了额外的适应性层，以应对 IT 下一步投入的任何事情。Sidecar 代理的设计模式是另一个令人兴奋和强大的功能，即使它有时会被过度宣传和过度设计来做用户和技术人员还没有准备好的事情。虽然社区在等着看哪个服务网格“生出”，这反映了之前过度炒作的编排战争，但未来我们将不可避免地看到更多专门构建的网格，并且可能会有更多的最终用户构建自己的控制平面和代理以满足他们的场景。&lt;/p>
&lt;h1 id="服务网格干扰">服务网格干扰&lt;/h1>
&lt;p>平台或基础设施控制层的价值怎么强调都不为过。然而，在服务网格世界中，我了解到入门的一个主要的挑战是，服务网格解决的核心问题通常甚至不是大多数服务网格项目交流的焦点！&lt;/p>
&lt;p>相反，来自服务网格项目的大部分交流都围绕着听起来很强大或令人兴奋但最终会让人分心的功能。这包括：&lt;/p>
&lt;h2 id="强复大杂的控制平面">强（复）大（杂）的控制平面&lt;/h2>
&lt;p>要很好地运行复杂的软件是非常困难的。这就是为什么如此多的组织使用云计算来使用完全托管的服务来减轻这一点的原因。那么为什么服务网格项目会让我们负责操作如此复杂的系统呢？系统的复杂性不是资产，而是负债，但大多数项目都在吹捧它们的功能集和可配置性。&lt;/p>
&lt;h2 id="多集群支持">多集群支持&lt;/h2>
&lt;p>多集群现在是一个热门话题。最终，大多数团队将运行多个 Kubernetes 集群。但是多集群的主要痛点是你的 Kubernetes 管理的网络被切分。服务网格有助于解决这个 Kubernetes 横向扩展问题，但它最终并没有带来任何新的东西。是的，多集群支持是必要的，但它对服务网格的承诺被过度宣传了。&lt;/p>
&lt;h2 id="envoy">Envoy&lt;/h2>
&lt;p>Envoy 是一个很棒的工具，但它被作为某种标准介绍，这是有问题的。Envoy 是众多开箱即用的代理之一，你可以将其作为服务网格平台的基础。但是 Envoy 并没有什么内在的特别之处，使其成为正确的选择。采用 Envoy 会给你的组织带来一系列重要问题，包括：&lt;/p>
&lt;ul>
&lt;li>运行时成本和性能（所有这些过滤器加起来！）&lt;/li>
&lt;li>计算资源需求以及如何随负载扩展&lt;/li>
&lt;li>如何调试错误或意外行为&lt;/li>
&lt;li>你的网格如何与 Envoy 交互以及配置生命周期是什么&lt;/li>
&lt;li>运作成熟的时间（这可能比你预期的要长）&lt;/li>
&lt;/ul>
&lt;p>服务网格中代理的选择应该是一个实现细节，而不是产品要求。&lt;/p>
&lt;h2 id="wasm">WASM&lt;/h2>
&lt;p>我是 Web Assembly (WASM) 的忠实拥趸，已经成功地使用它在 &lt;a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor">Blazor&lt;/a> 中构建前端应用程序。然而，WASM 作为定制服务网格代理行为的工具，让你处于获得一个全新的软件生命周期开销的境地，这与你现有的软件生命周期完全正交！如果你的组织还没有准备好构建、测试、部署、维护、监控、回滚和版本代码（影响通过其系统运行的每个请求），那么你还没有准备好使用 WASM。&lt;/p>
&lt;h2 id="ab-测试">A/B 测试&lt;/h2>
&lt;p>直到为时已晚，我才意识到 A/B 测试实际上是一个应用程序级别的问题。在基础设施层提供原语来实现它是很好的，但是没有简单的方法来完全自动化大多数组织需要的 A/B 测试水平。通常，应用程序需要定义独特的指标来定义测试的积极信号。如果组织想要在服务网格级别投入 A/B 测试，那么解决方案需要支持以下内容：&lt;/p>
&lt;ol>
&lt;li>对部署和回滚的精细控制，因为它可能同时进行多个不同的“测试”&lt;/li>
&lt;li>能够捕获系统知道的自定义指标并可以根据这些指标做出决策&lt;/li>
&lt;li>根据请求的特征暴露对流量方向的控制，其中可能包括解析整个请求正文&lt;/li>
&lt;/ol>
&lt;p>这需要实现很多，没有哪个服务网格是开箱即用的。最终，我们的组织选择了网格之外的特征标记解决方案，其以最小的努力取得了巨大的成功。&lt;/p>
&lt;h2 id="我们在哪里结束">我们在哪里结束&lt;/h2>
&lt;p>最终，我们面临的挑战并不是服务网格独有的。我们工作的组织有一系列限制条件，要求我们对解决的问题以及如何解决问题采取务实的态度。我们面临的问题包括：&lt;/p>
&lt;ul>
&lt;li>一个拥有大量不同技能的开发人员的大型组织&lt;/li>
&lt;li>云计算和 SaaS 能力普遍不成熟&lt;/li>
&lt;li>为非云计算软件优化的流程&lt;/li>
&lt;li>碎片化的软件工程方法和信念&lt;/li>
&lt;li>有限的资源&lt;/li>
&lt;li>激进的截止日期&lt;/li>
&lt;/ul>
&lt;p>简而言之，我们人少，问题多，需要快速输出价值。我们必须支持主要不是 Web 或云计算的开发者，我们需要扩大规模以支持有不同方法和流程的大型工程组织来做云计算。我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上。&lt;/p>
&lt;p>最后，当面对我们自己的服务网格决策时，我们决定建立在 &lt;a href="https://linkerd.io/">Linkerd 服务网格&lt;/a>上，因为它最符合我们的优先事项：低运营成本（计算和人力）、低认知开销、支持性社区以及透明的管理——同时满足我们的功能要求和预算。在 Linkerd 指导委员会工作了一段时间后（他们喜欢诚实的反馈和社区参与），我了解到它与我自己的工程原则有多么的契合。Linkerd 最近&lt;a href="https://www.cncf.io/announcements/2021/07/28/cloud-native-computing-foundation-announces-linkerd-graduation/">在 CNCF 达到毕业状态&lt;/a>，这是一个漫长的过程，强调了该项目的成熟及其广泛采用。&lt;/p>
&lt;h2 id="关于作者">关于作者&lt;/h2>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/10/15/16342274995874.jpg" alt="">&lt;/p>
&lt;p>&lt;strong>Chris Campbell&lt;/strong> 担任软件工程师和架构师已有十多年，与多个团队和组织合作落地云原生技术和最佳实践。他在与业务领导者合作采用加速业务的软件交付策略和与工程团队合作交付可扩展的云基础架构之间分配时间。他对提高开发人员生产力和体验的技术最感兴趣。&lt;/p></description></item><item><title>容器神话 Docker 是如何一分为二的</title><link>https://atbug.com/how-docker-broke-in-half/</link><pubDate>Mon, 20 Sep 2021 08:01:30 +0800</pubDate><guid>https://atbug.com/how-docker-broke-in-half/</guid><description>
&lt;p>译者点评：&lt;/p>
&lt;p>最近听了很多资深的人士关于开源，以及商业化的分析。开源与商业化，听起来就是一对矛盾的所在，似乎大家都在尝试做其二者的平衡。是先有开源，还是先有商业化？俗话说“谈钱不伤感情”，近几年背靠开源的创业公司如雨后春笋般涌现，即使是开发人员也是需要生活的。&lt;/p>
&lt;p>容器神话 Docker 曾经无比风光，盛极一时。即使这样一个备受瞩目，大获风投的热捧的独角兽也未能免俗，并付出了不小的代价。&lt;/p>
&lt;p>今天这篇文章讲述了 Docker 这家公司从诞生到巅峰到没落，这一路上所做的抉择，并最终做了开源与商业的分离，再一次从开源踏上找寻商业化之路。这些都是值得我们参考和思考的，不管是已经开源或者准备从事开源的。&lt;/p>
&lt;h2 id="这篇文章翻译自how-docker-broke-in-halfhttpswwwinfoworldcomarticle3632142how-docker-broke-in-halfhtml">这篇文章翻译自&lt;a href="https://www.infoworld.com/article/3632142/how-docker-broke-in-half.html">How Docker broke in half&lt;/a>&lt;/h2>
&lt;blockquote>
&lt;p>这家改变游戏规则的容器公司是其昔日的外衣。作为云时代最热门的企业技术业务之一的它到底发生了什么？&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://www.infoworld.com/article/3204171/what-is-docker-the-spark-for-the-container-revolution.html">Docker 并没有发明容器&lt;/a>——将计算机代码打包成紧凑单元的方法，可以轻松地从笔记本电脑移植到服务器——但它确实通过创建一套通用的开源工具和可重用的镜像使其成为主流，这使所有开发人员只需构建一次软件即可在任何地方运行。&lt;/p>
&lt;p>Docker 使开发人员能够轻松地将他们的代码“容器化”并将其从一个系统移动到另一个系统，迅速将其确立为行业标准，颠覆了在虚拟机 (VM) 上部署应用程序的主要方式，并使 Docker 成为新一代最快被采用的企业技术之一。&lt;/p>
&lt;p>今天，Docker 仍然活着，但它只是它可能成为的公司的一小部分，从未成功地将这种技术创新转化为可持续的商业模式，最终导致其企业业务于 &lt;a href="https://twitter.com/QuinnyPig/status/1194687851587198977?s=20">2019 年 11 月出售给 Mirantis&lt;/a>。InfoWorld 采访了十几位前任和现任 Docker 员工、开源贡献者、客户和行业分析师，了解 Docker 如何分崩离析的故事。&lt;/p>
&lt;h2 id="docker-诞生了">Docker 诞生了&lt;/h2>
&lt;p>2008 年由 Solomon Hykes 在巴黎创立的 DotCloud，这个后来成为 Docker 的公司最初被设计为供开发人员轻松构建和发布他们的应用程序的&lt;a href="https://www.infoworld.com/article/3223434/what-is-paas-a-simpler-way-to-build-software-applications.html">平台即服务 (PaaS)&lt;/a>。&lt;/p>
&lt;p>在 2010 年夏天一起搬到硅谷&lt;a href="https://blog.ycombinator.com/solomon-hykes-docker-dotcloud-interview/">参加著名的 Y Combinator 计划&lt;/a>之前，Hykes 很快就加入了他的朋友兼程序员同事 Sebastien Pahl。已经被拒绝一次的 Hykes 和 Pahl 重新申请，Pahl 的父亲在他们面试前几周把去旧金山的机票钱放在他们面前。唉，这对夫妇再次被拒绝，直到 YC 校友 James Lindenbaum，一家名为 &lt;a href="https://www.infoworld.com/article/3614210/the-decline-of-heroku.html">Heroku 的竞争公司的创始人&lt;/a>，出面为他们担保。&lt;/p>
&lt;p>我们所知道的 Docker 于 &lt;a href="https://www.youtube.com/watch?v=362sHaO5eGU">2013 年 3 月在 PyCon 上由 Hykes 首次演示&lt;/a>，他解释说开发人员一直要求访问支持 DotCloud 平台的底层技术。他在那次谈话中说：“我们一直认为能够说“是”会很酷，这是我们的底层部分，现在你可以和我们一起做 Linux 容器，做任何你想做的事，去构建你的平台，这就是我们正在做的，”。&lt;/p>
&lt;p>Docker 首席执行官 Ben Golub 2013 年至 2017 年之间，告诉 InfoWorld，“这听起来很老套，但 Solomon 和我谈论的是预发布，我们可以看到所有集装箱船进入奥克兰港，我们正在谈论集装箱在航运界的价值。事实上，从世界的一侧运送汽车比将应用程序从一个服务器带到另一个服务器更容易，这似乎是一个需要解决的问题。”&lt;/p>
&lt;p>Docker 开源项目迅速崛起，吸引了成千上万的用户，与&lt;a href="https://www.docker.com/blog/docker-microsoft-partnership/">微软&lt;/a>、&lt;a href="https://aws.amazon.com/blogs/apn/docker-trusted-registry-aws-marketplace/">AWS&lt;/a> 和 &lt;a href="https://www.eweek.com/cloud/ibm-partners-with-docker-launches-containers-service/">IBM&lt;/a> 等公司建立了备受瞩目的合作伙伴关系，并获得了&lt;a href="https://www.infoworld.com/article/3622960/another-day-another-multi-billion-ipo-for-open-source.html?nsdr=true">满满一车的风险投资&lt;/a>，包括 Benchmark 的 Peter Fenton 和 Trinity Ventures 的 Dan Scholnick 的早期投资。调整后的公司更名为 Docker，并从 Benchmark、Coatue Management、Goldman Sachs 和 Greylock Partners 等公司筹集了近 3 亿美元。然而，与许多基于开源软件的公司一样，它很难找到一种可盈利的商业模式，而这些投资者从未得到他们的大笔回报。。&lt;/p>
&lt;p>RedMonk 分析师 James Governor 说，“Solomon 建立了过去 20 年来最引人注目的技术之一，并且在将观点打包并使其对大量开发人员非常有价值的业务中。Docker 是否做出了错误的决定？显然是的，但风投都疯了，他们向他们投入的钱意味着他们一定觉得他们可以做任何事情，这是有问题的。”&lt;/p>
&lt;p>快进到 2021 年，这个故事的简版是，备受欢迎的开源容器编排工具 &lt;a href="https://www.infoworld.com/article/3268073/what-is-kubernetes-your-next-application-platform.html">Kubernetes&lt;/a> 通过取代其主要利润来源：一个名为 Docker Swarm 的企业版容器编排工具，吃掉了 Docker（业务）的午餐. 然而，真实的故事要复杂得多。&lt;/p>
&lt;h2 id="开源商业化很难">开源商业化很难&lt;/h2>
&lt;p>巨额的风险投资、快速增长的竞争格局以及云行业巨头都想分一杯羹的阴影，将这家年轻的公司带入了一个犹如压力锅的运营环境。&lt;/p>
&lt;p>“有一种说法是&amp;rsquo;大象打架，草被践踏'，我们很清楚这不仅是针对 Docker，还有云供应商的相互竞争。他们都想把我们拉向不同的方向。既要保持我们的价值观和根基，又要建立一个企业，这个根本就是个困局”Golub 说。&lt;/p>
&lt;p>这位前 CEO 指出，随着 Docker 的发展，所有这些因素都造成了“自然的紧张关系”。Golub 说：“我们希望建立伟大的社区并通过开发者产品获利，同时还希望建立一个伟大的运营商产品，让客户能够大规模构建和部署容器。这就是我们的愿景，很快我们意识到我们必须迅速扩大规模，而且没有太多时间来平衡社区和成为一家商业企业&amp;hellip;&amp;hellip;在一家初创公司，你每天要做出 100 个决定，你希望 80 个是对的。”&lt;/p>
&lt;p>2014 年左右，Docker 开始认真考虑将其在容器领域的领先地位货币化的商业战略，当时该公司将部分风险投资资金用于 &lt;a href="https://techcrunch.com/2014/10/07/docker-acquires-koality-in-engineering-talent-grab/%23:~:text=Docker%2520decided%2520to%2520use%2520some,today%2520for%2520an%2520undisclosed%2520price.">2014 年 Koality 的收购&lt;/a>和 &lt;a href="https://www.docker.com/blog/docker-acquires-tutum/">2015 年的 Tutum 的收购&lt;/a>，同时还推出了自己的企业支持计划的第一次迭代。&lt;/p>
&lt;p>这些投资催生了像 Docker Hub 这样的产品——你可以认为它有点像 Docker 镜像的 GitHub（&lt;a href="https://www.infoworld.com/article/3623291/github-container-registry-available-for-production-use.html">现在也存在&lt;/a>）—— 以及最终的 Docker 企业版。但这些产品都没有真正受到企业客户的欢迎，他们通常乐于与更成熟的合作伙伴合作，或者构建而不是购买解决方案，尽管 Docker 努力生产客户真正想要的一系列产品。&lt;/p>
&lt;p>今年夏天在法国度假时，Hykes 告诉 InfoWorld：“我们从未发布过出色的商业产品，原因是我们没有集中注意力。我们试图做每件事的一点点。维持开发者社区的增长并构建一个伟大的商业产品已经够难的了，更不用说三四个了，而且基本不可能同时做到，但这就是我们试图做的，我们花了大量的钱来做这些事。 ”&lt;/p>
&lt;p>DockerDocker 业务发展和技术联盟的前副总裁、最早的员工之一 Nick Stinemates 说：“在开源之外出现了零技术交付，根本无法交付商业软件。”&lt;/p>
&lt;p>事后看来，Hykes 认为 Docker 应该花更少的时间来运送产品，而应该花更多的时间倾听客户的意见。Hykes 说：“我本来不会急于扩大商业产品的规模，而是投入更多资金从我们的社区收集见解，并建立一个致力于了解他们的商业需求的团队。我们在 2014 年有一个转折点，当时我们觉得我们等不及了，但我认为我们等待的时间比我们意识到的要多。”&lt;/p>
&lt;p>其他人认为 Docker 过早地免费提供了太多东西。今年早些时候，&lt;a href="https://increment.com/containers/docker-ce-and-ee/">谷歌的 Kelsey Hightower 告诉 Increment 杂志&lt;/a>： “他们免费推出了一些东西，这就是本垒打。他们解决了整个问题并达到了这个问题的天花板：创建一个映像、构建它、将它存储在某个地方、然后运行它。还需要做什么？”&lt;/p>
&lt;p>Hykes 不同意这种说法。他说：“我认为这是错误的，一般来说，核心开源产品创造了巨大的增长，这首先创造了商业化的机会。许多公司成功地将 Docker 商业化，但 Docker 没有。有很多东西可以商业化，只是 Docker 未能将其商业化。”&lt;/p>
&lt;p>例如，&lt;a href="https://www.openshift.com/blog/red-hat-puts-docker-kubernetes-at-the-center-of-its-openshift-3-paas">红帽&lt;/a>和 &lt;a href="https://tanzu.vmware.com/content/blog/pivotal-cloud-foundry-has-supported-docker-for-a-long-time-now-pivotal-web-services-does-too">Pivotal&lt;/a>（现在是 VMware 的一部分）都是 Docker 的早期合作伙伴，将 Docker 容器集成到他们的商业 &lt;a href="https://www.infoworld.com/article/3223434/what-is-paas-a-simpler-way-to-build-software-applications.html">PaaS&lt;/a> 产品（分别是 OpenShift 和 Cloud Foundry）中，并为开源项目做出了贡献。&lt;/p>
&lt;p>Stinemates 说：“如果我是慷慨的，红帽公司早期的贡献让 Solomon 有点失控了。&lt;a href="https://news.ycombinator.com/threads?id=shykes">Solomon 烧掉了很多桥梁，Hacker News 上有关于他与反对者争吵的帖子&lt;/a>。企业合作伙伴不可能与 Solomon 一起拥有这些。”&lt;/p>
&lt;p>今天，Hykes 说他犯了混淆“社区与生态系统”的错误。红帽特别“不是社区的一部分，他们从来没有为 Docker 的成功而生根，”他说。“我们的错误是非常希望他们成为社区的一部分。回想起来，我们永远不会从这种伙伴关系中受益。”&lt;/p>
&lt;p>因此，旅游科技公司 Amadeus 等早期客户在 2015 年转向红帽，以填补他们认为 Docker 留下的企业级空白。Amadeus 的云平台负责人 Edouard Hubin 通过电子邮件告诉 InfoWorld：“我们直接从使用 [Docker 的] 开源版本的先驱模式转变为与红帽建立强大的合作伙伴关系，他们为我们提供容器技术的支持。容器化是远离虚拟化的技术变革的第一步。真正的游戏规则变革者是容器编排解决方案。显然，Docker 输给了 Kubernetes，这对他们来说是一个非常困难的局面。”&lt;/p>
&lt;h2 id="kubernetes-的决定">Kubernetes 的决定&lt;/h2>
&lt;p>Docker 会后悔之前做的一系列决定，因为它拒绝真正接受 Kubernetes 作为首选的新兴容器编排工具 —— Kubernetes 允许客户大规模、一致地运行容器队列——而不是短视地推进自己专有的 Docker Swarm 编排器（&lt;a href="https://boxboat.com/2019/12/10/migrate-docker-swarm-to-kubernetes/">RIP&lt;/a>）。&lt;/p>
&lt;p>Docker 最早也是服务时间最长的员工之一 Jérôme Petazzoni 说：“最大的错误是错过了 Kubernetes。我们处于集体思想泡沫中，我们在内部认为 Kubernetes 太复杂了，而 Swarm 会成功得多。没有意识到这一点是我们的集体失败。”&lt;/p>
&lt;p>事实是，Docker 在 2014 年有机会与谷歌的 Kubernetes 团队密切合作，并在此过程中可能拥有整个容器生态系统。Stinemates 说：“我们本可以让 Kubernetes 成为 GitHub 上 Docker 旗帜下的一流 Docker 项目。事后看来，Swarm 上市太晚是一个重大错误。”&lt;/p>
&lt;p>据在场的多名人士称，谷歌旧金山办公室的早期讨论是技术性的和紧张的，因为双方对如何进行容器编排持不同的意见。&lt;/p>
&lt;p>Kubernetes 联合创始人、现任 VMware 副总裁 Craig McLuckie 表示，他提出将 Kubernetes 捐赠给 Docker，但双方未能达成协议。他告诉 InfoWorld：“那里有一个相互傲慢的因素，从他们那里我们不了解开发人员的经验，但相互的感觉是这些年轻的新贵真的不了解分布式系统管理。”其他人则表示讨论更为非正式，并且侧重于容器技术的联合开发。无论哪种方式，团队从来没有意见一致并最终分道扬镳，&lt;a href="https://kubernetes.io/blog/2018/07/20/the-history-of-kubernetes-the-community-behind-it/">谷歌在 2014 年夏天推出了 Kubernetes&lt;/a>。&lt;/p>
&lt;p>Hykes 对 Google 向 Docker 提供 Kubernetes 项目的所有权提出异议，称他们“有机会像其他人一样成为生态系统的一部分”。&lt;/p>
&lt;p>Hykes 承认当时 Docker 和 Google 团队之间处于紧张关系。Hykes 说：“有那么一刻，自负占了上风。谷歌的很多聪明和有经验的人都被 Docker 的完全局外人蒙蔽了双眼。我们没有在谷歌工作，我们没有去斯坦福，我们没有计算机科学博士学位。有些人觉得这是他们的事，所以有一场自我之战。结果并不是 Docker 和 Kubernetes 团队之间的良好合作，而此时合作确实有意义。”&lt;/p>
&lt;p>Stinemates 说：“一方面是基本的自我，另一方面是与 [Kubernetes 联合创始人] Joe Beda、Brendan Burns 和 Craig McLuckie 的紧张关系——他们对服务级别 API 的需求有强烈的看法，而从简单的角度来看 Docker 在技术上对单个 API 有自己的看法，这意味着我们无法达成一致。”&lt;/p>
&lt;p>Hykes 承认，当时 Docker 面临着为想要扩展容器使用规模的客户寻找编排解决方案的压力，但当时Kubernetes 将成为该解决方案并不明确。Hykes 说：“Kubernetes 太早了，而且是几十个中的一个，我们并没有神奇地猜测它会占据主导地位，甚至不清楚谷歌对它的承诺。我问我们的工程师和架构师该怎么做，他们建议我们继续使用 Swarm。”&lt;/p>
&lt;p>甚至 McLuckie 也承认他“不知道 Kubernetes 会变成 Kubernetes。回顾历史很容易认为这是一个糟糕的选择。”&lt;/p>
&lt;p>不管它失败了，Kubernetes 最终赢得了容器编排之战，其余的成为软件行业的匆匆过客。&lt;/p>
&lt;p>451 Research 的分析师 Jay Lyman 说：“Kubernetes 来了，并抢走了所有的风头。它代表了谷歌在开发和开源方面对容器的使用，这在很多方面都超过了对 Docker 的关注。[Docker] 将 Docker Swarm 视为他们通过软件获利的方式。如果他们可以回去，他们可能会从一开始就与 Kubernetes 更紧密地集成。他们过于专注于独自行动。”&lt;/p>
&lt;p>McLuckie 说：“我最深切的遗憾之一是我们没有找到谈判的方法。Docker 提供了一些非凡的体验，而 Kubernetes 提供的东西，从体验的角度来看，并不那么引人注目。” 或者，正如 Docker 联合创始人 Sebastien Pahl 指出的那样：“简单并没有获胜。我喜欢 Kubernetes，但它&lt;a href="https://www.infoworld.com/article/3614850/no-one-wants-to-manage-kubernetes-anymore.html">不适合普通人&lt;/a>。”&lt;/p>
&lt;h2 id="高层的紧张气氛">高层的紧张气氛&lt;/h2>
&lt;p>在 2015 年以 10 亿美元的“独角兽”估值完成 9500 万美元的大型 D 轮融资之后，Docker 最终达到了炒作周期的巅峰。&lt;/p>
&lt;p>Steinmates 说：“这设定了非常高的期望，并暴露了我们作为一家公司将面临的一些基本问题。我认为 Ben [Golub，首席执行官] 对公司的想法与 Solomon 不同，两人没有意见一致应该不是什么秘密。董事会大量掺和努力让创始人开心，并给 CEO 足够的回旋余地，使公司取得成功。如果由 Solomon 决定，我们会坚持以社区为导向的路线来创造病毒式传播。如果由 Ben 决定，我们会更早地转向业务方面。这种紧张局势导致我们对两者都采取了半途而废的方式。”&lt;/p>
&lt;p>&lt;a href="https://increment.com/containers/docker-ce-and-ee/">这种方法有效地催生了两个Docker&lt;/a>：Docker 社区版（面向开发人员的广受欢迎的命令行工具和开源项目）和 Docker 企业版（面向希望大规模采用容器的企业客户的商业工具套件）。不幸的是，公司的动作太慢了，无法正式进行拆分并相应地分配资源。&lt;/p>
&lt;p>Golub 承认他们“应该比实际更早地拆分业务”，而 Hykes 同意 Docker “从未找到连接公司这两部分的方法”。&lt;/p>
&lt;p>到了 2018 年，裂缝开始显现，因为该公司努力在&lt;a href="https://www.techrepublic.com/article/why-doesnt-anyone-weep-for-docker/">日益不满的开源社区&lt;/a>、强大的合作伙伴和要求在生产中运行容器的企业客户之间找到可行的道路。&lt;/p>
&lt;p>不久之后，Hykes 于 2018 年 3 月离开了他在公司的日常角色，他在一篇博客文章中指出，“作为创始人，我当然有复杂的情绪。当你创建一家公司时，你的工作是确保它有一天可以在没有你的情况下取得成功。然后最终有一天会到来，庆祝活动可能是苦乐参半。对于创始人来说，放弃一生的工作绝非易事。”&lt;/p>
&lt;p>今天回想起来，Hykes 更简单。“我意识到我不属于这家公司。留下对我来说没有任何意义，所以我离开了&amp;hellip;&amp;hellip;我多半是个应该继续担任首席执行官或离开的不快乐创始人。”&lt;/p>
&lt;h2 id="docker-一分为二">Docker 一分为二&lt;/h2>
&lt;p>面对&lt;a href="https://www.cnbc.com/2019/09/27/docker-is-trying-to-raise-money-following-arrival-of-ceo-rob-bearden.html">日益严重的资金问题&lt;/a>，Docker 轮换了新任 CEO，Golub 于 2017 年 5 月让位给前 SAP 执行官 Steve Singh，然后 Singh 于 2019 年 6 月让位给前 Hortonworks 首席执行官 Rob Bearden。&lt;/p>
&lt;p>最终要接受批评的是 Bearden。上任后不久，&lt;a href="https://twitter.com/QuinnyPig/status/1194687851587198977?s=20">Docker 于 2019 年 11 月将其企业部分业务出售给 Mirantis&lt;/a>，Docker Enterprise 并入 Mirantis Kubernetes Engine。&lt;/p>
&lt;p>Bearden 当时在一份新闻稿中说：“在与管理团队和董事会进行彻底分析后，我们确定 Docker 有两个截然不同的业务：一个是活跃的开发者业务，另一个是成长中的企业业务。我们还发现产品和财务模型大不相同。”&lt;/p>
&lt;h2 id="docker-如今在哪里">Docker 如今在哪里？&lt;/h2>
&lt;p>有了原始投资者 Insight Venture Partners 和 Benchmark Capital 的 3500 万美元现金注入，剩下的 Docker 由原始 Docker Engine 容器运行时、Docker Hub 镜像存储库和 Docker 桌面应用程序支撑，在 7 年公司资深人士 Scott Johnston 的领导下活了下来。&lt;/p>
&lt;p>Johnston 告诉 InfoWorld，他正试图通过“像激光一样重新关注开发人员的需求”，让公司回归本源。“我们认为该公司比以往任何时候都更强大，因为三点：以客户为中心、统一的市场定位和生态系统友好的商业模式。”&lt;/p>
&lt;p>上周 Docker 宣布更改 Docker 软件的许可条款。很快，为大公司工作的 Docker Desktop 专业用户将不得不注册付费订阅才能继续使用该应用程序。&lt;/p>
&lt;p>Johnston 决心不重蹈覆辙，专注于为公司的核心软件开发人员受众提供价值。他说：“我们的野心更大，因为开发人员生态系统要面向的是世界上的每个开发人员，而不仅仅是那些与我们的运行时一致的开发人员。”&lt;/p>
&lt;p>Johnston 认为“Docker 2.0”的增长机会在于为安全、已验证的镜像构建新的开发人员工具和&lt;a href="https://www.docker.com/press-release/docker-expands-trusted-content-offerings">可信内容&lt;/a>，以及新兴计算模型（如&lt;a href="https://www.infoworld.com/article/3406501/what-is-serverless-serverless-computing-explained.html">无服务器&lt;/a>、&lt;a href="https://www.infoworld.com/article/3214424/what-is-machine-learning-intelligence-derived-from-data.html">机器学习&lt;/a>和&lt;a href="https://www.networkworld.com/article/3207535/what-is-iot-the-internet-of-things-explained.html">物联网&lt;/a> 以容器技术为基础的工作负载）背后的持续动力。&lt;/p>
&lt;p>同时，Docker 仍然是行业标准的容器运行时，&lt;a href="https://www.docker.com/blog/docker-index-shows-continued-massive-developer-adoption-and-activity-to-build-and-share-apps-with-docker/">如今 Docker Desktop 安装在 330 万台机器上&lt;/a>。此外，在 &lt;a href="https://insights.stackoverflow.com/survey/2021">Stack Overflow 的 2021 年开发者调查&lt;/a>中，49% 的受访者表示他们经常使用该工具。&lt;/p>
&lt;p>尽管如此，人们仍然对可能发生的事情深感失望。Stinemates 说：“如果我想要轻率，我会问今天 Docker 是否存在。从职业角度来看，这很可悲。我仍在寻找一家像 Docker 一样令人兴奋和充满活力的公司，并能创造出火花。”&lt;/p>
&lt;p>Hykes 说：“可以公平地说，Docker 未能实现其商业化的潜力……到目前为止。我很高兴 Docker 在这么多年之后有再次有实现商业化的机会。这是对基础项目和品牌的证明。”&lt;/p></description></item><item><title>开源评估框架</title><link>https://atbug.com/a-framework-for-open-source-evaluation/</link><pubDate>Mon, 09 Aug 2021 08:36:21 +0800</pubDate><guid>https://atbug.com/a-framework-for-open-source-evaluation/</guid><description>
&lt;p>本文由本人翻译自 &lt;a href="">Bilgin Ibryam&lt;/a> 的 &lt;a href="https://monetize.substack.com/p/a-framework-for-open-source-evaluation">A Framework for Open Source Evaluation&lt;/a>，首发在&lt;a href="https://cloudnative.to/blog/a-framework-for-open-source-evaluation/">云原生社区博客&lt;/a>。&lt;/p>
&lt;hr>
&lt;p>如今，真&lt;a href="https://www.linuxjournal.com/content/open-vs-fauxpen">假&lt;/a>开源无处不在。最近开源项目转为闭源的案例越来越多，同时也有不少闭源项目（按照 &lt;a href="https://opensource.org/osd">OSI 定义&lt;/a>）像开源一样构建社区的例子。这怎么可能，开源项目不应该始终如此吗？&lt;/p>
&lt;p>开源不是非黑即白，它具有开放性、透明、协作性和信任性的多个&lt;a href="https://monetize.substack.com/p/a-holistic-vision-of-open-source">维度&lt;/a>。有些开源是 Github 上的任何项目，有些必须通过 OSI 定义，有些是必须遵守不成文但普遍接受的开源规范。这里通过看一些商业和技术方面，再讨论社区管理习惯，来同大家分享一下我对评估开源项目的看法。&lt;/p>
&lt;h3 id="免责声明">免责声明&lt;/h3>
&lt;ul>
&lt;li>这些是我的个人观点，与我的雇主或我所属的软件基金会和项目无关。&lt;/li>
&lt;li>这不是法律或专业意见（我不是律师，也不是专门从事 OSS 评估的），而是外行的意见。
更新：我收到了多位开源律师的反馈并更新了文章！&lt;/li>
&lt;li>这篇博文由&lt;a href="https://monetize.substack.com/">订阅&lt;/a>和&lt;a href="https://twitter.com/bibryam/status/1371045284751507463">分享&lt;/a>按钮赞助，点击这些按钮表示支持。&lt;/li>
&lt;/ul>
&lt;h3 id="知识产权">知识产权&lt;/h3>
&lt;p>关于“开源”项目的第一个问题是关于知识产权的所有权。好消息是，即使不了解这些法律含义，你可以应用一个简单的 Litmus 测试。该项目是否属于你信任的信誉良好的开源基金会？例如，&lt;a href="https://www.fsf.org/">FSF&lt;/a> 拥有其托管项目的版权，更多情况下拥有基金会（如 &lt;a href="https://www.apache.org/foundation/">ASF&lt;/a>、&lt;a href="https://www.linuxfoundation.org/">LF&lt;/a>) 通过贡献者许可协议，聚合对其项目的贡献许可权。在任何一种情况下，你都可以相信他们将充当良好的去中心化管家，并且不会在一夜之间改变项目的未来方向。如果一个项目不属于信誉良好的软件基金会，而是由一家公司提供支持，那么问题是你是否信任该公司作为供应链合作伙伴。如果这些问题的答案是肯定的，请转到下一部分。如果答案是否定的，那么你最好调查一下版权所有者是谁，以及他们对你的长期前景和潜在风险是什么。今天的单一供应商开源项目，明天可能会变成闭源。&lt;/p>
&lt;h3 id="许可">许可&lt;/h3>
&lt;p>商标出现在许可之前的原因是软件的权利人（通常是作者）通过许可授予最终用户使用一个或多个软件副本的许可。自由软件许可证是一种说明，它授予源代码或其二进制形式的使用者修改和重新分发该软件的权利。如果没有许可，这些行为将受到版权法的禁止。这里的重点是权利人可以改变主意并更改许可。权利持有人可以决定在多个许可证下分发软件或随时将许可证更改为非开源许可证。该软件也可能在&lt;a href="https://opensource.org/node/878">公共领域&lt;/a>，在这种情况下，它不受版权法的限制。公共领域并不等同于开源许可证，这是一种不太流行的方法，我们可以在这里忽略。&lt;/p>
&lt;p>同样，如果不是律师，这是一个外行对许可的 Litmus 测试：该项目是否根据 OSI 批准的&lt;a href="https://opensource.org/licenses/alphabetical">许可清单&lt;/a>获得的许可？如果答案是肯定的，那么你可以依靠这些基金会的尽职调查来审查、分类许可并指出任何限制。如果答案是否定的，请让你公司的律师来查看和解释许可上的每个字以及可能的许可兼容性影响。&lt;/p>
&lt;h3 id="治理">治理&lt;/h3>
&lt;p>在余下的检查中，我们正在从更多的商业和法律方面转向涉及开源项目领域的技术和社区。&lt;/p>
&lt;p>&lt;img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F7f06c148-d675-4bb7-803e-b3704f0016ef_3309x2473.png" alt="开源评估框架">&lt;/p>
&lt;p>假设不担心商标持有方（未来的合作伙伴）、许可（使用开源软件的条款），下一个问题是治理。&lt;a href="https://www.oasis-open.org/policies-guidelines/open-projects-process/">治理&lt;/a>是项目决定谁来做什么、他们应该如何做以及何时做的规则或习惯。它定义了与不同项目角色相关的职责、特权和权限，以及人们如何分配到角色和从角色中删除。此处的示例是小型日常活动，例如谁有权批准拉取请求、投票给候选发布、就项目架构达成共识、定义项目路线图以及选举项目治理委员会。&lt;/p>
&lt;p>如果你正在评估对你的组织具有战略意义的项目，你想知道谁负责。不仅如此，你甚至可能&lt;a href="https://hackernoon.com/reciprocity-in-open-source-e60fb98ee1cc">希望&lt;/a>你的开发人员对项目的方向有发言权。&lt;/p>
&lt;p>还有一个简单的 Litmus 测试：对于开源基金会的项目，对于谁可以对重要决策进行投票，以及如何成为决策委员会的一部分，都有明确的规则。在某些基金会（例如 ASF）中，它基于社区成员的个人功绩，而在某些基金会（例如&lt;a href="https://www.cncf.io/"> CNCF &lt;/a>）中，它从成为付费成员组织的员工开始。在基于区块链的开源项目中，它是基于&lt;a href="https://bit.ly/devprtcl">令牌（Token）&lt;/a>的投票持有人。其他基金会有不同的规则，但都力求在多个参与者之间实现中立和权力下放。如果一个项目由一家公司或一个人管理，你相信他们会为项目和社区的利益做出最佳决策。其中一些项目可能已经写下了他们遵循的治理规则，而有些可能根本没有。由你来确定治理动态及其对你的项目参与的重要性。除了具有治理透明度和公开决策之外，另一个方面是治理机构的信任度和声誉。当你查看项目的治理委员会时，是否有一位或一组具有经过验证的技术和社交技能的领导者，让你相信他们可以将项目提升到一个新的水平？或者你是否看到一个在政治斗争中不断争论的团体？这些是开源项目是否会成功并长期发展的一些指标，还是可以预期的头痛和停滞。&lt;/p>
&lt;h3 id="基础设施">基础设施&lt;/h3>
&lt;p>拥有开源许可可能在技术上有资格作为开源项目，但这并不能说明项目是否以开源方式构建。有许多在 OSI 批准的许可下发布的软件示例，但它们是在封闭的基础设施之后开发的。通过基础设施，我的意思是用户快速提问的聊天频道。进行更深入的开发人员讨论的论坛和邮件列表。审查拉取请求的源代码管理系统，以及运行测试和每晚创建二进制文件的构建服务器。&lt;/p>
&lt;p>对于关注开源项目的商务人士和律师来说，这些可能并不重要，但对于将要使用开源项目的技术人员来说，这些是一些假设的好处。这里要做的检查是探索软件是否是使用开放式基础设施以开源方式开发的，而不是闭门造车。以下是几个示例问题：&lt;/p>
&lt;ul>
&lt;li>用户可以在项目聊天中提出问题并在没有中间人的情况下从另一个用户那里得到答案吗？&lt;/li>
&lt;li>开发人员能否与项目提交者联系并获得深入的技术问题的答案？&lt;/li>
&lt;li>你能否运行最新版本并确认已知的错误已修复？&lt;/li>
&lt;li>架构师可以参加每周一次的社区电话会议并确定项目的未来方向吗？（原文 Can an architect the weekly community call and figure out the future direction of the project? ）&lt;/li>
&lt;/ul>
&lt;p>对于封闭的基础架构，你必须创建支持工单并付费才能获得类似问题的答案。通过开放的基础设施和开放的参与，那些知道如何以开源方式工作的人可以获得答案。&lt;/p>
&lt;h3 id="社区和采用">社区和采用&lt;/h3>
&lt;p>开源软件的主要好处之一是它允许好创意的发展和传播。你可能拥有最先进的技术、最宽松的许可和开放式开发，但如果该软件没有不断壮大的社区和不断提高的采用率，那就是一个值得调查的迹象。不同的项目会有不同的采用率。有些可能会迅速成长为主流或被其他同类型项目所取代。一些项目可能有一个小但持续的增长率和一个持续数十年的生态社区。社区规模和采用率是开源项目的最终寿命指标。以下是你可以提出的一些示例问题：&lt;/p>
&lt;ul>
&lt;li>项目中有多少活跃的开发人员（提交者），平均提交率是多少？&lt;/li>
&lt;li>上个月有多少用户订阅了用户论坛以及提出了多少问题？&lt;/li>
&lt;li>软件的最新稳定版本已被下载多少次？&lt;/li>
&lt;li>还有哪些项目和服务&lt;a href="https://libraries.io/">依赖&lt;/a>并使用这个项目？&lt;/li>
&lt;li>有多少商业组织支持这个项目？&lt;/li>
&lt;li>是否有商业组织围绕它提供产品、支持和服务？&lt;/li>
&lt;li>这个项目有多少 StackOverflow 问题？&lt;/li>
&lt;li>有多少书籍、会议演讲和职位描述提到了这个项目？&lt;/li>
&lt;/ul>
&lt;p>执行这些问题会给你一个指示，即该项目是在增长并成为其领域的事实上的标准，还是停滞不前并可能被下一个大项目所取代。&lt;/p>
&lt;p>通常，开源与快节奏的开发和创新有关。同时，开源也是一种创建广泛采用和创建非官方标准的机制。许多开源项目已经变成了标准，例如用于容器编排的 Kubernetes、用于流处理的 Apache Kafka、用于 Web 服务器的 Apache httpd 等。软件中最昂贵的事情之一是找到具有合适技能的人。使用采用率高的开源项目将使你有更好的机会找到技术娴熟的人，并让他们能够更长时间地重复使用他们的技能。&lt;/p>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>根据开源项目的关键程度，有不同的风险和评估标准。对于战略性的、难以替代的项目，这将是你的 IT 基础设施的基础，你需要是已经成为其领域事实上的开源标准的完善项目。在这里确定谁拥有该项目的商标以及谁将成为你的长期合作伙伴非常重要。通常，这些合作伙伴是项目所属软件基金会的成员组织或持有项目 IP 的单个公司。对于后者，你可能需要考虑长期风险，例如核心开发人员分叉项目的机会、提供项目即服务的超大规模者、公司收购等。&lt;/p>
&lt;p>对于交付速度最重要的非战略性、战术性、短期项目，你可以让你的开发人员根据开放性、社区协作和热度（对于某些前端技术很重要）来推动选择和挑选项目。在这里，定期的安全修复、开发人员支持和许可兼容性检查等中短期风险可能就足够了。&lt;/p>
&lt;p>在任何一种情况下，都没有适合所有情况的单一评估标准。你必须在长期商业风险、技术稳定性与最新热度、创新和开发人员满意度之间取得平衡。这里的框架将为你概括需要探索的领域和需要考虑的一些风险。祝你好运！&lt;/p></description></item><item><title>Kubernetes 必备工具：2021</title><link>https://atbug.com/translation-kuberletes-essential-tools-2021/</link><pubDate>Thu, 15 Jul 2021 08:10:22 +0800</pubDate><guid>https://atbug.com/translation-kuberletes-essential-tools-2021/</guid><description>
&lt;p>有别于前些天的文章 - &lt;a href="https://mp.weixin.qq.com/s/uU2zmT5yyVcKZ5XmLSRqtg">常用的几款工具让 Kubernetes 集群上的工作更容易&lt;/a> 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。&lt;/p>
&lt;p>文档翻译自 &lt;a href="https://itnext.io/kubernetes-essential-tools-2021-def12e84c572">Kubernetes Essential Tools: 2021&lt;/a>，篇幅较长，做了部分增删。&lt;/p>
&lt;hr>
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>在本文中，我将尝试总结我最喜欢的 &lt;a href="https://kubernetes.io/">Kubernetes&lt;/a> 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。&lt;/p>
&lt;p>这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。&lt;/p>
&lt;h2 id="k3d">K3D&lt;/h2>
&lt;p>&lt;a href="https://k3d.io/">K3D&lt;/a> 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常&lt;strong>轻巧且&lt;/strong>速度非常快。它是使用 &lt;strong>Docker&lt;/strong> 围绕 &lt;a href="https://k3s.io/">K3S&lt;/a> 的包装器。所以，你只需要 Docker 来运行它并且资源使用率非常低。唯一的问题是&lt;strong>它不完全符合 K8s 标准&lt;/strong>，但这不应该是本地开发的问题。对于测试环境，你可以使用其他解决方案。K3D 比 Kind 快，但 Kind 完全兼容。&lt;/p>
&lt;h3 id="备选">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://k3s.io/">&lt;strong>K3S&lt;/strong>&lt;/a> 物联网或者边缘计算&lt;/li>
&lt;li>&lt;a href="https://kind.sigs.k8s.io/">&lt;strong>Kind&lt;/strong>&lt;/a> 完全兼容 Kubernetes 的备选&lt;/li>
&lt;li>&lt;a href="https://microk8s.io/">&lt;strong>MicroK8s&lt;/strong>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://minikube.sigs.k8s.io/docs/">&lt;strong>MiniKube&lt;/strong>&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="krew">Krew&lt;/h2>
&lt;p>&lt;a href="https://krew.sigs.k8s.io/">Krew&lt;/a> 是管理的必备工具 &lt;strong>Kubectl 插件&lt;/strong>，这是一个必须有任何 K8S 用户。我不会详细介绍超过 145 个可用&lt;a href="https://krew.sigs.k8s.io/plugins/">插件&lt;/a>，但至少安装 &lt;a href="https://github.com/ahmetb/kubectx">&lt;strong>kubens&lt;/strong>&lt;/a> 和 &lt;a href="https://github.com/ahmetb/kubectx">&lt;strong>kubectx&lt;/strong>&lt;/a>。&lt;/p>
&lt;h2 id="lens">Lens&lt;/h2>
&lt;p>&lt;a href="https://k8slens.dev/">Lens&lt;/a> 是适用于 SRE、Ops 和开发人员的 K8s &lt;strong>IDE&lt;/strong>。它适用于任何 Kubernetes 发行版：本地或云端。它快速、易于使用并提供实时可观察性。使用 Lens 可以非常轻松地管理多个集群。如果你是集群操作员，这是必须的。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263041512885.jpg" alt="">&lt;/p>
&lt;h3 id="备选-1">备选&lt;/h3>
&lt;ul>
&lt;li>对于那些喜欢轻量级终端替代品的人来说，&lt;a href="https://k9scli.io/">K9s&lt;/a> 是一个很好的选择。K9s 会持续观察 Kubernetes 的变化，并提供后续命令来与你观察到的资源进行交互。&lt;/li>
&lt;/ul>
&lt;h2 id="helm">Helm&lt;/h2>
&lt;p>&lt;a href="https://helm.sh/">Helm&lt;/a> 不需要介绍，它是 Kubernetes 最著名的包管理器。你应该在 K8s 中使用包管理器，就像在编程语言中使用它一样。Helm 允许你将应用程序打包到 &lt;a href="https://artifacthub.io/">Charts&lt;/a> 中，将复杂的应用程序抽象为易于定义、安装和更新的可重用简单组件。&lt;/p>
&lt;p>它还提供了强大的模板引擎。Helm 很成熟，有很多预定义的 charts，很好的支持，而且很容易使用。&lt;/p>
&lt;h3 id="备选-2">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://kustomize.io/">&lt;strong>Kustomize&lt;/strong>&lt;/a> 是 helm 的一个更新和伟大的替代品，它不使用模板引擎，而是一个覆盖引擎，在其中你有基本的定义和覆盖在它们之上。&lt;/li>
&lt;/ul>
&lt;h2 id="argocd">ArgoCD&lt;/h2>
&lt;p>我相信 &lt;a href="https://www.gitops.tech/">GitOps&lt;/a> 是过去十年中最好的想法之一。在软件开发中，我们应该使用单一的事实来源来跟踪构建软件所需的所有移动部分，而 &lt;strong>Git&lt;/strong> 是做到这一点的完美工具。我们的想法是拥有一个 Git 存储库，其中包含应用程序代码以及表示所需生产环境状态的基础设施 (&lt;a href="https://en.wikipedia.org/wiki/Infrastructure_as_Code">IaC&lt;/a>) 的声明性描述；以及使所需环境与存储库中描述的状态相匹配的自动化过程。&lt;/p>
&lt;blockquote>
&lt;p>GitOps: versioned CI/CD on top of declarative infrastructure. Stop scripting and start shipping.&lt;/p>
&lt;p>— Kelsey Hightower&lt;/p>
&lt;/blockquote>
&lt;p>尽管使用 &lt;a href="https://www.terraform.io/">Terraform&lt;/a> 或类似工具，你可以实现基础设施即代码（&lt;a href="https://en.wikipedia.org/wiki/Infrastructure_as_code">IaC&lt;/a>），但这还不足以将你在 Git 中的所需状态与生产同步。我们需要一种方法来持续监控环境并确保没有配置漂移。使用 Terraform，你将不得不编写脚本来运行&lt;code>terraform apply&lt;/code>并检查状态是否与 Terraform 状态匹配，但这既乏味又难以维护。&lt;/p>
&lt;p>Kubernetes 从头开始​​构建控制循环的思想，这意味着 Kubernetes 一直在监视集群的状态以确保它与所需的状态匹配，例如，运行的副本数量与所需的数量相匹配复制品。GitOps 的想法是将其扩展到应用程序，因此你可以将你的服务定义为代码，例如，通过定义 Helm Charts，并使用利用 K8s 功能的工具来监控你的应用程序的状态并相应地调整集群。也就是说，如果更新你的代码存储库或Helm Chart，生产集群也会更新。这是真正的&lt;a href="https://en.wikipedia.org/wiki/Continuous_deployment">持续部署&lt;/a>。核心原则是应用程序部署和生命周期管理应该自动化、可审计且易于理解。&lt;/p>
&lt;p>对我来说，这个想法是革命性的，如果做得好，将使组织能够更多地关注功能，而不是编写自动化脚本。这个概念可以扩展到软件开发的其他领域，例如，你可以将文档存储在代码中以跟踪更改的历史并确保文档是最新的；或使用&lt;a href="https://github.com/jamesmh/architecture_decision_record">ADR&lt;/a>跟踪架构决策。&lt;/p>
&lt;p>在我看来，在最好的 GitOps 工具 &lt;strong>Kubernetes&lt;/strong> 是 &lt;a href="https://argoproj.github.io/argo-cd/">ArgoCD&lt;/a>。你可以在&lt;a href="https://argoproj.github.io/argo-cd/core_concepts/">此处&lt;/a>阅读更多信息。ArgoCD 是 &lt;strong>Argo&lt;/strong> 生态系统的一部分，其中包括一些其他很棒的工具，其中一些我们将在稍后讨论。&lt;/p>
&lt;p>使用 &lt;strong>ArgoCD&lt;/strong>，你可以在代码存储库中拥有每个环境，你可以在其中定义该环境的所有配置。Argo CD 在指定的目标环境中自动部署所需的应用程序状态。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263046811430.jpg" alt="">&lt;/p>
&lt;p>ArgoCD 被实现为一个 kubernetes 控制器，它持续监控正在运行的应用程序并将当前的实时状态与所需的目标状态（如 Git 存储库中指定的）进行比较。ArgoCD 报告并可视化差异，并且可以自动或手动将实时状态同步回所需的目标状态。&lt;/p>
&lt;h3 id="备选-3">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://fluxcd.io/">Flux&lt;/a> 刚刚发布了一个具有许多改进的新版本。它提供了非常相似的功能。&lt;/li>
&lt;/ul>
&lt;h2 id="argo-工作流workflows和-argo-事件events">Argo 工作流（Workflows）和 Argo 事件（Events）&lt;/h2>
&lt;p>在 Kubernetes 中，你可能还需要运行批处理作业或复杂的工作流。这可能是你的数据管道、异步流程甚至 CI/CD 的一部分。最重要的是，你甚至可能需要运行对某些事件做出反应的驱动微服务，例如文件上传或消息发送到队列。对于所有这些，我们有 &lt;a href="https://argoproj.github.io/argo-workflows/">Argo Workflows&lt;/a> 和 &lt;a href="https://argoproj.github.io/argo-events/">Argo Events&lt;/a>。&lt;/p>
&lt;p>尽管它们是独立的项目，但它们往往会被部署在一起。&lt;/p>
&lt;p>Argo Workflows 是一个类似于 &lt;a href="https://airflow.apache.org/">Apache Airflow&lt;/a> 的编排引擎，但它是 Kubernetes 原生的。它使用自定义 CRD 来定义复杂的工作流程，使用 YAML 的步骤或 &lt;strong>DAG&lt;/strong>，这在 K8s 中感觉更自然。它有一个漂亮的 UI、重试机制、基于 cron 的作业、输入和输出跟踪等等。你可以使用它来编排数据管道、批处理作业等等。&lt;/p>
&lt;p>有时，你可能希望将管道与异步服务（如 &lt;strong>Kafka&lt;/strong> 等流引擎、队列、webhooks 或深度存储服务）集成。例如，你可能想要对上传到 S3 的文件等事件做出反应。为此，你将使用 &lt;a href="https://argoproj.github.io/argo-events/">Argo 事件（Event）&lt;/a>。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263049343226.jpg" alt="">&lt;/p>
&lt;p>这两个工具组合为你的所有管道需求提供了一个简单而强大的解决方案，包括 CI/CD 管道，它允许你在 Kubernetes 中本地运行 CI/CD 管道。&lt;/p>
&lt;h3 id="备选-4">备选&lt;/h3>
&lt;ul>
&lt;li>对于 ML 管道，你可以使用 &lt;a href="https://www.kubeflow.org/">Kubeflow&lt;/a>。&lt;/li>
&lt;li>对于 CI/CD 管道，你可以使用 &lt;a href="https://tekton.dev/docs/pipelines/pipelines/">Tekton&lt;/a>。&lt;/li>
&lt;/ul>
&lt;h2 id="kaniko">Kaniko&lt;/h2>
&lt;p>我们刚刚看到了如何使用 Argo Workflows 运行 Kubernetes 原生 &lt;strong>CI/CD&lt;/strong> 管道。一个常见的任务是构建 &lt;strong>Docker 镜像&lt;/strong>，这在 Kubernetes 中通常是乏味的，因为构建过程实际上是在容器本身上运行的，你需要使用变通方法来使用主机的 Docker 引擎。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263050297533.jpg" alt="">&lt;/p>
&lt;p>底线是你不应该使用 &lt;strong>Docker&lt;/strong> 来构建你的镜像：改用 &lt;a href="https://github.com/GoogleContainerTools/kaniko">&lt;strong>Kanico&lt;/strong>&lt;/a>。Kaniko 不依赖于 Docker 守护进程，而是完全在用户空间中执行 Dockerfile 中的每个命令。这使得在无法轻松或安全地运行 Docker 守护程序的环境中构建容器镜像成为可能，例如标准的 Kubernetes 集群。这消除了在 K8s 集群中构建镜像的所有问题。&lt;/p>
&lt;h2 id="istio">Istio&lt;/h2>
&lt;p>&lt;a href="https://istio.io/">Istio&lt;/a> 是市场上最著名的&lt;a href="https://en.wikipedia.org/wiki/Service_mesh">服务网格&lt;/a>，它是开源的并且非常受欢迎。我不会详细介绍什么是服务网格，因为它是一个巨大的话题，但是如果你正在构建&lt;a href="https://microservices.io/">微服务&lt;/a>，并且可能应该这样做，那么你将需要一个服务网格来管理通信、可观察性、错误处理、安全性。与其用重复的逻辑污染每个微服务的代码（译者：SDK 侵入），不如利用服务网格为你做这件事。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263052962147.jpg" alt="">&lt;/p>
&lt;p>简而言之，服务网格是一个专用的基础设施层，你可以将其添加到你的应用程序中。它允许你透明地添加可观察性、流量管理和安全性等功能，而无需将它们添加到你自己的代码中。&lt;/p>
&lt;p>如果 &lt;strong>Istio&lt;/strong> 用于运行微服务，尽管你可以在任何地方运行 Istio 并使用微服务，但 Kubernetes 已被一次又一次地证明是运行它们的最佳平台。&lt;strong>Istio&lt;/strong> 还可以将你的 K8s 集群扩展到其他服务，例如 VM，允许你拥有在迁移到 Kubernetes 时非常有用的混合环境。&lt;/p>
&lt;h3 id="备选-5">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://linkerd.io/">&lt;strong>Linkerd&lt;/strong>&lt;/a> 是一种更轻巧且可能更快的服务网格。Linkerd 从头开始​​为安全性而构建，包括&lt;a href="https://linkerd.io/2.10/features/automatic-mtls/">默认 mTLS&lt;/a>、&lt;a href="https://github.com/linkerd/linkerd2-proxy">使用 Rust 构建的数据平面&lt;/a>、&lt;a href="https://github.com/linkerd/linkerd2-proxy">内存安全语言&lt;/a>和&lt;a href="https://github.com/linkerd/linkerd2/blob/main/SECURITY_AUDIT.pdf">定期安全审计&lt;/a>等功能&lt;/li>
&lt;li>&lt;a href="https://www.consul.io/">&lt;strong>Consul&lt;/strong>&lt;/a> 是为任何运行时和云提供商构建的服务网格，因此它非常适合跨 K8s 和云提供商的混合部署。如果不是所有的工作负载都在 Kubernetes 上运行，这是一个不错的选择。&lt;/li>
&lt;/ul>
&lt;h2 id="argo-rollouts">Argo Rollouts&lt;/h2>
&lt;p>我们已经提到，你可以使用 Kubernetes 使用 Argo Workflows 或使用 Kanico 构建图像的类似工具来运行 CI/CD 管道。下一个合乎逻辑的步骤是继续并进行持续部署。由于涉及高风险，这在真实场景中是极具挑战性的，这就是为什么大多数公司只做持续交付，这意味着他们已经实现了自动化，但他们仍然需要手动批准和验证，这个手动步骤是这是因为团队&lt;strong>不能完全信任他们的自动化&lt;/strong>。&lt;/p>
&lt;p>那么，你如何建立这种信任以摆脱所有脚本并完全自动化从源代码到生产的所有内容？答案是：可观察性。你需要将资源更多地集中在指标上，并收集准确表示应用程序状态所需的所有数据。目标是使用一组指标来建立这种信任。如果你在 &lt;a href="https://prometheus.io/">Prometheus&lt;/a> 中拥有所有数据，那么你可以自动部署，因为你可以根据这些指标自动逐步推出应用程序。&lt;/p>
&lt;p>简而言之，你需要比 K8s 开箱即用的&lt;a href="https://www.educative.io/blog/kubernetes-deployments-strategies">&lt;strong>滚动更新&lt;/strong>&lt;/a>更高级的部署技术。我们需要使用&lt;a href="https://semaphoreci.com/blog/what-is-canary-deployment">金丝雀部署&lt;/a>进行渐进式交付。目标是逐步将流量路由到应用程序的新版本，等待收集指标，分析它们并将它们与预定义的规则进行匹配。如果一切正常，我们增加流量；如果有任何问题，我们会回滚部署。
要在 Kubernetes 中执行此操作，你可以使用提供 Canary 发布等的 Argo Rollouts 。&lt;/p>
&lt;blockquote>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">Argo Rollouts&lt;/a> 是一个 &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">Kubernetes 控制器&lt;/a>和一组 &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">CRD&lt;/a>，可提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和向 Kubernetes 的渐进式交付功能。&lt;/p>
&lt;/blockquote>
&lt;p>尽管像 &lt;a href="https://istio.io/">Istio&lt;/a> 这样的服务网格提供 Canary 发布，但 Argo Rollouts 使这个过程变得更加容易并且以开发人员为中心，因为它是专门为此目的而构建的。除此之外，Argo Rollouts 可以与任何服务网格集成。&lt;/p>
&lt;p>Argo Rollouts 功能：&lt;/p>
&lt;ul>
&lt;li>蓝绿更新策略&lt;/li>
&lt;li>金丝雀更新策略&lt;/li>
&lt;li>细粒度、加权的流量转移&lt;/li>
&lt;li>自动回滚和促销或人工判断&lt;/li>
&lt;li>可定制的指标查询和业务 KPI 分析&lt;/li>
&lt;li>入口控制器集成：NGINX、ALB&lt;/li>
&lt;li>服务网格集成：Istio、Linkerd、SMI&lt;/li>
&lt;li>指标提供者集成：Prometheus、Wavefront、Kayenta、Web、Kubernetes Jobs&lt;/li>
&lt;/ul>
&lt;h3 id="备选-6">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://istio.io/">Istio&lt;/a> 作为 Canary 版本的服务网格。Istio 不仅仅是一个渐进式交付工具，它还是一个完整的服务网格。Istio 不会自动部署，Argo Rollouts 可以与 Istio 集成来实现这一点。&lt;/li>
&lt;li>&lt;a href="https://flagger.app/">Flagger&lt;/a> 与 Argo Rollouts 非常相似，并且与 &lt;a href="https://fluxcd.io/">Flux&lt;/a> 很好地集成在一起，因此如果你使用 Flux，请考虑使用 Flagger。&lt;/li>
&lt;li>&lt;a href="https://spinnaker.io/">Spinnaker&lt;/a> 是 Kubernetes 的第一个持续交付工具，它具有许多功能，但使用和设置起来有点复杂。&lt;/li>
&lt;/ul>
&lt;h2 id="crossplane">Crossplane&lt;/h2>
&lt;p>&lt;a href="https://crossplane.io/">&lt;strong>Crossplane&lt;/strong>&lt;/a> 是我最喜欢的 K8s 工具，我对这个项目感到非常兴奋，因为它给 Kubernetes 带来了一个关键的缺失部分：像管理 K8s 资源一样管理第三方服务。这意味着，你可以使用 &lt;strong>YAML&lt;/strong> 中定义的 K8s 资源来配置云提供商数据库，例如 &lt;a href="https://aws.amazon.com/rds/">&lt;strong>AWS RDS&lt;/strong>&lt;/a> 或 &lt;strong>GCP Cloud SQL&lt;/strong>，就像你在 K8s 中配置数据库一样。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263059248680.jpg" alt="">&lt;/p>
&lt;p>使用 Crossplane，无需使用不同的工具和方法分离基础设施和代码。&lt;strong>你可以使用 K8s 资源定义一切&lt;/strong>。这样，你就无需学习 &lt;a href="https://www.terraform.io/">Terraform&lt;/a> 等新工具并将它们分开保存。&lt;/p>
&lt;blockquote>
&lt;p>Crossplane 是一个开源 Kubernetes 附加组件，它使平台团队能够组装来自多个供应商的基础设施，并公开更高级别的自助 API 供应用程序团队使用，而无需编写任何代码。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Crossplane&lt;/strong> 扩展了你的 Kubernetes 集群，为你提供适用于任何基础架构或托管云服务的 &lt;strong>CRD&lt;/strong>。此外，它允许你完全实现持续部署，因为与 Terraform 等其他工具相反，Crossplane 使用现有的 K8s 功能（例如控制循环）来持续观察你的集群并自动检测任何对其起作用的配置漂移。例如，如果你定义了一个托管数据库实例并且有人手动更改它，Crossplane 将自动检测问题并将其设置回以前的值。这将实施基础设施即代码和 GitOps 原则。Crossplane 与 ArgoCD 配合使用效果很好，它可以查看源代码并确保你的代码存储库是唯一的真实来源，并且代码中的任何更改都会传播到集群以及外部云服务。如果没有 Crossplane，你只能在 K8s 服务中实现 GitOps，而不能在不使用单独进程的情况下在云服务中实现，现在你可以做到这一点，这太棒了。&lt;/p>
&lt;h3 id="备选-7">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.terraform.io/">Terraform&lt;/a> 是最著名的 IaC 工具，但它不是 K8s 原生的，需要新技能并且不会自动监视配置漂移。&lt;/li>
&lt;li>&lt;a href="https://www.pulumi.com/">Pulumi&lt;/a> 是一种 Terraform 替代品，它使用开发人员可以测试和理解的编程语言工作。&lt;/li>
&lt;/ul>
&lt;h2 id="knative">Knative&lt;/h2>
&lt;p>如果你在云中开发应用程序，你可能已经使用了一些无服务器技术，例如 &lt;a href="https://aws.amazon.com/lambda/">AWS Lambda &lt;/a>，它是一种称为 &lt;a href="https://en.wikipedia.org/wiki/Function_as_a_service">FaaS&lt;/a> 的事件驱动范例。&lt;/p>
&lt;p>我过去已经讨论过 &lt;a href="https://en.wikipedia.org/wiki/Serverless_computing">Serverless&lt;/a>，因此请查看我&lt;a href="https://itnext.io/scaling-my-app-serverless-vs-kubernetes-cdb8adf446e1">之前的文章&lt;/a>以了解更多信息。Serverless 的问题在于它与云提供商紧密耦合，因为提供商可以为事件驱动的应用程序创建一个很好的生态系统。&lt;/p>
&lt;p>对于 Kubernetes，如果你希望将函数作为代码运行并使用事件驱动架构，那么你最好的选择是 &lt;a href="https://itnext.io/scaling-my-app-serverless-vs-kubernetes-cdb8adf446e1">Knative&lt;/a>。Knative 旨在在 Kubernetes 上运行函数，在 Pod 之上创建一个抽象。&lt;/p>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>针对常见应用程序用例的具有更高级别抽象的重点 API。&lt;/li>
&lt;li>在几秒钟内建立一个可扩展、安全、无状态的服务。&lt;/li>
&lt;li>松散耦合的功能让你可以使用所需的部分。&lt;/li>
&lt;li>可插拔组件让你可以使用自己的日志记录和监控、网络和服务网格。&lt;/li>
&lt;li>Knative 是可移植的：在 Kubernetes 运行的任何地方运行它，不用担心供应商锁定。&lt;/li>
&lt;li>惯用的开发者体验，支持 GitOps、DockerOps、ManualOps 等常用模式。&lt;/li>
&lt;li>Knative 可以与常见的工具和框架一起使用，例如 Django、Ruby on Rails、Spring 等等。&lt;/li>
&lt;/ul>
&lt;h3 id="备选-8">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://argoproj.github.io/argo-events/">Argo Events&lt;/a> 为 Kubernetes 提供了一个事件驱动的工作流引擎，可以与 AWS Lambda 等云引擎集成。它不是 FaaS，而是为 Kubernetes 提供了一个事件驱动的架构。&lt;/li>
&lt;li>&lt;a href="https://www.openfaas.com/">OpenFaas&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="kyverno">Kyverno&lt;/h2>
&lt;p>Kubernetes 提供了极大的灵活性，以赋予敏捷的自治团队权力，但能力越大，责任越大。必须有一组&lt;strong>最佳实践和规则&lt;/strong>，以确保以一致且有凝聚力的方式来部署和管理符合公司政策和安全要求的工作负载。&lt;/p>
&lt;p>有几种工具可以实现这一点，但没有一个是 Kubernetes 原生的…… 直到现在。&lt;a href="https://kyverno.io/">Kyverno&lt;/a> 是为 Kubernetes 设计的策略引擎，策略作为 Kubernetes 资源进行管理，并且不需要新的语言来编写策略。Kyverno 策略可以验证、改变和生成 Kubernetes 资源。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263062593080.jpg" alt="">&lt;/p>
&lt;p>你可以应用有关最佳实践、网络或安全性的任何类型的策略。例如，你可以强制所有服务都有标签或所有容器都以非 root 身份运行。你可以在&lt;a href="https://github.com/kyverno/policies/">此处&lt;/a>查看一些政策示例。策略可以应用于整个集群或给定的命名空间。你还可以选择是只想审核策略还是强制执行它们以阻止用户部署资源。&lt;/p>
&lt;h3 id="备选-9">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.openpolicyagent.org/">Open Policy Agent&lt;/a> 是著名的云原生基于策略的控制引擎。它使用自己的声明性语言，并且可以在许多环境中运行，而不仅仅是在 Kubernetes 上。它比 &lt;a href="https://kyverno.io/">Kyverno&lt;/a> 更难管理，但更强大。&lt;/li>
&lt;/ul>
&lt;h2 id="kubevela">Kubevela&lt;/h2>
&lt;p>Kubernetes 的一个问题是开发人员需要非常了解和理解平台和集群配置。许多人会争辩说 &lt;strong>K8s 的抽象级别太低&lt;/strong>，这会给只想专注于编写和交付应用程序的开发人员带来很多摩擦。&lt;/p>
&lt;p>在开放式应用程序模型（&lt;a href="https://oam.dev/">OAM&lt;/a>）的设立是为了克服这个问题。这个想法是围绕应用程序创建更高级别的抽象，它独立于底层运行时。你可以在此处阅读规范。&lt;/p>
&lt;blockquote>
&lt;p>专注于应用程序而不是容器或协调器，开放应用程序模型 [OAM] 带来了模块化、可扩展和可移植的设计，用于使用更高级别但一致的 API 对应用程序部署进行建模。&lt;/p>
&lt;/blockquote>
&lt;p>&lt;a href="https://kubevela.io/">Kubevela&lt;/a> 是 OAM 模型的一个实现。KubeVela 与运行时无关，可本地扩展，但最重要的是，以应用程序为中心 。在 Kubevela 中，应用程序是作为 Kubernetes 资源实现的一等公民。**集群运营商（Platform Team）和开发者（Application Team）**是有区别的。集群操作员通过定义组件（组成应用程序的可部署/可配置实体，如 Helm Chart）和特征来管理集群和不同的环境。开发人员通过组装组件和特征来定义应用程序。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263064905339.jpg" alt="">&lt;/p>
&lt;p>KubeVela 是一个&lt;a href="https://cncf.io/">云原生计算基金会&lt;/a>沙箱项目，虽然它仍处于起步阶段，但它可以在不久的将来改变我们使用 Kubernetes 的方式，让开发人员无需成为 Kubernetes 专家即可专注于应用程序。但是，我确实对 &lt;strong>OAM&lt;/strong> 在现实世界中的适用性有一些担忧，因为系统应用程序、ML 或大数据过程等一些服务在很大程度上依赖于低级细节，这些细节可能很难融入 OAM 模型中。&lt;/p>
&lt;h3 id="备选-10">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.shipa.io/getting-started/">Shipa&lt;/a> 遵循类似的方法，使平台和开发团队能够协同工作，轻松将应用程序部署到 Kubernetes。&lt;/li>
&lt;/ul>
&lt;h2 id="snyk">Snyk&lt;/h2>
&lt;p>任何开发过程中一个非常重要的方面是安全性，这一直是 Kubernetes 的一个问题，因为想要迁移到 Kubernetes 的公司无法轻松实现其当前的安全原则。&lt;/p>
&lt;p>&lt;a href="https://snyk.io/">Snyk&lt;/a> 试图通过提供一个可以轻松与 Kubernetes 集成的安全框架来缓解这种情况。它可以检测容器映像、你的代码、开源项目等中的漏洞。&lt;/p>
&lt;h3 id="备选-11">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://falco.org/">Falco&lt;/a> 是 Kubernetes 的运行时安全线程检测工具。&lt;/li>
&lt;/ul>
&lt;h2 id="velero">Velero&lt;/h2>
&lt;p>如果你在 Kubernetes 中运行工作负载并使用卷来存储数据，则需要创建和管理备份。&lt;a href="https://velero.io/">Velero&lt;/a> 提供简单的备份/恢复过程、灾难恢复机制和数据迁移。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263066379840.jpg" alt="">&lt;/p>
&lt;p>与其他直接访问 Kubernetes etcd 数据库执行备份和恢复的工具不同，Velero 使用 Kubernetes API 来捕获集群资源的状态并在必要时恢复它们。此外，Velero 使你能够在配置的同时备份和恢复你的应用程序持久数据。&lt;/p>
&lt;h2 id="schema-hero">Schema Hero&lt;/h2>
&lt;p>软件开发中的另一个常见过程是在使用关系数据库时管理&lt;strong>模式演变&lt;/strong>。&lt;/p>
&lt;p>&lt;a href="https://schemahero.io/">SchemaHero&lt;/a> 是一种开源数据库架构迁移工具，可将架构定义转换为可应用于任何环境的迁移脚本。它使用 Kubernetes 声明性来管理数据库模式迁移。你只需指定所需的状态，然后 &lt;strong>SchemaHero&lt;/strong> 管理其余的。&lt;/p>
&lt;h3 id="备选-12">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.liquibase.org/">LiquidBase&lt;/a> 是最著名的替代品。它更难使用，且不是 Kubernetes 原生的，但它具有更多功能。&lt;/li>
&lt;/ul>
&lt;h2 id="bitnami-sealed-secrets">Bitnami Sealed Secrets&lt;/h2>
&lt;p>我们已经介绍了许多 &lt;strong>GitOps&lt;/strong> 工具，例如 &lt;a href="https://argoproj.github.io/argo-cd/">ArgoCD&lt;/a>。我们的目标是将所有内容保留在 Git 中，并使用 Kubernetes 声明性来保持环境同步。我们刚刚看到我们如何（并且应该）在 Git 中保留真实来源，并让自动化流程处理配置更改。&lt;/p>
&lt;p>在 Git 中通常很难保留的一件事是诸如数据库密码或 API 密钥之类的秘密，这是因为你永远不应该在代码存储库中存储秘密。一种常见的解决方案是使用外部保管库（例如 &lt;a href="https://aws.amazon.com/secrets-manager/">AWS Secret Manager&lt;/a> 或 &lt;a href="https://www.vaultproject.io/">HashiCorp Vault&lt;/a>）来存储机密，但这会产生很多摩擦，因为你需要有一个单独的流程来处理机密。理想情况下，我们希望有一种方法可以像任何其他资源一样安全地在 Git 中存储机密。&lt;/p>
&lt;p>&lt;a href="https://github.com/bitnami-labs/sealed-secrets">&lt;strong>Sealed Secrets&lt;/strong>&lt;/a> 旨在克服这个问题，允许你使用强加密将敏感数据存储在 Git 中。Bitnami &lt;strong>Sealed Secrets&lt;/strong> 本地集成在 Kubernetes 中，允许你仅通过在 Kubernetes 中运行的 Kubernetes 控制器而不是其他任何人来解密密钥。控制器将解密数据并创建安全存储的原生 K8s 机密。这使我们能够将所有内容作为代码存储在我们的 repo 中，从而允许我们安全地执行持续部署，而无需任何外部依赖。&lt;/p>
&lt;p>Sealed Secrets 由两部分组成：&lt;/p>
&lt;ul>
&lt;li>集群端控制器&lt;/li>
&lt;li>客户端实用程序：&lt;code>kubeseal&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>该 &lt;code>kubeseal&lt;/code> 实用程序使用非对称加密来加密只有控制器才能解密的机密。这些加密的秘密被编码在一个 &lt;code>SealedSecret&lt;/code> K8s 资源中，你可以将其存储在 Git 中。&lt;/p>
&lt;h3 id="备选-13">备选&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://aws.amazon.com/secrets-manager/">AWS Secret Manager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.vaultproject.io/">HashiCorp Vault&lt;/a>）&lt;/li>
&lt;/ul>
&lt;h2 id="capsule">Capsule&lt;/h2>
&lt;p>许多公司使用&lt;strong>多租户&lt;/strong>来管理不同的客户。这在软件开发中很常见，但在 Kubernetes 中很难实现。&lt;strong>命名空间&lt;/strong>是将集群的逻辑分区创建为隔离切片的好方法，但这不足以安全地隔离客户，我们需要强制执行网络策略、配额等。你可以为每个名称空间创建网络策略和规则，但这是一个难以扩展的乏味过程。此外，租户将不能使用多个命名空间，这是一个很大的限制。&lt;/p>
&lt;p>创建&lt;strong>分层命名空间&lt;/strong>是为了克服其中一些问题。这个想法是为每个租户拥有一个父命名空间，为租户提供公共网络策略和配额，并允许创建子命名空间。这是一个很大的改进，但它在安全和治理方面没有对租户的本地支持。此外，它还没有达到生产状态，但 1.0 版预计将在未来几个月内发布。&lt;/p>
&lt;p>当前解决此问题的常用方法是为每个客户创建一个集群，这是安全的并提供租户所需的一切，但这很难管理且非常昂贵。&lt;/p>
&lt;p>&lt;a href="https://github.com/clastix/capsule">&lt;strong>Capsule&lt;/strong>&lt;/a> 是一种为单个集群中的多个租户提供原生 Kubernetes 支持的工具。使用 Capsule，你可以为所有租户拥有一个集群。Capsule 将为租户提供 “几乎” 原生体验（有一些小限制），他们将能够创建多个命名空间并使用集群，因为它们完全可用，隐藏了集群实际上是共享的事实。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263070902872.jpg" alt="Capsule 架构">&lt;/p>
&lt;p>在单个集群中，Capsule Controller 在称为 &lt;strong>Tenant&lt;/strong> 的轻量级 Kubernetes 抽象中聚合多个命名空间，这是一组 Kubernetes 命名空间。在每个租户内，用户可以自由创建他们的命名空间并共享所有分配的资源，而策略引擎则使不同的租户彼此隔离。&lt;/p>
&lt;p>租户级别定义的网络和安全策略、资源配额、限制范围、RBAC 和其他策略由租户中的所有命名空间自动继承，类似于分层命名空间。然后用户可以自由地自治操作他们的租户，而无需集群管理员的干预。
Capsule 是 GitOps 就绪的，因为它是声明性的，并且所有配置都可以存储在 Git 中。&lt;/p>
&lt;h2 id="vcluster">vCluster&lt;/h2>
&lt;p>&lt;a href="https://www.vcluster.com/">vCluster&lt;/a> 在多租户方面更进了一步，它在 Kubernetes 集群内提供了虚拟集群。每个集群都在一个常规命名空间上运行，并且是完全隔离的。虚拟集群有自己的 API 服务器和独立的数据存储，所以你在 vCluster 中创建的每个 Kubernetes 对象都只存在于 vcluster 内部。此外，您可以将 kube 上下文与虚拟集群一起使用，以像使用常规集群一样使用它们。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263071651487.jpg" alt="">&lt;/p>
&lt;p>只要您可以在单个命名空间内创建部署，您就可以创建虚拟集群并成为该虚拟集群的管理员，租户可以创建命名空间、安装 CRD、配置权限等等。&lt;/p>
&lt;p>&lt;strong>vCluster&lt;/strong> 使用 &lt;a href="https://k3s.io/">&lt;strong>k3s&lt;/strong>&lt;/a> 作为其 API 服务器，使虚拟集群超轻量级且经济高效；由于 k3s 集群 100% 合规，虚拟集群也 100% 合规。&lt;strong>vClusters&lt;/strong> 是超轻量级的（1 个 pod），消耗很少的资源并且可以在任何 Kubernetes 集群上运行，而无需对底层集群进行特权访问。与 &lt;strong>Capsule&lt;/strong> 相比，它确实使用了更多的资源，但它提供了更多的灵活性，因为多租户只是用例之一。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/07/15/16263072079021.jpg" alt="">&lt;/p>
&lt;h2 id="其他工具">其他工具&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/cloud-bulldozer/kube-burner">kube-burner&lt;/a> 用于&lt;strong>压力测试&lt;/strong>。它提供指标和警报。&lt;/li>
&lt;li>混沌工程的 &lt;a href="https://github.com/litmuschaos/litmus">Litmus&lt;/a>。&lt;/li>
&lt;li>&lt;a href="https://github.com/bitnami-labs/kubewatch">kubewatch&lt;/a> 用于监控，但主要关注基于 Kubernetes 事件（如资源创建或删除）的推送通知。它可以与 Slack 等许多工具集成。&lt;/li>
&lt;li>&lt;a href="https://www.botkube.io/">BotKube&lt;/a> 是一个消息机器人，用于监控和调试 Kubernetes 集群。与 kubewatch 类似，但更新并具有更多功能。&lt;/li>
&lt;li>&lt;a href="https://getmizu.io/">Mizu&lt;/a> 是一个 API 流量查看器和调试器。&lt;/li>
&lt;li>&lt;a href="https://github.com/senthilrch/kube-fledged">kube-fledged&lt;/a> 是一个 Kubernetes 插件，用于直接在 Kubernetes 集群的工作节点上创建和管理容器镜像的缓存。因此，&lt;strong>应用程序 pod 几乎立即启动&lt;/strong>，因为不需要从注册表中提取图像。&lt;/li>
&lt;/ul>
&lt;h2 id="结论">结论&lt;/h2>
&lt;p>在本文中，我们回顾了我最喜欢的 Kubernetes 工具。我专注于可以合并到任何 Kubernetes 发行版中的开源项目。我没有涵盖诸如 &lt;a href="https://www.openshift.com/">OpenShift&lt;/a> 或 Cloud Providers Add-Ons 之类的商业解决方案，因为我想让它保持通用性，但我鼓励您探索如果您在云上运行 Kubernetes 或使用商业工具，您的云提供商可以为您提供什么。&lt;/p>
&lt;p>我的目标是向您展示您可以在 &lt;strong>Kubernetes&lt;/strong> 中完成您在本地所做的一切。我还更多地关注鲜为人知的工具，我认为这些工具可能具有很大的潜力，例如 &lt;a href="https://crossplane.io/">Crossplane&lt;/a>、&lt;a href="https://kubevela.io/">Argo Rollouts&lt;/a>或 &lt;a href="https://kubevela.io/">Kubevela&lt;/a>。我更感兴趣的工具是 &lt;a href="https://www.vcluster.com/">vCluster&lt;/a>、&lt;a href="https://crossplane.io/">Crossplane&lt;/a> 和 ArgoCD/Workflows。&lt;/p></description></item><item><title>Kubernetes 的魔力在于企业标准化，而不是应用程序的可移植性</title><link>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</link><pubDate>Sun, 11 Jul 2021 08:05:42 +0800</pubDate><guid>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</guid><description>
&lt;p>笔者：Kubernetes 抽象了资源和工作负载的操作模式，统一了工具集，实现人机接口的标准化。正如类 Docker 工具提供了应用运行时的操作模式；Spring Framework 提供了 Java 应用的开发模式。&lt;/p>
&lt;blockquote>
&lt;p>Kubernetes 是关于跨云的技能、工具和实践的可移植性。不是工作负载的可移植性。 &amp;ndash; Bilgin Lbryam @bibryam&lt;/p>
&lt;/blockquote>
&lt;p>本文翻译自 &lt;a href="https://www.techrepublic.com/google-amp/article/kubernetes-magic-is-in-enterprise-standardization-not-app-portability">Kubernetes magic is in enterprise standardization, not app portability&lt;/a>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>Kubernetes 不会神奇地使你的应用程序具有可移植性，但它可能会给你带来更好的东西。&lt;/p>
&lt;/blockquote>
&lt;p>云为企业提供了看似无限的选择。然而，根据 &lt;a href="https://juju.is/cloud-native-kubernetes-usage-report-2021">Canonical-sponsored 的一项调查&lt;/a>，这并不是大多数企业采用 &lt;a href="https://www.techrepublic.com/article/kubernetes-the-smart-persons-guide/">Kubernetes&lt;/a> 等云友好技术的原因。相反，Kubernetes 的主要目标是标准化——外观和操作与其他人一样。&lt;/p>
&lt;h2 id="可移植性不是目标">可移植性不是目标&lt;/h2>
&lt;p>我之前已经讨论过这个问题，参考了 Gartner 关于 Kubernetes 和可移植性的指南。许多人认为 Kubernetes（和&lt;a href="https://www.techrepublic.com/article/containers-the-smart-persons-guide">容器&lt;/a>）可以让他们在云之间轻松移植，但事实证明并不是这样的。正如 Gartner 分析师 &lt;a href="https://blogs.gartner.com/marco-meinardi/2020/09/04/adopting-kubernetes-application-portability-not-good-idea/">Marco Meinardi 所写&lt;/a>，当被问及公司是否应该采用“Kubernetes 使他们的应用程序可移植&amp;hellip;&amp;hellip;答案是：不。” 再说一次？&lt;/p>
&lt;blockquote>
&lt;p>调查显示，[在云提供商之间移动应用程序] 的可能性实际上非常低。一旦部署在供应商中，应用程序往往会留在那里。这是因为数据湖难以移植且成本高昂，因此最终成为迁移的重心。&lt;/p>
&lt;/blockquote>
&lt;p>因此 Kubernetes 通常不会被公司接受，以增强应用程序的可移植性；相反，谈论人员可移植性或换言之，技能可移植性更接近事实。Weaveworks 首席执行官亚历克西斯·理查森（Alexis Richardson）&lt;a href="https://twitter.com/monadic/status/1302257531025264642">将这个主题打回家&lt;/a>：&lt;/p>
&lt;blockquote>
&lt;p>重点是“技能可移植性”，因为使用标准操作模型和工具链。大型组织希望开发人员使用标准的工作方式，因为这可以降低培训成本，并消除员工在不同项目之间转移的障碍。如果你的“平台”（或多个平台）基于相同的核心云原生工具集，那么它也可以更轻松、更便宜地应用策略。&lt;/p>
&lt;/blockquote>
&lt;p>这让我们回到规范调查。&lt;/p>
&lt;h2 id="samesies">Samesies&lt;/h2>
&lt;p>当被问及确定与采用 Kubernetes 等云原生技术相关的技术目标时，调查受访者将可移植性排在最后，将更直接的问题排在第一位：&lt;/p>
&lt;ul>
&lt;li>改进维护、监控和自动化 - 64.6%。&lt;/li>
&lt;li>基础设施现代化 - 46.4%。&lt;/li>
&lt;li>更快的上线时间 - 26.5%。&lt;/li>
&lt;li>删除供应商依赖项 - 12.8%。&lt;/li>
&lt;li>全球覆盖率 - 12.5%。&lt;/li>
&lt;li>围绕流量高峰的敏捷性 - 9.2%。&lt;/li>
&lt;li>确保便携性 - 8.9%&lt;/li>
&lt;/ul>
&lt;p>我喜欢 Google Cloud 的开发者倡导者 Kelsey Hightower 在调查报告中评论这些结果的方式：&lt;/p>
&lt;blockquote>
&lt;p>很多人认为组织转向 Kubernetes 是因为规模，或者因为他们想成为超大规模者，或者与 Twitter 拥有相同的流量水平。对于大多数组织而言，情况并非一定如此。很多人都喜欢 K8s 中内置了许多决策，例如日志记录、监控和负载平衡。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>人们往往会忘记事情有多么复杂，只是为了构建一个没有所有自动化的应用程序。如果你在公有云上，你可以使用一些本机集成和工具。但是，如果你在本地，那不是给定的——你必须自己将解决方案粘合在一起。使用 Kubernetes，你几乎将 25 种不同的工具合二为一。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>这就是人们所说的“现代基础设施”的意思——他们并不是在谈论做一些以前从未做过的事情。他们谈论的是过去 10 年或 15 年一直在生产的东西。Kubernetes 是当今所有“现代模式”的检查站。&lt;/p>
&lt;/blockquote>
&lt;p>换句话说，人们真正想要从 Kubernetes 获得的是一种标准的基础设施思考方式。回到 Richardson 之前的观点，虽然 Kubernetes 和云原生技术使公司能够以更高的速度运营，但最大的好处可能是使技能在组织之间可以互换——这为雇主和员工都创造了巨大的绩效收益。这是企业不断增加对 Kubernetes 投资的另一个原因。&lt;/p>
&lt;p>&lt;em>声明：我为 AWS 工作，但此处表达的观点是我的。&lt;/em>&lt;/p></description></item><item><title>Kubernetes 的自动伸缩你用对了吗？</title><link>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</link><pubDate>Wed, 09 Jun 2021 00:34:25 +0800</pubDate><guid>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</guid><description>
&lt;p>本文翻译自 learnk8s 的 &lt;a href="https://learnk8s.io/kubernetes-autoscaling-strategies#when-autoscaling-pods-goes-wrong">Architecting Kubernetes clusters — choosing the best autoscaling strategy&lt;/a>，&lt;!-- raw HTML omitted -->有增删部分内容&lt;!-- raw HTML omitted -->。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2159402x.png" alt="">&lt;/p>
&lt;p>TL;DR: 在默认设置下，扩展 Kubernetes 集群中的 pod 和节点可能需要几分钟时间。了解如何调整集群节点的大小、配置水平和集群自动缩放器以及过度配置集群以加快扩展速度。&lt;/p>
&lt;h2 id="自动扩展器">自动扩展器&lt;/h2>
&lt;p>在 Kubernetes 中，常说的“自用扩展”有：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA：Pod 水平缩放器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">VPA：Pod 垂直缩放器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">CA：集群自动缩放器&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>不同类型的自动缩放器，使用的场景不一样。&lt;/p>
&lt;h3 id="hpa">HPA&lt;/h3>
&lt;p>HPA 定期检查内存和 CPU 等指标，自动调整 Deployment 中的副本数，比如流量变化：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2206552x.png" alt="调整前">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2207122x.png" alt="调整后">&lt;/p>
&lt;h3 id="vpa">VPA&lt;/h3>
&lt;p>有些时候无法通过增加 Pod 数来扩容，比如数据库。这时候可以通过 VPA 增加 Pod 的大小，比如调整 Pod 的 CPU 和内存：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2212162x.png" alt="调整前">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2212342x.png" alt="调整后">&lt;/p>
&lt;h3 id="ca">CA&lt;/h3>
&lt;p>当集群资源不足时，CA 会自动配置新的计算资源并添加到集群中：&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2213392x.png" alt="调整前">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2214182x.png" alt="调整后">&lt;/p>
&lt;h2 id="自动缩放-pod-出错时">自动缩放 Pod 出错时&lt;/h2>
&lt;p>比如一个应用需要 1.5 GB 内存和 0.25 个 vCPU。一个 8GB 和 2 个 vCPU 的节点，可以容纳 4 个这样的 Pod，完美！&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2216192x.png" alt="">&lt;/p>
&lt;p>做如下配置：&lt;/p>
&lt;ol>
&lt;li>HPA：每增加 10 个并发，增加一个副本。即 40 个并发的时候，自动扩展到 4 个副本。（这里使用自定义指标，比如来自 Ingress Controller 的 QPS）&lt;/li>
&lt;li>CA：在资源不足的时候，增加计算节点。&lt;/li>
&lt;/ol>
&lt;p>当并发达到 30 的时候，系统是下面这样。完美！HPA 工作正常，CA 没工作。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2223102x.png" alt="">&lt;/p>
&lt;p>当增加到 40 个并发的时候，系统是下面的情况：&lt;/p>
&lt;ol>
&lt;li>HPA 增加了一个 Pod&lt;/li>
&lt;li>Pod 挂起&lt;/li>
&lt;li>CA 增加了一个节点&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2224462x.png" alt="HPA 工作">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2225022x.png" alt="CA 工作">&lt;/p>
&lt;p>&lt;em>为什么 Pod 没有部署成功？&lt;/em>&lt;/p>
&lt;p>节点上的操作系统进程和 kubelet 也会消耗一部分资源，8G 和 2 vCPU 并不是全都可以提供给 Pod 用的。并且还有一个&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#eviction-thresholds">驱逐阈值&lt;/a>：在节点系统剩余资源达到阈值时，会驱逐 Pod，避免 OOM 的发生。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2230402x.png" alt="">&lt;/p>
&lt;p>当然上面的这些都是&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable">可配置&lt;/a>的。&lt;/p>
&lt;p>&lt;em>那为什么在创建该 Pod 之前，CA 没有增加新的节点呢？&lt;/em>&lt;/p>
&lt;h2 id="ca-如何工作">CA 如何工作？&lt;/h2>
&lt;p>&lt;strong>CA 在触发自动缩放时，不会查看可用的内存或 CPU。&lt;/strong>&lt;/p>
&lt;p>CA 是面向事件工作的，并每 10 秒检查一次是否存在不可调度（Pending）的 Pod。&lt;/p>
&lt;p>当调度器无法找到可以容纳 Pod 的节点时，这个 Pod 是不可调度的。&lt;/p>
&lt;p>此时，CA 开始创建新节点：CA 扫描集群并检查是否有不可调度的 Pod。&lt;/p>
&lt;p>当集群有多种节点池，CA 会通过选择下面的一种策略：&lt;/p>
&lt;ul>
&lt;li>&lt;code>random&lt;/code>：默认的扩展器，随机选择一种节点池&lt;/li>
&lt;li>&lt;code>most-pods&lt;/code>：能够调度最多 Pod 的节点池&lt;/li>
&lt;li>&lt;code>least-waste&lt;/code>：选择扩展后，资源空闲最少的节点池&lt;/li>
&lt;li>&lt;code>price&lt;/code>：选择成本最低的节点池&lt;/li>
&lt;li>&lt;code>priority&lt;/code>：选择用户分配的具有最高优先级的节点池&lt;/li>
&lt;/ul>
&lt;p>确定类型后，CA 会调用相关 API 来创建资源。（云厂商会实现 API，比如 AWS 添加 EC2；Azure 添加 Virtual Machine；阿里云增加 ECS；GCP 增加 Compute Engine）&lt;/p>
&lt;p>计算资源就绪后，就会进行&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">节点的初始化&lt;/a>。&lt;/p>
&lt;p>注意，这里需要一定的耗时，通常比较慢。&lt;/p>
&lt;h2 id="探索-pod-自动缩放的前置时间">探索 Pod 自动缩放的前置时间&lt;/h2>
&lt;p>四个因素：&lt;/p>
&lt;ol>
&lt;li>HPA 的响应耗时&lt;/li>
&lt;li>CA 的响应耗时&lt;/li>
&lt;li>节点的初始化耗时&lt;/li>
&lt;li>Pod 的创建时间&lt;/li>
&lt;/ol>
&lt;p>默认情况下，&lt;a href="https://github.com/kubernetes/kubernetes/blob/2da8d1c18fb9406bd8bb9a51da58d5f8108cb8f7/pkg/kubelet/kubelet.go#L1855">kubelet 每 10 秒抓取一次 Pod 的 CPU 和内存占用情况&lt;/a>。&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-often-metrics-are-scraped">每分钟，Metrics Server 会将聚合的指标开放&lt;/a>给 Kubernetes API 的其他组件使用。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2256302x.png" alt="">&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-does-scale-up-work">CA 每 10 秒排查不可调度的 Pod。&lt;/a>&lt;/p>
&lt;ul>
&lt;li>少于 100 个节点，且每个节点最多 30 个 Pod，时间不超过 30s。平均延迟大约 5s。&lt;/li>
&lt;li>100 到 1000个节点，不超过 60s。平均延迟大约 15s。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2300242x.png" alt="">&lt;/p>
&lt;p>节点的配置时间，取决于云服务商。通常在 3~5 分钟。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2301312x.png" alt="">&lt;/p>
&lt;p>容器运行时创建 Pod：启动容器的几毫秒和&lt;strong>下载镜像的几秒钟&lt;/strong>。如果不做镜像缓存，几秒到 1 分钟不等，取决于层的大小和梳理。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2302382x.png" alt="">&lt;/p>
&lt;p>对于小规模的集群，最坏的情况是 6 分 30 秒。对于 100 个以上节点规模的集群，可能高达 7 分钟。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">HPA delay: 1m30s +
CA delay: 0m30s +
Cloud provider: 4m +
Container runtime: 0m30s +
=========================
Total 6m30s
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>突发情况，比如流量激增，你是否愿意等这 7 分钟？&lt;/em>&lt;/p>
&lt;p>&lt;em>这 7 分钟，如何优化压缩？&lt;/em>&lt;/p>
&lt;ul>
&lt;li>HPA 的刷新时间，默认 15 秒，通过 &lt;code>--horizontal-pod-autoscaler-sync-period&lt;/code> 标志控制。&lt;/li>
&lt;li>Metrics Server 的指标抓取时间，默认 60 秒，通过 &lt;code>metric-resolution&lt;/code> 控制。&lt;/li>
&lt;li>CA 的扫描间隔，默认 10 秒，通过 &lt;code>scan-interval&lt;/code> 控制。&lt;/li>
&lt;li>节点上缓存镜像，比如 &lt;a href="https://github.com/senthilrch/kube-fledged">kube-fledged&lt;/a> 等工具。&lt;/li>
&lt;/ul>
&lt;p>即使调小了上述设置，依然会受云服务商的时间限制。&lt;/p>
&lt;p>&lt;em>那么，如何解决？&lt;/em>&lt;/p>
&lt;p>两种尝试：&lt;/p>
&lt;ol>
&lt;li>尽量避免被动创建新节点&lt;/li>
&lt;li>主动创建新节点&lt;/li>
&lt;/ol>
&lt;h2 id="为-kubernetes-选择最佳规格的节点">为 Kubernetes 选择最佳规格的节点&lt;/h2>
&lt;p>&lt;strong>这会对扩展策略产生巨大影响。&lt;/strong>&lt;/p>
&lt;p>&lt;em>这样的场景&lt;/em>&lt;/p>
&lt;p>应用程序需要 1GB 内存和 0.1 vCPU；有一个 4GB 内存和 1 个 vCPU 的节点。&lt;/p>
&lt;p>排除操作系统、kubelet 和阈值保留空间后，有 2.5GB 内存和 0.7 个 vCPU 可用。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2314032x.png" alt="">&lt;/p>
&lt;p>最多只能容纳 2 个 Pod，扩展副本时最长耗时 7 分钟（HPA、CA、云服务商的资源配置耗时）&lt;/p>
&lt;p>假如节点的规格是 64GB 内存和 16 个 vCPU，可用的资源为 58.32GB 和 15.8 个 vCPU。&lt;/p>
&lt;p>&lt;strong>这个节点可以托管 58 个 Pod。只有扩容第 59 个副本时，才需要创建新的节点。&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2316562x.png" alt="CleanShot 2021-06-08 at 23.16.56@2x">&lt;/p>
&lt;p>这样触发 CA 的机会更少。&lt;/p>
&lt;p>选择大规格的节点，还有另外一个好处：资源的利用率会更高。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2317562x.png" alt="">&lt;/p>
&lt;p>&lt;strong>节点上可以容纳的 Pod 数量，决定了效率的峰值。&lt;/strong>&lt;/p>
&lt;p>物极必反！更大的实例，并不是一个好的选择：&lt;/p>
&lt;ol>
&lt;li>爆炸半径（Blast radius）：节点故障时，少节点的集群和多节点的集群，前者影响更大。&lt;/li>
&lt;li>自动缩放的成本效益低：增加一个大容量的节点，其利用率会比较低（调度过去的 Pod 数少）&lt;/li>
&lt;/ol>
&lt;p>&lt;em>即使选择了正确规格的节点，配置新的计算单元时，延迟仍然存在。怎么解决？&lt;/em>&lt;/p>
&lt;p>&lt;em>能否提前创建节点？&lt;/em>&lt;/p>
&lt;h2 id="为集群过度配置节点">为集群过度配置节点&lt;/h2>
&lt;p>即为集群增加备用节点，可以：&lt;/p>
&lt;ol>
&lt;li>创建一个节点，并留空 （比如 SchedulingDisabled）&lt;/li>
&lt;li>一旦空节点中有了一个 Pod，马上创建新的空节点&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2325582x.png" alt="">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2326262x.png" alt="CleanShot 2021-06-08 at 23.26.26@2x">&lt;/p>
&lt;p>&lt;strong>这种会产生额外的成本，但是效率会提升。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>CA 并不支持此功能 &amp;ndash; 总是保留一个空节点。&lt;/strong>&lt;/p>
&lt;p>但是，可以伪造。创建一个只占用资源，不使用资源的 Pod 占用整个 Node 节点。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2329322x.png" alt="">&lt;/p>
&lt;p>一旦有了真正的 Pod，驱逐占位的 Pod。
&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2330332x.png" alt="">&lt;/p>
&lt;p>待后台完成新的节点配置后，将“占位” Pod 再次占用整个节点。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2331062x.png" alt="">&lt;/p>
&lt;p>这个“占位”的 Pod 可以通过永久休眠来实现空间的保留。&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">pause&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">k8s.gcr.io/pause&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;1739m&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;5.9G&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>使用&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/">优先级和抢占&lt;/a>，来实现创建真正的 Pod 后驱逐“占位”的 Pod。&lt;/p>
&lt;p>使用 &lt;code>PodPriorityClass&lt;/code> 在配置 Pod 优先级：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">scheduling.k8s.io/v1beta1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">PriorityClass&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">value&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>-&lt;span class="m">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">#默认的是 0，这个表示比默认的低&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">globalDefault&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="kc">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">description&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;Priority class used by overprovisioning.&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>为“占位” Pod 配置优先级：&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">apps/v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Deployment&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">replicas&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">selector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">matchLabels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">template&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">run&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">priorityClassName&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">overprovisioning&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c">#HERE&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">reserve-resources&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">k8s.gcr.io/pause&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;1739m&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;5.9G&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>已经做完过度配置，应用程序是否需要优化？&lt;/em>&lt;/p>
&lt;h2 id="为-pod-选择正确的内存和-cpu-请求">为 Pod 选择正确的内存和 CPU 请求&lt;/h2>
&lt;p>Kubernetes 是根据 Pod 的内存和 CPU 请求，为其分配节点。&lt;/p>
&lt;p>如果 Pod 的资源请求配置不正确，可能会过晚（或过早）触发自动缩放器。&lt;/p>
&lt;p>这样一个场景：&lt;/p>
&lt;ul>
&lt;li>应用程序平均负载下消耗 512MB 内存和 0.25 个 vCPU。&lt;/li>
&lt;li>高峰时，消耗 4GB 内存 和 1 个 vCPU。（即资源限制，Limit）&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2338292x.png" alt="">&lt;/p>
&lt;p>有三种请求的配置选择：&lt;/p>
&lt;ol>
&lt;li>远低于平均使用量&lt;/li>
&lt;li>匹配平均使用量&lt;/li>
&lt;li>尽量接近限制&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2344462x.png" alt="2">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2345002x.png" alt="2">&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2346042x.png" alt="3">&lt;/p>
&lt;p>第一种的问题在于&lt;strong>超卖严重，过度使用节点&lt;/strong>。kubelet 负载高，稳定性差。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2346452x.png" alt="1">&lt;/p>
&lt;p>第三种，会造成资源的利用率低，浪费资源。这种通常被称为 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QoS：Quality of Service class&lt;/a> 中的 &lt;code>Guaranteed&lt;/code> 级别，Pod 不会被终止和驱逐。
&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/06/09/cleanshot-20210608-at-2347382x.png" alt="3">&lt;/p>
&lt;p>&lt;em>如何在稳定性和资源使用率间做权衡？&lt;/em>&lt;/p>
&lt;p>这就是 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#qos-classes">QoS：Quality of Service class&lt;/a> 中的 &lt;code>Burstable&lt;/code> 级别，即 Pod 偶尔会使用更多的内存和 CPU。&lt;/p>
&lt;ol>
&lt;li>如果节点中有可用资源，应用程序会在返回基线（baseline）前使用这些资源。&lt;/li>
&lt;li>如果资源不足，Pod 将竞争资源（CPU），kubelet 也有可能尝试驱逐 Pod（内存）。&lt;/li>
&lt;/ol>
&lt;p>在 &lt;code>Guaranteed&lt;/code> 和 &lt;code>Burstable&lt;/code> 之前如何做选择？取决于：&lt;/p>
&lt;ol>
&lt;li>想尽量减少 Pod 的重新调度和驱逐，应该是用 &lt;code>Guaranteed&lt;/code>。&lt;/li>
&lt;li>如果想充分利用资源时，使用 &lt;code>Burstable&lt;/code>。比如弹性较大的服务，Web 或者 REST 服务。&lt;/li>
&lt;/ol>
&lt;p>&lt;em>如何做出正确的配置？&lt;/em>&lt;/p>
&lt;p>应该分析应用程序，并测算空闲、负载和峰值时的内存和 CPU 消耗。&lt;/p>
&lt;p>甚至可以通过部署 VPA 来自动调整。&lt;/p>
&lt;h2 id="如何进行集群缩容">如何进行集群缩容？&lt;/h2>
&lt;p>&lt;strong>每 10 秒，当请求（request）利用率低于 50%时，CA 才会决定删除节点。&lt;/strong>&lt;/p>
&lt;p>CA 会汇总同一个节点上的所有 Pod 的 CPU 和内存请求。小于节点容量的一半，就会考虑对当前节点进行缩减。&lt;/p>
&lt;blockquote>
&lt;p>需要注意的是，CA 不考虑实际的 CPU 和内存使用或者限制（limit），只看请求（request）。&lt;/p>
&lt;/blockquote>
&lt;p>移除节点之前，CA 会：&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node">检查 Pod&lt;/a> 确保可以调度到其他节点上。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#i-have-a-couple-of-nodes-with-low-utilization-but-they-are-not-scaled-down-why">检查节点&lt;/a>，避免节点被过早的销毁，比如两个节点的请求都低于 50%。&lt;/li>
&lt;/ol>
&lt;p>检查都通过之后，才会删除节点。&lt;/p>
&lt;h2 id="为什么不根据内存或-cpu-进行自动缩放">为什么不根据内存或 CPU 进行自动缩放？&lt;/h2>
&lt;p>&lt;strong>基于内存和 CPU 的自动缩放器，不关心 pod。&lt;/strong>&lt;/p>
&lt;p>比如配置缩放器在节点的 CPU 达到总量的 80%，就自动增加节点。&lt;/p>
&lt;p>当你创建 3 个副本的 Deployment，3 个节点的 CPU 达到了 85%。这时会创建一个节点，但你并不需要第 4 个副本，新的节点就空闲了。&lt;/p>
&lt;p>&lt;strong>不建议使用这种类型的自动缩放器。&lt;/strong>&lt;/p>
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>定义和实施成功的扩展策略，需要掌握以下几点：&lt;/p>
&lt;ul>
&lt;li>节点的可分配资源。&lt;/li>
&lt;li>微调 Metrics Server、HPA 和 CA 的刷新间隔。&lt;/li>
&lt;li>设计集群和节点的规格。&lt;/li>
&lt;li>缓存容器镜像到节点。&lt;/li>
&lt;li>应用程序的基准测试和分析。&lt;/li>
&lt;/ul>
&lt;p>配合适当的监控工具，可以反复测试扩展策略并调整集群的缩放速度和成本。&lt;/p></description></item><item><title>云上细粒度访问管理的参考架构</title><link>https://atbug.com/translation-access-management-reference-architecture/</link><pubDate>Wed, 28 Apr 2021 08:02:11 +0800</pubDate><guid>https://atbug.com/translation-access-management-reference-architecture/</guid><description>
&lt;p>本文由 &lt;a href="https://mp.weixin.qq.com/mp/profile_ext?action=home&amp;amp;__biz=MjM5OTg2MTM0MQ==&amp;amp;scene=124#wechat_redirect">Addo Zhang&lt;/a> 翻译自 &lt;a href="https://www.infoq.com/articles/access-management-reference-architecture/">A Reference Architecture for Fine-Grained Access Management on the Cloud&lt;/a>&lt;/p>
&lt;h1 id="什么是访问管理">什么是访问管理？&lt;/h1>
&lt;p>访问管理是识别用户或一组用户是否应该能够访问给定资源（例如主机、服务或数据库）的过程。例如，对于开发人员来说是否可以使用 SSH 登录生产应用程序服务器，如果可以，那么可以登录多长时间？如果 SRE 在非支持时间尝试访问数据库，他们这样做？如果数据工程师已转移到其他团队，他们是否应该继续访问 ETL 管道的 S3 存储桶？&lt;/p>
&lt;h1 id="现在如何进行访问管理">现在如何进行访问管理？&lt;/h1>
&lt;p>在云上各种基础设施和数据服务激增之前，访问管理是 DevOps 和 Security 团队要解决的相对简单的问题。VPN 和堡垒主机是（现在仍然是）在网络级别封锁所有关键资源的首选机制。用户必须先通过 VPN 服务器进行身份验证，或者登录到堡垒主机，然后才能访问专用网络上的所有资源。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/27/16195301216852.jpg" alt="">&lt;/p>
&lt;p>当资源是静态的并且它们的数量相对较小时，此方法效果很好。但是，随着越来越多的资源动态地涌入专用网络的各处，VPN / 堡垒主机解决方案变得站不住脚。&lt;/p>
&lt;p>具体来说，在三个方面，VPN 和堡垒主机不足以作为一种有效的访问管理机制。&lt;/p>
&lt;ul>
&lt;li>&lt;strong>它们作用于网络层面&lt;/strong>：用户通过 VPN 进行身份验证并获得对专用网络的访问权限后，他们实际上就可以访问其上运行的所有服务。无法根据用户的身份在基础架构或数据服务的粒度上管理访问。&lt;/li>
&lt;li>&lt;strong>凭据是攻击的媒介&lt;/strong>：VPN 和堡垒主机都要求用户记住并存储凭据。过期和轮换凭证作为安全策略非常困难，尤其是在涉及大量用户的情况下，凭证因此成为潜在的攻击媒介。&lt;/li>
&lt;li>&lt;strong>不能管理第三方 SaaS 工具&lt;/strong>：SaaS 工具（如 Looker、Tableau 和 Periscope Data）需要直接访问数据端点。因此，使用这些工具访问数据的任何人都无法通过使用了相同的机制和凭据的基础设施进行身份验证。&lt;/li>
&lt;/ul>
&lt;h1 id="云上访问管理的新架构">云上访问管理的新架构&lt;/h1>
&lt;p>在本文中，我们将定义新的参考架构，为那些正在寻求简化访问管理云资源（从 SSH 主机、数据库、数据仓库到消息管道和云存储终结点）解决方案的云原生企业。&lt;/p>
&lt;p>它解决了 VPN 和堡垒主机无法克服的以下特定挑战：&lt;/p>
&lt;ul>
&lt;li>在细粒度的服务级别上进行访问鉴权&lt;/li>
&lt;li>消除共享凭据和个人帐户管理&lt;/li>
&lt;li>通过第三方 SaaS 工具控制访问&lt;/li>
&lt;/ul>
&lt;p>此外，它为具有敏感数据的组织带来以下商业利益：&lt;/p>
&lt;ul>
&lt;li>通过跨所有服务的会话记录和活动监视来满足 FedRamp 和 SOC2 等合规性标准的可审核性&lt;/li>
&lt;li>基于访问者的身份，通过细粒度的授权策略来限制或清除敏感数据，从而实现隐私和数据治理&lt;/li>
&lt;/ul>
&lt;p>该架构建立在以下三个核心原则的基础上，这些原则的实现使 DevOps 和 Security 团队可以在对所有环境进行全面控制的同时，通过简单而一致的体验来提高用户的工作效率。&lt;/p>
&lt;ul>
&lt;li>为访问资源的用户建立不可否认的身份&lt;/li>
&lt;li>使用短期的短暂令牌和证书代替静态凭证和密钥&lt;/li>
&lt;li>在一处集中所有资源类型的细粒度访问策略&lt;/li>
&lt;/ul>
&lt;p>下图显示了参考架构及其组件。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/27/16195323349746.jpg" alt="">&lt;/p>
&lt;p>上图中的 VPN / 堡垒主机已替换为接入网关（Access Gateway）。接入网关实际上是微服务的集合，负责验证单个用户、基于特定属性授权他们的请求，并最终授予他们访问专用网络中的基础结构和数据服务的权限。&lt;/p>
&lt;p>接下来，让我们看一下各个组件，以了解之前概括的核心原理是如何实现的。&lt;/p>
&lt;h2 id="访问控制器">访问控制器&lt;/h2>
&lt;p>支持此体系结构的关键见解是将用户身份验证委派给单个服务（访问控制器），而不是将责任分配给用户可能需要访问的服务。这种联合在 SaaS 应用程序世界中很常见。由单一服务负责身份验证，可以简化应用程序所有者的用户配置和接触配置，并加快应用程序开发。&lt;/p>
&lt;p>对于实际的身份验证序列，访问控制器本身通常会与身份提供商集成，例如 &lt;a href="https://auth0.com/">Auth0&lt;/a> 或 &lt;a href="https://www.okta.com/">Okta&lt;/a>，因此，可以跨提供者和协议提供有用的抽象。最终，身份提供商以签名的 SAML 声明\JWT 令牌或临时证书的形式保证用户的身份不可否认。这样就无需依赖受信任的子网作为用户身份的代理。与 VPN 允许用户访问网络上的所有服务不同，它还允许将访问策略配置到服务的粒度。&lt;/p>
&lt;p>将身份验证委派给身份提供者的另一个好处是，可以使用零信任原则对用户进行身份验证。 具体来说，可以创建身份提供者策略以强制执行以下操作：&lt;/p>
&lt;ul>
&lt;li>禁止从信誉不佳的地理位置和 IP 地址访问&lt;/li>
&lt;li>禁止从已知漏洞的设备（未修补的 OS、较旧的浏览器等）进行访问&lt;/li>
&lt;li>成功进行 SAML 交换后立即触发 MFA&lt;/li>
&lt;/ul>
&lt;h3 id="身份验证序列如何工作">身份验证序列如何工作：&lt;/h3>
&lt;ol>
&lt;li>用户首先通过访问控制器进行身份验证，访问控制器又将身份验证委派给身份提供者。&lt;/li>
&lt;li>成功登录到身份提供者后，访问控制器将生成一个短暂的临时证书，进行签名并将其返回给用户。或者，它可以代替证书生成令牌。只要证书或令牌有效，就可以将其用于连接到 接入网关管理的任何授权基础设施或数据服务。到期后，必须获取新的证书或令牌。&lt;/li>
&lt;li>用户将在步骤（2）中获得的证书传递给他们选择的工具，然后连接到接入网关。根据用户请求访问的服务，基础设施网关或数据网关将首先允许访问控制器验证用户的证书，然后再允许他们访问该服务。因此，访问控制器充当用户与其访问的服务之间的 CA，因此为每个用户提供了不可否认的身份。&lt;/li>
&lt;/ol>
&lt;h2 id="策略引擎">策略引擎&lt;/h2>
&lt;p>当访问控制器强制对用户进行身份验证时，策略引擎会对用户的请求强制进行细粒度的授权。它以易于使用的 YAML 语法接受授权规则（查看最后的示例），并根据用户请求和响应对它们进行评估。&lt;/p>
&lt;p>开放策略代理（OPA）是一个开源的 CNCF 项目，是策略引擎的一个很好的例子。它可以自己作为微服务运行，也可以用作其他微服务进程空间中的库。OPA 中的策略以称为 Rego 的语言编写。另外，也可以在 Rego 之上轻松构建一个简单的 YAML 界面，以简化政策规范。&lt;/p>
&lt;p>具有独立于基础结构和数据服务的安全模型的独立策略引擎的优点如下：&lt;/p>
&lt;ul>
&lt;li>可以以与服务和位置无关的方式指定安全策略
&lt;ul>
&lt;li>例如在所有 SSH 服务器上禁止特权命令&lt;/li>
&lt;li>例如强制执行 MFA 检查所有服务（基础设施和数据）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>策略可以保存在一个地方并进行版本控制
&lt;ul>
&lt;li>策略可以作为代码签入 GitHub 存储库&lt;/li>
&lt;li>每项变更在提交之前都要经过协作审核流程&lt;/li>
&lt;li>存在版本历史记录，可以轻松地还原策略更改&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>基础设施网关和数据网关都依赖于策略引擎，以分别评估用户的基础设施和数据活动。&lt;/p>
&lt;h2 id="基础设施网关">基础设施网关&lt;/h2>
&lt;p>基础设施网关管理和监控对基础设施服务的访问，例如 SSH 服务器和 Kubernetes 集群。它与策略引擎连接，以确定细化的授权规则，并在用户会话期间对所有基础设施活动强制执行这些规则。 为了实现负载平衡，网关可以包含一组工作节点，可以在 AWS 上部署为自动扩展组，也可以在 Kubernetes 集群上作为副本集运行。&lt;/p>
&lt;p>&lt;a href="https://www.boundaryproject.io/">Hashicorp 边界&lt;/a> 是基础设施网关的示例。这是一个开源项目，使开发人员、DevOps 和 SRE 可以使用细粒度的授权来安全地访问基础设施服务（SSH 服务器、Kubernetes 群集），而无需直接访问网络，同时又禁止使用 VPN 或堡垒主机。&lt;/p>
&lt;p>基础设施网关支持 SSH 服务器和 Kubernetes 客户端使用的各种连接协议，并提供以下关键功能：&lt;/p>
&lt;h3 id="会话记录">会话记录&lt;/h3>
&lt;p>这涉及复制用户在会话期间执行的每个命令。捕获的命令通常会附加其他信息，例如用户的身份、他们所属的各种身份提供者组、当天的时间、命令的持续时间以及响应的特征（是否成功、是否有错误、是否已读取或写入数据等）。&lt;/p>
&lt;h3 id="活动监控">活动监控&lt;/h3>
&lt;p>监控使会话记录的概念更进一步。除了捕获所有命令和响应，基础设施网关还将安全策略应用于用户的活动。在发生违规的情况下，它可以选择触发警报、阻止有问题的命令及其响应或完全终止用户的会话。&lt;/p>
&lt;h2 id="数据网关">数据网关&lt;/h2>
&lt;p>数据网关管理和监控对数据服务的访问，例如 MySQL、PostgreSQL 和 MongoDB 等托管数据库、AWS RDS 等 DBaaS 端点、Snowflake 和 Bigquery 等数据仓库、AWS S3 等云存储以及 Kafka 和 Kinesis。它与策略引擎连接，以确定细化的授权规则，并在用户会话期间对所有数据活动强制执行这些规则。&lt;/p>
&lt;p>与基础设施网关类似，数据网关可以包含一组工作节点，可以在 AWS 上部署为自动扩展组，也可以在 Kubernetes 集群上作为副本集运行。&lt;/p>
&lt;p>由于与基础设施服务相比，数据服务的种类更多，因此数据网关通常将支持大量的连接协议和语法。&lt;/p>
&lt;p>此类数据网关的示例是 &lt;a href="https://cyral.com/">Cyral&lt;/a>，这是一种轻量级的拦截服务，以边车（sidecar）的方式部署来监控和管理对现代数据终端节点的访问，如 AWS RDS、Snowflake、Bigquery，、AWS S3、Apache Kafka 等。其功能包括：&lt;/p>
&lt;h3 id="会话记录-1">会话记录&lt;/h3>
&lt;p>这类似于记录基础设施活动，并且涉及用户在会话期间执行的每个命令的副本，并使用丰富的审计信息进行注释。&lt;/p>
&lt;h3 id="活动监控-1">活动监控&lt;/h3>
&lt;p>同样，这类似于监视基础设施活动。例如，以下策略阻止数据分析人员读取敏感的客户 PII。&lt;/p>
&lt;h3 id="隐私权执行">隐私权执行&lt;/h3>
&lt;p>与基础设施服务不同，数据服务授予用户对通常位于数据库、数据仓库、云存储和消息管道中的与客户、合作伙伴和竞争对手有关的敏感数据的读写访问权限。 出于隐私原因，对数据网关的一个非常普遍的要求是能够清理（也称为令牌化或屏蔽）PII，例如电子邮件、姓名、社会保险号、信用卡号和地址。&lt;/p>
&lt;h2 id="那么这种体系结构如何简化访问管理">那么这种体系结构如何简化访问管理？&lt;/h2>
&lt;p>让我们看一些常见的访问管理方案，以了解与使用 VPN 和堡垒主机相比，接入网关架构如何提供细粒度的控制。&lt;/p>
&lt;h2 id="特权活动监控pam">特权活动监控（PAM）&lt;/h2>
&lt;p>这是一个简单的策略，可以在一个地方监视所有基础设施和数据服务中的特权活动：&lt;/p>
&lt;ul>
&lt;li>仅允许属于 Admins 和 SRE 组的个人在 SSH 服务器、Kubernetes 集群和数据库上运行特权命令。&lt;/li>
&lt;li>虽然可以运行特权命令，但是有一些例外形式的限制。具体来说，以下命令是不允许的：
&lt;ul>
&lt;li>“sudo” 和 “yum” 命令可能无法在任何 SSH 服务器上运行&lt;/li>
&lt;li>“kubectl delete” 和 “kubectl taint” 命令可能无法在任何 Kubernetes 集群上运行&lt;/li>
&lt;li>“drop table” 和 “create user” 命令可能无法在任何数据库上运行&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/27/16195354276750.jpg" alt="">&lt;/p>
&lt;h2 id="零特权zsp执行">零特权（ZSP）执行&lt;/h2>
&lt;p>The next policy shows an example of enforcing zero standing privileges &amp;ndash; a paradigm where no one has access to an infrastructure or data service by default. Access may be obtained only upon satisfying one or more qualifying criteria:&lt;/p>
&lt;ul>
&lt;li>Only individuals belonging to the Support group are allowed access&lt;/li>
&lt;li>An individual must be on-call to gain access. On call status may be determined by checking their schedule in an incident response service such as PagerDuty&lt;/li>
&lt;li>A multi-factor authentication (MFA) check is triggered upon successful authentication&lt;/li>
&lt;li>They must use TLS to connect to the infrastructure or data service&lt;/li>
&lt;li>Lastly, if a data service is being accessed, full table scans (e.g. SQL requests lacking a WHERE or a LIMIT clause that end up reading an entire dataset) are disallowed.&lt;/li>
&lt;/ul>
&lt;p>下一个策略显示了一个实施零特权的示例 &amp;ndash; 一种默认情况下没有人可以访问基础设施或数据服务的范例。只有满足一个或多个合格标准，才能获得访问权限：&lt;/p>
&lt;ul>
&lt;li>只允许属于支持组的个人访问&lt;/li>
&lt;li>个人必须 on-call 才能获得访问权限。可以通过检查事件响应服务（例如 PagerDuty）中的时间表来确定通话状态&lt;/li>
&lt;li>成功通过身份验证后会触发多因子身份验证（MFA）检查&lt;/li>
&lt;li>他们必须使用 TLS 连接到基础设施或数据服务&lt;/li>
&lt;li>最后，如果正在访问数据服务，则不允许进行全表扫描（例如，缺少 WHERE 或 LIMIT 子句的 SQL 请求最终将读取整个数据集）。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/27/16195356881012.jpg" alt="">&lt;/p>
&lt;h2 id="隐私和数据保护">隐私和数据保护&lt;/h2>
&lt;p>The last policy shows an example of data governance involving data scrubbing:&lt;/p>
&lt;ul>
&lt;li>If anyone from Marketing is accessing PII (social security number (SSN), credit card number (CCN), age), scrub the data before returning&lt;/li>
&lt;li>If anyone is accessing PII using the Looker or Tableau services, also scrub the data&lt;/li>
&lt;li>Scrubbing rules are defined by the specific type of the PII
&lt;ul>
&lt;li>For SSNs, scrub the first 5 digits&lt;/li>
&lt;li>For CCNs, scrub the last  4 digits&lt;/li>
&lt;li>For ages, scrub the last digit i.e., the requestor will know the age brackets but never the actual ages&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>最后一条策略显示了涉及数据清理的数据治理示例：&lt;/p>
&lt;ul>
&lt;li>如果市场营销人员正在访问 PII（社会保险号（SSN）、信用卡号（CCN）、年龄），先清洗数据然后再返回&lt;/li>
&lt;li>如果有人正在使用 Looker 或 Tableau 服务访问 PII，同时清洗数据&lt;/li>
&lt;li>清理规则由 PII 的特定类型定义
&lt;ul>
&lt;li>对于 SSN，清洗前 5 位数字&lt;/li>
&lt;li>对于 CCN，清洗最后 4 位数字&lt;/li>
&lt;li>对于年龄，请清洗最后一位数字，即请求者将知道年龄段，但从不知道实际年龄&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/27/16195358881245.jpg" alt="">&lt;/p>
&lt;h2 id="概括">概括&lt;/h2>
&lt;p>我们看到，对于高度动态的云环境，VPN 和堡垒主机不足以作为高效云环境中的有效访问管理机制。一种新的访问管理体系结构，其重点是不可否认的用户身份，短暂的证书或令牌以及集中的细粒度授权引擎，可有效解决 VPN 和堡垒主机无法解决的难题。除了为访问关键基础设施和数据服务的用户提供全面的安全性之外，该体系结构还可以帮助组织实现其审核、合规性、隐私和保护目标。&lt;/p>
&lt;p>我们还讨论了该架构的参考实现，其中使用了以开发人员为中心的著名开源解决方案，例如 Hashicorp Boundary 和 OPA 以及 Cyral（一种用于现代数据服务的快速且无状态的辅助工具）。 他们一起可以在云上提供细粒度且易于使用的访问管理解决方案。&lt;/p>
&lt;h2 id="关于作者">关于作者&lt;/h2>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/04/27/16195361264391.jpg" alt="">&lt;/p>
&lt;p>&lt;strong>Manav Mital&lt;/strong> 是 Cyral 的联合创始人兼首席执行官，Cyral 是首个为数据云提供可见性、访问控制和保护的云原生安全服务。Cyral 成立于 2018 年，与各种组织合作 - 从云原生初创企业到财富 500 强企业，因为它们采用 DevOps 文化和云技术来管理和分析数据。 Manav 拥有 UCLA 的计算机科学硕士学位和坎普尔的印度理工学院的计算机科学学士学位。&lt;/p>
&lt;h2 id="关于译者">关于译者&lt;/h2>
&lt;p>&lt;strong>Addo Zhang&lt;/strong> 云原生从业人员，爱好各种代码。更多翻译：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/beRHn9l2K4eiS8M1IevcRA">分布式系统在 Kubernetes 上的进化&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/V6lO9sT_6hJVled9sOI4IA">2021 年及未来的云原生预测&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/mw9LhDPiTyooUAXAoKHwTA">应用架构：为什么要随着市场演进&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>分布式系统在 Kubernetes 上的进化</title><link>https://atbug.com/translation-distributed-systems-kubernetes/</link><pubDate>Mon, 29 Mar 2021 23:11:25 +0800</pubDate><guid>https://atbug.com/translation-distributed-systems-kubernetes/</guid><description>
&lt;p>本文译自 &lt;a href="https://www.infoq.com/articles/distributed-systems-kubernetes/">The Evolution of Distributed Systems on Kubernetes&lt;/a>&lt;/p>
&lt;p>在 3 月份的 QCon 上，我做了一个关于 Kubernetes 的分布式系统进化的演讲。首先，我想先问一个问题，微服务之后是什么？我相信大家都有各自的答案，我也有我的答案。你会在最后发现我的想法是什么。为了达到这个目的，我建议大家看看分布式系统的需求是什么？以及这些需求在过去是如何发展的，从单体应用开始到 Kubernetes，再到最近的 Dapr、Istio、Knative 等项目，它们是如何改变我们做分布式系统的方式。我们将尝试对未来做一些预测。&lt;/p>
&lt;h2 id="现代分布式应用">现代分布式应用&lt;/h2>
&lt;p>为了给这个话题提供更多的背景信息，我认为的分布式系统是由数百个组件组成的系统。这些组件可以是有状态的、无状态的或者无服务器的。此外，这些组件可以用不同的语言创建，运行在混合环境上，并开发开源技术、开放标准和互操作性。我相信你可以使用闭源软件来构建这样的系统，也可以在 AWS 和其他地方构建。具体到这次演讲，我将关注 Kubernetes 生态系统，以及你如何在 Kubernetes 平台上构建这样一个系统。&lt;/p>
&lt;p>我们从分布式系统的需求讲起。我认为是我们要创建一个应用或者服务，并写一些业务逻辑。那从运行时的平台到构建分布式系统，我们还需要什么呢？在底层，最开始是我们要一些生命周期的能力。当你用任一语言开发你的应用时，我们希望有能力把这个应用可靠地打包和部署、回滚、健康检查。并且能够把应用部署到不同的节点上，并实现资源隔离、扩展、配置管理，以及所有这些。这些都是你创建分布式应用所需要的第一点。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/55image0011616431697020.jpg" alt="">&lt;/p>
&lt;p>第二点是围绕网络。我们有了应用之后，我们希望它能够可靠地连接到其他服务，无论该服务是在集群内部还是在外部。我们希望其具有服务发现、负载均衡的能力。为了不同的发布策略或是其他的一些原因的我们希望有流量转移的能力。然后我们还希望其具有与其他系统进行弹性通信的能力，无论是通过重试、超时还是断路器。要有适当的安全保障，并且要有足够的监控、追踪、可观察性等等。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/25image0021616431698392.jpg" alt="">&lt;/p>
&lt;p>我们有了网络之后，接下来就是我们希望有能力与不同的 API 和端点交互，即资源绑定&amp;ndash;与其他协议和不同的数据格式交互。甚至能够从一种数据格式转换成另一种数据格式。我还会在这里加入诸如滤光的功能，也就是说，当我们订阅一个主题时，我们也许只对某些事件感兴趣。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/45image0031616431697873.jpg" alt="">&lt;/p>
&lt;p>你认为最后一类是什么？是状态。当我在说状态和有状态的抽象时，我并不是在谈论实际的状态管理，比如数据库或者文件系统的功能。我要说的更多是有关幕后依赖状态的开发人员抽象。可能，你需要具有工作流管理的能力。也许你想管理运行时间长的进程或者做临时调度或者某些定时任务来定期运行服务。也许你还想进行分布式缓存，具有幂等性或者支持回滚。所有这些都是开发人员级的原语，但在幕后，它们依赖于具有某种状态。你想随意使用这些抽象俩创建完善的分布式系统。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/26image0041616431697348.jpg" alt="">&lt;/p>
&lt;p>我们将使用这个分布式系统原语的框架来评估它们在 Kubernetes 和其他项目上的变化情况。&lt;/p>
&lt;h2 id="单体架构----传统中间件功能">单体架构 &amp;ndash; 传统中间件功能&lt;/h2>
&lt;p>假设我们从单体架构以及如何获得这些能力开始。在那种情况下，首先是当我说单体的时候，在分布式应用的情况下我想到的是 ESB。ESB 是相当强大的，当我们检查我们的需求列表时，我们会说 ESB 对所有有状态的抽象有很好的支持。&lt;/p>
&lt;p>使用 ESB，你可以进行长时间运行的流程的编排、分布式事务、回滚和幂等。此外，ESB 还提供了出色的资源绑定能力，并且有数百个连接器，支持转换、编排，甚至有联网功能。最后，ESB 甚至可以做服务发现和负载均衡。&lt;/p>
&lt;p>它具有围绕网络连接的弹性的所有功能，因此它可以进行重试。可能 ESB 本质上不是很分布式，所以它不需要非常高级的网络和发布能力。ESB 欠缺的主要是生命周期管理。因为它是单一运行时，所以第一件事就是你只能使用一种语言。通常是创建实际运行时的语言，Java、.NET、或者其他的语言。然后，因为是单一运行时，我们不能轻松地进行声明式的部署或者自动防止。部署是相当大且非常重的，所以它通常涉及到人机交互。这种单体架构的另一个难点是扩展：“我们无法扩展单个组件。”&lt;/p>
&lt;p>最后却并非最不重要的一点是，围绕隔离，无论是资源隔离还是故障隔离。使用单体架构无法完成所有这些工作。从我们的需求框架来看，ESB 的单体架构不符合条件。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/40image0051616431696438.jpg" alt="">&lt;/p>
&lt;h2 id="云原生架构----微服务和-kubernetes">云原生架构 &amp;ndash; 微服务和 Kubernetes&lt;/h2>
&lt;p>接下来，我建议我们研究一下云原生架构以及这些需求是如何变化的。如果我们从一个非常高的层面来看，这些架构是如何发生变化的，云原生可能始于微服务运动。微服务使我们可以按业务领域进行拆分单体应用。事实证明，容器和 Kubernetes 实际上是管理这些微服务的优秀平台。让我们来看一下 Kubernetes 对于微服务特别有吸引力的一些具体特性和功能。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/13image0061616431699209.jpg" alt="">&lt;/p>
&lt;p>从一开始，进行健康状况探测的能力就是 Kubernetes 受欢迎的原因。在实践中，这意味着当你将容器部署到 Pod 中时，Kubernetes 会检查进程的运行状况。通常情况下，该过程模型还不够好。你可能仍然有一个已启动并正在运行的进程，但是它并不健康。这就是为什么还可以使用就绪度和存活度检查的原因。Kubernetes 会做一个就绪度检查，以确定你的应用在启动期间何时准备接受流量。它将进行活跃度检查，以检查服务的运行状况。在 Kubernetes 之前，这并不是很流行，但今天几乎所有语言、所有框架、所有运行时都有健康检查功能，你可以在其中快速启动端点。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/29image0071616431696697.jpg" alt="">&lt;/p>
&lt;p>Kubernetes 引入的下一个特性是围绕应用程序的托管生命周期&amp;ndash;我的意思是，你不再控制何时启动、何时关闭服务。你相信平台可以做到这一点。Kubernetes 可以启动你的应用；它可以将其关闭，然后在不同的节点上移动它。为此，你必须正确执行平台在应用启动和关闭期间告诉你的事件。&lt;/p>
&lt;p>Kubernetes 刘兴的另一件特性是围绕着声明式部署。这意味着你不再需要启动服务；检查日志是否已经启动。你不必手动升级实例&amp;ndash;支持声明式部署的 Kubernetes 可以为你做到这一点。根据你选择的策略，它可以停止旧实例并启动新实例。此外，如果出现问题，可以进行回滚。&lt;/p>
&lt;p>另外就是声明你的资源需求。创建服务时，将其容器化。最好告诉平台该服务将需要多少 CPU 和内存。Kubernetes 利用这些信息为你的工作负载找到最佳节点。在使用 Kubernetes 之前，我们必须根据我们的标准将实例手动放置到一个节点上。现在，我们可以根据自己的偏好来指导 Kubernetes，它将为我们做出最佳的决策。&lt;/p>
&lt;p>如今，在 Kubernetes 上，你可以进行多语言配置管理。无需在应用程序运行时进行配置查找就可以进行任何操作。Kubernetes 会确保配置最终在工作负载所在的同一节点上。这些配置被映射为卷或环境变量，以供你的应用程序使用。&lt;/p>
&lt;p>事实证明，我刚才谈到的那些特定功能也是相关的。比如说，如果要进行自动放置，则必须告诉 Kubernetes 服务的资源需求。然后，你必须告诉它要使用的部署策略。为了让策略正确运行，你的应用程序必须执行来自环境的事件。它必须执行健康检查。一旦采用了所有这些最佳实践并使用所有这些功能，你的应用就会成为出色的云原生公民，并且可以在 Kubernetes 上实现自动化了（这是在 Kubernetes 上运行工作负载的基本模式）。最后，还有围绕着构建 Pod 中的容器、配置管理和行为，还有其他模式。&lt;/p>
&lt;p>我要简要介绍的下一个主题是工作负载。从生命周期的角度来看，我们希望能够运行不同的工作负载。我们也可以在 Kubernetes 上做到这一点。运行十二要素应用程序和无状态微服务非常简单。Kubernetes 可以做到这一点。这不是你将要承担的唯一工作量。可能你还有有状态的工作负载，你可以使用有状态集在 Kubernetes 上完成此工作。&lt;/p>
&lt;p>你可能还有的另一个工作负载是单例。也许你希望某个应用程序的实例是整个集群中应用程序的唯一一个实例&amp;ndash;你希望它成为可靠的单例。如果失败，则重新启动。因此，你可以根据需求以及是否希望单例至少具有一种或最多一种语义来在有状态集和副本集之间进行选择。你可能还有的另一个工作负载是围绕作业和定时作业&amp;ndash;有了 Kubernetes，你也可以实现这些。&lt;/p>
&lt;p>如果我们将所有这些 Kubernetes 功能映射到我们的需求，则 Kubernetes 可以满足生命周期需求。我通常创建的需求列表主要是由 Kubernetes 今天提供给我们的。这些是任何平台上的预期功能，而 Kubernetes 可以为你的部署做的是配置管理、资源隔离和故障隔离。此外，除了无服务器本身之外，它还支持其他工作负载。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/12image0081616431698134.jpg" alt="">&lt;/p>
&lt;p>然后，如果这就是 Kubernetes 给开发者提供的全部功能，那么我们该如何扩展 Kubernetes 呢？以及如何使它具有更多功能？因此，我想描述当今使用的两种常用方法。&lt;/p>
&lt;h2 id="进程外扩展机制">进程外扩展机制&lt;/h2>
&lt;p>首先是 Pod 的概念，Pod 是用于在节点上部署容器的抽象。此外，Pod 给我们提供了两组保证：&lt;/p>
&lt;ul>
&lt;li>第一组是部署保证 &amp;ndash; Pod 中的所有容器始终位于同一个节点上。这意味着它们可以通过 localhost 相互通信，也可以使用文件系统或通过其他 IPC 机制进行异步通信。&lt;/li>
&lt;li>Pod 给我们的另一组保证是围绕生命周期的。Pod 中的所有容器并非都相等。&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/22image0091616431698660.jpg" alt="">&lt;/p>
&lt;p>根据使用的是 init 容器还是应用程序容器，你会获得不同的保证。例如，init 容器在开始时运行；当 Pod 启动时，它按顺序一个接一个地运行。他们仅在之前的容器已成功完成时运行。它们有助于实现由容器驱动的类似工作流的逻辑。&lt;/p>
&lt;p>另一方面，应用程序容器是并行运行的。它们在整个 Pod 的生命周期中运行，这也是 sidecar 模式的基础。sidecar 可以运行多个容器，这些容器可以协作并共同为用户提供价值。这也是当今我们看到的扩展 Kubernetes 附加功能的主要机制之一。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/9image0101616431695489.jpg" alt="">&lt;/p>
&lt;p>为了解释以下功能，我必须简要地告诉你 Kubernetes 内部的工作方式。它是基于调谐循环的。调谐循环的思想是将期望状态驱动到实际状态。在 Kubernetes 中，很多功能都是靠这个来实现的。例如，当你说我要两个 Pod 实例，这系统的期望状态。有一个控制循环不断地运行，并检查你的 Pod 是否有两个实例。如果不存在两个实例，它将计算差值。它将确保存在两个实例。&lt;/p>
&lt;p>这方面的例子有很多。一些是副本集或有状态集。资源定义映射到控制器是什么，并且每个资源定义都有一个控制器。该控制器确保现实世界与所需控制器相匹配，你甚至可以编写自己的自定义控制器。&lt;/p>
&lt;p>当在 Pod 中运行应用程序时，你将无法在运行时加载任何配置文件更改。然而，你可以编写一个自定义控制器，检测 config map 的变化，重新启动 Pod 和应用程序&amp;ndash;从而获取配置更改。&lt;/p>
&lt;p>事实证明，即使 Kubernetes 拥有丰富的资源集合，但它们并不能满足你的所有不同需求。Kubernetes 引入了自定义资源定义的概念。这意味着你可以对需求进行建模并定义适用于 Kubernetes 的 API。它与其他 Kubernetes 原生资源共存。你可以用能理解模型的任何语言编写自己的控制器。你可以设计一个用 Java 实现的 ConfigWatcher，描述我们前面所解释的内容。这就是 operator 模式，即与自定义资源定义一起使用的控制器。如今，我们看到很多 operator 假如，这就是第二种扩展 Kubernetes 附加功能的方式。&lt;/p>
&lt;p>接下来，我想简单介绍一下基于 Kubernetes 构建的一些平台，这些平台大量使用 sidecar 和 operator 来给开发者提供额外的功能。&lt;/p>
&lt;h2 id="什么是服务网格">什么是服务网格？&lt;/h2>
&lt;p>让我们从服务网格开始，什么是服务网格？&lt;/p>
&lt;p>我们有两个服务，服务 A 要调用服务 B，并且可以用任何语言。把这个当做是我们的应用工作负载。服务网格使用 sidecar 控制器，并在我们的服务旁边注入一个代理。你最终会在 Pod 中得到两个容器。代理是一个透明的代理，你的应用对这个代理完全无感知&amp;ndash;它拦截所有传入和传出的流量。此外，代理还充当数据防火墙。&lt;/p>
&lt;p>这些服务代理的集合代表了你的数据平面，并且很小且无状态。为了获得所有状态和配置，它们依赖于控制平面。控制平面是保持所有配置，收集指标，做出决定并与数据平面进行交互的有状态部分。此外，它们是不同控制平面和数据平面的正确选择。事实证明，我们还需要一个组件-一个 API 网关，以将数据获取到我们的集群中。一些服务网格具有自己的 API 网关，而某些使用第三方。如果你研究下所有这些组件，它们将提供我们所需的功能。&lt;/p>
&lt;p>API 网关主要专注于抽象我们服务的实现。它隐藏细节并提供边界功能。服务网格则相反。在某种程度上，它增强了服务内的可见性和可靠性。可以说，API 网关和服务网格共同提供了所有网络需求。要在 Kubernetes 上获得网络功能，仅使用服务是不够的：“你需要一些服务网格。”&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/19image0111616431696146.jpg" alt="">&lt;/p>
&lt;h2 id="什么是-knative">什么是 Knative？&lt;/h2>
&lt;p>我要讨论的下一个主题是 Knative，这是 Google 几年前启动的一个项目。它是 Kubernetes 之上的一层，可为您提供无服务器功能，并具有两个主要模块：&lt;/p>
&lt;ul>
&lt;li>Knative 服务 - 围绕着请求-应答交互，以及&lt;/li>
&lt;li>Knative Eventing - 更多的是用于事件驱动的交互。&lt;/li>
&lt;/ul>
&lt;p>只是让你感受一下，Knative Serving 是什么？通过 Knative Serving，你可以定义服务，但这不同于 Kubernetes 服务。这是 Knative 服务。使用 Knative 服务定义工作负载后，你就会得到具有无服务器的特征的部署。你不需要有启动并运行实例。它可以在请求到达时从零开始。你得到的是无服务器的能力；它可以迅速扩容，也可以缩容到零。&lt;/p>
&lt;p>Knative Eventing 为我们提供了一个完全声明式的事件管理系统。假设我们有一些要与之集成的外部系统，以及一些外部的事件生产者。在底部，我们将应用程序放在具有 HTTP 端点的容器中。借助 Knative Eventing，我们可以启动代理，该代理可以触发 Kafka 映射的代理，也可以在内存或者某些云服务中。此外，我们可以启动连接到外部系统的导入器，并将事件导入到我们的代理中。这些导入器可以基于，例如，具有数百个连接器的 Apache Camel。&lt;/p>
&lt;p>一旦我们将事件发送给代理，然后用 YAML 文件声明，我们可以让容器订阅这些事件。在我们的容器中，我们不需要任何消息客户端&amp;ndash;比如 Kafka 客户端。我们的容器将使用云事件通过 HTTP POST 获取事件。这是一个完全平台管理的消息传递基础设施。作为开发人员，你必须在容器中编写业务代码，并且不处理任何消息传递逻辑。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/8image0121616431698919.jpg" alt="">&lt;/p>
&lt;p>从我们的需求的角度来看，Knative 可以满足其中的一些要求。从生命周期的角度来看，它为我们的工作负载提供了无服务器的功能，因此能够将其扩展到零，并从零开始激活。从网络的角度来看，如果服务网格之间存在某些重叠，则 Knative 也可以进行流量转移。从绑定的角度来看，它对使用 Knative 导入程序进行绑定提供了很好的支持。它可以使我们进行发布/订阅，或点对点交互，甚至可以进行一些排序。它可以满足几类需求。&lt;/p>
&lt;h2 id="什么是-dapr">什么是 Dapr？&lt;/h2>
&lt;p>另一个使用 sidecar 和 operator 的项目是 &lt;a href="https://dapr.io/">Dapr&lt;/a>，它是微软几个月前才开始并且正在迅速流行起来。此外，1.0 版本 &lt;a href="https://www.infoq.com/news/2021/02/dapr-production-ready/">被认为是生产可用的&lt;/a>。它是一个作为 sidecar 的分布式系统工具包&amp;ndash;Dapr 中的所有内容都是作为 sidecar 提供的，并且有一套他们所谓的构件或功能集的集合。&lt;/p>
&lt;p>这些功能是什么呢？第一组功能是围绕网络。Dapr 可以进行服务发现和服务之间的点对点集成。同样，它也可以进行服务网格的追踪、可靠通信、重试和恢复。第二套功能是围绕资源绑定：&lt;/p>
&lt;ul>
&lt;li>它有很多云 API、不同系统的连接器，以及&lt;/li>
&lt;li>也可以做消息发布/订阅和其他逻辑。&lt;/li>
&lt;/ul>
&lt;p>有趣的是，Dapr 还引入了状态管理的概念。除了 Knative 和服务网格提供的功能外，Dapr 在状态存储之上进行了抽象。此外，你通过存储机制支持与 Dapr 进行基于键值的交互。&lt;/p>
&lt;p>在较高的层次上，架构是你的应用程序位于顶部，可以使用任何语言。你可以使用 Dapr 提供的客户端库，但你不必这样做。你可以使用语言功能来执行称为 sidecar 的 HTTP 和 gRPC。与 服务网格的区别在于，这里的 Dapr sidecar 不是一个透明的代理。它是一个显式代理，你必须从你的应用中调用它，并通过 HTTP 或 gRPC 与之交互。根据你需要的功能，Dapr 可以与其他如云服务的系统对话。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/18image0131616431699532.jpg" alt="">&lt;/p>
&lt;p>在 Kubernetes 上，Dapr 是作为 sidecar 部署的，并且可以在 Kubernetes 之外工作（不仅仅是 Kubernetes）。此外，它还有一个 operator &amp;ndash; 而 sidecar 和 Operator 是主要的扩展机制。其他一些组件管理证书、处理基于 actor 的建模并注入 sidecar。你的工作负载与 sidecar 交互，并尽其所能与其他服务对话，让你与不同的云提供商进行互操作。它还为你提供了额外的分布式系统功能。&lt;/p>
&lt;p>综上所述，这些项目所提供的功能，我们可以说 ESB 是分布式系统的早期化身，其中我们有集中式的控制平面和数据平面&amp;ndash;但是扩展性不好。在云原生中，集中式控制平面仍然存在，但是数据平面是分散的&amp;ndash;并且具有隔音功能和高度的可扩展性。&lt;/p>
&lt;p>我们始终需要 Kubernetes 来做良好的生命周期管理，除此之外，你可能还需要一个或多个附加组件。你可能需要 Istio 来进行高级联网。你可能会使用 Knative 来进行无服务器工作负载，或者使用 Dapr 来做集成。这些框架可与 Istio 和 Envoy 很好的配合使用。从 Dapr 和 Knative 的角度来看，你可能必须选择一个。它们共同以云原生的方式提供了我们过去在 ESB 上拥有的东西。&lt;/p>
&lt;h2 id="未来云原生趋势--生命周期趋势">未来云原生趋势&amp;ndash;生命周期趋势&lt;/h2>
&lt;p>在接下来的部分，我列出了一些我认为在这些领域正在发生令人振奋的发展的项目。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/8image0141616431695762.jpg" alt="">&lt;/p>
&lt;p>我想从生命周期开始。通过 Kubernetes，我们可以为应用程序提供一个有用的生命周期，这可能不足以进行更复杂的生命周期管理。比如，如果你有一个更复杂的有状态应用，则可能会有这样的场景，其中 Kubernetes 中的部署原语不足以为应用提供支持。&lt;/p>
&lt;p>在这些场景下，你可以使用 operator 模式。你可以使用一个 operator 来进行部署和升级，还可以将 S3 作为服务备份的存储介质。此外，你可能还会发现 Kubernetes 的实际健康检查机制不够好。假设存活检查和就绪检查不够好。在这种情况下，你可以使用 operator 对你的应用进行更智能的存活和就绪检查，然后在此基础上进行恢复。&lt;/p>
&lt;p>第三个领域就是自动伸缩和调整。你可以让 operator 更好的了解你的应用，并在平台上进行自动调整。目前，编写 operator 的框架主要有两个，一个是 Kubernetes 特别兴趣小组的 Kubebuilder，另一个是红帽创建的 operator 框架的一部分&amp;ndash;operator SDK。它有以下几个方面的内容：&lt;/p>
&lt;p>Operator SDK 让你可以编写 operator &amp;ndash; operator 生命周期管理器来管理 operator 的生命周期，以及可以发布你的 operator 到 OperatorHub。如今在 OperatorHub，你会看到 100 多个 operator 用于管理数据库、消息队列和监控工具。从生命周期空间来看，operator 可能是 Kubernetes 生态系统中发展最活跃的领域。&lt;/p>
&lt;h2 id="网络趋势---envoy">网络趋势 - Envoy&lt;/h2>
&lt;p>我选的另一个项目是 &lt;a href="https://www.envoyproxy.io/">Envoy&lt;/a>。服务网格接口规范的引入将使你更轻松地切换不同的服务网格实现。在部署上 Istio 对架构进行了一些整合。你不再需要为控制平面部署 7 个 Pod；现在，你只需要部署一次就可以了。更有趣的是在 Envoy 项目的数据平面上所正在发生的：越来越多的第 7 层协议被添加到 Envoy 中。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/11image0151616431697613.jpg" alt="">&lt;/p>
&lt;p>服务网格增加了对更多协议的支持，比如 MongoDB、ZooKeeper、MySQL、Redis，而最新的协议是 Kafka。我看到 Kafka 社区现在正在进一步改进他们的协议，使其对服务网格更加友好。我们可以预料将会有更紧密的集成、更多的功能。最有可能的是，会有一些桥接的能力。你可以从服务中在你的应用本地做一个 HTTP 调用，而代理将在后台使用 Kafka。你可以在应用外部，在 sidecar 中针对 Kafka 协议进行转换和加密。&lt;/p>
&lt;p>另一个令人兴奋的发展是引入了 HTTP 缓存。现在 Envoy 可以进行 HTTP 缓存。你不必在你的应用中使用缓存客户端。所有这些都是在 sidecar 中透明地完成的。有了 tap 过滤器，你可以 tap 流量并获得流量的副本。最近，WebAssembly 的引入，意味着如果你要为 Envoy 编写一些自定义的过滤器，你不必用 C++ 编写，也不必编译整个 Envoy 运行时。你可以用 WebAssembly 写你的过滤器，然后在运行时进行部署。这些大多数还在进行中。它们不存在，说明数据平面和服务网格无意停止，仅支持 HTTP 和 gRPC。他们有兴趣支持更多的应用层协议，为你提供更多的功能，以实现更多的用例。最主要的是，随着 WebAssembly 的引入，你现在可以在 sidecar 中编写自定义逻辑。只要你没有在其中添加一些业务逻辑就可以了。&lt;/p>
&lt;h2 id="绑定趋势---apache-camel">绑定趋势 - Apache Camel&lt;/h2>
&lt;p>&lt;a href="https://camel.apache.org/">Apache Camel&lt;/a> 是一个用于集成的项目，它具有很多使用企业集成模式连接到不同系统的连接器。 比如 &lt;a href="https://camel.apache.org/releases/release-3.0.0/">Camel version 3&lt;/a> 就深度集成到了 Kubernetes 中，并且使用了我们到目前为止所讲的那些原语，比如 operator。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/7image0161616431694981.jpg" alt="">&lt;/p>
&lt;p>你可以在 Camel 中用 Java、JavaScript 或 YAML 等语言编写你的集成逻辑。最新的版本引入了一个 Camel operator，它在 Kubernetes 中运行并理解你的集成。当你写好 Camel 应用，将其部署到自定义资源中，operator 就知道如何构建容器或查找依赖项。根据平台的能力，不管是只用 Kubernetes，还是带有 Knative 的 Kubernetes，它都可以决定要使用的服务以及如何实现集成。在运行时之外有相当多的智能 &amp;ndash; 包括 operator &amp;ndash; 所有这些都非常快地发生。为什么我会说这是一个绑定的趋势？主要是因为 Apache Camel 提供的连接器的功能。这里有趣的一点是它如何与 Kubernetes 深度集成。&lt;/p>
&lt;h2 id="状态趋势---cloudstate">状态趋势 - Cloudstate&lt;/h2>
&lt;p>另一个我想讨论的项目是 &lt;a href="https://cloudstate.io/">Cloudstate&lt;/a> 和与状态相关的趋势。Cloudstate 是 Lightbend 的一个项目，主要致力于无服务器和功能驱动的开发。最新发布的版本，正在使用 sidecar 和 operator 与 Kubernetes 进行深度集成。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/8image0171616431996943.jpg" alt="">&lt;/p>
&lt;p>这个创意是，当你编写你的功能时，你在功能中要做的就是使用 gRPC 来获取状态并与之进行交互。整个状态管理在与其他 sidecar 群集的 sidear 中进行。它使你能够进行事件溯源、CQRS、键值查询、消息传递。&lt;/p>
&lt;p>从应用程序角度来看，你并不了解所有这些复杂性。你所做的只是调用一个本地的 sidecar，而 sidecar 会处理这些复杂的事情。它可以在后台使用两个不同的数据源。而且它拥有开发人员所需的所有有状态抽象。&lt;/p>
&lt;p>到目前为止，我们已经看到了云原生生态系统中的最新技术以及一些仍在进行中的开发。我们如何理解这一切？&lt;/p>
&lt;h2 id="多运行时微服务已经到来">多运行时微服务已经到来&lt;/h2>
&lt;p>如果你看微服务在 Kubernetes 上的样子，则将需要使用某些平台功能。此外，你将需要首先使用 Kubernetes 的功能进行生命周期管理。然后，很有可能透明地，你的服务会使用某些服务网格（例如 Envoy）来获得增强的网络功能，无论是流量路由、弹性、增强的安全性，甚至出于监控的目的。除此之外，根据你的场景和使用的工作负载可能需要 Dapr 或者 Knative。所有这些都代表了进程外附加的功能。剩下的就是编写业务逻辑，不是放在最上面而是作为一个单独的运行时来编写。未来的微服务很有可能将是由多个容器组成的这种多运行时。有些是透明的，有些则是非常明确的。&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/6image0181616431996411.jpg" alt="">&lt;/p>
&lt;h2 id="智能的-sidecar-和愚蠢的管道">智能的 sidecar 和愚蠢的管道&lt;/h2>
&lt;p>如果更深入地看，那可能是什么样的，你可以使用一些高级语言编写业务逻辑。是什么并不重要，不必仅是 Java，因为你可以使用任何其他语言并在内部开发自定义逻辑。&lt;/p>
&lt;p>你的业务逻辑与外部世界的所有交互都是通过 sidecar 发生的，并与平台集成进行生命周期管理。它为外部系统执行网络抽象，为你提供高级的绑定功能和状态抽象。sidecar 是你不需要开发的东西。你可以从货架上拿到它。你用一点 YAML 或 JSON 配置它，然后就可以使用它。这意味着你可以轻松地更新 sidecar，因为它不再被嵌入到你的运行时。这使得打补丁、更新变得更加更容易。它为我们的业务逻辑启用了多语言运行时。&lt;/p>
&lt;h2 id="微服务之后是什么">微服务之后是什么？&lt;/h2>
&lt;p>这让我想到了最初的问题，微服务之后是什么？&lt;/p>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/6image0201616431995910.jpg" alt="">&lt;/p>
&lt;p>如果我们看下架构的发展历程，应用架构在很高的层面上是从单体应用开始的。然而微服务给我们提供了如何把一个单体应用拆分成独立的业务域的指导原则。之后又出现了无服务器和功能即服务（FaaS），我们说过可以按操作将其进一步拆分，从而实现极高的可扩展性-因为我们可以分别扩展每个操作。&lt;/p>
&lt;p>我想说的是 FaaS 并不是最好的模式 &amp;ndash; 因为功能并不是实现合理的复杂服务的最佳模式，在这种情况下，当多个操作必须与同一个数据集进行交互时，你希望它们驻留在一起。可能是多运行时（我把它称为 &lt;a href="https://www.infoq.com/articles/multi-runtime-microservice-architecture/">Mecha 架构&lt;/a>），在该架构中你将业务逻辑放在一个容器中，而所有与基础设施相关的关注点作为一个单独的容器存在。它们共同代表多运行时微服务。也许这是一个更合适的模型，因为它有更好的属性。&lt;/p>
&lt;p>你可以获得微服务的所有好处。仍然将所有域和所有限界上下文放在一处。你将所有的基础设施和分布式应用需求放在一个单独的容器中，并在运行时将它们组合在一起。大概，现在最接近这种模型的是 Dapr。他们正在遵循这种模型。如果你仅对网络方面感兴趣，那么可能使用 Envoy 也会接近这种模型。&lt;/p>
&lt;h2 id="关于作者">关于作者&lt;/h2>
&lt;p>&lt;img src="https://atbug.oss-cn-hangzhou.aliyuncs.com/2021/03/30/21bilgin-ibryam15886810412341616480845087.jpeg" alt="">&lt;/p>
&lt;p>&lt;strong>Bilgin Ibryam&lt;/strong> 是红帽公司的产品经理和前架构师、提交人，并且是 Apache 软件基金会的成员。他是开源布道者，经常写博客、发表演讲，是 &lt;a href="https://k8spatterns.io/">Kubernetes Patterns&lt;/a> 和 Camel Design Patterns 书籍的作者。Bilgin 目前的工作主要集中在分布式系统、事件驱动架构以及可重复的云原生应用开发模式和实践上。请关注他 @bibryam 了解未来类似主题的更新。&lt;/p></description></item><item><title>【译】2021 年及未来的云原生预测</title><link>https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/</link><pubDate>Tue, 09 Feb 2021 06:43:54 +0800</pubDate><guid>https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/</guid><description>
&lt;p>本文译自 &lt;a href="https://www.cncf.io/blog/2021/01/29/cloud-native-predictions-for-2021-and-beyond/">Cloud Native Predictions for 2021 and Beyond&lt;/a>&lt;/p>
&lt;p>原文发布在 &lt;a href="https://www.aniszczyk.org/2021/01/19/cloud-native-predictions-for-2021-and-beyond/">Chris Aniszczyk 的个人博客&lt;/a>&lt;/p>
&lt;p>我希望每个人都有一个美好的假期，因为 2021 年 1 月的前几周一直非常疯狂，从叛乱到新的 COVID 菌株。在云原生国度，CNCF 最近发布了关于我们去年完成的所有工作的&lt;a href="https://www.cncf.io/cncf-annual-report-2020/">年度报告&lt;/a>。我建议大家找个机会去看一下这份报告，在疫情大流行的这一年，我们收获颇丰。&lt;a href="https://twitter.com/CloudNativeFdn/status/1343914259177222145">https://twitter.com/CloudNativeFdn/status/1343914259177222145&lt;/a>&lt;/p>
&lt;p>作为我工作的一部分，我对云原生趋势有一个独特的观点，送给所有与我合作的会员公司和开发人员，所以我想我会分享我对 2021 年及以后云原生发展的想法。&lt;/p>
&lt;p>&lt;strong>云原生的 IDE&lt;/strong>&lt;/p>
&lt;p>作为一个在 Eclipse 基金会内部从事开发者工具工作的人，我对最近的技术状态进展感到无比兴奋。未来，开发生命周期（代码、构建、调试）将主要发生在云端，而不是你本地的 Emacs 或 VSCode。你将每一个拉动请求最终得到一个完整的开发环境设置，预先配置并连接到他们自己的部署，以协助你的开发和调试需求。今天这种技术的一个具体例子是通过 GitHub &lt;a href="https://github.com/features/codespaces">Codespaces&lt;/a> 和 &lt;a href="https://gitpod.io/">GitPod&lt;/a> 实现的。虽然 GitHub Codespaces 还处于测试阶段，但今天你可以通过 GitPod 来体验，以 &lt;a href="https://gitpod.io/#https://github.com/prometheus/prometheus">Prometheus&lt;/a> 为例。一分钟左右，你就拥有了一个有编辑器和预览环境的完全实时的开发环境。最疯狂的是，这个开发环境（工作空间）是 &lt;a href="https://github.com/prometheus/prometheus/blob/master/.gitpod.yml">用代码描述&lt;/a>，并且可以像其他代码工件一样，与你团队的其他开发者共享。&lt;/p>
&lt;p>最后，我期望在接下来的一年里，能看到云原生 IDE 领域出现令人难以置信的创新，特别是随着 GitHub Codespaces 进入测试版之后，并得到广泛地使用，让开发者可以体验到这个新概念，并爱上它。&lt;/p>
&lt;p>&lt;strong>边缘的 Kubernetes&lt;/strong>&lt;/p>
&lt;p>Kubernetes 是通过在大规模数据中心的使用而诞生的，但 Kubernetes 会像 Linux 一样为新的环境而进化。Linux 所发生的事情是，终端用户最终对内核进行了扩展，以支持从移动、嵌入式等各种新的部署场景。我坚信 Kubernetes 也会经历类似的进化，我们已经见证了 Telcos（和其他初创公司）通过将 VNFs 转化为 &lt;a href="https://github.com/cncf/cnf-wg">云原生网络功能&lt;/a>（CNFs），以及 &lt;a href="https://k3s.io/">k3s&lt;/a>、KubeEdge、k0s、&lt;a href="https://www.lfedge.org/">LFEdge&lt;/a>、Eclipse ioFog 等开源项目，来探索 Kubernetes 作为边缘平台。推动超大规模云服务支持电信公司和边缘的能力，再加上重用云原生软件的能力，以及建立在现有庞大的生态系统基础上的能力，将巩固 Kubernetes 在未来几年内成为边缘计算的主导平台。&lt;/p>
&lt;p>&lt;strong>云原生 + Wasm&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://webassembly.org/">Web Assembly&lt;/a>(Wasm) 是一项新的技术，但我预计它将成为云原生生态系统中不断增长的实用工具和工作负载，特别是随着 &lt;a href="https://wasi.dev/">WASI&lt;/a> 的成熟，以及 Kubernetes 更多地作为边缘编排工具使用，如前所述。一个场景是增强扩展机制，就像 Envoy 对过滤器和 LuaJIT 所做的那样。你可以与一个支持各种编程语言的更小的优化运行时协同，而不是直接与 Lua 打交道。Envoy 项目目前正处于 &lt;a href="https://www.solo.io/blog/the-state-of-webassembly-in-envoy-proxy/">采用 Wasm&lt;/a> 的过程中，我预计任何使用脚本语言作为流行扩展机制的环境都会出现被 Wasm 全盘取代的情况。&lt;/p>
&lt;p>在 Kubernetes 方面，有像微软的 &lt;a href="https://deislabs.io/posts/introducing-krustlet/">Krustlet&lt;/a> 这样的项目，正在探索如何在 Kubernetes 中支持基于 WASI 的运行时。这不应该太令人惊讶，因为 Kubernetes 已经在通过 CRD 和其他机制扩展，以运行不同类型的工作负载，如 VM（&lt;a href="https://kubevirt.io/">KubeVirt&lt;/a>）等等。&lt;/p>
&lt;p>另外，如果你是 Wasm 的新手，我推荐 Linux 基金会的这本新的 &lt;a href="https://www.edx.org/course/introduction-to-webassembly-runtime">入门课程&lt;/a>，它对其进行了介绍，以及优选的文档。&lt;/p>
&lt;p>&lt;strong>FinOps 的崛起（CFM）&lt;/strong>&lt;/p>
&lt;p>新冠病毒的爆发加速了向云原生的转变。至少有一半的公司在危机中加快了他们的云计划。近 60% 的受访者表示，由于 COVID-19 大流行，云计算的使用量将超过之前的计划 (&lt;a href="https://info.flexera.com/SLO-CM-REPORT-State-of-the-Cloud-2020">2020 年云计算现状报告&lt;/a>)。除此之外，云财务管理 (或 FinOps) 对许多公司来说是一个日益严重的问题和 &lt;a href="https://www.wsj.com/articles/cloud-bills-will-get-loftier-1518363001">关注&lt;/a>，老实说，在过去 6 个月里，我与正在进行云原生之旅的公司进行的讨论中，大约有一半的讨论都会提到这个问题。你也可以说，云提供商没有动力让云财务管理变得更容易，因为这将使客户更容易减少支出，然而，在我看来，真正的痛苦是缺乏围绕云财务管理的开源创新和标准化（所有的云都以不同的方式进行成本管理）。在 CNCF 的背景下，试图让 FinOps 变得更容易的开源项目并不多，有 &lt;a href="https://github.com/kubecost/cost-model">KubeCost&lt;/a> 项目，但还相当早期。&lt;/p>
&lt;p>另外，Linux 基金会最近推出了 &lt;a href="https://www.finops.org/blog/linux-foundation">FinOps 基金会&lt;/a> 来帮助这个领域的创新，他们在这个领域有一些 &lt;a href="https://www.edx.org/course/introduction-to-finops">很棒的入门材料&lt;/a>。我期望在未来几年，在 FinOps 领域能看到更多的开源项目和规范。&lt;/p>
&lt;p>&lt;strong>云原生中更多的使用 Rust&lt;/strong>&lt;/p>
&lt;p>Rust 仍然是一门年轻而小众的编程语言，特别是如果你以 Redmonk 的 &lt;a href="https://redmonk.com/sogrady/2020/07/27/language-rankings-6-20/">编程语言排名&lt;/a> 为例。然而，我的感觉是，鉴于已经有一些 &lt;a href="https://www.cncf.io/blog/2020/06/22/rust-at-cncf/">使用 Rust 的 CNCF 项目&lt;/a>，以及它出现在像 microvm &lt;a href="https://firecracker-microvm.github.io/">Firecracker&lt;/a> 这样有趣的基础设施项目中，你将在未来一年中看到 Rust 出现在更多的云原生项目中。虽然 CNCF 目前有超多的项目是用 Golang 编写的，但我预计随着 &lt;a href="https://blog.rust-lang.org/2020/08/18/laying-the-foundation-for-rusts-future.html">Rust 社区的成熟&lt;/a>，几年后基于 Rust 的项目将与基于 Go 的项目平起平坐。&lt;/p>
&lt;p>&lt;strong>GitOps+CD/PD 增长显著&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://www.weave.works/blog/what-is-gitops-really">GitOps&lt;/a> 是云原生技术的一种操作模式，提供了一套统一部署、管理和监控应用程序的最佳实践 (最初是由 Weaveworks 名气很大的 Alexis Richardson&lt;a href="https://www.weave.works/blog/gitops-operations-by-pull-request">创造&lt;/a>)。GitOps 最重要的方面是通过声明的方式描述所需的在 Git 中版本化的系统状态，这基本上可以使一系列复杂的系统变更被正确地应用，然后进行验证（通过 Git 和其他工具启用的漂亮的审计日志）。从实用的角度来看，GitOps 改善了开发者的体验，随着 Argo、GitLab、Flux 等项目的发展，我预计今年 GitOps 工具会更多地冲击企业。如果你看过 GitLab 的 &lt;a href="https://about.gitlab.com/blog/2020/07/14/gitops-next-big-thing-automation/">数据&lt;/a>，GitOps 还是一个大部分公司还没有探索出来的新兴的实践，但随着越来越多的公司大规模采用云原生软件，我认为 GitOps 自然会随之而来。如果你有兴趣了解更多关于这个领域的信息，我推荐你去看看 CNCF 中 &lt;a href="https://codefresh.io/devops/announcing-gitops-working-group/">新&lt;/a> 成立的 &lt;a href="https://github.com/gitops-working-group/gitops-working-group">GitOps 工作组&lt;/a>。&lt;/p>
&lt;p>&lt;strong>服务目录2.0：云原生开发者仪表盘&lt;/strong>&lt;/p>
&lt;p>服务目录的概念并不是一个新事物，对于我们一些在 &lt;a href="https://en.wikipedia.org/wiki/ITIL">ITIL&lt;/a> 时代成长起来的老人们来说，你可能还记得 &lt;a href="https://en.wikipedia.org/wiki/Configuration_management_database">CMDBs&lt;/a> （恐怖）等东西。然而，随着微服务和云原生开发的兴起，对服务进行编目和索引各种实时服务元数据的能力对于推动开发者自动化是至关重要的。这可以包括使用服务目录来了解所有权来处理事件管理、管理 SLO 等。&lt;/p>
&lt;p>在未来，你将看到开发人员仪表盘的趋势，它不仅是一个服务目录，而且提供了通过各种自动化功能在扩展仪表盘的能力。这方面的典范开源例子是 Lyft 的 &lt;a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/">Backstage&lt;/a> 和 &lt;a href="https://eng.lyft.com/announcing-clutch-the-open-source-platform-for-infrastructure-tooling-143d00de9713">Clutch&lt;/a>，然而，任何拥有相当现代的云原生部署的公司往往都有一个平台基础设施团队，他们已经尝试构建类似的东西。随着开源开发者仪表盘与 &lt;a href="https://backstage.io/plugins">大型插件生态系统&lt;/a> 的成熟，你会看到其被各地的平台工程团队加速采用。&lt;/p>
&lt;p>&lt;strong>跨云变得更真实&lt;/strong>&lt;/p>
&lt;p>Kubernetes 和云原生运动已经证明了云原生和多云方式在生产环境中是可行的，数据很清楚地表明“93% 的企业都有使用微软 Azure、亚马逊网络服务和谷歌云等多个提供商的策略” (&lt;a href="https://info.flexera.com/SLO-CM-REPORT-State-of-the-Cloud-2020">2020 年云计算现状报告&lt;/a>)。事实上，Kubernetes 这些年伴随着云市场的发展而更加成熟，将有望解锁程序化的跨云管理服务。这种方法的一个具体例子体现在 Crossplane 项目中，该项目提供了一个开源的跨云控制平面，利用 Kubernetes API 的可扩展性来实现跨云工作负载管理（参见 &lt;a href="https://thenewstack.io/gitlab-deploys-the-crossplane-control-plane-to-offer-multicloud-deployments/">&amp;ldquo;GitLab 部署 Crossplane 控制平面，提供多云部署 &amp;ldquo;&lt;/a>）。&lt;/p>
&lt;p>&lt;strong>主流 eBPF&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Berkeley_Packet_Filter">eBPF&lt;/a> 允许你在不改变内核代码或加载模块的情况下，在 Linux 内核中运行程序，你可以把它看作是一种沙箱扩展机制。eBPF 允许 &lt;a href="https://ebpf.io/projects">新一代软件&lt;/a> 从改进的网络、监控和安全等各种不同的方向扩展 Linux 内核的行为。从历史上看，eBPF 的缺点是它需要一个现代的内核版本来利用它，在很长一段时间里，这对许多公司来说都不是一个现实的选择。然而，事情正在发生变化，甚至新版本的 RHEL 终于支持 eBPF，所以你会看到更多的项目利用其 [优势]（https://sysdig.com/blog/sysdig-and-falco-now-powered-by-ebpf/）。如果你看过 Sysdig 最新的 &lt;a href="https://sysdig.com/blog/sysdig-2021-container-security-usage-report/">容器报告&lt;/a>，你会发现 Falco 的采用率最近在上升，虽然 Sysdig 的报告可能有点偏颇，但它反映在生产使用上。所以请继续关注，并期待未来更多基于 eBPF 的项目。&lt;/p>
&lt;p>&lt;strong>最后，祝大家 2021 年快乐！&lt;/strong>&lt;/p>
&lt;p>我还有一些预测和趋势要分享，尤其是围绕终端用户驱动的开源、服务网格拆解/标准化、Prometheus+OTel、保障软件供应链安全的 KYC 等等，但我会把这些留到更详细的文章中去，9 个预测足以开启新的一年！总之，感谢大家的阅读，希望在 2021 年 5 月的 &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">KubeCon+CloudNativeCon EU&lt;/a> 上与大家见面，报名已开始！&lt;/p></description></item><item><title>【译】应用架构：为什么要随着市场演进</title><link>https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/</link><pubDate>Sun, 17 Jan 2021 21:37:23 +0800</pubDate><guid>https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/</guid><description>
&lt;p>本文译自 &lt;a href="https://www.cncf.io/blog/2021/01/07/application-architecture-why-it-should-evolve-with-the-market/">Application architecture: why it should evolve with the market&lt;/a>
最初由Mia Platform团队发布在&lt;a href="https://blog.mia-platform.eu/en/application-architecture-why-it-should-evolve-with-the-market">Mia Platform的博客&lt;/a>上&lt;/p>
&lt;p>如今，IT 挑战在于通过有效选择应用架构来适应市场和业务需求的发展。为了满足业务和客户的需求，IT 部门应能够对技术和&lt;strong>方法&lt;/strong>采取行动以确保软件具有灵活性，并实现产品和服务的持续创新流程，从而做出更快的反应 。&lt;/p>
&lt;p>当然，过去的单体应用程序和刚性基础设施无法做到这一点。相反，它可以通过&lt;strong>为演化而设计的架构&lt;/strong>来实现，该架构在需要时易于更新和重构。容器化实践的广泛应用（根据 &lt;a href="https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co">Gartner&lt;/a>，到2022年，大公司的就业人数将从目前的 30％ 增长到 75％），这种情况下采用云原生方法重新设计微服务应用是成功的关键。&lt;/p>
&lt;h2 id="如何构建不断发展的应用架构">如何构建不断发展的应用架构&lt;/h2>
&lt;p>海外专家称它们为&lt;strong>可演进的架构&lt;/strong>，以将它们与当今阻碍或无助于改变的传统架构区分开。应用架构基于&lt;a href="https://blog.mia-platform.eu/it/architettura-a-microservizi-i-vantaggi-per-il-business-e-per-lit">微服务架构风格&lt;/a> ，被设计成在现代虚拟化 IT 和云环境中发挥最佳性能。&lt;/p>
&lt;p>基本思想是&lt;strong>创建可以轻松“分解”的应用程序，其组件可以在其他上下文或组合中重用&lt;/strong>，如 Lego 系列。开发一系列微服务，每个微服务都用于执行单个业务功能（根据“单一职责原则”），可以在应用本身的开发和演进中获得相当大的灵活性。实际上，可以根据支持功能的特定生命周期独立开发、更新和测试服务。&lt;/p>
&lt;p>此外，谈到部署，微服务应用的架构具有很大的优势：可以根据需要在内部或云中通过使用可用资源来扩展单个微服务。&lt;/p>
&lt;p>为此，微服务应用获得&lt;strong>基于容器的基础设施&lt;/strong>的支持，该基础设施通过业务编排系统（通常为 &lt;a href="https://blog.mia-platform.eu/en/kubernetes-why-it-is-so-popular-and-who-should-use-it">Kubernetes&lt;/a>）进行管理，该流程可以自动化并促进公司系统之间以及从这些系统到云提供商服务的软件作业的迁移。&lt;/p>
&lt;h2 id="随着业务发展的应用架构的优势">随着业务发展的应用架构的优势&lt;/h2>
&lt;p>基于微服务的应用架构在开发和部署方面具有更大的自治权。如我们所见，微服务可以在其他应用程序中单独实现、“分解”、更新和重用。因此，通过产品或客户需求的演变，它有降低&lt;strong>减少市场所需的每个新产品的设计/开发时间和成本&lt;/strong>。&lt;/p>
&lt;p>此外，通过使用容器化实践，可以简化在本地、云、多云或混合环境的任何环境中应用程序的部署，从而优化成本。&lt;/p>
&lt;p>在微服务架构风格的优点中，我们还发现有可能在各种服务之间的对话及其健康状况上获得更大的&lt;strong>透明度&lt;/strong>：更好的可观察性意味着可以轻松解决复杂应用的问题。实际上，管理员可以&lt;strong>更快地定位和解决性能和安全性问题&lt;/strong>，在运维和代码层面实施措施，从而使响应速度与变更的长期有效性保持一致。&lt;/p>
&lt;p>通过采用微服务以及新的开发和部署方法，可以创建能够随时间发展的应用架构。除了 IT 团队必须掌握的新技能外，还必须对公司的未来有一个清晰的愿景，以确保所提供的服务对业务发展有用。&lt;/p>
&lt;h2 id="创建可演进的应用架构">创建可演进的应用架构&lt;/h2>
&lt;p>我们已经看到了基于微服务的现代应用架构如何保证软件的灵活性，并允许你利用本地和按需使用的所有资源，在可以&lt;strong>方便地&lt;/strong>获得所需性能、降低成本或保护数据的&lt;strong>位置分配作业&lt;/strong>。&lt;/p>
&lt;p>为了使之成为可能，有必要在云和混合环境中创建和管理虚拟化的 IT 环境，并&lt;strong>采用最合适的方法和策略&lt;/strong>。例如，在用于将开发和运维活动链接在一起的DevOps领域中，&lt;strong>持续集成/持续交付&lt;/strong>（CI / CD）策略的方法学支持可帮助提高更新速度和应用软件的质量。&lt;/p>
&lt;p>此外，微服务可促进对&lt;a href="https://blog.mia-platform.eu/it/da-monolite-a-microservizi-come-far-evolvere-unapplicazione-legacy">遗留应用程序的集成&lt;/a>，从而使公司更加敏捷，并利用市场上最&lt;strong>先进的解决方案&lt;/strong>。除了需要新的技术和工作方法外，现在还需要可演进的应用架构来&lt;strong>支持数字化转型所决定的不断变化的需求&lt;/strong>。&lt;/p></description></item><item><title>Tekton 0.9.0 更新</title><link>https://atbug.com/tekton-0.9.0-release/</link><pubDate>Sun, 19 Jan 2020 14:33:17 +0800</pubDate><guid>https://atbug.com/tekton-0.9.0-release/</guid><description>
&lt;p>翻译整理自 &lt;a href="https://cd.foundation/blog/2019/12/12/whats-new-in-tekton-0-9/">What’s New in Tekton 0.9&lt;/a>&lt;/p>
&lt;h2 id="功能及bug修复">功能及Bug修复&lt;/h2>
&lt;h3 id="脚本模式">脚本模式&lt;/h3>
&lt;p>以前如果要在容器里运行个简单的 bash 脚本, 需要这么写:&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">hello&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ubuntu&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">command&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;bash&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">args&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- -&lt;span class="l">c&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>- &lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> set -ex
&lt;/span>&lt;span class="sd"> echo &amp;#34;hello&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在 0.9 之后, 可以更加简单, 不需要再写command 和讨厌的&lt;code>-c&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-yaml" data-lang="yaml">- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">hello&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ubuntu&lt;/span>&lt;span class="w">
&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">|&lt;/span>&lt;span class="sd">
&lt;/span>&lt;span class="sd"> #!/bin/bash
&lt;/span>&lt;span class="sd"> echo &amp;#34;hello&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="性能">性能&lt;/h3>
&lt;p>通过一系列的工作, 每个 PipelineRun 的运行时间缩短了 5-20 秒&lt;/p>
&lt;h2 id="api-变化">API 变化&lt;/h2>
&lt;p>为了 beta 版本的发布, API 做了一些调整:&lt;/p>
&lt;h3 id="镜像摘要输出路径的标准化">镜像摘要输出路径的标准化&lt;/h3>
&lt;p>Tekton 目前提供了一个机制用于存储 task 构建出的镜像的摘要. 这个机制早于&lt;code> PipelineResource&lt;/code>子系统, 并要求 Task 编写者镜像这些摘要写到指定的位置&lt;code>/builder/image-outputs&lt;/code>. 从现在开始, 有了输出资源的标准路径&lt;code>/workspace/output/&amp;lt;resource-name&amp;gt;&lt;/code>&lt;/p>
&lt;h3 id="简化集群资源">简化集群资源&lt;/h3>
&lt;p>集群&lt;code>PipelineResource&lt;/code>使从 Tasks 内部部署和使用 Kubernetes 集群变得简单. 它为用户提供了声明集群的位置以及如何进行身份验证的机制. 然后再执行 Task 过程中, 他们会自动配置&lt;code>.kubeconfig&lt;/code>文件, 以便 Kubernetes 工具可以找到该集群.&lt;/p>
&lt;p>这个版本保函了一些更改, 使集群&lt;code> PipelineResource&lt;/code>更易于使用.&lt;/p>
&lt;p>以前用户必须两次指定名字参数: 一次在资源名称中指定, 一次作为资源参数. 现在第二个参数不需要了.&lt;/p>
&lt;h2 id="基础工作">基础工作&lt;/h2>
&lt;p>每个Tekton版本中包含的大部分工作都是针对某些功能的, 这些功能要等到以后的版本才能公开.况。&lt;/p>
&lt;h3 id="改进的-pipelineresource">改进的 PipelineResource&lt;/h3>
&lt;p>源于&lt;a href="https://github.com/tektoncd/pipeline/issues/1673">Pipeline Resource Redesign&lt;/a>&lt;/p>
&lt;h3 id="api-的版本控制">API 的版本控制&lt;/h3>
&lt;p>源于&lt;a href="https://github.com/tektoncd/pipeline/issues/1526">Create a v1alpha2 apiVersion&lt;/a>&lt;/p>
&lt;h2 id="独立的包">独立的包&lt;/h2>
&lt;p>Tekton项目发展惊人. 除了这里提到的&lt;a href="https://github.com/tektoncd/pipeline">Pipeline&lt;/a>更新, 其他比如&lt;a href="https://github.com/tektoncd/triggers">Triggers&lt;/a>, &lt;a href="https://github.com/tektoncd/cli">CLI&lt;/a>, &lt;a href="https://github.com/tektoncd/dashboard">Dashboard&lt;/a>也有显著的成果.&lt;/p>
&lt;p>Triggers 现在支持开箱即用的 Github 和 Gitlab 校验.
CLI加入了交互式创建&lt;code>PipelineResource&lt;/code>和启动 task 的支持.
Dashboard 接下来也会假如可视化特性.&lt;/p></description></item><item><title>神秘的Eureka自我保护</title><link>https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/</link><pubDate>Sun, 05 Jan 2020 14:14:03 +0800</pubDate><guid>https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/</guid><description>
&lt;p>本文翻译自&lt;a href="https://dzone.com/articles/the-mystery-of-eurekas-self-preservation">The Mystery of Eureka Self-Preservation&lt;/a>&lt;/p>
&lt;p>根据CAP定理, Eureka是一个AP系统, 这就导致了在网络分区期间多个注册表中的信息不一致. 自我保护功能则是为了尽可能降低这种不一致.&lt;/p>
&lt;h2 id="自我保护的定义">自我保护的定义&lt;/h2>
&lt;p>自我保护(self preservation)是Eureka的一项功能, Eureka注册表在未收到实例的心跳情况超过一定阈值时停止驱逐过期的实例.&lt;/p>
&lt;h3 id="从一个健康的系统开始">从一个健康的系统开始&lt;/h3>
&lt;p>把下面看成一个健康的系统&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/addozhang/oss/master/uPic/n5wZMX.jpg" alt="The healthy system — before encountering network partition">&lt;/p>
&lt;p>假设所有的微服务都处于健康的状态并成功注册到Eureka注册表中.&lt;/p>
&lt;p>多个注册表间会同步注册表记录, 所有的微服务实例都处于UP状态. 假设实例2从注册中心发现里实例4, 并调用实例4上的服务.&lt;/p>
&lt;h3 id="突发网络分区">突发网络分区&lt;/h3>
&lt;p>假设出现了网络分区, 系统变成下面的状态.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/addozhang/oss/master/uPic/MznWWr.jpg" alt="During network partition  -  enters self-preservation">&lt;/p>
&lt;p>由于网络分区, 实例4和5丢失了注册中心的连接, 但是实例2仍然可以连接到实例4. Eureka服务端因为没有收到实例4和5的心跳(超过一定时间后), 将他们驱逐. 然后Eureka服务端意识到突然丢失了超过15%(2/5)的心跳, 因此其进入&lt;em>自我保护&lt;/em>模式&lt;/p>
&lt;p>从此时开始, Eureka服务端不在驱逐任何实例, 即使实例真正的下线了.&lt;/p>
&lt;p>&lt;img src="https://raw.githubusercontent.com/addozhang/oss/master/uPic/7c9eHt.jpg" alt="During self-preservation  -  stops expiring instances">&lt;/p>
&lt;p>实例3下线, 但其始终存在注册表中.&lt;/p>
&lt;p>但此时注册表还会接受新实例的注册.&lt;/p>
&lt;h2 id="自我保护的基本原理">自我保护的基本原理&lt;/h2>
&lt;p>自我保护功能在下面两种情况下是合理的:&lt;/p>
&lt;ul>
&lt;li>Eureka服务端因为弱网分区问题没有收到心跳(这并不意味着客户端下线), 但是这种问题可能会很快被修复.&lt;/li>
&lt;li>即使Eureka服务端和客户端的连接断开, 客户端间还可以继续保持连接. (比如上面实例2仍然可以连接到实例4)&lt;/li>
&lt;/ul>
&lt;h3 id="配置-默认">配置 (默认)&lt;/h3>
&lt;p>下面的配置会直接或间接影响到自我保护的行为.&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">eureka.instance.lease-renewal-interval-in-seconds = 30
&lt;/code>&lt;/pre>&lt;/div>&lt;p>客户端发送心跳的频率. 服务端会以此在计算期望收到心跳数, 默认30秒&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">eureka.instance.lease-expiration-duration-in-seconds = 90
&lt;/code>&lt;/pre>&lt;/div>&lt;p>多长时间未收到心跳后, 实例才可以被驱逐, 默认90秒&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">eureka.server.eviction-interval-timer-in-ms = 60 * 1000
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Eureka服务端驱逐操作的执行频率, 默认60秒&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">eureka.server.renewal-percent-threshold = 0.85
&lt;/code>&lt;/pre>&lt;/div>&lt;p>期望心跳数达到该阈值后, 就会进入自我保护模式, 默认0.85&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">eureka.server.renewal-threshold-update-interval-ms = 15 * 60 * 1000
&lt;/code>&lt;/pre>&lt;/div>&lt;p>期望心跳数的计算间隔, 默认15分钟&lt;/p>
&lt;div class="highlight">&lt;pre class="chroma">&lt;code class="language-fallback" data-lang="fallback">eureka.server.enable-self-preservation = true
&lt;/code>&lt;/pre>&lt;/div>&lt;p>是否允许Eureka服务端进入自我保护模式, 默认开启&lt;/p>
&lt;h2 id="理解配置">理解配置&lt;/h2>
&lt;p>Eureka服务端在&amp;quot;上一分钟实际收到的心跳数&amp;quot;小于&amp;quot;每分钟期望的心跳数&amp;quot;时就会进入自我保护模式&lt;/p>
&lt;h3 id="期望的每分钟心跳数">期望的每分钟心跳数&lt;/h3>
&lt;p>假设&lt;code>renewal-percent-threshold&lt;/code>设置为&lt;code>0.85&lt;/code>&lt;/p>
&lt;p>计算方式:&lt;/p>
&lt;ul>
&lt;li>单个实例每分钟期望的心跳数是: 2&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>N个实例的每分钟期望的心跳数: 2 * N&lt;/li>
&lt;li>期望的上一分钟最小心跳数: 2 * N * 0.85&lt;/li>
&lt;/ul>
&lt;h3 id="实际的每分钟心跳数">实际的每分钟心跳数&lt;/h3>
&lt;p>正如上面所述, 两个定时调度器独立地运行计算&lt;em>实际&lt;/em>和&lt;em>期望&lt;/em>的心跳数. 此外还有另一个调度任务&lt;code>EvictionTask&lt;/code>进行结果比较, 并识别当前系统是否在自我保护状态.&lt;/p>
&lt;p>这个调度任务每个&lt;code>eviction-interval-timer-in-ms&lt;/code>时间执行一次, 并决定是否驱逐实例.&lt;/p>
&lt;h2 id="结论">结论&lt;/h2>
&lt;ul>
&lt;li>基于使用的经验, 大多数情况下自我保护模式都是错误的, 它错误地认为一些下线的微服务实例是不良的网络分区&lt;/li>
&lt;li>自我保护永远不会过期, 除非下线的实例重新上线&lt;/li>
&lt;li>如果启用了自我保留, 则无法对实例的心跳间隔进行微调, 因为自我保护在计算期望心跳数是按照30s间隔来计算的&lt;/li>
&lt;li>除非环境中经常出现类似的网络分区故障, 否则建议关闭&lt;/li>
&lt;/ul>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>这个值是固定的, 源于默认的心跳间隔是30s, 故每分钟2次. 见eureka-core-1.7.2的&lt;code>AbstractInstanceRegistryL226&lt;/code> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item></channel></rss>