<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>翻译 on 乱世浮生</title><link>https://atbug.com/categories/%E7%BF%BB%E8%AF%91/</link><description>Recent content in 翻译 on 乱世浮生</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 12 Feb 2023 20:26:32 +0800</lastBuildDate><atom:link href="https://atbug.com/categories/%E7%BF%BB%E8%AF%91/index.xml" rel="self" type="application/rss+xml"/><item><title>【译】分拆：技术栈的自然演进</title><link>https://atbug.com/the-unbundling-of-tech-stack-translation/</link><pubDate>Sun, 12 Feb 2023 20:26:32 +0800</pubDate><guid>https://atbug.com/the-unbundling-of-tech-stack-translation/</guid><description>本文翻译自 Bilgin Lbryam 的 Unbundling: The Natural Evolution of Tech Stacks，翻译难免有所疏漏，有建议请反馈。
“unbundling” 如何翻译，有点纠结，我一度将其翻译成“解耦”，但解耦是 “decoupling” 的翻译。这里我将其翻译成分拆，如果你有更好的翻译请告知。
译者注 作者应该是去年 7 月离开红帽加入了基于 Dapr 的创业公司 Diagrid，曾写过 Multi-Runtime Microservices Architecture 介绍多运行时，多运行时实际上也是分拆的体现。
作者从多种技术和团队触发，介绍在演进中分拆的体现。除了文中提到，我认为可以分拆的是计算资源。将计算资源拆分：虚拟机、多租户、多集群、多云、混合云，以降低成本、避免供应商绑定、提升性能和可靠性。在计算资源拆分过程中，也衍生出了与之配套的技术来解决拆分后带来的不便。
随着 IT 领域的不断发展，新的软件架构、开发技术和工具层出不穷。包括微服务、微前端、零信任、服务网格和数据网格，并将其网格化。尽管这些技术和方法间存在着明显的不同，但它们都被一个共同趋势联系在一起：技术栈和团队的分拆。这种趋势包括将系统分解成更小的、独立的组件，并将工作组织成更小、更专注的团队，以实现更高的灵活性和模块化。
他们都是如何体现分拆的？
微服务 的出现是为了应对单体架构的局限性，随着应用程序的增长单体架构灵活性不足，并且扩展和维护困难。通过将单体应用程序分解为更小的、独立服务，就可以独立开发、部署和扩展应用程序的每一部分，从而缩短开发周期并提高灵活性。 六边形架构 的出现是为了通过将组件解耦并提供与它们交互的标准接口来提高 3 层应用程序的灵活性和可维护性。 领域驱动设计 (DDD) 是一种软件开发方法，可以帮助将整体应用程序分解成更小的、松耦合的、代表不同的业务领域或上下文的模块。 微前端 架构是一种设计方法，是将大型单体前端应用程序分解为较小的、独立的、可以单独开发和部署的模块。 JAMstack 通过将构成用户界面的 HTML、CSS 和 JavaScript 与为应用程序提供支持的服务器端代码和数据库分离，实现应用程序的前端和后端分离。由于系统的一部分的变更无需变更其他部分，从而可以更轻松地维护应用。 服务网格 将分布式应用程序的网络职责（例如路由、负载平衡和服务发现）与应用程序本身分离，使开发人员可以专注于构建业务逻辑和功能，而无需担心底层网络基础设施。 与微服务类似，数据网格 将大型复杂系统分解为更小的独立组件。它将数据治理和管理实践分解为更小的、独立组件，这些组件可以跨不同的数据源和系统一致地实现和执行。 2 个比萨团队 模型是一种在组织中组织团队和工作的策略，它提倡更小的团队能够更快地响应变化、沟通和协作，并可以更快地做出决策并更有效地解决问题。 每种技术趋势的最终结果都是分拆。将技术栈分解为独立的组件，将团队分解为更小、更专注的团队，这些团队可能会扩展到所有其他领域。在前端、数据、网络、安全之后，下一个拆分领域你认为会是什么？ 和我一起 致力于 Dapr 和分拆集成。 也可以在 @bibryam 上关注我，并大声说出关于 分拆 主题的任何想法和评论。</description></item><item><title>译：在 Kubernetes 上实施零信任</title><link>https://atbug.com/implementing-zero-trust-on-kubernetes/</link><pubDate>Thu, 29 Dec 2022 12:42:40 +0800</pubDate><guid>https://atbug.com/implementing-zero-trust-on-kubernetes/</guid><description>本文翻译自 ContainerJournal 的 2022 年度文章之一 《Implementing Zero-Trust on Kubernetes》，作者 Deepak Goel 在文中分享了 Kubernetes 上实施零信任的三个最佳实践。
作为云原生社区的基石，Kubernetes 帮助企业在生产环境中更高效地部署和管理容器。尽管 Kubernetes 最初设计时提供了基本的 安全功能，但广泛且迅速的采用以及日益复杂的威胁形势使 Kubernetes 更容易受到攻击。开发人员和安全专家当下的任务是扩展 Kubernetes 的内置安全性，以有效防范更复杂、更多样和更频繁的网络攻击。
以往“信任但要验证”的方式已被证明对云计算复杂的分布式特性无效，因此 Kubernetes 必须转向“从不信任，始终验证”的零信任模型思想，为业务提供更大的保护。
零信任模型的基本概念 基于“从不信任，始终验证”的原则，可以用三个基本概念来解释零信任模型：
安全网络：始终认为网络是敌对的和有威胁的。网络上的内部和外部数据和信息不断暴露在安全威胁之下。 安全资源：网络上存在的任何信息源，无论位于何处，对其都应持怀疑态度。 身份验证：默认情况下不应信任来自内部或外部网络的用户、设备和流量。零信任应该基于使用正确的身份验证和授权的访问控制。 零信任的三个最佳实践 Kubernetes 提供了灵活性，既是优势但也增加了复杂性，为了在不同的网络环境中运行，为服务和工作负载引入了许多配置选项。Kubernetes 部署考虑以下三个零信任模型的最佳实践，以提升安全保护和工作效率。
优化软件配置和访问权限 团队需要为服务和跨集群操作提供一致的配置。虽然 Kubernetes 提供了多种配置选项，但过多的选项会增加安全问题出现的几率。使用零信任框架，组织可以对服务进行持续验证并将其部署到多个集群，而不会危及任何安全性。通过在授予它们对应用程序和服务的任何安全权限之前仔细检查这些配置，组织可以加强分布式 Kubernetes 集群的安全性。</description></item><item><title>【译】eBPF 和服务网格：还不能丢掉 Sidecar</title><link>https://atbug.com/translate-ebpf-service-mesh/</link><pubDate>Mon, 31 Oct 2022 20:51:17 -0400</pubDate><guid>https://atbug.com/translate-ebpf-service-mesh/</guid><description>服务网格以典型的 sidecar 模型为人熟知，将 sidecar 容器与应用容器部署在同一个 Pod 中。虽说 sidecar 并非很新的模型（操作系统的 systemd、initd、cron 进程；Java 的多线程），但是以这种与业务逻辑分离的方式来提供服务治理等基础能力的设计还是让人一亮。
随着 eBPF 等技术的引入，最近关于服务网格是否需要 sidecar （也就是 sidecarless）的讨论渐增。
笔者认为任何问题都有其起因，长久困扰服务网格的不外乎性能和资源占用。这篇文章翻译自 Buoyant 的 Flynn 文章 eBPF and the Service Mesh: Don&amp;rsquo;t Dismiss the Sidecar Yet。希望这篇文章能帮助大家穿透迷雾看透事物的本质。
本文要点 eBPF 是一个旨在通过（谨慎地）允许在内核中运行一些用户代码来提高性能的工具。 在可预见的未来，服务网格所需的第 7 层处理在 eBPF 中不太可能实现，这意味着网格仍然需要代理。 与 sidecar 代理相比，每个主机代理增加了操作复杂性并降低了安全性。 可以通过更小、更快的 Sidecar 代理来解决有关 Sidecar 代理的典型性能问题。 目前，sidecar 模型对服务网格仍是最有意义的。 关于 eBPF 的故事已经在云原生世界中泛滥了一段时间，有时将其描述为自切片面包以来最伟大的事物，有时则嘲笑它是对现实世界的无用干扰。当然，现实要微妙得多，因此仔细研究一下 eBPF 能做什么和不能做什么似乎是有必要的——技术毕竟只是工具，使用的工具应该适合手头的任务。</description></item><item><title>译：零信任对 Kubernetes 意味着什么</title><link>https://atbug.com/translate-zero-trust-for-k8s/</link><pubDate>Thu, 08 Sep 2022 21:45:44 +0800</pubDate><guid>https://atbug.com/translate-zero-trust-for-k8s/</guid><description>这篇是 Buoyant 的创始人 William Morgan 文章《What Does Zero Trust Mean for Kubernetes?》 的翻译，文章很好的解释了什么是零信任、为什么要实施零信任，以及服务网格如何以最小的代码实现零信任。
零信任是营销炒作，还是新的机会，各位看官你怎么看？
要点
零信任是一种被大量炒作的安全模型，但尽管存在营销噪音，但它对于具有安全意识的组织具有一些具体而直接的价值。 零信任的核心是，将授权从“在边界验证一次”转变为“随时随地验证”。 为此，零信任要求我们重新思考身份的概念，并摆脱基于位置的身份，例如 IP 地址。 Kubernetes 采用者在网络层实现零信任时具有明显的优势，这要归功于基于 Sidecar 的服务网格，它提供无需更改应用程序就可实现的最细粒度的身份验证和授权。 虽然服务网格可以提供帮助，但 Kubernetes 安全性仍然是一个复杂而微妙的话题，需要从多个层次进行了解。 零信任是一种位于现代安全实践前沿的强大的安全模型。这也是一个容易引起轰动和炒作的术语，因此很难消除噪音。那么，究竟什么是零信任，对于 Kubernetes，它究竟意味着什么？在本文中，我们将从工程的角度探讨什么是零信任，并构建一个基本框架来理解它对 Kubernetes 运维和安全团队等的影响。
介绍 如果你正在构建现代云软件，无论是采用 Kubernetes 还是其他软件，可能都听说过“零信任”一词。零信任的安全模式变得如此重要，以至于美国联邦政府已经注意到：白宫最近发布了一份联邦零信任战略的备忘录，要求所有美国联邦机构在年底前满足特定的零信任安全标准。2024财年；国防部创建了零信任参考架构；美国国家安全局发布了一份Kubernetes 强化指南，专门描述了 Kubernetes 中零信任安全的最佳实践。</description></item><item><title>译：Kubernetes 最佳实践</title><link>https://atbug.com/translate-kubernetes-best-practices/</link><pubDate>Tue, 05 Jul 2022 07:53:30 +0800</pubDate><guid>https://atbug.com/translate-kubernetes-best-practices/</guid><description>本文翻译自 Jack Roper 的文章 Kubernetes Best Practice。
译者：文章中作者从应用程序开发、治理和集群配置三个方面给出了一些 Kubernetes 的最佳实践，同时翻译过程中也加入了我过往的一些使用经验。有误的地方，也欢迎大家指正。
在这篇文章中，我将介绍一些使用 Kubernetes (K8s) 时的最佳实践。
作为最流行的容器编排系统，K8s 是现代云工程师掌握的事实标准。众所周知，不管使用还是维护 K8s 复杂的系统，因此很好地掌握它应该做什么和不应该做什么，并知道什么是可能的，将是一个好的开局。
这些建议包含 3 大类中的常见问题，即应用程序开发、治理和集群配置。
最佳实践目录 使用命名空间 使用就绪和存活探针（译者注：还有启动探针） 使用自动缩放 使用资源请求和约束 使用 Deployment、DaemonSet、ReplicaSet 或者 StatefulSet 跨节点部署 Pod 使用多节点 使用基于角色的访问控制（RBAC） 在外部托管Kubernetes集群（使用云服务） 升级Kubernetes版本 监控集群资源和审计策略日志 使用版本控制系统 使用基于Git的工作流程（GitOps） 缩小容器的大小 用标签整理对象（译者注：或理解为资源） 使用网络策略 使用防火墙 使用命名空间 K8s 中的命名空间对于组织对象、在集群中创建逻辑分区以及安全方面的考虑至关重要。 默认情况下，K8s 集群中有 3 个命名空间，default、kube-public 和 kube-system。</description></item><item><title>译：边缘计算的 4 种类型（大致分类）</title><link>https://atbug.com/translate-4-types-edge-computing-by-latency/</link><pubDate>Tue, 31 May 2022 07:12:31 +0800</pubDate><guid>https://atbug.com/translate-4-types-edge-computing-by-latency/</guid><description>本篇文章译自 SUNKU RANGANATH 的 4 Types of Edge Computing - Broadly Categorized。文章通过 往返终端设备和数据中心的延迟 来对边缘计算的类型进行大致的分类，通俗易懂，方便大家对边缘计算有个大概的了解。
边缘计算被认为是分布式计算的下一个前沿。 然而，并不是每个人都知道什么是边缘计算以及存在多种类型的边缘计算。
简单地说，边缘计算通过将计算靠近最终用户，实现了过多设备（通常在 5G 中称为用户设备 UE）和电信数据中心核心之间的连接。
促成边缘计算的关键因素是 UE 和需要高度分布式架构的计算服务器之间的往返通信所带来的延迟。
了解各种类型边缘计算的一种简单方法是基于其靠近终端设备的程度以及与数据中心的往返延迟。 将延迟作为主要因素，边缘计算可以大致分为以下几类：
IoT 边缘 本地边缘 接入边缘 网络边缘 IoT 边缘 这种边缘的延迟预期通常小于 1 毫秒。
这几乎涵盖了任何连接到私有或公共网络（如互联网）的设备。该设备可以是能够处理数据的智能设备，例如移动电话或中继周围环境信息的简单传感器。
包括但不限于零售亭、摄像头、工厂传感器、联网汽车、无人机、联网路灯、智能停车咪表、远程手术设备等。</description></item><item><title>追踪 Kubernetes 中的网络流量</title><link>https://atbug.com/tracing-path-of-kubernetes-network-packets/</link><pubDate>Sat, 22 Jan 2022 10:15:31 +0800</pubDate><guid>https://atbug.com/tracing-path-of-kubernetes-network-packets/</guid><description>译者注：
这篇文章很全面的罗列出了 Kubernetes 中涉及的网络知识，从 Linux 内核的网络内容，到容器、Kubernetes，一一进行了详细的说明。
​文章篇幅有点长，不得不说，网络是很复杂很麻烦的一层，但恰恰这层多年来变化不大。希望翻译的内容对大家能有所帮助，有误的地方，也欢迎大家指正。
本文翻译获得 Learnk8s 的授权，原文 Tracing the path of network traffic in Kubernetes 作者 Kristijan Mitevski。
TL;DR： 本文将代理了解 Kubernetes 集群内外的数据流转。从最初的 Web 请求开始，一直到托管应用程序的容器。
目录 目录 Kubernetes 网络要求 Linux 网络命名空间如何在 pod 中工作 Pause 容器创建 Pod 中的网络命名空间 为 Pod 分配了 IP 地址 检查集群中 pod 到 pod 的流量 Pod 命名空间连接到以太网桥接器 跟踪同一节点上 pod 间的流量 跟踪不同节点上 pod 间的通信 位运算的工作原理 容器网络接口 - CNI 检查 pod 到服务的流量 使用 Netfilter 和 Iptables 拦截和重写流量 检查服务的响应 回顾 Kubernetes 网络要求 在深入了解 Kubernetes 中的数据流转之前，让我们先澄清下 Kubernetes 网络的要求。</description></item><item><title>eBPF 和 Wasm：探索服务网格数据平面的未来</title><link>https://atbug.com/ebpf-wasm-service-mesh/</link><pubDate>Tue, 11 Jan 2022 10:40:56 +0800</pubDate><guid>https://atbug.com/ebpf-wasm-service-mesh/</guid><description>本文翻译自 Vivian Hu 的 《eBPF and Wasm: Exploring the Future of the Service Mesh Data Plane》。
在 2021 年 12 月 2 日，Cilium 项目宣布了 Cilium Service Mesh 项目的测试版。在 2020 年 8 月 Google Cloud 宣布基于 eBPF 的 Google Kubernetes 服务（GKS）的数据平面 V2 的一年后，Cilium Service Mesh 带来了 “无边车服务网格”（sidecarless service mesh）的想法。它扩展了 Cilium eBPF 产品来处理服务网格中的大部分边车代理功能，包括 7 层路由和负载均衡、TLS 终止、访问策略、健康检查、日志和跟踪，以及内置的 Kubernetes Ingress。</description></item><item><title>OpenFaaS - 以自己的方式运行容器化函数</title><link>https://atbug.com/openfaas-case-study-zh/</link><pubDate>Fri, 17 Dec 2021 09:13:59 +0800</pubDate><guid>https://atbug.com/openfaas-case-study-zh/</guid><description>译者注： 本文篇幅较长，有助于了解 FaaS 和 OpenFaaS。作者分别从开发人员和运维人员的视角来了解 OpenFaaS，对了解新的技术是个很好的方式。
本文翻译自 Ivan Velichko 的 OpenFaaS - Run Containerized Functions On Your Own Terms。
长期以来，无服务器（serverless） 对我来说无非就是 AWS Lambda 的代名词。Lambda 提供了一种方便的途径，可以将任意代码附加到平台事件（云实例的状态变更、DynamoDB 记录的更新或新的 SNS 消息）中。但是，我时不时会想到某个逻辑，但其又没大到足以有自己的服务，同时有不适合任何现有服务的范围。因此，我经常将其放入函数中，以便日后使用 CLI 命令或者 HTTP 调用来调用它。
几年前，我来开了 AWS，自那以后，我一直怀念部署无服务器功能的便利性。因此，当我得知 OpenFaaS 项目时惊喜万分。它将在 Kubernetes 集群上部署函数变得简单，甚至仅需要 Containerd 就可以部署到虚拟机上。</description></item><item><title>低复杂度 - 服务网格的下一站</title><link>https://atbug.com/service-mesh-unnecessary-complexity/</link><pubDate>Fri, 15 Oct 2021 07:58:25 +0800</pubDate><guid>https://atbug.com/service-mesh-unnecessary-complexity/</guid><description>译者：
作为一个曾经在制造业企业的基础架构团队任职，为支持公司的“互联网基因”和“数字化转型”落地了云原生基础设施平台，并在尝试采用服务网格未成的我来说，看到这篇文章深有感触。尤其是文中所说的“人少，问题多，需要快速输出价值”，直戳到了痛处。有限的人手有限的时间，我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上，要想很好的运行复杂的系统是非常困难的。
服务网格是一个新的基础设施层，可以承载很多的功能，未来还会有更大的想象空间和光明的未来。
以上的种种原因，也促使我后来选择进入一家提供服务网格的产品企业，也希望服务网格可以被更简单的使用。
“道阻且长，行则将至！”
本文翻译自 Chris Campbell 的 How Unnecessary Complexity Gave the Service Mesh a Bad Name
关键要点 采用服务网格有巨大的价值，但必须以轻量级的方式进行，以避免不必要的复杂性。 在实施服务网时，要采取务实的方法，与技术的核心功能保持一致，并小心干扰（译者：注意力的分散）。 服务网格的一些核心特性包括标准化监控、自动加密和身份识别、智能路由、可靠的重试和网络可扩展性。 服务网格可以提供强大的功能，但这些功能会分散本应对核心优势的关注，并且这些功能也不是实施服务网格的主要原因。 在初始实施服务网格时没有必要去关注那些明显会分散注意力的功能，比如复杂的控制平面、多集群支持、Envoy、WASM 和 A/B 测试。 服务网格是 Kubernetes 世界中的一个热门话题，但许多潜在的采用者已经有些失望了。服务网格的落地受到压倒性的复杂性和看似无穷无尽的供应商解决方案的限制。在我亲自浏览了这个领域之后，我发现采用服务网格具有巨大的价值，但它必须以轻量级的方式完成，以避免不必要的复杂性。尽管普遍存在幻灭感，但服务网格的未来依然光明。
在工作中学习 我进入服务网格的世界始于我在一家老牌的财富 500 强技术公司担任云计算架构师的角色。在开始我们的服务网格之旅时，我身边有许多强大的工程师，但大多数人几乎没有云计算开发经验。我们的组织诞生于云计算之前，完全实现云计算的价值需要时间。我们的传统业务线主要集中在技术栈的硬件元素上，云计算的决策最初是由为运送硬件或为该硬件提供固件和驱动程序而开发的流程驱动的。
随着该组织经历其“数字化转型”，它越来越依赖于提供高质量的软件服务，并逐渐开发出更好的方法。但作为云计算架构师，我仍在为优先考虑硬件的业务流程，以及具有不同技能、流程和信念的工程团队导航。随着时间的推移，我和我的团队在将 .</description></item><item><title>容器神话 Docker 是如何一分为二的</title><link>https://atbug.com/how-docker-broke-in-half/</link><pubDate>Mon, 20 Sep 2021 08:01:30 +0800</pubDate><guid>https://atbug.com/how-docker-broke-in-half/</guid><description>译者点评：
最近听了很多资深的人士关于开源，以及商业化的分析。开源与商业化，听起来就是一对矛盾的所在，似乎大家都在尝试做其二者的平衡。是先有开源，还是先有商业化？俗话说“谈钱不伤感情”，近几年背靠开源的创业公司如雨后春笋般涌现，即使是开发人员也是需要生活的。
容器神话 Docker 曾经无比风光，盛极一时。即使这样一个备受瞩目，大获风投的热捧的独角兽也未能免俗，并付出了不小的代价。
今天这篇文章讲述了 Docker 这家公司从诞生到巅峰到没落，这一路上所做的抉择，并最终做了开源与商业的分离，再一次从开源踏上找寻商业化之路。这些都是值得我们参考和思考的，不管是已经开源或者准备从事开源的。
这篇文章翻译自How Docker broke in half 这家改变游戏规则的容器公司是其昔日的外衣。作为云时代最热门的企业技术业务之一的它到底发生了什么？
Docker 并没有发明容器——将计算机代码打包成紧凑单元的方法，可以轻松地从笔记本电脑移植到服务器——但它确实通过创建一套通用的开源工具和可重用的镜像使其成为主流，这使所有开发人员只需构建一次软件即可在任何地方运行。
Docker 使开发人员能够轻松地将他们的代码“容器化”并将其从一个系统移动到另一个系统，迅速将其确立为行业标准，颠覆了在虚拟机 (VM) 上部署应用程序的主要方式，并使 Docker 成为新一代最快被采用的企业技术之一。
今天，Docker 仍然活着，但它只是它可能成为的公司的一小部分，从未成功地将这种技术创新转化为可持续的商业模式，最终导致其企业业务于 2019 年 11 月出售给 Mirantis。InfoWorld 采访了十几位前任和现任 Docker 员工、开源贡献者、客户和行业分析师，了解 Docker 如何分崩离析的故事。
Docker 诞生了 2008 年由 Solomon Hykes 在巴黎创立的 DotCloud，这个后来成为 Docker 的公司最初被设计为供开发人员轻松构建和发布他们的应用程序的平台即服务 (PaaS)。</description></item><item><title>开源评估框架</title><link>https://atbug.com/a-framework-for-open-source-evaluation/</link><pubDate>Mon, 09 Aug 2021 08:36:21 +0800</pubDate><guid>https://atbug.com/a-framework-for-open-source-evaluation/</guid><description>本文由本人翻译自 Bilgin Ibryam 的 A Framework for Open Source Evaluation，首发在云原生社区博客。
如今，真假开源无处不在。最近开源项目转为闭源的案例越来越多，同时也有不少闭源项目（按照 OSI 定义）像开源一样构建社区的例子。这怎么可能，开源项目不应该始终如此吗？
开源不是非黑即白，它具有开放性、透明、协作性和信任性的多个维度。有些开源是 Github 上的任何项目，有些必须通过 OSI 定义，有些是必须遵守不成文但普遍接受的开源规范。这里通过看一些商业和技术方面，再讨论社区管理习惯，来同大家分享一下我对评估开源项目的看法。
免责声明 这些是我的个人观点，与我的雇主或我所属的软件基金会和项目无关。 这不是法律或专业意见（我不是律师，也不是专门从事 OSS 评估的），而是外行的意见。 更新：我收到了多位开源律师的反馈并更新了文章！ 这篇博文由订阅和分享按钮赞助，点击这些按钮表示支持。 知识产权 关于“开源”项目的第一个问题是关于知识产权的所有权。好消息是，即使不了解这些法律含义，你可以应用一个简单的 Litmus 测试。该项目是否属于你信任的信誉良好的开源基金会？例如，FSF 拥有其托管项目的版权，更多情况下拥有基金会（如 ASF、LF) 通过贡献者许可协议，聚合对其项目的贡献许可权。在任何一种情况下，你都可以相信他们将充当良好的去中心化管家，并且不会在一夜之间改变项目的未来方向。如果一个项目不属于信誉良好的软件基金会，而是由一家公司提供支持，那么问题是你是否信任该公司作为供应链合作伙伴。如果这些问题的答案是肯定的，请转到下一部分。如果答案是否定的，那么你最好调查一下版权所有者是谁，以及他们对你的长期前景和潜在风险是什么。今天的单一供应商开源项目，明天可能会变成闭源。
许可 商标出现在许可之前的原因是软件的权利人（通常是作者）通过许可授予最终用户使用一个或多个软件副本的许可。自由软件许可证是一种说明，它授予源代码或其二进制形式的使用者修改和重新分发该软件的权利。如果没有许可，这些行为将受到版权法的禁止。这里的重点是权利人可以改变主意并更改许可。权利持有人可以决定在多个许可证下分发软件或随时将许可证更改为非开源许可证。该软件也可能在公共领域，在这种情况下，它不受版权法的限制。公共领域并不等同于开源许可证，这是一种不太流行的方法，我们可以在这里忽略。
同样，如果不是律师，这是一个外行对许可的 Litmus 测试：该项目是否根据 OSI 批准的许可清单获得的许可？如果答案是肯定的，那么你可以依靠这些基金会的尽职调查来审查、分类许可并指出任何限制。如果答案是否定的，请让你公司的律师来查看和解释许可上的每个字以及可能的许可兼容性影响。</description></item><item><title>Kubernetes 必备工具：2021</title><link>https://atbug.com/translation-kuberletes-essential-tools-2021/</link><pubDate>Thu, 15 Jul 2021 08:10:22 +0800</pubDate><guid>https://atbug.com/translation-kuberletes-essential-tools-2021/</guid><description>有别于前些天的文章 - 常用的几款工具让 Kubernetes 集群上的工作更容易 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。
文档翻译自 Kubernetes Essential Tools: 2021，篇幅较长，做了部分增删。
介绍 在本文中，我将尝试总结我最喜欢的 Kubernetes 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。
这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。
K3D K3D 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常轻巧且速度非常快。它是使用 Docker 围绕 K3S 的包装器。所以，你只需要 Docker 来运行它并且资源使用率非常低。唯一的问题是它不完全符合 K8s 标准，但这不应该是本地开发的问题。对于测试环境，你可以使用其他解决方案。K3D 比 Kind 快，但 Kind 完全兼容。</description></item><item><title>Kubernetes 的魔力在于企业标准化，而不是应用程序的可移植性</title><link>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</link><pubDate>Sun, 11 Jul 2021 08:05:42 +0800</pubDate><guid>https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/</guid><description>笔者：Kubernetes 抽象了资源和工作负载的操作模式，统一了工具集，实现人机接口的标准化。正如类 Docker 工具提供了应用运行时的操作模式；Spring Framework 提供了 Java 应用的开发模式。
Kubernetes 是关于跨云的技能、工具和实践的可移植性。不是工作负载的可移植性。 &amp;ndash; Bilgin Lbryam @bibryam
本文翻译自 Kubernetes magic is in enterprise standardization, not app portability
Kubernetes 不会神奇地使你的应用程序具有可移植性，但它可能会给你带来更好的东西。
云为企业提供了看似无限的选择。然而，根据 Canonical-sponsored 的一项调查，这并不是大多数企业采用 Kubernetes 等云友好技术的原因。相反，Kubernetes 的主要目标是标准化——外观和操作与其他人一样。
可移植性不是目标 我之前已经讨论过这个问题，参考了 Gartner 关于 Kubernetes 和可移植性的指南。许多人认为 Kubernetes（和容器）可以让他们在云之间轻松移植，但事实证明并不是这样的。正如 Gartner 分析师 Marco Meinardi 所写，当被问及公司是否应该采用“Kubernetes 使他们的应用程序可移植&amp;hellip;&amp;hellip;答案是：不。” 再说一次？</description></item><item><title>Kubernetes 的自动伸缩你用对了吗？</title><link>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</link><pubDate>Wed, 09 Jun 2021 00:34:25 +0800</pubDate><guid>https://atbug.com/auto-scaling-best-practice-in-kubernetes/</guid><description>本文翻译自 learnk8s 的 Architecting Kubernetes clusters — choosing the best autoscaling strategy，有增删部分内容。
TL;DR: 在默认设置下，扩展 Kubernetes 集群中的 pod 和节点可能需要几分钟时间。了解如何调整集群节点的大小、配置水平和集群自动缩放器以及过度配置集群以加快扩展速度。
自动扩展器 在 Kubernetes 中，常说的“自用扩展”有：
HPA：Pod 水平缩放器 VPA：Pod 垂直缩放器 CA：集群自动缩放器 不同类型的自动缩放器，使用的场景不一样。
HPA HPA 定期检查内存和 CPU 等指标，自动调整 Deployment 中的副本数，比如流量变化：
VPA 有些时候无法通过增加 Pod 数来扩容，比如数据库。这时候可以通过 VPA 增加 Pod 的大小，比如调整 Pod 的 CPU 和内存：</description></item><item><title>使用 Quarkus 和 MicroProfile 实现微服务特性</title><link>https://atbug.com/microservicilities-quarkus/</link><pubDate>Wed, 26 May 2021 07:37:04 +0800</pubDate><guid>https://atbug.com/microservicilities-quarkus/</guid><description>Quarkus 的文章之前写过三篇了，讲过了 Quarkus 的小而快。
Hello, Quarkus 应&amp;quot;云&amp;quot;而生的 Java 框架 Quarkus：构建本机可执行文件 谁说 Java 不能用来跑 Serverless？ 一直在酝酿写一篇 Quarkus 生态相关的，因为最近一直在忙 Meetup 的事情而搁浅。正好看到了这篇文章，就拿来翻译一下，补全云原生中的“微服务”这一块。
本文译自《Implementing Microservicilities with Quarkus and MicroProfile》 。
为什么要使用微服务特性？ 在微服务架构中，一个应用程序是由几个相互连接的服务组成的，这些服务一起工作来实现所需的业务功能。
因此，典型的企业微服务架构如下所示：
刚开始，使用微服务架构实现应用程序看起来很容易。
但是，因为有了单体架构没有一些新的挑战，因此做起来并不容器
举几个例子，比如容错、服务发现、扩展性、日志记录和跟踪。
为了解决这些挑战，每个微服务都应实现我们在 Red Hat 所说的“微服务特性”。
该术语是指除业务逻辑以外，服务还必须实现来解决的跨领域关注点清单，如下图所示： 可以用任何语言（Java、Go、JavaScript）或任何框架（Spring Boot、Quarkus）实现业务逻辑，但是围绕业务逻辑，应实现以下关注点：</description></item><item><title>云上细粒度访问管理的参考架构</title><link>https://atbug.com/translation-access-management-reference-architecture/</link><pubDate>Wed, 28 Apr 2021 08:02:11 +0800</pubDate><guid>https://atbug.com/translation-access-management-reference-architecture/</guid><description>本文由 Addo Zhang 翻译自 A Reference Architecture for Fine-Grained Access Management on the Cloud
什么是访问管理？ 访问管理是识别用户或一组用户是否应该能够访问给定资源（例如主机、服务或数据库）的过程。例如，对于开发人员来说是否可以使用 SSH 登录生产应用程序服务器，如果可以，那么可以登录多长时间？如果 SRE 在非支持时间尝试访问数据库，他们这样做？如果数据工程师已转移到其他团队，他们是否应该继续访问 ETL 管道的 S3 存储桶？
现在如何进行访问管理？ 在云上各种基础设施和数据服务激增之前，访问管理是 DevOps 和 Security 团队要解决的相对简单的问题。VPN 和堡垒主机是（现在仍然是）在网络级别封锁所有关键资源的首选机制。用户必须先通过 VPN 服务器进行身份验证，或者登录到堡垒主机，然后才能访问专用网络上的所有资源。
当资源是静态的并且它们的数量相对较小时，此方法效果很好。但是，随着越来越多的资源动态地涌入专用网络的各处，VPN / 堡垒主机解决方案变得站不住脚。
具体来说，在三个方面，VPN 和堡垒主机不足以作为一种有效的访问管理机制。
它们作用于网络层面：用户通过 VPN 进行身份验证并获得对专用网络的访问权限后，他们实际上就可以访问其上运行的所有服务。无法根据用户的身份在基础架构或数据服务的粒度上管理访问。 凭据是攻击的媒介：VPN 和堡垒主机都要求用户记住并存储凭据。过期和轮换凭证作为安全策略非常困难，尤其是在涉及大量用户的情况下，凭证因此成为潜在的攻击媒介。 不能管理第三方 SaaS 工具：SaaS 工具（如 Looker、Tableau 和 Periscope Data）需要直接访问数据端点。因此，使用这些工具访问数据的任何人都无法通过使用了相同的机制和凭据的基础设施进行身份验证。 云上访问管理的新架构 在本文中，我们将定义新的参考架构，为那些正在寻求简化访问管理云资源（从 SSH 主机、数据库、数据仓库到消息管道和云存储终结点）解决方案的云原生企业。</description></item><item><title>分布式系统在 Kubernetes 上的进化</title><link>https://atbug.com/translation-distributed-systems-kubernetes/</link><pubDate>Mon, 29 Mar 2021 23:11:25 +0800</pubDate><guid>https://atbug.com/translation-distributed-systems-kubernetes/</guid><description>本文译自 The Evolution of Distributed Systems on Kubernetes
在 3 月份的 QCon 上，我做了一个关于 Kubernetes 的分布式系统进化的演讲。首先，我想先问一个问题，微服务之后是什么？我相信大家都有各自的答案，我也有我的答案。你会在最后发现我的想法是什么。为了达到这个目的，我建议大家看看分布式系统的需求是什么？以及这些需求在过去是如何发展的，从单体应用开始到 Kubernetes，再到最近的 Dapr、Istio、Knative 等项目，它们是如何改变我们做分布式系统的方式。我们将尝试对未来做一些预测。
现代分布式应用 为了给这个话题提供更多的背景信息，我认为的分布式系统是由数百个组件组成的系统。这些组件可以是有状态的、无状态的或者无服务器的。此外，这些组件可以用不同的语言创建，运行在混合环境上，并开发开源技术、开放标准和互操作性。我相信你可以使用闭源软件来构建这样的系统，也可以在 AWS 和其他地方构建。具体到这次演讲，我将关注 Kubernetes 生态系统，以及你如何在 Kubernetes 平台上构建这样一个系统。
我们从分布式系统的需求讲起。我认为是我们要创建一个应用或者服务，并写一些业务逻辑。那从运行时的平台到构建分布式系统，我们还需要什么呢？在底层，最开始是我们要一些生命周期的能力。当你用任一语言开发你的应用时，我们希望有能力把这个应用可靠地打包和部署、回滚、健康检查。并且能够把应用部署到不同的节点上，并实现资源隔离、扩展、配置管理，以及所有这些。这些都是你创建分布式应用所需要的第一点。
第二点是围绕网络。我们有了应用之后，我们希望它能够可靠地连接到其他服务，无论该服务是在集群内部还是在外部。我们希望其具有服务发现、负载均衡的能力。为了不同的发布策略或是其他的一些原因的我们希望有流量转移的能力。然后我们还希望其具有与其他系统进行弹性通信的能力，无论是通过重试、超时还是断路器。要有适当的安全保障，并且要有足够的监控、追踪、可观察性等等。
我们有了网络之后，接下来就是我们希望有能力与不同的 API 和端点交互，即资源绑定&amp;ndash;与其他协议和不同的数据格式交互。甚至能够从一种数据格式转换成另一种数据格式。我还会在这里加入诸如滤光的功能，也就是说，当我们订阅一个主题时，我们也许只对某些事件感兴趣。
你认为最后一类是什么？是状态。当我在说状态和有状态的抽象时，我并不是在谈论实际的状态管理，比如数据库或者文件系统的功能。我要说的更多是有关幕后依赖状态的开发人员抽象。可能，你需要具有工作流管理的能力。也许你想管理运行时间长的进程或者做临时调度或者某些定时任务来定期运行服务。也许你还想进行分布式缓存，具有幂等性或者支持回滚。所有这些都是开发人员级的原语，但在幕后，它们依赖于具有某种状态。你想随意使用这些抽象俩创建完善的分布式系统。
我们将使用这个分布式系统原语的框架来评估它们在 Kubernetes 和其他项目上的变化情况。
单体架构 &amp;ndash; 传统中间件功能 假设我们从单体架构以及如何获得这些能力开始。在那种情况下，首先是当我说单体的时候，在分布式应用的情况下我想到的是 ESB。ESB 是相当强大的，当我们检查我们的需求列表时，我们会说 ESB 对所有有状态的抽象有很好的支持。</description></item><item><title>【译】2021 年及未来的云原生预测</title><link>https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/</link><pubDate>Tue, 09 Feb 2021 06:43:54 +0800</pubDate><guid>https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/</guid><description>本文译自 Cloud Native Predictions for 2021 and Beyond
原文发布在 Chris Aniszczyk 的个人博客
我希望每个人都有一个美好的假期，因为 2021 年 1 月的前几周一直非常疯狂，从叛乱到新的 COVID 菌株。在云原生国度，CNCF 最近发布了关于我们去年完成的所有工作的年度报告。我建议大家找个机会去看一下这份报告，在疫情大流行的这一年，我们收获颇丰。https://twitter.com/CloudNativeFdn/status/1343914259177222145
作为我工作的一部分，我对云原生趋势有一个独特的观点，送给所有与我合作的会员公司和开发人员，所以我想我会分享我对 2021 年及以后云原生发展的想法。
云原生的 IDE
作为一个在 Eclipse 基金会内部从事开发者工具工作的人，我对最近的技术状态进展感到无比兴奋。未来，开发生命周期（代码、构建、调试）将主要发生在云端，而不是你本地的 Emacs 或 VSCode。你将每一个拉动请求最终得到一个完整的开发环境设置，预先配置并连接到他们自己的部署，以协助你的开发和调试需求。今天这种技术的一个具体例子是通过 GitHub Codespaces 和 GitPod 实现的。虽然 GitHub Codespaces 还处于测试阶段，但今天你可以通过 GitPod 来体验，以 Prometheus 为例。一分钟左右，你就拥有了一个有编辑器和预览环境的完全实时的开发环境。最疯狂的是，这个开发环境（工作空间）是 用代码描述，并且可以像其他代码工件一样，与你团队的其他开发者共享。</description></item><item><title>【译】应用架构：为什么要随着市场演进</title><link>https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/</link><pubDate>Sun, 17 Jan 2021 21:37:23 +0800</pubDate><guid>https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/</guid><description>本文译自 Application architecture: why it should evolve with the market 最初由Mia Platform团队发布在Mia Platform的博客上
如今，IT 挑战在于通过有效选择应用架构来适应市场和业务需求的发展。为了满足业务和客户的需求，IT 部门应能够对技术和方法采取行动以确保软件具有灵活性，并实现产品和服务的持续创新流程，从而做出更快的反应 。
当然，过去的单体应用程序和刚性基础设施无法做到这一点。相反，它可以通过为演化而设计的架构来实现，该架构在需要时易于更新和重构。容器化实践的广泛应用（根据 Gartner，到2022年，大公司的就业人数将从目前的 30％ 增长到 75％），这种情况下采用云原生方法重新设计微服务应用是成功的关键。
如何构建不断发展的应用架构 海外专家称它们为可演进的架构，以将它们与当今阻碍或无助于改变的传统架构区分开。应用架构基于微服务架构风格 ，被设计成在现代虚拟化 IT 和云环境中发挥最佳性能。
基本思想是创建可以轻松“分解”的应用程序，其组件可以在其他上下文或组合中重用，如 Lego 系列。开发一系列微服务，每个微服务都用于执行单个业务功能（根据“单一职责原则”），可以在应用本身的开发和演进中获得相当大的灵活性。实际上，可以根据支持功能的特定生命周期独立开发、更新和测试服务。
此外，谈到部署，微服务应用的架构具有很大的优势：可以根据需要在内部或云中通过使用可用资源来扩展单个微服务。
为此，微服务应用获得基于容器的基础设施的支持，该基础设施通过业务编排系统（通常为 Kubernetes）进行管理，该流程可以自动化并促进公司系统之间以及从这些系统到云提供商服务的软件作业的迁移。
随着业务发展的应用架构的优势 基于微服务的应用架构在开发和部署方面具有更大的自治权。如我们所见，微服务可以在其他应用程序中单独实现、“分解”、更新和重用。因此，通过产品或客户需求的演变，它有降低减少市场所需的每个新产品的设计/开发时间和成本。
此外，通过使用容器化实践，可以简化在本地、云、多云或混合环境的任何环境中应用程序的部署，从而优化成本。
在微服务架构风格的优点中，我们还发现有可能在各种服务之间的对话及其健康状况上获得更大的透明度：更好的可观察性意味着可以轻松解决复杂应用的问题。实际上，管理员可以更快地定位和解决性能和安全性问题，在运维和代码层面实施措施，从而使响应速度与变更的长期有效性保持一致。
通过采用微服务以及新的开发和部署方法，可以创建能够随时间发展的应用架构。除了 IT 团队必须掌握的新技能外，还必须对公司的未来有一个清晰的愿景，以确保所提供的服务对业务发展有用。</description></item><item><title>神秘的 Eureka 自我保护</title><link>https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/</link><pubDate>Sun, 05 Jan 2020 14:14:03 +0800</pubDate><guid>https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/</guid><description>本文翻译自The Mystery of Eureka Self-Preservation
根据CAP定理, Eureka是一个AP系统, 这就导致了在网络分区期间多个注册表中的信息不一致. 自我保护功能则是为了尽可能降低这种不一致.
自我保护的定义 自我保护(self preservation)是Eureka的一项功能, Eureka注册表在未收到实例的心跳情况超过一定阈值时停止驱逐过期的实例.
从一个健康的系统开始 把下面看成一个健康的系统
假设所有的微服务都处于健康的状态并成功注册到Eureka注册表中.
多个注册表间会同步注册表记录, 所有的微服务实例都处于UP状态. 假设实例2从注册中心发现里实例4, 并调用实例4上的服务.
突发网络分区 假设出现了网络分区, 系统变成下面的状态.
由于网络分区, 实例4和5丢失了注册中心的连接, 但是实例2仍然可以连接到实例4. Eureka服务端因为没有收到实例4和5的心跳(超过一定时间后), 将他们驱逐. 然后Eureka服务端意识到突然丢失了超过15%(2/5)的心跳, 因此其进入自我保护模式
从此时开始, Eureka服务端不在驱逐任何实例, 即使实例真正的下线了.
实例3下线, 但其始终存在注册表中.
但此时注册表还会接受新实例的注册.
自我保护的基本原理 自我保护功能在下面两种情况下是合理的:
Eureka服务端因为弱网分区问题没有收到心跳(这并不意味着客户端下线), 但是这种问题可能会很快被修复.</description></item></channel></rss>