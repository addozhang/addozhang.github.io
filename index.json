[{"categories":["笔记"],"contents":"在 上一篇 文章中分享了分布式运行时 Dapr 的使用，在示例中将状态存储能力分离到 Dapr 运行时中，应用通过 Dapr API 来使用该能力。这篇文章将介绍如何通过 Ingress Controller（入口控制器）来访问 Dapr 应用。\n方案 如何公开 Dapr 应用的访问，方案有两种：\n 像传统用法一样，配置应用的 Service 作为后端，由入口控制器直接将流量转发到应用容器，简单说就是支持自动配置的 L7 负载均衡器。 不直接访问应用容器，而是通过 Daprd 运行时来访问。这时，我们就需要将入口控制器也声明为 Dapr 应用，为其注入 Daprd 运行时容器。此时创建入口规则指向的后端，则是 Ingress 的 Dapr Service。  两种方案各有优劣，前者架构简单；后者虽然引入 Daprd 容器，架构复杂，消耗了更多的资源，但也因此可以使用 Dapr 的服务治理能力，如超时、重试、访问控制等等。\n接下来我们将通过示例来分别验证两种方案。\n演示 前置条件  helm dapr cli  安装集群 export INSTALL_K3S_VERSION=v1.23.8+k3s2 curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 安装 Dapr dapr init --kubernetes --wait 安装入口控制器 这里使用 Flomesh 的入口控制器。通过 valus.yaml 为入口控制器添加 dapr 相关的注解。这里需要注意由于默认情况下 Daprd 运行时监听的端口是绑定在回环地址上的，而 Ingress 向后端转发请求时使用的是 Pod IP，因此需要注解 dapr.io/sidecar-listen-addresses: \u0026quot;[::],0.0.0.0\u0026quot; 让 Daprd 运行时可以接收来自 Pod IP 的请求。\n# values.yaml fsm: ingress: podAnnotations: dapr.io/sidecar-listen-addresses: \u0026#34;[::],0.0.0.0\u0026#34; dapr.io/enabled: \u0026#34;true\u0026#34; dapr.io/app-id: \u0026#34;ingress\u0026#34; dapr.io/app-port: \u0026#34;8000\u0026#34; dapr.io/enable-api-logging: \u0026#34;true\u0026#34; dapr.io/config: \u0026#34;ingressconfig\u0026#34; helm repo add fsm https://charts.flomesh.io helm repo update helm install \\  --namespace flomesh \\  --create-namespace \\  --version=0.2.1-alpha.3 \\  -f values.yaml \\  fsm fsm/fsm 部署示例应用 示例应用我们使用 kennethreitz/httpbin，为其添加 dapr 相关的注解。\nkubectl create ns httpbin kubectl apply -n httpbin -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: labels: app: httpbin name: httpbin spec: replicas: 1 selector: matchLabels: app: httpbin strategy: {} template: metadata: annotations: dapr.io/enabled: \u0026#34;true\u0026#34; dapr.io/app-id: \u0026#34;httpbin\u0026#34; dapr.io/app-port: \u0026#34;80\u0026#34; dapr.io/enable-api-logging: \u0026#34;true\u0026#34; dapr.io/config: \u0026#34;httpbinconfig\u0026#34; labels: app: httpbin spec: containers: - image: kennethreitz/httpbin name: httpbin resources: {} --- apiVersion: v1 kind: Service metadata: labels: app: httpbin name: httpbin namespace: httpbin spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: app: httpbin EOF 方案 1 参考 Flomesh Ingress 的 文档，创建入口规则使用 Service httpbin 作为后端。\nkubectl apply -n httpbin -f - \u0026lt;\u0026lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: httpbin annotations: pipy.ingress.kubernetes.io/rewrite-target-from: ^/httpbin/? pipy.ingress.kubernetes.io/rewrite-target-to: / spec: ingressClassName: pipy rules: - http: paths: - backend: service: name: httpbin port: number: 80 path: /httpbin pathType: Prefix EOF 使用主机 IP 地址和入口控制器的 80 端口来访问 /httpbin/get，由于上面配置了路径重写，最终请求 /get 将会被发送到目标 Pod。\ncurl HOST_IP:80/httpbin/get { \u0026#34;args\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;Content-Length\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;10.0.0.13:80\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.86.0\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;10.42.0.18\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://10.0.0.13:80/get\u0026#34; } 方案 2 方案 2 也同样需要配置入口规则，只是这次后端服务配置的入口控制器的 Dapr Service，通过 kubectl get svc -n flomesh 就可以找到名字带有 -dapr 后缀的 Service。\nkubectl apply -n flomesh -f - \u0026lt;\u0026lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: dapr annotations: pipy.ingress.kubernetes.io/rewrite-target-from: ^/dapr/? pipy.ingress.kubernetes.io/rewrite-target-to: / spec: ingressClassName: pipy rules: - http: paths: - backend: service: name: ingress-dapr port: number: 80 path: /dapr pathType: Prefix EOF 仍然是访问 httpbin 的路径 /get，但是访问方式要遵循 Dapr 服务调用的 API：通过请求头指定 dapr-app-id 为 httpbin.httpbin。由于入口控制器和目标应用不在同一命名空间下，在应用 id 需要带上其命名空间。\n同样是成功返回，但是可以看到最终应用收到请求头部会多了一些信息，这些信息都是来自 Daprd 运行时，比如 Dapr-Caller-App-Id、\u0026quot;Dapr-Callee-App-Id 标识表示请求的发起方和接收方；Forwarded 标识了发起请求的主机名。如果开启了 tracing，还会有链路相关的信息（本示例中并未开启）。\ncurl HOST_IP:80/dapr/get -H \u0026#39;dapr-app-id:httpbin.httpbin\u0026#39; { \u0026#34;args\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;Dapr-App-Id\u0026#34;: \u0026#34;httpbin.httpbin\u0026#34;, \u0026#34;Dapr-Callee-App-Id\u0026#34;: \u0026#34;httpbin\u0026#34;, \u0026#34;Dapr-Caller-App-Id\u0026#34;: \u0026#34;ingress\u0026#34;, \u0026#34;Forwarded\u0026#34;: \u0026#34;for=10.42.0.18;by=10.42.0.18;host=fsm-ingress-pipy-864d8d9c76-4kb7r\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;Proto\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;Protomajor\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Protominor\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Referer\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Traceparent\u0026#34;: \u0026#34;00-00000000000000000000000000000000-0000000000000000-00\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.86.0\u0026#34;, \u0026#34;X-Forwarded-Host\u0026#34;: \u0026#34;fsm-ingress-pipy-864d8d9c76-4kb7r\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;10.42.0.18\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://fsm-ingress-pipy-864d8d9c76-4kb7r/get\u0026#34; } 访问控制 文章开头提到使用 方案 2 虽然带来了延迟增加了复杂度，但是可以使用 Dapr 的服务治理能力。这里以 Dapr 的访问控制功能 为例。\n不知道大家注意到没，在部署入口控制器和示例应用时，有个注解 dapr.io/config: xxx。这个注解是为应用的 Daprd 运行时指定配置，运行时启动时会从名为 xxx 的 Configuration 资源读取配置。\n执行下面的命令为 httpbin 应用设置访问控制规则：默认拒绝所有请求，只允许 flomesh 命名空间下的 id 为 ingress 的应用通过 GET 方式访问路径 /get。更多访问控制配置，可以参考 Dapr 访问控制官方文档。\nkubectl apply -n httpbin -f - \u0026lt;\u0026lt;EOF apiVersion: dapr.io/v1alpha1 kind: Configuration metadata: name: httpbinconfig spec: accessControl: defaultAction: deny trustDomain: \u0026#34;public\u0026#34; policies: - appId: ingress defaultAction: deny trustDomain: \u0026#39;public\u0026#39; namespace: \u0026#34;flomesh\u0026#34; operations: - name: /get httpVerb: [\u0026#39;GET\u0026#39;] action: allow EOF 应用配置之后，需要重启应用。\nkubectl rollout restart deploy httpbin -n httpbin 等到重启完成，再访问 /get 路径，正常响应。\ncurl HOST_IP:80/dapr/get -H \u0026#39;dapr-app-id:httpbin.httpbin\u0026#39; { \u0026#34;args\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;Dapr-App-Id\u0026#34;: \u0026#34;httpbin.httpbin\u0026#34;, \u0026#34;Dapr-Callee-App-Id\u0026#34;: \u0026#34;httpbin\u0026#34;, \u0026#34;Dapr-Caller-App-Id\u0026#34;: \u0026#34;ingress\u0026#34;, \u0026#34;Forwarded\u0026#34;: \u0026#34;for=10.42.0.40;by=10.42.0.40;host=fsm-ingress-pipy-864d8d9c76-jctcx\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;127.0.0.1:80\u0026#34;, \u0026#34;Proto\u0026#34;: \u0026#34;HTTP/1.1\u0026#34;, \u0026#34;Protomajor\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Protominor\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Referer\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Traceparent\u0026#34;: \u0026#34;00-00000000000000000000000000000000-0000000000000000-00\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.86.0\u0026#34;, \u0026#34;X-Forwarded-Host\u0026#34;: \u0026#34;fsm-ingress-pipy-864d8d9c76-jctcx\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;10.42.0.40\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://fsm-ingress-pipy-864d8d9c76-jctcx/get\u0026#34; } 但是，如果是访问路径 /headers，会收到下面的响应：被禁止访问 /headers。\ncurl HOST_IP:80/dapr/headers -H \u0026#39;dapr-app-id:httpbin.httpbin\u0026#39; { \u0026#34;errorCode\u0026#34;: \u0026#34;ERR_DIRECT_INVOKE\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;fail to invoke, id: httpbin.httpbin, err: rpc error: code = PermissionDenied desc = access control policy has denied access to appid: ingress operation: headers verb: GET\u0026#34; } 总结 文中使用的两种入口方案各有优缺点，方案 1 架构简单，也是我们常用的方案；方案 2 中相当于将入口控制器声明为 Dapr 应用，实际上暴露的是 Dapr 的 API，在实现上更像是一个全局的应用运行时。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-gutsbyjan-n-15272406-2.jpg","permalink":"https://atbug.com/access-dapr-application-with-ingress-controller/","tags":["Dapr","Kubernetes","Ingress"],"title":"使用 Ingress 访问 Dapr 应用"},{"categories":["翻译"],"contents":"本文翻译自 Bilgin Lbryam 的 Unbundling: The Natural Evolution of Tech Stacks，翻译难免有所疏漏，有建议请反馈。\n“unbundling” 如何翻译，有点纠结，我一度将其翻译成“解耦”，但解耦是 “decoupling” 的翻译。这里我将其翻译成分拆，如果你有更好的翻译请告知。\n译者注 作者应该是去年 7 月离开红帽加入了基于 Dapr 的创业公司 Diagrid，曾写过 Multi-Runtime Microservices Architecture 介绍多运行时，多运行时实际上也是分拆的体现。\n作者从多种技术和团队触发，介绍在演进中分拆的体现。除了文中提到，我认为可以分拆的是计算资源。将计算资源拆分：虚拟机、多租户、多集群、多云、混合云，以降低成本、避免供应商绑定、提升性能和可靠性。在计算资源拆分过程中，也衍生出了与之配套的技术来解决拆分后带来的不便。\n 随着 IT 领域的不断发展，新的软件架构、开发技术和工具层出不穷。包括微服务、微前端、零信任、服务网格和数据网格，并将其网格化。尽管这些技术和方法间存在着明显的不同，但它们都被一个共同趋势联系在一起：技术栈和团队的分拆。这种趋势包括将系统分解成更小的、独立的组件，并将工作组织成更小、更专注的团队，以实现更高的灵活性和模块化。\n他们都是如何体现分拆的？\n 微服务 的出现是为了应对单体架构的局限性，随着应用程序的增长单体架构灵活性不足，并且扩展和维护困难。通过将单体应用程序分解为更小的、独立服务，就可以独立开发、部署和扩展应用程序的每一部分，从而缩短开发周期并提高灵活性。 六边形架构 的出现是为了通过将组件解耦并提供与它们交互的标准接口来提高 3 层应用程序的灵活性和可维护性。 领域驱动设计 (DDD) 是一种软件开发方法，可以帮助将整体应用程序分解成更小的、松耦合的、代表不同的业务领域或上下文的模块。 微前端 架构是一种设计方法，是将大型单体前端应用程序分解为较小的、独立的、可以单独开发和部署的模块。 JAMstack 通过将构成用户界面的 HTML、CSS 和 JavaScript 与为应用程序提供支持的服务器端代码和数据库分离，实现应用程序的前端和后端分离。由于系统的一部分的变更无需变更其他部分，从而可以更轻松地维护应用。 服务网格 将分布式应用程序的网络职责（例如路由、负载平衡和服务发现）与应用程序本身分离，使开发人员可以专注于构建业务逻辑和功能，而无需担心底层网络基础设施。 与微服务类似，数据网格 将大型复杂系统分解为更小的独立组件。它将数据治理和管理实践分解为更小的、独立组件，这些组件可以跨不同的数据源和系统一致地实现和执行。 2 个比萨团队 模型是一种在组织中组织团队和工作的策略，它提倡更小的团队能够更快地响应变化、沟通和协作，并可以更快地做出决策并更有效地解决问题。  每种技术趋势的最终结果都是分拆。将技术栈分解为独立的组件，将团队分解为更小、更专注的团队，这些团队可能会扩展到所有其他领域。在前端、数据、网络、安全之后，下一个拆分领域你认为会是什么？ 和我一起 致力于 Dapr 和分拆集成。 也可以在 @bibryam 上关注我，并大声说出关于 分拆 主题的任何想法和评论。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/evolution.jpeg","permalink":"https://atbug.com/the-unbundling-of-tech-stack-translation/","tags":["Service Mesh","云原生","微服务","架构"],"title":"【译】分拆：技术栈的自然演进"},{"categories":["笔记"],"contents":"Dapr 分布式应用运行时 Distributed Application Runtime 的首字母缩写。有关多运行时，可以看下 Bilgin Ibryam 的 Multi-Runtime Microservices Architecture，不想看英文的可以看下我之前的翻译。\nDapr 是一个分布式系统工具包，通过提供 API 实现应用程序与外围组件的解耦合，让开发人员更加聚焦于业务逻辑的研发。解耦也是与传统 SDK 的很大区别，能力不再是通过应用程序中加入库的方式提供，而是通过应用附近的边车（sidecar）运行时提供（sidecar 不是广为人知的服务网格 sidecar - pod 中的容器，而是广泛使用在系统软件设计中的一种模式，比如操作系统的 initd、日志采集组件，甚至是 Java 中的多线程。）。因此这里说的 Dapr sidecar 可能是个独立的进程，也可能是 pod 中的一个容器。\n在 Dapr 中我们可以看到很多常见 SDK 的能力：\n 如 SpringCloud、Netflix OSS 的 服务调用，以及超时、熔断、重试等 弹性策略 如 Spring Data KeyValue 一样提供 状态存储 的抽象，简化各种持久存储的访问 如 Kafka、NATS、MQTT 等消息代理，提供 发布/订阅 抽象供服务通过消息进行通信 如 Kafka、MQTT、RabbitMQ 提供以事件触发应用的抽象：绑定 如 Redis 一样的 分布式锁 如 Consul、Kubernetes 等的 名称解析 \u0026hellip;  以上能力都是通过 HTTP 和 gRPC API 暴露给应用，这些 API 在 Dapr 中被叫做 构建块（building blocks），并且也 仅提供抽象，也就是说你可以随意替换底层实现（Dapr 中也叫做 组件）而无需修改任何应用代码。\n比如你的应用需要在存储中保存状态，在开发时可以使用 内存 作为存储组件，其他环境中可以使用 Mysql、Redis 等持久化组件。\n接下来，就借助官方的入门指南体验 Dapr 的。Dapr 提供了 多种入门指南，这里我选了其中的 hello-kubernetes，但实际操作可能与官方有些许差异，也正式这些差异能让（坑）我对 Dapr 有更多的了解。\n环境 安装 Dapr CLI Dapr CLI 是操作 Dapr 的工具，对可以用来安装、管理 Dapr 实例，以及进行 debug。参考官方的 安装文档，我使用的是 macOS 选择 homebrew 来安装。\nbrew install dapr-cli 目前最新的版本是 1.9.1。\ndapr version CLI version: 1.9.1 Runtime version: n/a 创建 Kubernetes 集群 使用 k3s v1.23.8+k3s2 作为实验环境集群。\nexport INSTALL_K3S_VERSION=v1.23.8+k3s2 curl -sfL https://get.k3s.io | sh -s - --disable traefik --disable servicelb --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 安装 Dapr 执行下面的命令将 Dapr 安装到集群中。\ndapr init --kubernetes --wait 检查组件是否正常运行。在 Kubernetes 环境下，我们的很多命令都要使用 --kubernetes 或者 -k 参数。\ndapr status -k NAME NAMESPACE HEALTHY STATUS REPLICAS VERSION AGE CREATED dapr-dashboard dapr-system True Running 1 0.11.0 47s 2023-02-11 08:30.25 dapr-sentry dapr-system True Running 1 1.9.6 47s 2023-02-11 08:30.25 dapr-sidecar-injector dapr-system True Running 1 1.9.6 47s 2023-02-11 08:30.25 dapr-operator dapr-system True Running 1 1.9.6 47s 2023-02-11 08:30.25 dapr-placement-server dapr-system True Running 1 1.9.6 47s 2023-02-11 08:30.25 示例应用 环境部署好之后，我们来看下要用的示例应用。\ngit clone https://github.com/dapr/quickstarts cd quickstarts/tutorials/hello-kubernetes 示例中包含了 2 个应用 pythonapp 和 nodeapp，以及 Redis。\n nodeapp 提供 HTTP 端点来创建和查询订单，订单信息保存在 Redis 中 pythonapp 会持续访问 nodeapp 的 HTTP 端点来创建订单  用到了 Dapr 的两个功能：服务调用和状态存储。\n创建应用命名空间 应用将部署在 dpar-test 命名空间下。\nkubectl create namespace dapr-test 状态存储 状态存储使用 Redis，先部署 Redis 到命名空间 store 下。简单起见，只使用单 master 节点，并设置密码 changeme。\nhelm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install redis bitnami/redis --namespace store --create-namespace \\  --set replica.replicaCount=0 \\  --set auth.password=changeme 创建组件 由于 Redis 设置了密码，需要为 Dapr 提供访问 Redis 的密码，通过 Secret 来传递。Secret 保存在 dapr-test 下。\nkubectl create secret generic redis -n dapr-test --from-literal=redis-password=changeme 根据 Redis store 规范 在 dapr-test 下创建组件 statetore：\n 组件类型 type 为 state.redis 版本 version=v1 访问地址 redisHost=redis-master.store:6379 Redis 的访问密码从秘钥 redis 的键 redis-password 获取 auth.secretStore 指定秘钥存储的类型是 Kubernetes  kubectl apply -n dapr-test -f - \u0026lt;\u0026lt;EOF apiVersion: dapr.io/v1alpha1 kind: Component metadata: name: statestore spec: type: state.redis version: v1 metadata: - name: redisHost value: redis-master.store:6379 - name: redisPassword secretKeyRef: name: redis key: redis-password auth: secretStore: kubernetes EOF 访问状态存储 通过 Dapr API 访问状态存储，请求格式：POST http://localhost:\u0026lt;daprPort\u0026gt;/v1.0/state/\u0026lt;storename\u0026gt;。\n下面截取了 nodeapp 中的部分代码，stateStoreName 就是上面创建的 statestore。应用和组件位于同一命名空间下，直接只用 statestore；否则，就要代码组件所在的命名空间 storeName.storeNamespace（由于代码中硬编码了组件名 statestore，所以在同命名空间下创建组件）。\nconst stateStoreName = `statestore`; const stateUrl = `http://localhost:${daprPort}/v1.0/state/${stateStoreName}`; const state = [{ key: \u0026#34;order\u0026#34;, value: data }]; const response = await fetch(stateUrl, { method: \u0026#34;POST\u0026#34;, body: JSON.stringify(state), headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; } }); 服务调用 调用方 pythonapp 的代码。\n 通过 sidecar daprd 的地址 localhost 和端口 3500 访问 HTTP API。 在请求头中通过 dapr-app-id 指定目标应用 id nodeapp。应用 id 是通过 Kubernetes 注解 dapr.io/app-id 来设置的，更多注解可参考 文档。 目标方法名通过请求路径来指定：/neworder  dapr_port = os.getenv(\u0026#34;DAPR_HTTP_PORT\u0026#34;, 3500) dapr_url = \u0026#34;http://localhost:{}/neworder\u0026#34;.format(dapr_port) n = 0 while True: n += 1 message = {\u0026#34;data\u0026#34;: {\u0026#34;orderId\u0026#34;: n}} try: response = requests.post(dapr_url, json=message, timeout=5, headers = {\u0026#34;dapr-app-id\u0026#34;: \u0026#34;nodeapp\u0026#34;} ) if not response.ok: print(\u0026#34;HTTP %d=\u0026gt; %s\u0026#34; % (response.status_code, response.content.decode(\u0026#34;utf-8\u0026#34;)), flush=True) except Exception as e: print(e, flush=True) time.sleep(1) 部署应用 kubectl apply -n dapr-test -f deploy/node.yaml kubectl wait --for=condition=ready pod -n dapr-test -l app=node --timeout=60s kubectl apply -n dapr-test -f deploy/python.yaml kubectl wait --for=condition=ready pod -n dapr-test -l app=python --timeout=60s 检查 node 容器的日志，可以接收到了来自 pythonapp 的请求，并成功持久化存储了订单。\nkubectl logs -f -n dapr-test -l app=node -c node Successfully persisted state for Order ID: 1 Got a new order! Order ID: 1 Successfully persisted state for Order ID: 2 Got a new order! Order ID: 2 Successfully persisted state for Order ID: 3 Got a new order! Order ID: 3 Successfully persisted state for Order ID: 4 Got a new order! Order ID: 4 Debug 原本官方的指南是将 Redis 和应用部署在同一个命名空间中，加上 nodeapp 中硬编码了存储组件名。而我实验的时候讲 Redis 部署在了另一个空间下，检查 node 容器日志时看到的是：\nGot a new order! Order ID: 1 Failed to persist state. daprd 容器中，只有下面的日志。\ntime=\u0026#34;2023-02-11T02:55:38.166259509Z\u0026#34; level=info msg=\u0026#34;HTTP API Called: POST /v1.0/state/statestore\u0026#34; app_id=nodeapp instance=nodeapp-857cf6f985-jnmzw scope=dapr.runtime.http-info type=log useragent=\u0026#34;node-fetch/1.0 (+https://github.com/bitinn/node-fetch)\u0026#34; ver=1.9.6 通过为 nodeapp 的 pod 添加注解 dapr.io/log-level=\u0026quot;debug\u0026quot; 让 daprd 容器输出 debug 日志。\ntime=\u0026#34;2023-02-11T03:05:07.663028821Z\u0026#34; level=debug msg=\u0026#34;{ERR_STATE_STORE_NOT_CONFIGURED state store is not configured}\u0026#34; app_id=nodeapp instance=nodeapp-59b754ff54-c4x4s scope=dapr.runtime.http type=log ver=1.9.6 更多 Debug 方式，参考官方的 Troubleshooting 文档。\n总结 Dapr 提供了与传统 SDK 方式完成不同的方法来实现系统集成，让开发者可以专注于业务逻辑，而无需考虑底层的实现；对组织来说，应用变得更加便携，可以使用不同的云环境。\n但是 Dapr 本身无法跨云跨集群，社区正在考虑与服务网格集成来实现混合多云环境下的服务调用，大家可以期待一下。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-narcisa-aciko-1292464.jpg","permalink":"https://atbug.com/first-sight-of-dapr/","tags":["Dapr","Kubernetes"],"title":"分布式应用运行时 Dapr：万物皆可 API"},{"categories":["笔记"],"contents":"假期给小朋友装上了叨叨许久的 Minecraft（我的世界），为了体验安装的是 开源启动器 HMCL。其实这游戏我也关注比较久了，不过感觉太耗时间。但被小朋友拉上一起玩，便研究了下自建服务器。GitHub 发现已经有人做好了 Minecraft 服务端容器镜像，先是在 HomeLab 上用 Docker 部署，通过多人连线就能玩起来了。\n由于不会玩几下被小朋友给打死，后来才发现还有“和平模式”。无聊转而研究下如何在公有云上部署：\n 我的 HomeLab 常年运行，由于没有重要的数据，不管是对硬件稳定性和数据备份都没有投入，担心游戏数据丢失被埋怨。放在公有云上使用公有云的对象存储，避免数据丢失 偶尔外出时玩的话，还需要 VPN 连回家才能玩 他有朋友一起玩时还能方便联机 最主要的原因还是去年加入微软 MVP 时，有送 Azure 的 credit，不用实属浪费  基于上面的原因，决定将服务器部署在 Azure 上，开一个 8c16g 的虚拟机并安装 k3s。数据呢，通过 blog-csi-driver 持久化存储在 Azure 的 Blob Storage 上。\n开始吧！\n安装 k3s 运行下面的命令进行安装，1.23 版本即可。\nexport INSTALL_K3S_VERSION=v1.23.8+k3s2 curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config k3s 安装之后，需要安装 blob storage 的 CSI 驱动。根据 文档说明 驱动要使用 v0.9.0 以上的版本，才能使用 存储账户 做动态配置。\n安装 CSI 驱动 curl -skSL https://raw.githubusercontent.com/kubernetes-sigs/blob-csi-driver/v1.19.0/deploy/install-driver.sh | bash -s v1.19.0 blobfuse-proxy -- 创建 StorageClass 先登录到 Azure Portal 在 存储账户 中创建账户，记得区域的选择和虚拟机相同；网络权限中选择只允许虚拟网络的访问。创建完成后，在账户的 访问秘钥 中可以获取到 key。\n执行下面的命令，使用前面的账户名和 key 创建 secert。\nkubectl create secret generic azure-secret --from-literal azurestorageaccountname=[ACCOUNT HERE] --from-literal azurestorageaccountkey=[KEY HERE] --type=Opaque 接下来就是使用该 secret 创建 StorageClass。\nkubectl apply -f - \u0026lt;\u0026lt;EOF --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: blob-fuse provisioner: blob.csi.azure.com allowVolumeExpansion: true parameters: csi.storage.k8s.io/provisioner-secret-name: azure-secret csi.storage.k8s.io/provisioner-secret-namespace: default csi.storage.k8s.io/node-stage-secret-name: azure-secret csi.storage.k8s.io/node-stage-secret-namespace: default EOF 创建 PVC 有了 StorageClass 之后，就可以创建 PersistentVolumeClaim 了，指定使用上面的 StorageClass blob-fuse。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: minecraft-pvc spec: storageClassName: blob-fuse accessModes: - ReadWriteMany resources: requests: storage: 100Gi EOF 部署 Minecraft 服务器 使用 Deployment 进行部署，并创建 NodePort Service。镜像使用 itzg/minecraft-server:java17，相关的配置可以参考 官方的文档（我怕再被打，启用了和平模式）。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: minecraft name: minecraft spec: replicas: 1 selector: matchLabels: app: minecraft strategy: {} template: metadata: creationTimestamp: null labels: app: minecraft spec: containers: - image: itzg/minecraft-server:java17 name: minecraft-server env: - name: EULA value: \u0026#34;TRUE\u0026#34; - name: ONLINE_MODE value: \u0026#34;FALSE\u0026#34; - name: DIFFICULTY value: peaceful - name: PVP value: \u0026#34;false\u0026#34; - name: UID value: \u0026#34;0\u0026#34; - name: GID value: \u0026#34;0\u0026#34; - name: MEMORY value: \u0026#34;4G\u0026#34; resources: {} ports: - containerPort: 25565 protocol: TCP volumeMounts: - name: persistent-storage mountPath: /data volumes: - name: persistent-storage persistentVolumeClaim: claimName: minecraft-pvc --- apiVersion: v1 kind: Service metadata: labels: app: minecraft name: minecraft spec: ports: - port: 25565 protocol: TCP targetPort: 25565 selector: app: minecraft type: NodePort EOF 测试 启动客户端，在多人游戏中添加服务器：地址是虚拟机的公共 IP，端口是 Service 的 NodePort。\n接下来就可以愉快的玩耍了。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2023/01/26/minecraft.png","permalink":"https://atbug.com/run-minecraft-on-kubernetes/","tags":["Kubernetes"],"title":"在 Kubernetes 上运行我的世界"},{"categories":["笔记","源码解析"],"contents":"这是 Kubernetes 网络学习的第五篇笔记，也是之前计划中的最后一篇。\n 深入探索 Kubernetes 网络模型和网络通信 认识一下容器网络接口 CNI 源码分析：从 kubelet、容器运行时看 CNI 的使用 从 Flannel 学习 Kubernetes VXLAN 网络 Kubernetes 网络学习之 Cilium 与 eBPF（本篇） \u0026hellip;  开始之前说点题外话，距离上一篇 Flannel CNI 的发布已经快一个月了。这篇本想趁着势头在去年底完成的，正好在一个月内完成计划的所有内容。但上篇发布后不久，我中招了花了一个多周的时间才恢复。然而，恢复后的状态让我有点懵，总感觉很难集中精力，很容易精神涣散。可能接近网上流传的“脑雾”吧，而且 Cilium 也有点类似一团迷雾。再叠加网络知识的不足，eBPF 也未从涉足，学习的过程中断断续续，我曾经一度怀疑这篇会不会流产。\n文章中不免会有问题，如果有发现问题或者建议，望不吝赐教。\n 背景 去年曾经写过一篇文章 《使用 Cilium 增强 Kubernetes 网络安全》 接触过 Cilium，借助 Cilium 的网络策略从网络层面对 pod 间的通信进行限制。但当时我不曾深入其实现原理，对 Kubernetes 网络和 CNI 的了解也不够深入。这次我们通过实际的环境来探寻 Cilium 的网络。\n这篇文章使用的 Cilium 版本是 v1.12.3，操作系统是 Ubuntu 20.04，内核版本是 5.4.0-91-generic。\nCilium 简介  Cilium 是一个开源软件，用于提供、保护和观察容器工作负载（云原生）之间的网络连接，由革命性的内核技术 eBPF 推动。\n eBPF 是什么？  Linux 内核一直是实现监控/可观测性、网络和安全功能的理想地方。 不过很多情况下这并非易事，因为这些工作需要修改内核源码或加载内核模块， 最终实现形式是在已有的层层抽象之上叠加新的抽象。 eBPF 是一项革命性技术，它能在内核中运行沙箱程序（sandbox programs）， 而无需修改内核源码或者加载内核模块。\n  将 Linux 内核变成可编程之后，就能基于现有的（而非增加新的）抽象层来打造更加智能、 功能更加丰富的基础设施软件，而不会增加系统的复杂度，也不会牺牲执行效率和安全性。\n Linux 的内核在网络栈上提供了一组 BPF 钩子，通过这些钩子可以触发 BPF 程序的执行。Cilium datapah 使用这些钩子加载 BPF 程序，创建出更高级的网络结构。\n通过阅读 Cilium 参考文档 eBPF Datapath 得知 Cilium 使用了下面几种钩子：\n XDP：这是网络驱动中接收网络包时就可以触发 BPF 程序的钩子，也是最早的点。由于此时还没有执行其他操作，比如将网络包写入内存，所以它非常适合运行删除恶意或意外流量的过滤程序，以及其他常见的 DDOS 保护机制。 Traffic Control Ingress/Egress：附加到流量控制（traffic control，简称 tc）ingress 钩子上的 BPF 程序，可以被附加到网络接口上。这种钩子在网络栈的 L3 之前执行，并可以访问网络包的大部分元数据。适合处理本节点的操作，比如应用 L3/L4 的端点 1 策略、转发流量到端点。CNI 通常使用虚拟机以太接口对 veth 将容器连接到主机的网络命名空间。使用附加到主机端 veth 的 tc ingress 钩子，可以监控离开容器的所有流量，并执行策略。同时将另一个 BPF 程序附加到 tc egress 钩子，Cilium 可以监控所有进出节点的流量并执行策略 . Socket operations：套接字操作钩子附加到特定的 cgroup 并在 TCP 事件上运行。Cilium 将 BPF 套接字操作程序附加到根 cgroup，并使用它来监控 TCP 状态转换，特别是 ESTABLISHED 状态转换。当套接字状态变为 ESTABLISHED 时，如果 TCP 套接字的对端也在当前节点（也可能是本地代理），则会附加 Socket send/recv 程序。 Socket send/recv：这个钩子在 TCP 套接字执行的每个发送操作上运行。此时钩子可以检查消息并丢弃消息、将消息发送到 TCP 层，或者将消息重定向到另一个套接字。Cilium 使用它来加速数据路径重定向。  因为后面会用到，这里着重介绍了这几种钩子。\n环境搭建 前面几篇文章，我都是使用 k3s 并手动安装 CNI 插件来搭建实验环境。这次，我们直接使用 k8e，因为 k8e 使用 Cilium 作为默认的 CNI 实现。\n还是在我的 homelab 上做个双节点（ubuntu-dev2: 192.168.1.12、ubuntu-dev3: 192.168.1.13）的集群。\nMaster 节点：\ncurl -sfL https://getk8e.com/install.sh | API_SERVER_IP=192.168.1.12 K8E_TOKEN=ilovek8e INSTALL_K8E_EXEC=\u0026#34;server --cluster-init --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config\u0026#34; sh - Worker 节点：\ncurl -sfL https://getk8e.com/install.sh | K8E_TOKEN=ilovek8e K8E_URL=https://192.168.1.12:6443 sh - 部署示例应用，将其调度到不同的节点上：\nNODE1=ubuntu-dev2 NODE2=ubuntu-dev3 kubectl apply -n default -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: labels: app: curl name: curl spec: containers: - image: curlimages/curl name: curl command: [\u0026#34;sleep\u0026#34;, \u0026#34;365d\u0026#34;] nodeName: $NODE1 --- apiVersion: v1 kind: Pod metadata: labels: app: httpbin name: httpbin spec: containers: - image: kennethreitz/httpbin name: httpbin nodeName: $NODE2 EOF 为了使用方便，将示例应用、cilium pod 等信息设置为环境变量：\nNODE1=ubuntu-dev2 NODE2=ubuntu-dev3 cilium1=$(kubectl get po -n kube-system -l k8s-app=cilium --field-selector spec.nodeName=$NODE1 -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) cilium2=$(kubectl get po -n kube-system -l k8s-app=cilium --field-selector spec.nodeName=$NODE2 -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) Debug 流量 还是以前的套路，从请求发起方开始一路追寻网络包。这次使用 Service 来进行访问：curl http://10.42.0.51:80/get。\nkubectl get po httpbin -n default -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES httpbin 1/1 Running 0 3m 10.42.0.51 ubuntu-dev3 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 第 1 步：Pod1 发送请求 检查 pod curl 的路由表：\nkubectl exec curl -n default -- ip route get 10.42.0.51 10.42.0.51 via 10.42.1.247 dev eth0 src 10.42.1.80 可知网络包就发往以太接口 eth0，然后从使用 arp 查到其 MAC 地址 ae:36:76:3e:c3:03：\nkubectl exec curl -n default -- arp -n ? (10.42.1.247) at ae:36:76:3e:c3:03 [ether] on eth0 查看接口 eth0 的信息：\nkubectl exec curl -n default -- ip link show eth0 42: eth0@if43: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP qlen 1000 link/ether f6:00:50:f9:92:a1 brd ff:ff:ff:ff:ff:ff 发现其 MAC 地址并不是 ae:36:76:3e:c3:03，从名字上的 @if43 可以得知其 veth 对的索引是 43，接着 登录到节点 NODE1 查询该索引接口的信息：\nip link | grep -A1 ^43 43: lxc48c4aa0637ce@if42: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether ae:36:76:3e:c3:03 brd ff:ff:ff:ff:ff:ff link-netns cni-407cd7d8-7c02-cfa7-bf93-22946f923ffd 我们看到这个接口 lxc48c4aa0637ce 的 MAC 正好就是 ae:36:76:3e:c3:03。\n按照 过往的经验，这个虚拟的以太接口 lxc48c4aa0637ce 是个 虚拟以太网口，位于主机的根网络命名空间，一方面与容器的以太接口 eth0 间通过隧道相连，发送到任何一端的网络包都会直达对端；另一方面应该与主机命名空间上的网桥相连，但是从上面的结果中并未找到网桥的名字。\n通过 ip link 查看：\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether fa:cb:49:4a:28:21 brd ff:ff:ff:ff:ff:ff 3: cilium_net@cilium_host: \u0026lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 36:d5:5a:2a:ce:80 brd ff:ff:ff:ff:ff:ff 4: cilium_host@cilium_net: \u0026lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 12:82:fb:78:16:6a brd ff:ff:ff:ff:ff:ff 5: cilium_vxlan: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether fa:42:4d:22:b7:d0 brd ff:ff:ff:ff:ff:ff 25: lxc_health@if24: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 3e:4f:b3:56:67:2b brd ff:ff:ff:ff:ff:ff link-netnsid 0 33: lxc113dd6a50a7a@if32: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 32:3a:5b:15:44:ff brd ff:ff:ff:ff:ff:ff link-netns cni-07cffbd8-83dd-dcc1-0b57-5c59c1c037e9 43: lxc48c4aa0637ce@if42: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether ae:36:76:3e:c3:03 brd ff:ff:ff:ff:ff:ff link-netns cni-407cd7d8-7c02-cfa7-bf93-22946f923ffd 我们看到了多个以太接口：cilium_net、cilium_host、cilium_vxlan、cilium_health 以及与容器网络命名空间的以太接口的隧道对端 lxcxxxx。\n网络包到了 lxcxxx 这里再怎么走？接下来就轮到 eBPF 出场了。\n注意 cilium_net、cilium_host 和 cilium_health 在文中不会涉及，因此不在后面的图中体现。\n第 2 步：Pod1 LXC BPF Ingress 进入到当前节点的 cilium pod 也就是前面设置的变量 $cilium1 中使用 bpftool 命令检查附加该 veth 上 BPF 程序。\nkubectl exec -n kube-system $cilium1 -c cilium-agent -- bpftool net show dev lxc48c4aa0637ce xdp: tc: lxc48c4aa0637ce(43) clsact/ingress bpf_lxc.o:[from-container] id 2901 flow_dissector: 也可以登录到节点 $NODE1 上使用 tc 命令来查询。注意，这里我们指定了 ingress，在文章开头 datapath 部分。因为容器的 eth0 与主机网络命名空间的 lxc 组成通道，因此容器的出口（Egress）流量就是 lxc 的入口 Ingress 流量。同理，容器的入口流量就是 lxc 的出口流量。\n#on NODE1 tc filter show dev lxc48c4aa0637ce ingress filter protocol all pref 1 bpf chain 0 filter protocol all pref 1 bpf chain 0 handle 0x1 bpf_lxc.o:[from-container] direct-action not_in_hw id 2901 tag d578585f7e71464b jited 可以通过程序 id 2901 查看详细信息。\nkubectl exec -n kube-system $cilium1 -c cilium-agent -- bpftool prog show id 2901 2901: sched_cls name handle_xgress tag d578585f7e71464b gpl loaded_at 2023-01-09T19:29:52+0000 uid 0 xlated 688B jited 589B memlock 4096B map_ids 572,86 btf_id 301 可以看出，这里加载了 BPF 程序 bpf_lxc.o 的 from-container 部分。到 Cilium 的源码 bpf_lxc.c 的 __section(\u0026quot;from-container\u0026quot;) 部分，程序名 handle_xgress：\nhandle_xgress #1 validate_ethertype(ctx, \u0026amp;proto) tail_handle_ipv4 #2 handle_ipv4_from_lxc #3 lookup_ip4_remote_endpoint =\u0026gt; ipcache_lookup4 #4 policy_can_access #5 if TUNNEL_MODE #6 encap_and_redirect_lxc ctx_redirect(ctx, ENCAP_IFINDEX, 0) if ENABLE_ROUTING ipv4_l3 return CTX_ACT_OK; (1)：网络包的头信息发送给 handle_xgress，然后检查其 L3 的协议。\n(2)：所有 IPv4 的网络包都交由 tail_handle_ipv4 来处理。\n(3)：核心的逻辑都在 handle_ipv4_from_lxc。tail_handle_ipv4 是如何跳转到 handle_ipv4_from_lxc，这里用到了 Tails Call 。Tails call 允许我们配置在某个 BPF 程序执行完成并满足某个条件时执行指定的另一个程序，且无需返回原程序。这里不做展开有兴趣的可以参考 官方的文档。\n(4)：接着从 eBPF map cilium_ipcache 中查询目标 endpoint，查询到 tunnel endpoint 192.168.1.13 ，这个地址是目标所在的节点 IP 地址，类型是。\nkubectl exec -n kube-system $cilium1 -c cilium-agent -- cilium map get cilium_ipcache | grep 10.42.0.51 10.42.0.51/32 identity=15773 encryptkey=0 tunnelendpoint=192.168.1.13 sync (5)：policy_can_access 这里是执行出口策略的检查，本文不涉及故不展开。\n(6)：之后的处理会有两种模式：\n 直接路由：交由内核网络栈进行处理，或者 underlaying SDN 的支持。 隧道：会将网络包再次封装，通过隧道传输，比如 vxlan。  这里我们使用的也是隧道模式。网络包交给 encap_and_redirect_lxc 处理，使用 tunnel endpoint 作为隧道对端。最终转发给 ENCAP_IFINDEX（这个值是接口的索引值，由 cilium-agent 启动时获取的），就是以太网接口 cilium_vxlan。\n第 3 步：NODE 1 vxlan BPF Egress 先看下这个接口上的 BPF 程序。\nkubectl exec -n kube-system $cilium1 -c cilium-agent -- bpftool net show dev cilium_vxlan xdp: tc: cilium_vxlan(5) clsact/ingress bpf_overlay.o:[from-overlay] id 2699 cilium_vxlan(5) clsact/egress bpf_overlay.o:[to-overlay] id 2707 flow_dissector: 容器的出口流量对 cilium_vxlan 来说也是 engress，因此这里的程序是 to-overlay。\n程序位于 bpf_overlay.c 中，这个程序的处理很简单，如果是 IPv6 协议会将封包使用 IPv6 的地址封装一次。这里是 IPv4 ，直接返回 CTX_ACT_OK。将网络包交给内核网络栈，进入 eth0 接口。\n第 4 步：NODE1 NIC BPF Egress 先看看 BPF 程序。\nkubectl exec -n kube-system $cilium1 -c cilium-agent -- bpftool net show dev eth0 xdp: tc: eth0(2) clsact/ingress bpf_netdev_eth0.o:[from-netdev] id 2823 eth0(2) clsact/egress bpf_netdev_eth0.o:[to-netdev] id 2832 flow_dissector: egress 程序 to-netdev 位于 bpf_host.c。实际上没做重要的处理，只是返回 CTX_ACT_OK 交给内核网络栈继续处理：将网络包发送到 vxlan 隧道发送到对端，也就是节点 192.168.1.13 。中间数据的传输，实际上用的还是 underlaying 网络，从主机的 eth0 接口经过 underlaying 网络到达目标主机的 eth0 接口。\n第 5 步：NODE2 NIC BPF Ingress vxlan 网络包到达节点的 eth0 接口，也会触发 BPF 程序。\nkubectl exec -n kube-system $cilium2 -c cilium-agent -- bpftool net show dev eth0 xdp: tc: eth0(2) clsact/ingress bpf_netdev_eth0.o:[from-netdev] id 4556 eth0(2) clsact/egress bpf_netdev_eth0.o:[to-netdev] id 4565 flow_dissector: 这次触发的是 from-netdev，位于 bpf_host.c 中。\nfrom_netdev if vlan allow_vlan return CTX_ACT_OK 对 vxlan tunnel 模式来说，这里的逻辑很简单。当判断网络包是 vxlan 的并确认允许 vlan 后，直接返回 CTX_ACT_OK 将处理交给内核网络栈。\n第 6 步：NODE2 vxlan BPF Ingress 网络包通过内核网络栈来到了接口 cilium_vxlan。\nkubectl exec -n kube-system $cilium2 -c cilium-agent -- bpftool net show dev cilium_vxlan xdp: tc: cilium_vxlan(5) clsact/ingress bpf_overlay.o:[from-overlay] id 4468 cilium_vxlan(5) clsact/egress bpf_overlay.o:[to-overlay] id 4476 flow_dissector: 程序位于 bpf_overlay.c 中。\nfrom_overlay validate_ethertype tail_handle_ipv4 handle_ipv4 lookup_ip4_endpoint 1# map_lookup_elem ipv4_local_delivery 2# tail_call_dynamic 3# (1)：lookup_ip4_endpoint 会在 eBPF map cilium_lxc 中检查目标地址是否在当前节点中（这个 map 只保存了当前节点中的 endpoint）。\nkubectl exec -n kube-system $cilium2 -c cilium-agent -- cilium map get cilium_lxc | grep 10.42.0.51 10.42.0.51:0 id=2826 flags=0x0000 ifindex=29 mac=96:86:44:A6:37:EC nodemac=D2:AD:65:4D:D0:7B sync 这里查到目标 endpoint 的信息：id、以太网口索引、mac 地址。在 NODE2 的节点上，查看接口信息发现，这个网口是虚拟以太网设备 lxc65015af813d1，正好是 pod httpbin 接口 eth0 的对端。\nip link | grep -B1 -i d2:ad 29: lxc65015af813d1@if28: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether d2:ad:65:4d:d0:7b brd ff:ff:ff:ff:ff:ff link-netns cni-395674eb-172b-2234-a9ad-1db78b2a5beb kubectl exec -n default httpbin -- ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 28: eth0@if29: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 96:86:44:a6:37:ec brd ff:ff:ff:ff:ff:ff link-netnsid (2)：ipv4_local_delivery 的逻辑位于 l3.h 中，这里会 tail-call 通过 endpoint 的 LXC ID（29）定位的 BPF 程序。\n第 7 步：Pod2 LXC BPF Egress 执行下面的命令并不会找到想想中的 egress to-container（与 from-container）。\nkubectl exec -n kube-system $cilium2 -c cilium-agent -- bpftool net show | grep 29 lxc65015af813d1(29) clsact/ingress bpf_lxc.o:[from-container] id 4670 前面用的 BPF 程序都是附加到接口上的，而这里是直接有 vxlan 附加的程序直接 tail call 的。to-container 可以在 bpf-lxc.c 中找到。\nhandle_to_container tail_ipv4_to_endpoint ipv4_policy #1 policy_can_access_ingress redirect_ep ctx_redirect (1)：ipv4_policy 会执行配置的策略\n(2)：如果策略通过，会调用 redirect_ep 将网络包发送到虚拟以太接口 lxc65015af813d1，进入到 veth 后会直达与其相连的容器 eth0 接口。\n第 8 步：到达 Pod2 网络包到达 pod2，附上一张完成的图。\n总结 说说个人看法吧，本文设计的内容还只是 Cilium 的冰山一角，对于内核知识和 C 语言欠缺的我来说研究起来非常吃力。Cilium 除此之外还有很多的内容，也还没有深入去研究。不得不感叹，Cilium 真是复杂，以我目前的了解，Cilium 维护了一套自己的数据在 BPF map 中，比如 endpoint、节点、策略、路由、连接状态等相当多的数据，这些都是保存在内核中；再就是 BPF 程序的开发和维护成本会随着功能的复杂度而膨胀，很难想象如果用 BPF 程序去开发 L7 的功能会多复杂。这应该是为什么会借助代理去处理 L7 的场景。\n最后分享下学习 Cilium 过程中的经验吧。\n首先是 BPF 程序的阅读，在项目的 bpf 的代码都是静态的代码，里面分布着很多的与配置相关的 if else，运行时会根据配置进行编译。这种情况下可以进入 Cilium pod，在目录 /run/cilium/state/templates 下有应用配置后的源文件，代码量会少很多；在 /run/cilium/state/globals/node_config 下是当前使用的配置，可以结合这些配置来阅读代码。\n脚注\n      Cilium 通过为容器分配 IP 地址使其在网络上可用。多个容器可以共享同一个 IP 地址，就像 一个 Kubernetes Pod 中可以有多个容器，这些容器之间共享网络命名空间，使用同一个 IP 地址。这些共享同一个地址的容器，Cilium 将其组合起来，成为 Endpoint（端点）。 \u0026#x21a9;\u0026#xfe0e;\n   ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2023/01/11/pexelsmarcosmiranda672597.jpg","permalink":"https://atbug.com/learn-cilium-and-ebpf/","tags":["Kubernetes","容器","Cilium","eBPF"],"title":"Kubernetes 网络学习之 Cilium 与 eBPF"},{"categories":["生活"],"contents":"快，是真的快，感觉 与 2021 再见 就在几天前，然而回头又想不到做了什么，该总结什么。\n是的，这篇与技术无关。\n工作 去年（我现在可以称其为去年）换了现在的工作，换了新的方向，对于未知当初的几个月充满了新鲜与忐忑。年初在老板的几次正向 “PUA” 下，对这个方向慢慢有了更新的认识，并坚定的走下去。\n远程办公又一年，走出最初半年的“蜜月期”之后，尤其上半年的那几个月，心中充满了阴郁。短短的几个月，有点将居家做到了极致的异味，以至于年底两个月的居家变得无感了。也好，快速地度过了远程办公的“阵痛期”。\n工作中仍然有挑战，但更多的是来自自己不曾接触的知识领域，无他唯有不断地探索学习。这一年，从工作从同事那边都学到了很多。\n何其有幸，继续做着自己喜欢的工作。\n学习 阅读方面，书看的是不够多，更多的是碎片化的阅读时间。“哪里不会学哪里”，遇到不会的不需要尴尬，也正式填充认知拼图的机会。学习之后，将其记录下来。写博客写公众号，实际上是我学习的一种方式。\n公众号，这是我坚持的第二年，因为是记录学习，内容会显得杂乱。年初有朋友建议稍微运营一下，确实有考虑过，但最终放弃。不想在这上面浪费时间，不想改变写作的初衷，唯有持续学习才是写下去的动力。\n统计了下，2020 年有 34 篇原创 5 篇翻译，虽然数量没有去年多，但是有在尝试进行思考和沉淀。内容虽然不会很多，但我希望继续保持下去，只发原创。\n何其有幸，还能继续学习。\n生活 明天与她进入第 19 个年头，明年我也 38 了，开始超过人生一半的时间；父母陪在身边，不住在一起但住得越来越近；儿子也一天天长大，可以一起开始打游戏了。\n今年与医院打交道的次数有点多，所幸都一切都算顺利，坦然面对。\n何其有幸，有你们相伴。\n总结 过去一年，过得非常快，有得有失，有求而不得，有失而复得。\n珍惜眼前，珍惜当下。\n何其有幸，2023，它来了。\n 希望你愿意听 我心中的旧日辉煌\n我的世界没有太阳 却依然明亮\n理想的种子它随风飘荡不知去向何方\n在不为人知的角落里开出了花\n不停地走 远方没有尽头\n目光所及 都是崭新的轮廓\n声色犬马 对错别再评价\n不必关心 明天会怎样\n万物生长 纵火烧干理想\n孤勇一腔 将漆黑的夜点亮\n义无反顾 继续乘风破浪\n人群散场 快背上行囊\n黑屋乐队《逆旅》\n ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/31/pexelslaleshaldarwish169976.jpg","permalink":"https://atbug.com/fare-well-2002/","tags":null,"title":"再见，超速的 2022"},{"categories":["翻译"],"contents":"本文翻译自 ContainerJournal 的 2022 年度文章之一 《Implementing Zero-Trust on Kubernetes》，作者 Deepak Goel 在文中分享了 Kubernetes 上实施零信任的三个最佳实践。\n 作为云原生社区的基石，Kubernetes 帮助企业在生产环境中更高效地部署和管理容器。尽管 Kubernetes 最初设计时提供了基本的 安全功能，但广泛且迅速的采用以及日益复杂的威胁形势使 Kubernetes 更容易受到攻击。开发人员和安全专家当下的任务是扩展 Kubernetes 的内置安全性，以有效防范更复杂、更多样和更频繁的网络攻击。\n以往“信任但要验证”的方式已被证明对云计算复杂的分布式特性无效，因此 Kubernetes 必须转向“从不信任，始终验证”的零信任模型思想，为业务提供更大的保护。\n零信任模型的基本概念 基于“从不信任，始终验证”的原则，可以用三个基本概念来解释零信任模型：\n 安全网络：始终认为网络是敌对的和有威胁的。网络上的内部和外部数据和信息不断暴露在安全威胁之下。 安全资源：网络上存在的任何信息源，无论位于何处，对其都应持怀疑态度。 身份验证：默认情况下不应信任来自内部或外部网络的用户、设备和流量。零信任应该基于使用正确的身份验证和授权的访问控制。  零信任的三个最佳实践 Kubernetes 提供了灵活性，既是优势但也增加了复杂性，为了在不同的网络环境中运行，为服务和工作负载引入了许多配置选项。Kubernetes 部署考虑以下三个零信任模型的最佳实践，以提升安全保护和工作效率。\n优化软件配置和访问权限 团队需要为服务和跨集群操作提供一致的配置。虽然 Kubernetes 提供了多种配置选项，但过多的选项会增加安全问题出现的几率。使用零信任框架，组织可以对服务进行持续验证并将其部署到多个集群，而不会危及任何安全性。通过在授予它们对应用程序和服务的任何安全权限之前仔细检查这些配置，组织可以加强分布式 Kubernetes 集群的安全性。\n使用零信任模型提高 Kubernetes 安全性的另一种方法是只为软件提供运行所需的权限和功能。虽然确定软件所需的确切权限和功能并不是那么容易，但更好地了解这些元素可以降低安全风险。对于云端的容器编排环境，相比本地环境，赋予有限的权限和能力更为重要。 译者注：持续验证、最小权限原则\n记录和监控数据 重要的是提供必要的安全数据，使开发人员和安全专家能够衡量、预测、避免和防御潜在的安全风险。例如，组织应该记录服务识别的用户 ID 或组 ID，尤其是在集群环境。这可确保组织使用所需的 ID 来帮助服务和软件团队更快地识别匿名攻击。日志记录也将是在云原生环境中提供安全可追溯性的信息的关键部分。\n有了足够的安全数据，团队还可以重新思考和优化他们的安全实践和应用程序更新，以应对不断变化的技术环境，帮助确保持续抵御攻击。\n译者注：提供数据支撑\n专注于人员和流程管理 除了来自外部网络的用户和设备之外，合作伙伴、利益相关者或任何有权访问组织的数据库和容器化应用程序的人都是潜在的 Kubernetes 安全威胁。因此，培训内部人员以避免潜在的内部威胁至关重要。如上所述，组织可以从记录和监控平台数据开始，同时让所有利益相关者了解市场上普遍存在的各种攻击策略。\n除了适当的培训之外，优化日常运营中的安全流程有助于支撑零信任模型，并最大限度地减少网络攻击对企业云上服务的影响。一些推荐的安全流程包括积极审查网络管理、防火墙清单以及定期检查容器和软件镜像。\n由于 air-gapped 为云中的复杂部署模式提供了军事级安全级别，我建议组织将这些操作流程与 air-gapped 实现相结合，为 Kubernetes 项目提供额外的安全级别。 译者注：人员培训、流程约束必不可少\n结论 在生产环境中部署和管理 Kubernetes 时，安全性不再是事后的想法。违规、中断和数据盗窃是严重的网络安全问题，可能对组织产生不利影响。数据和信息记录、员工安全培训和流程优化等零信任实践是保护 Kubernetes 项目和 IT 基础架构的有效且实用的方法。通过实施这些实践，组织可以更好地保护 Kubernetes 部署。遵循这种零信任模型将使开发人员和运维人员无需担心集群和基础设施安全问题，同时使安全团队能够专注于安全性，而不是迷失在 Kubernetes 配置中。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/29/16722890416327.png","permalink":"https://atbug.com/implementing-zero-trust-on-kubernetes/","tags":["安全","Kubernetes","零信任"],"title":"译：在 Kubernetes 上实施零信任"},{"categories":["笔记"],"contents":"这是 Kubernetes 网络学习的第四篇笔记。\n 深入探索 Kubernetes 网络模型和网络通信 认识一下容器网络接口 CNI 源码分析：从 kubelet、容器运行时看 CNI 的使用 从 Flannel 学习 Kubernetes VXLAN 网络（本篇） Kubernetes 网络学习之 Cilium 与 eBPF \u0026hellip;   Flannel 介绍 Flannel 是一个非常简单的 overlay 网络（VXLAN），是 Kubernetes 网络 CNI 的解决方案之一。Flannel 在每台主机上运行一个简单的轻量级 agent flanneld 来监听集群中节点的变更，并对地址空间进行预配置。Flannel 还会在每台主机上安装 vtep flannel.1（VXLAN tunnel endpoints），与其他主机通过 VXLAN 隧道相连。\nflanneld 监听在 8472 端口，通过 UDP 与其他节点的 vtep 进行数据传输。到达 vtep 的二层包会被原封不动地通过 UDP 的方式发送到对端的 vtep，然后拆出二层包进行处理。简单说就是用四层的 UDP 传输二层的数据帧。\n在 Kubernetes 发行版 K3S 中将 Flannel 作为默认的 CNI 实现。K3S 集成了 flannel，在启动后 flannel 以 go routine 的方式运行。\n环境搭建 Kubernetes 集群使用 k3s 发行版，但在安装集群的时候，禁用 k3s 集成的 flannel，使用独立安装的 flannel 进行验证。\n安装 CNI 的 plugin，需要在所有的 node 节点上执行下面的命令，下载 CNI 的官方 bin。\nsudo mkdir -p /opt/cni/bin curl -sSL https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz | sudo tar -zxf - -C /opt/cni/bin 安装 k3s 的控制平面。\nexport INSTALL_K3S_VERSION=v1.23.8+k3s2 curl -sfL https://get.k3s.io | sh -s - --disable traefik --flannel-backend=none --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 安装 Flannel。这里注意，Flannel 默认的 Pod CIRD 是 10.244.0.0/16，我们将其修改为 k3s 默认的 10.42.0.0/16。\ncurl -s https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml | sed \u0026#39;s|10.244.0.0/16|10.42.0.0/16|g\u0026#39; | kubectl apply -f - 添加另一个节点到集群。\nexport INSTALL_K3S_VERSION=v1.23.8+k3s2 export MASTER_IP=\u0026lt;MASTER_IP\u0026gt; export NODE_TOKEN=\u0026lt;TOKEN\u0026gt; curl -sfL https://get.k3s.io | K3S_URL=https://${MASTER_IP}:6443 K3S_TOKEN=${NODE_TOKEN} sh - 查看节点状态。\nkubectl get node NAME STATUS ROLES AGE VERSION ubuntu-dev3 Ready \u0026lt;none\u0026gt; 13m v1.23.8+k3s2 ubuntu-dev2 Ready control-plane,master 17m v1.23.8+k3s2 运行两个 pod：curl 和 httpbin，为了探寻\nNODE1=ubuntu-dev2 NODE2=ubuntu-dev3 kubectl apply -n default -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: labels: app: curl name: curl spec: containers: - image: curlimages/curl name: curl command: [\u0026#34;sleep\u0026#34;, \u0026#34;365d\u0026#34;] nodeName: $NODE1 --- apiVersion: v1 kind: Pod metadata: labels: app: httpbin name: httpbin spec: containers: - image: kennethreitz/httpbin name: httpbin nodeName: $NODE2 EOF 网络配置 接下来，一起看下 CNI 插件如何配置 pod 网络。\n初始化 Flannel 是通过 Daemonset 的方式部署的，每台节点上都会运行一个 flannel 的 pod。通过挂载本地磁盘的方式，在 Pod 启动时会通过初始化容器将二进制文件和 CNI 的配置复制到本地磁盘中，分别位于 /opt/cni/bin/flannel 和 /etc/cni/net.d/10-flannel.conflist。\n通过查看 kube-flannel.yml 中的 ConfigMap，可以找到 CNI 配置，flannel 默认委托（见 flannel-cni 源码 flannel_linux.go#L78）给 bridge 插件 进行网络配置，网络名称为 cbr0；IP 地址的管理，默认委托（见 flannel-cni 源码 flannel_linux.go#L40） host-local 插件 完成。\n#cni-conf.json 复制到 /etc/cni/net.d/10-flannel.conflist { \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } 还有 Flannel 的网络配置，配置有我们设置的 Pod CIDR 10.42.0.0/16 以及后端（backend）的类型 vxlan。这也是 flannel 默认的类型，此外还有 多种后端类型 可选，如 host-gw、wireguard、udp、Alloc、IPIP、IPSec。\n#net-conf.json 挂载到 pod 的 /etc/kube-flannel/net-conf.json { \u0026#34;Network\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } } Flannel Pod 运行启动 flanneld 进程，指定了参数 --ip-masq 和 --kube-subnet-mgr，后者开启了 kube subnet manager 模式。\n运行 集群初始化时使用了默认的 Pod CIDR 10.42.0.0/16，当有节点加入集群，集群会从该网段上为节点分配 属于节点的 Pod CIDR 10.42.X.1/24。\nflannel 在 kube subnet manager 模式下，连接到 apiserver 监听节点更新的事件，从节点信息中获取节点的 Pod CIDR。\nkubectl get no ubuntu-dev2 -o jsonpath={.spec} | jq { \u0026#34;podCIDR\u0026#34;: \u0026#34;10.42.0.0/24\u0026#34;, \u0026#34;podCIDRs\u0026#34;: [ \u0026#34;10.42.0.0/24\u0026#34; ], \u0026#34;providerID\u0026#34;: \u0026#34;k3s://ubuntu-dev2\u0026#34; } 然后在主机上写子网配置文件，下面展示的是其中一个节点的子网配置文件的内容。另一个节点的内容差异在 FLANNEL_SUBNET=10.42.1.1/24，使用的是对应节点的 Pod CIDR。\n#node 192.168.1.12 cat /run/flannel/subnet.env FLANNEL_NETWORK=10.42.0.0/16 FLANNEL_SUBNET=10.42.0.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true CNI 插件执行 CNI 插件的执行是由容器运行时触发的，具体细节可以看上一篇 《源码解析：从 kubelet、容器运行时看 CNI 的使用》。\nflannel 插件 flannel CNI 插件（/opt/cni/bin/flannel）执行的时候，接收传入的 cni-conf.json，读取上面初始化好的 subnet.env 的配置，输出结果，委托给 bridge 进行下一步。\ncat /var/lib/cni/flannel/e4239ab2706ed9191543a5c7f1ef06fc1f0a56346b0c3f2c742d52607ea271f0 | jq { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;ipMasq\u0026#34;: false, \u0026#34;ipam\u0026#34;: { \u0026#34;ranges\u0026#34;: [ [ { \u0026#34;subnet\u0026#34;: \u0026#34;10.42.0.0/24\u0026#34; } ] ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34; } ], \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34; }, \u0026#34;isDefaultGateway\u0026#34;: true, \u0026#34;isGateway\u0026#34;: true, \u0026#34;mtu\u0026#34;: 1450, \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34; } bridge 插件 bridge 使用上面的输出连同参数一起作为输入，根据配置完成如下操作：\n 创建网桥 cni0（节点的根网络命名空间） 创建容器网络接口 eth0（ pod 网络命名空间） 创建主机上的虚拟网络接口 vethX（节点的根网络命名空间） 将 vethX 连接到网桥 cni0 委托 ipam 插件分配 IP 地址、DNS、路由 将 IP 地址绑定到 pod 网络命名空间的接口 eth0 上 检查网桥状态 设置路由 设置 DNS  最后输出如下的结果：\ncat /var/li/cni/results/cbr0-a34bb3dc268e99e6e1ef83c732f5619ca89924b646766d1ef352de90dbd1c750-eth0 | jq .result { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;dns\u0026#34;: {}, \u0026#34;interfaces\u0026#34;: [ { \u0026#34;mac\u0026#34;: \u0026#34;6a:0f:94:28:9b:e7\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cni0\u0026#34; }, { \u0026#34;mac\u0026#34;: \u0026#34;ca:b4:a9:83:0f:d4\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;veth38b50fb4\u0026#34; }, { \u0026#34;mac\u0026#34;: \u0026#34;0a:01:c5:6f:57:67\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;sandbox\u0026#34;: \u0026#34;/var/run/netns/cni-44bb41bd-7c41-4860-3c55-4323bc279628\u0026#34; } ], \u0026#34;ips\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;10.42.0.5/24\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.42.0.1\u0026#34;, \u0026#34;interface\u0026#34;: 2, \u0026#34;version\u0026#34;: \u0026#34;4\u0026#34; } ], \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;10.42.0.0/16\u0026#34; }, { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;10.42.0.1\u0026#34; } ] } port-mapping 插件 该插件会将来自主机上一个或多个端口的流量转发到容器。\nDebug 让我们在第一个节点上，使用 tcpdump 对接口 cni0 进行抓包。\ntcpdump -i cni0 port 80 -vvv 从 pod curl 中使用 pod httpbin 的 IP 地址 10.42.1.2 发送请求：\nkubectl exec curl -n default -- curl -s 10.42.1.2/get cni0 从在 cni0 上的抓包结果来看，第三层的 IP 地址均为 Pod 的 IP 地址，看起来就像是两个 pod 都在同一个网段。\nhost eth0 文章开头提到 flanneld 监听 udp 8472 端口。\nnetstat -tupln | grep 8472 udp 0 0 0.0.0.0:8472 0.0.0.0:* - 我们直接在以太网接口上抓取 UDP 的包：\ntcpdump -i eth0 port 8472 -vvv 再次发送请求，可以看到抓取到 UDP 数据包，传输的负载是二层的封包。\nOverlay 网络下的跨节点通信 在系列的第一篇中，我们研究 pod 间的通信时提到不同 CNI 插件的处理方式不同，这次我们探索了 flannel 插件的工作原理。希望通过下面的图可以对 overlay 网络处理跨节点的网络通信有个比较直观的认识。\n当发送到 10.42.1.2 流量到达节点 A 的网桥 cni0，由于目标 IP 并不属于当前阶段的网段。根据系统的路由规则，进入到接口 flannel.1，也就是 VXLAN 的 vtep。这里的路由规则也由 flanneld 来维护，当节点上线或者下线时，都会更新路由规则。\n#192.168.1.12 Destination Gateway Genmask Flags Metric Ref Use Iface default _gateway 0.0.0.0 UG 0 0 0 eth0 10.42.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.42.1.0 10.42.1.0 255.255.255.0 UG 0 0 0 flannel.1 192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 #192.168.1.13 Destination Gateway Genmask Flags Metric Ref Use Iface default _gateway 0.0.0.0 UG 0 0 0 eth0 10.42.0.0 10.42.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.42.1.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 flannel.1 将原始的以太封包使用 UDP 协议重新封装，将其发送到目标地址 10.42.1.0 （目标的 MAC 地址通过 ARP 获取）。对端的 vtep 也就是 flannel.1 的 UDP 端口 8472 收到消息，解帧出以太封包，然后对以太封包进行路由处理，发送到接口 cni0，最终到达目标 pod 中。\n响应的数据传输与请求的处理也是类似，只是源地址和目的地址调换。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/14/pexelsdickscholten1096793.jpg","permalink":"https://atbug.com/cross-node-traffic-on-flannel-vxlan-network/","tags":["Kubernetes","网络","CNI"],"title":"从 Flannel 学习 Kubernetes overlay 网络"},{"categories":["笔记","源码解析"],"contents":"这是 Kubernetes 网络学习的第三篇笔记。\n 深入探索 Kubernetes 网络模型和网络通信 认识一下容器网络接口 CNI 源码分析：从 kubelet、容器运行时看 CNI 的使用（本篇） 从 Flannel 学习 Kubernetes VXLAN 网络 Kubernetes 网络学习之 Cilium 与 eBPF \u0026hellip;  在上一篇中，通过对 CNI 规范的解读了解了网络配置的操作和相关的流程。在网络的几个操作中除了 CNI_COMMAND 外，有另外三个参数几乎每次都要提供 CNI_CONTAINERID、CNI_IFNAME 和 CNI_NETNS，这些参数无外乎都来自容器运行时。这篇将结合 Kubernetes 和 Containerd 源码，来分析一下 CNI 的使用。\nKubernetes 的源码来自分支 release-1.24，Containerd 的来自分支 release/1.6。\nCNI 的使用 创建 Pod 在之前做过的 kubelet 源码分析 中曾提到 Kubelet#syncLoop() 会持续监控来自 文件、apiserver、http 的变更，来更新 pod 的状态。写那篇文章的时候，分析到这里就结束了。因为这之后的工作就交给容器运行时来完成 sandbox 和各种容器的创建和运行，见 kubeGenericRuntimeManager#SyncPod()。\nkubelet 封装 sandbox 和容器创建、运行请求，调用容器运行时的接口，将具体工作交由容器运行时来完成来完成（容器运行时接口 Container Runtime Interface，简称 CRI，找时间再进行研究）。\n参考源码  pkg/kubelet/kubelet.go:1985 pkg/kubelet/kuberuntime/kuberuntime_manager.go:711  Sandbox 容器 记得在 系列的第一篇 中，当我们在节点上查看命名空间时，网络命名空间的进程是 /pause 。\nlsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531992 net 126 1 root unassigned /lib/systemd/systemd --system --deserialize 31 4026532247 net 1 83224 uuidd unassigned /usr/sbin/uuidd --socket-activation 4026532317 net 4 129820 65535 0 /run/netns/cni-607c5530-b6d8-ba57-420e-a467d7b10c56 /pause Kubernetes 在创建 pod 时，会先一个 sandbox 容器（使用 pause 镜像，启动时执行 /pause 进入休眠状态）。我们知道 Kubernetes 的 pod 中是允许多容器的，由这个 sandbox 容器来创建和维持网络命名空间，pod 的其他容器会加入到该命名空间中。因为 pause 镜像足够简单，不会出错导致网络管理空间在出错时被删除。sandbox 容器发挥着至关重要的作用，它在 PID 进程空间的进程树中作为 PID 为 1 的进程，其他容器进程都将其作为父进程。当其他容器的进程成为孤儿进程时，可以得到清理。\n创建 Sandbox 容器 CRI 的 RuntimeServiceServer 定义了运行时对外提供的服务接口，除了管理 sandbox、容器相关的操作外，还有 streaming 相关的操作，即常用的 exec、attach、portforward。streaming 相关的内容，可以参考之前的一篇 《源码解析 kubectl port-forward 工作原理》。\n让我们来看容器相关的部分。\nContainerd 的 criService 实现了 RuntimeServiceServer 的接口。创建 sandbox 容器的请求通过 CRI 的 UDS（Unix domain socket） 接口 /runtime.v1.RuntimeService/RunPodSandbox，进入到 criService 的处理流程中。在 criService#RunPodSandbox()，负责创建和运行 sandbox 容器，并保证容器状态正常。\n 容器运行时首先初始化容器对象，产生必要的参数 CNI_CONTAINERID 会创建 pod 网络命名空间，产生必要的参数 CNI_NETNS 然后调用 CNI 的接口来对 pod 的网络空间进行配置，比如创建网络接口、分配 IP 地址、创建 veth、设置路由等等一系列的操作。这些操作正是由具体的网络插件实现完成，不同插件之间的实现存在差异。了解了规范之后之后，网络的配置就不难了，其中 2 和 3 可能执行多次：  读取网络配置 查找二进制文件 执行二进制文件 向容器运行时反馈结果   最后便是创建 sandbox 容器，这个过程与操作系统的类型相关，会调用对应操作系统的方法来完成容器的创建。  从零开始学习容器，推荐阅读 Ivan Velichko 的 《Learning Containers From The Bottom Up》\n参考源码：  pkg/cri/server/sandbox_run.go:61 pkg/cri/server/sandbox_run.go:422  创建其他容器 接下来就是创建 pod 内的其他容器：临时（ephemeral）、初始化（init）和普通容器，创建这些容器的时候，会将 sandbox 容器的。会加入到 sandox 的网络命名空间中。这里不展开，详细逻辑可参考 containerd 的 containerStore#Create()。\n参考源码  kubernetes：pkg/kubelet/kuberuntime/kuberuntime_manager.go:913 containerd：pkg/cri/server/container_create.go:51  总结 接着上篇 CNI 的规范介绍，这次又介绍了 CNI 的使用，以及如何与容器运行时的交互、Pod 的创建流程。\n不同的 CNI 插件，实现了不一样的网络功能。下篇，将以 Flannel 为例来了解下 CNI 的实现，以及 Kubernetes VXLAN 网络。\n为什么介绍 flannel？因为我常用的开发环境之一 k3s 默认就用的 flannel 网络。另一个开发环境是 k8e ，k8e 默认用的是 Cilium，cilium 的 cni 也是系列的文章之一。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/08/pexelsthomgonzalez13019937.jpg","permalink":"https://atbug.com/how-kubelete-container-runtime-work-with-cni/","tags":["Kubernetes","容器","CNI","CRI"],"title":"源码解析：从 kubelet、容器运行时看 CNI 的使用"},{"categories":["笔记"],"contents":"写在最前，周末写到这篇的时候我就发现可能是给自己挖了很大的坑，整个 Kubernetes 网关相关的内容会非常复杂且庞大。\n 深入探索 Kubernetes 网络模型和网络通信 认识一下容器网络接口 CNI（本篇） 源码分析：从 kubelet、容器运行时看 CNI 的使用 从 Flannel 学习 Kubernetes VXLAN 网络 Kubernetes 网络学习之 Cilium 与 eBPF \u0026hellip;  看自己能学到哪一步~\n 在 《深入探索 Kubernetes 网络模型和网络通信》 文章中，我们介绍了网络命名空间（network namespace） 如何在 Kubernetes 网络模型中工作，通过示例分析 pod 间的流量传输路径。整个传输过程需要各种不同组件的参与才完成，而这些组件与 pod 相同的生命周期，跟随 pod 的创建和销毁。容器的维护由 kubelet 委托给容器运行时（container runtime）来完成，而容器的网络命名空间则是由容器运行时委托网络插件共同完成。\n 创建 pod（容器）的网络命名空间 创建接口 创建 veth 对 设置命名空间网络 设置静态路由 配置以太网桥接器 分配 IP 地址 创建 NAT 规则 \u0026hellip;  上篇我们也提到不同网络插件对 Kubernetes 网络模型有不同的实现，主要集中在跨节点的 pod 间通信的实现上。用户可以根据需要选择合适的网络插件，这其中离不开 CNI（container network interface）。这些网络插件都实现了 CNI 标准，可以与容器编排系统和运行时良好的集成。\nCNI 是什么 CNI 是 CNCF 下的一个项目，除了提供了最重要的 规范 、用来 CNI 与应用集成的 库、实行 CNI 插件的 CLI cnitool，以及 可引用的插件。本文发布时，最新版本为 v1.1.2。\nCNI 只关注容器的网络连接以及在容器销毁时清理/释放分配的资源，也正因为这个，即使容器发展迅速，CNI 也依然能保证简单并被 广泛支持。\nCNI 规范 CNI 的规范涵盖了以下几部分：\n 网络配置文件格式 容器运行时与网络插件交互的协议 插件的执行流程 将委托其他插件的执行流程 返回给运行时的执行结果数据类型  1. 网络配置格式 这里贴出规范中的配置示例，规范 中定义了网络配置的格式，包括必须字段、可选字段以及各个字段的功能。示例使用定义了名为 dbnet 的网络，配置了插件 bridge 和 tuning，这两个插件。\nCNI 的插件一般分为两种：\n 接口插件（interface plugin）：用来创建网络接口，比如示例中的 bridge。 链式插件（chained）：用来调整已创建好的网络接口，比如示例中的 tuning。  { \u0026#34;cniVersion\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;dbnet\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, // plugin specific parameters \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;keyA\u0026#34;: [\u0026#34;some more\u0026#34;, \u0026#34;plugin specific\u0026#34;, \u0026#34;configuration\u0026#34;], \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, // ipam specific \u0026#34;subnet\u0026#34;: \u0026#34;10.1.0.0/16\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.1.0.1\u0026#34;, \u0026#34;routes\u0026#34;: [ {\u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;} ] }, \u0026#34;dns\u0026#34;: { \u0026#34;nameservers\u0026#34;: [ \u0026#34;10.1.0.1\u0026#34; ] } }, { \u0026#34;type\u0026#34;: \u0026#34;tuning\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;mac\u0026#34;: true }, \u0026#34;sysctl\u0026#34;: { \u0026#34;net.core.somaxconn\u0026#34;: \u0026#34;500\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true} } ] } 2. 容器运行时与网络插件交互的协议 CNI 为容器运行时提供 四个不同的操作：\n ADD - 将容器添加到网络，或修改配置 DEL - 从网络中删除容器，或取消修改 CHECK - 检查容器网络是否正常，如果容器的网络出现问题，则返回错误 VERSION - 显示插件的版本  规范对操作的输入和输出内容进行了定义。主要几个核心的字段有：\n CNI_COMMAND：上面的四个操作之一 CNI_CONTAINERID：容器 ID CNI_NETNS：容器的隔离域，如果用的网络命名空间，这里的值是网络命名空间的地址 CNI_IFNAME：要在容器中创建的接口名，比如 eth0 CNI_ARGS：执行参数时传递的参数 CNI_PATH：插件可执行文件的朝招路径  3. 插件的执行流程 CNI 将容器上网络配置的 ADD、DELETE 和 CHECK 操作，成为附加（attachment）。\n容器网络配置的操作，需要一个或多个插件的共同操作来完成，因此插件有一定的执行顺序。比如前面的示例配置中，要先创建接口，才能对接口进行调优。\n拿 ADD 操作为例，首先执行的一般是 interface plugin，然后在执行 chained plugin。以前一个插件的 输出 PrevResult 与下一个插件的配置会共同作为下一个插件的 输入。 如果是第一个插件，会将网络配置作为输入的一部分。插件可以将前一个插件的 PrevResult 最为自己的输出，也可以结合自身的操作对 PrevResult 进行更新。最后一个插件的输出 PrevResult 作为 CNI 的执行结果返回给容器运行时，容器运行时会保存改结果并将其作为其他操作的输入。\nDELETE 的执行与 ADD 的顺序正好相反，要先移除接口上的配置或者释放已经分配的 IP，最后才能删除容器网络接口。DELETE 操作的输入就是容器运行时保存的 ADD 操作的结果。\n除了定义单次操作中插件的执行顺序，CNI 还对操作的并行操作、重复操作等进行了说明。\n4. 插件委托 有一些操作，无论出于何种原因，都不能合理地作为一个松散的链接插件来实现。相反，CNI 插件可能希望将某些功能委托给另一个插件。一个常见的例子是 IP 地址管理（IP Adress Management，简称 IPAM），主要是为容器接口分配/回收 IP 地址、管理路由等。\nCNI 定义了第三种插件 \u0026ndash; IPAM 插件。CNI 插件可以在恰当的时机调用 IPAM 插件，IPAM 插件会将执行的结果返回给委托方。IPAM 插件会根据指定的协议（如 dhcp）、本地文件中的数据、或者网络配置文件中 ipam 字段的信息来完成操作：分配 IP、设置网关、路由等等。\n\u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, // ipam specific \u0026#34;subnet\u0026#34;: \u0026#34;10.1.0.0/16\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.1.0.1\u0026#34;, \u0026#34;routes\u0026#34;: [ {\u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;} ] } 5. 执行结果 插件可以返回一下三种结果之一，规范对 结果的格式 进行了定义。\n Success：同时会包含 PrevResult 信息，比如 ADD 操作后的 PrevResult 返回给容器运行时。 Error：包含必要的错误提示信息。 Version：这个是 VERSION 操作的返回结果。  库 CNI 的库是指 libcni，用于 CNI 和应用程序集成，定义了 CNI 相关的接口和配置。\ntype CNI interface { AddNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) CheckNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error DelNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error GetNetworkListCachedResult(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) GetNetworkListCachedConfig(net *NetworkConfigList, rt *RuntimeConf) ([]byte, *RuntimeConf, error) AddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error) CheckNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error DelNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error GetNetworkCachedResult(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) GetNetworkCachedConfig(net *NetworkConfig, rt *RuntimeConf) ([]byte, *RuntimeConf, error) ValidateNetworkList(ctx context.Context, net *NetworkConfigList) ([]string, error) ValidateNetwork(ctx context.Context, net *NetworkConfig) ([]string, error) } 以添加网络的部分代码为例：\nfunc (c *CNIConfig) addNetwork(ctx context.Context, name, cniVersion string, net *NetworkConfig, prevResult types.Result, rt *RuntimeConf) (types.Result, error) { ... return invoke.ExecPluginWithResult(ctx, pluginPath, newConf.Bytes, c.args(\u0026#34;ADD\u0026#34;, rt), c.exec) } 执行的逻辑简单来说就是：\n 查找可执行文件 加载网络配置 执行 ADD 操作 结果处理  总结 这篇学习了 CNI 规范的内容、网络插件的执行流程，对 CNI 抽象的网络管理接口有了大致的了解。\n下一篇将结合源码的分析，了解 kubelet、容器运行时、CNI 网络插件之间如何进行交互。\n参考  https://www.tigera.io/learn/guides/kubernetes-networking/kubernetes-cni/ https://github.com/containernetworking/cni https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/06/pexelsquangnguyenvinh6346488.jpg","permalink":"https://atbug.com/deep-dive-cni-spec/","tags":["Kubernetes","CNI","网络","容器"],"title":"认识一下容器网络接口 CNI"},{"categories":["笔记"],"contents":"这是 Kubernetes 网络学习的第一篇笔记。\n 深入探索 Kubernetes 网络模型和网络通信（本篇） 认识一下容器网络接口 CNI 源码分析：从 kubelet、容器运行时看 CNI 的使用 从 Flannel 学习 Kubernetes VXLAN 网络 Kubernetes 网络学习之 Cilium 与 eBPF  Kubernetes 定义了一种简单、一致的网络模型，基于扁平网络结构的设计，无需将主机端口与网络端口进行映射便可以进行高效地通讯，也无需其他组件进行转发。该模型也使应用程序很容易从虚拟机或者主机物理机迁移到 Kubernetes 管理的 pod 中。\n这篇文章主要深入探索 Kubernetes 网络模型，并了解容器、pod 间如何进行通讯。对于网络模型的实现将会在后面的文章介绍。\nKubernetes 网络模型 该模型定义了：\n 每个 pod 都有自己的 IP 地址，这个 IP 在集群范围内可达 Pod 中的所有容器共享 pod IP 地址（包括 MAC 地址），并且容器之前可以相互通信（使用 localhost） Pod 可以使用 pod IP 地址与集群中任一节点上的其他 pod 通信，无需 NAT Kubernetes 的组件之间可以相互通信，也可以与 pod 通信 网络隔离可以通过网络策略实现  上面的定义中提到了几个相关的组件：\n Pod：Kubernetes 中的 pod 有点类似虚拟机有唯一的 IP 地址，同一个节点上的 pod 共享网络和存储。 Container：pod 是一组容器的集合，这些容器共享同一个网络命名空间。pod 内的容器就像虚拟机上的进程，进程之间可以使用 localhost 进行通信；容器有自己独立的文件系统、CPU、内存和进程空间。需要通过创建 Pod 来创建容器。 Node：pod 运行在节点上，集群中包含一个或多个节点。每个 pod 的网络命名空间都会连接到节点的命名空间上，以打通网络。  讲了这么多次网络命名空间，那它到底是如何运作的呢？\n网络命名空间如何工作 在 Kubernetes 的发行版 k3s 创建一个 pod，这个 pod 有两个容器：发送请求的 curl 容器和提供 web 服务的 httpbin 容器。\n虽然使用发行版，但是其仍然使用 Kubernetes 网络模型，并不妨碍我们了解网络模型。\napiVersion: v1 kind: Pod metadata: name: multi-container-pod spec: containers: - image: curlimages/curl name: curl command: [\u0026#34;sleep\u0026#34;, \u0026#34;365d\u0026#34;] - image: kennethreitz/httpbin name: httpbin 登录到节点上，通过 lsns -t net 当前主机上的网络命名空间，但是并没有找到 httpbin 的进程。有个命名空间的命令是 /pause，这个 pause 进程实际上是每个 pod 中 不可见 的 sandbox 容器进程。关于 sanbox 容器的作用，将会在下一篇容器网络和 CNI 中介绍。\nlsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531992 net 126 1 root unassigned /lib/systemd/systemd --system --deserialize 31 4026532247 net 1 83224 uuidd unassigned /usr/sbin/uuidd --socket-activation 4026532317 net 4 129820 65535 0 /run/netns/cni-607c5530-b6d8-ba57-420e-a467d7b10c56 /pause 既然每个容器都有独立的进程空间，我们换下命令查看进程类型的空间：\nlsns -t pid NS TYPE NPROCS PID USER COMMAND 4026531836 pid 127 1 root /lib/systemd/systemd --system --deserialize 31 4026532387 pid 1 129820 65535 /pause 4026532389 pid 1 129855 systemd-network sleep 365d 4026532391 pid 2 129889 root /usr/bin/python3 /usr/local/bin/gunicorn -b 0.0.0.0:80 httpbin:app -k gevent 通过进程 PID 129889 可以找到其所属的命名空间：\nip netns identify 129889 cni-607c5530-b6d8-ba57-420e-a467d7b10c56 然后可以在该命名空间下使用 exec 执行命令：\nip netns exec cni-607c5530-b6d8-ba57-420e-a467d7b10c56 ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether f2:c8:17:b6:5f:e5 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.42.1.14/24 brd 10.42.1.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f0c8:17ff:feb6:5fe5/64 scope link valid_lft forever preferred_lft forever 从结果来看 pod 的 IP 地址 10.42.1.14 绑定在接口 eth0 上，而 eth0 被连接到 17 号接口上。\n在节点主机上，查看 17 号接口信息。veth7912056b 是主机根命名空间下的虚拟以太接口（vitual ethernet device），是连接 pod 网络和节点网络的 隧道，对端是 pod 命名空间下的接口 eth0。\nip link | grep -A1 ^17 17: veth7912056b@if2: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default link/ether d6:5e:54:7f:df:af brd ff:ff:ff:ff:ff:ff link-netns cni-607c5530-b6d8-ba57-420e-a467d7b10c56 上面的结果看到，该 veth 连到了个网桥（network bridge）cni0 上。\n 网桥工作在数据链路层（OSI 模型的第 2 层），连接多个网络（可多个网段）。当请求到达网桥，网桥会询问所有连接的接口（这里 pod 通过 veth 以网桥连接）是否拥有原始请求中的 IP 地址。如果有接口响应，网桥会将匹配信息（IP -\u0026gt; veth）记录，并将数据转发过去。\n 那如果没有接口响应怎么办？具体流程就要看各个网络插件的实现了。我准备在后面的文章中介绍常用的网络插件，比如 Calico、Flannel、Cilium 等。\n接下来看下 Kubernetes 中的网络通信如何完成，一共有几种类型：\n 同 pod 内容器间通信 同节点上的 pod 间通信 不同节点上的 pod 间通信  Kubernetes 网络如何工作 同 pod 内的容器间通信 同 pod 内的容器间通信最简单，这些容器共享网络命名空间，每个命名空间下都有 lo 回环接口，可以通过 localhost 来完成通信。\n同节点上的 pod 间通信 当我们将 curl 容器和 httpbin 分别在两个 pod 中运行，这两个 pod 有可能调度到同一个节点上。curl 发出的请求根据容器内的路由表到达了 pod 内的 eth0 接口。然后通过与 eth0 相连的隧道 veth1 到达节点的根网络空间。\nveth1 通过网桥 cni0 与其他 pod 相连虚拟以太接口 vethX 相连，网桥会询问所有相连的接口是否拥有原始请求中的 IP 地址（比如这里的 10.42.1.9）。收到响应后，网桥会记录映射信息（10.42.1.9 =\u0026gt; veth0），同时将数据转发过去。最终数据经过 veth0 隧道进入 pod httpbin 中。\n不同节点的 pod 间通信 跨节点的 pod 间通信会复杂一些，且 不同网络插件的处理方式不同，这里选择一种容易理解的方式来简单说明下。\n前半部分的流程与同节点 pod 间通信类似，当请求到达网桥，网桥询问哪个 pod 拥有该 IP 但是没有得到回应。流程进入主机的路由寻址过程，到更高的集群层面。\n在集群层面有一张路由表，里面存储着每个节点的 Pod IP 网段（节点加入到集群时会分配一个 Pod 网段（Pod CIDR），比如在 k3s 中默认的 Pod CIDR 是 10.42.0.0/16，节点获取到的网段是 10.42.0.0/24、10.42.1.0/24、10.42.2.0/24，依次类推）。通过节点的 Pod IP 网段可以判断出请求 IP 的节点，然后请求被发送到该节点。\n总结 现在应该对 Kubernetes 的网络通信有初步的了解了吧。\n整个通信的过程需要各种组件的配合，比如 Pod 网络命名空间、pod 以太网接口 eth0、虚拟以太网接口 vethX、网桥（network bridge） cni0 等。其中有些组件与 pod 一一对应，与 pod 同生命周期。虽然可以通过手动的方式创建、关联和删除，但对于 pod 这种非永久性的资源会被频繁地创建和销毁，太多人工的工作也是不现实的。\n实际上这些工作都是由容器委托给网络插件来完成的，而网络插件所遵循的规范 CNI（Container Network Interface）。\n网络插件都做了什么？\n 创建 pod（容器）的网络命名空间 创建接口 创建 veth 对 设置命名空间网络 设置静态路由 配置以太网桥接器 分配 IP 地址 创建 NAT 规则 \u0026hellip;  参考  https://www.tigera.io/learn/guides/kubernetes-networking/ https://kubernetes.io/docs/concepts/services-networking/ https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-networking-guide-beginners.html https://learnk8s.io/kubernetes-network-packets  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/04/deepdiveintokuberntesnetwork.jpg","permalink":"https://atbug.com/deep-dive-k8s-network-mode-and-communication/","tags":["Kubernetes","网络","CNI"],"title":"深入探索 Kubernetes 网络模型和网络通信"},{"categories":["云原生"],"contents":"上周在写 K8s 多集群的流量调度 的 demo 部分时需要不停地在多个集群中安装组件、部署应用，或者执行各种命令。当时是通过 Linux shell 脚本并通过工具 kubectx 进行集群的切换，像这样：\n或者这样：\n操作繁琐，很是痛苦。\n今天偶然间发现了一个 kubectl 插件 kubectl foreach ，可以在多个集群（contexts）上执行 kubectl 命令。比如 kubectl foreach cluster-1 cluster-2 -- get po -n kube-system 。\n插件安装和使用很简单，通过 krew 进行安装：\nkubectl krew install foreach 使用也很简单：\nkubectl foreach -h Usage: kubectl foreach [OPTIONS] [PATTERN]... -- [KUBECTL_ARGS...] Patterns can be used to match context names from kubeconfig: (empty): matches all contexts NAME: matches context with exact name /PATTERN/: matches context with regular expression ^NAME: remove context with exact name from the matched results ^/PATTERN/: remove contexts matching the regular expression from the results Options: -c=NUM Limit parallel executions (default: 0, unlimited) -I=VAL Replace VAL occurring in KUBECTL_ARGS with context name -q Disable and accept confirmation prompts ($KUBECTL_FOREACH_DISABLE_PROMPTS) -h/--help Print help Examples: # get nodes on contexts named a b c kubectl foreach a b c -- get nodes # get nodes on all contexts named c0..9 except c1 (note the escaping) kubectl foreach \u0026#39;/^c[0-9]/\u0026#39; ^c1\t-- get nodes # get nodes on all contexts that has \u0026#34;prod\u0026#34; but not \u0026#34;foo\u0026#34; kubectl foreach /prod/ ^/foo/ -- get nodes # use \u0026#39;kubectl tail\u0026#39; plugin to follow logs of pods in contexts named *test* kubectl foreach -I _ /test/ -- tail --context=_ -l app=foo 接下来测试下，使用 k3d 创建 3 个集群 （k3d 貌似不支持同时创建多个集群，还是需要 for 脚本来操作）：\nfor CLUSTER_NAME in cluster-1 cluster-2 cluster-3 do k3d cluster create ${CLUSTER_NAME} \\  --image docker.io/rancher/k3s:v1.23.8-k3s2 \\  --servers-memory 4g \\  --k3s-arg \u0026#34;--disable=traefik@server:0\u0026#34; \\  --no-lb \\  --timeout 120s \\  --wait done 集群安装完成：\nk3d cluster list NAME SERVERS AGENTS LOADBALANCER cluster-1 1/1 0/0 false cluster-2 1/1 0/0 false cluster-3 1/1 0/0 false 注意，k3d 安装的集群的 context 都带有前缀 k3d- ，在使用 kubectl foreach 的时候要注意：\nkubectx k3d-cluster-1 k3d-cluster-2 k3d-cluster-3 比如查看各个集群中的 kube-sysmte 下的 pod：\nkubectl foreach -q k3d-cluster-1 k3d-cluster-2 k3d-cluster-3 -- get po -n kube-system 或者试试创建 deployment，这次我们不列出完整的 context name，而是使用正则 /cluster/：\nkubectl foreach -q /cluster/ -- create deploy pipy --image flomesh/pipy -n default Will run command in context(s): - k3d-cluster-1 - k3d-cluster-2 - k3d-cluster-3 k3d-cluster-1 | deployment.apps/pipy created k3d-cluster-3 | deployment.apps/pipy created k3d-cluster-2 | deployment.apps/pipy created 然后查看下 pod：\nkubectl foreach -q /cluster/ -- get pod -n default Will run command in context(s): - k3d-cluster-1 - k3d-cluster-2 - k3d-cluster-3 k3d-cluster-1 | NAME READY STATUS RESTARTS AGE k3d-cluster-1 | pipy-df659b55f-bnr27 1/1 Running 0 25s k3d-cluster-3 | NAME READY STATUS RESTARTS AGE k3d-cluster-3 | pipy-df659b55f-p9j49 1/1 Running 0 25s k3d-cluster-2 | NAME READY STATUS RESTARTS AGE k3d-cluster-2 | pipy-df659b55f-9bjgf 1/1 Running 0 25s 查看日志\nkubectl foreach -q /cluster/ -- logs -l app=pipy -n default --tail 3 Will run command in context(s): - k3d-cluster-1 - k3d-cluster-2 - k3d-cluster-3 k3d-cluster-2 | 2022-11-30 10:40:56.520 [INF] [listener] Listening on TCP port 8080 at 0.0.0.0 k3d-cluster-2 | 2022-11-30 10:40:56.520 [INF] [listener] Listening on TCP port 8081 at 0.0.0.0 k3d-cluster-2 | 2022-11-30 10:40:56.520 [INF] [listener] Listening on TCP port 8082 at 0.0.0.0 k3d-cluster-1 | 2022-11-30 10:40:56.551 [INF] [listener] Listening on TCP port 8080 at 0.0.0.0 k3d-cluster-1 | 2022-11-30 10:40:56.551 [INF] [listener] Listening on TCP port 8081 at 0.0.0.0 k3d-cluster-1 | 2022-11-30 10:40:56.551 [INF] [listener] Listening on TCP port 8082 at 0.0.0.0 k3d-cluster-3 | 2022-11-30 10:40:55.813 [INF] [listener] Listening on TCP port 8080 at 0.0.0.0 k3d-cluster-3 | 2022-11-30 10:40:55.813 [INF] [listener] Listening on TCP port 8081 at 0.0.0.0 k3d-cluster-3 | 2022-11-30 10:40:55.813 [INF] [listener] Listening on TCP port 8082 at 0.0.0.0 注意，多集群的操作要谨慎，尤其是使用正则来匹配 context name；还有 -q 参数会跳过要操作的集群提醒，直接执行命令。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/11/30/toolsg7ceb2b2061280.jpg","permalink":"https://atbug.com/multi-clusters-operation-with-kubectl-foreach/","tags":["Kubernetes","多集群","工具"],"title":"kubectl foreach 在多个集群中执行 kubectl 命令"},{"categories":["云原生"],"contents":"由于各种原因，采用 Kubernetes 的企业内部存在着几个、几十甚至上百个集群。比如处于研发流程上的考虑，不同环境下都存在独立的集群；监管层面的考虑，就地存储的用户数据需要搭配应用集群；单个集群的容量限制，无法满足业务体量；可用性要求的多云、多地、多中心；Kubernetes 原地升级成本大进而考虑新建集群，等等各种原因。然而，Kubernetes 设计之初并没有考虑多集群。\n这些集群彼此之间看似独立，但又有着千丝万缕的关系。比如高可用性的多集群，实现了集群级的灾备，但集群中的服务如何实现跨集群的故障迁移？\n我们先看下集群内的应用如何对集群外提供服务。由于 Kubernetes 网络隔离特性，存在着天然的网络边界，需要借助某些方案（如 Ingress、NodePort）来将服务暴露到集群外。虽然解决了连通性的问题，但是服务的注册和发现还无法解决。\n通常我们将 Service 作为 Kubernetes 平台的服务注册和发现，今天要介绍的 Multi-Cluster Service（多集群服务 API，简称 MCS API） 则可以看成是 跨 Kubernetes 集群的服务注册发现。\nMCS 介绍 MCS API 来自 Kubernetes Enhancement Proposal KEP-1645: Multi-Cluster Services API 目前还处于提案阶段。\nMCS 的目标是既然多集群不可避免，那就定义一套 API 来实现服务的跨集群注册和发现，并且这套 API 足够轻量级，做到能够像访问本地服务一样访问其他集群的服务。\n注意 MCS API 只提供 API 和实现的设计指导，不提供具体的实现。\n术语  clusterset：集群集，集合内的集群间共享服务。 mcs-controller：多集群服务控制器，在多集群间完成服务的注册和发现。 cluster name：集群的唯一标识，用以标识注册的服务来自哪个集群。  CRD  ServiceExport 定义对集群集中其他集群暴露的服务，与 Service 一样必须创建在与负载相同的命名空间下，且与 Service 同名。 ServiceImport 定义了导入的其他集群的 Service。  type ServiceExport struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Status ServiceExportStatus `json:\u0026#34;status,omitempty\u0026#34;` } type ServiceImport struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec ServiceImportSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status ServiceImportStatus `json:\u0026#34;status,omitempty\u0026#34;` } type ServiceImportSpec struct { Ports []ServicePort `json:\u0026#34;ports\u0026#34;` IPs []string `json:\u0026#34;ips,omitempty\u0026#34;` Type ServiceImportType `json:\u0026#34;type\u0026#34;` SessionAffinity corev1.ServiceAffinity `json:\u0026#34;sessionAffinity\u0026#34;` SessionAffinityConfig *corev1.SessionAffinityConfig `json:\u0026#34;sessionAffinityConfig\u0026#34;` } 跨集群服务注册 如果多个集群都以同样的名字和命名空间对外声明服务，这些服务将被视作单一的组合服务。例如 1 个集群集下有 4 个集群，其中 1 个集群在命名空间 bar 下有个名为 foo 的 Service。其他集群要想使用该集群的 Service，该集群需要对外声明该 Service 是可以跨集群提供服务，完成多集群服务的注册。\n多集群服务的注册通过在 Service 的同命名空间下创建同名的 ServiceExport 资源来完成。ServiceExport 可以手动创建，也可以实现支持服务的自动注册：根据标识将集群内或者指定命名空间下的服务自动注册。\n更多的设计细节，可以参考 Service Exporting 设计细节。\n跨集群服务发现 在 ServiceExport 资源创建完成后，该服务会被集群集下的其他集群“发现”：同名的命名空间下出现同名的 ServiceImport 资源，并且该 ServiceImport 与服务注册的 Endpoint 相关联。\n同样，如果一个服务下线或者停止跨集群服务，删除 ServiceExport 资源后，其他集群中的 ServiceImport 也会相应的地被销毁。ServiceImport 的管理，由 MCS 实现中的 mcs-controller 来完成。\n值得注意的是，关于 ServiceImport 的使用，该标准中并未界定具体的实现细节。由于 Pod 在 Kubernetes 中是非永久性资源，我们访问应用的时候通常使用 Service。多集群服务的理想情况，是在使用 Service 的时候能自动使用其他集群的 Endpoints，做到对应用的无感知。\n一种方案是修改 Service API 的实现（比如 kube-proxy） 进行来使用 ServiceImport。\n另一种方案则是不需要修改现在 Service API 的实现，由 mcs-controller 来创建影子服务，改服务与新创建的 EndpointSlice 关键关联，后者聚合了该跨服务的的所有 Endpoints。一个 ServiceImport 下可能有来自多个集群的 EndpointSlice，每个 EndpointSlice 上有各自集群的标识。\n在官方的设计细节中建议了 ClusterIP 和 DNS 的方案并提供了细节，但不是唯一的实现方案。\n总结 MCS API 希望在 API 的层面提供多集群服务的定义，但由于其实现的复杂性，该提案的进展缓慢。从 2020.2 提出方案，到 2020.8 提供 API 的 alpha 实现之后没有多少进展。\n即便如此，多集群服务的注册和发现机制可以给后面的实现带来一定的参考价值。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/11/27/sunsetg55251915a1280.jpg","permalink":"https://atbug.com/kubernetes-multi-cluster-api/","tags":["Kubernetes","多集群"],"title":"认识一下 Kubernetes 多集群服务 API"},{"categories":["生活"],"contents":"首先这篇文章不讲任何政策的东西，没有对政策和人员的吐槽。相反，整个过程中感受到的都是理解和支持。正如标题所写回家的过程有曲折，但影响不大，而结果让我有点想笑：就这？\n作为一名程序员，有必要对问题做个复盘，摸清整个流程，了解下支撑咱们疫情防控的部分系统情况，进而思考下我们在数字化改造过程中可能有哪些不足。\n背景 上个月底我因公去美国底特律参加了 KubeCon + CloudNativeCon 2022 NA ，从走之前的各种忐忑，到出去之后的坦然，以及最后顺利的满载而归。收获很多，这里暂且不表，后面我可能也抽时间写一下。\n目前国内的入境防疫政策是 7 天的酒店集中隔离以及 3 天的居家健康监测。我在 11 月 2 日落地上海并顺利入住隔离酒店，开始了 7 天的隔离。家人帮忙从居委会咨询了可以回广州居家检测，往返机场两边都会提供封闭转运。因此我选择了在上海隔离 7 天，也早早预定了 7 天后就是 11 月 9 日返穗的机票。在 9 号解离前一天提供居住所在地的接收证明即可。\n各地的政策不同，有的城市是可以提供盖章的证明，有些城市已经完成了无纸化的流程。这就像我们做系统集成的时候，要考虑针对不同系统的情况选择合适的集成方案。\n过程 广州正是完成了数字化的城市之一（其他城市我也不了解），只要在微信小程序 粤省事 上找到 入粤健康申报，按照提示信息如实填写即可。提交之后，申报会派发到目的地所在的居委会，居委会的工作人员根据情况进行审批即可。\n按照我和工作人员的理解，流程就是这样，很简单呐。然而不然，系统会有所差异。\n上面是我经过两次提交申请，并跟工作人员多次沟通后“悟”出的流量。\n起初在第 2 步就遇到了问题，提交健康申报后工作人员没有看到申请，而是过了几个小时候才看到。\n另一个问题就是在第 4 步，工作人员审批完健康申报并通知我，我在小程序中查询的还是待审批的状态。打多方电话无果，取消申请重新提交，然后重新经历前面的等待，并最终又卡在了第 4 步。于是，有了下面的对话。\n A：“之前回国的人员，都是这么处理的。” B：“\u0026hellip;\u0026hellip;” A：“你是不是还提交了一个居家管理条件审核的？” B：“那是什么东西，我没有提交！从哪里提交？” A：“我也不知道，但是里面是你的信息，我审核但是审核不了，因为里面的地址不正确。” B：“能不能给我个截图看看？” (内心：卧槽，肯定是系统自动下发的)\n 于是就有了下面这张图，两个大大的 null 映入眼帘。按照我的理解这种强流程约束的，遇到这种问题很棘手。如果不能紧急发布修复的话，基本上就是手动去修改数据库了。这种时候，不管哪种措施的成本都并不低。\n果不其然，最终居委工作人员告知系统问题暂时解决不了，还是开具了纸质的证明，明天我可以回家了。\n思考 需要再次感谢整个过程中两地工作人员细心且耐心的帮助。\n作为一个从业人员，不得不思考下经常说的“数字化转型”，真正要做的其实并不仅仅在技术上，反而技术之外的才是决定最后能否落地的“最后 1km”。\n 系统的技术 bug 可以通过其他的手段来尽量避免，毕竟不存在没有 bug 的系统。但是当业务数字化之后，由于技术问题阻碍了流程，有没有补救的措施、能否再用非数字化的方式来解决。 人员缺乏培训，缺少必要的用户手册或者手册更新不及时。我不怀疑一线人员的能力，现在居委工作人员年轻化、高学历，从服务态度和专业程度上都很大的提升（我所接触到的，其他地方的不清楚）。这个过程给我的感觉是系统更新了流程，但是一线人员并不清楚，且遇到问题无从下手。  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/11/08/pexelsrobertonickson2559941.jpg","permalink":"https://atbug.com/thoughts-after-encountering-null/","tags":null,"title":"因为 null null 隔离结束的我差点不能回家"},{"categories":["翻译"],"contents":"服务网格以典型的 sidecar 模型为人熟知，将 sidecar 容器与应用容器部署在同一个 Pod 中。虽说 sidecar 并非很新的模型（操作系统的 systemd、initd、cron 进程；Java 的多线程），但是以这种与业务逻辑分离的方式来提供服务治理等基础能力的设计还是让人一亮。\n随着 eBPF 等技术的引入，最近关于服务网格是否需要 sidecar （也就是 sidecarless）的讨论渐增。\n笔者认为任何问题都有其起因，长久困扰服务网格的不外乎性能和资源占用。这篇文章翻译自 Buoyant 的 Flynn 文章 eBPF and the Service Mesh: Don\u0026rsquo;t Dismiss the Sidecar Yet。希望这篇文章能帮助大家穿透迷雾看透事物的本质。\n 本文要点  eBPF 是一个旨在通过（谨慎地）允许在内核中运行一些用户代码来提高性能的工具。 在可预见的未来，服务网格所需的第 7 层处理在 eBPF 中不太可能实现，这意味着网格仍然需要代理。 与 sidecar 代理相比，每个主机代理增加了操作复杂性并降低了安全性。 可以通过更小、更快的 Sidecar 代理来解决有关 Sidecar 代理的典型性能问题。 目前，sidecar 模型对服务网格仍是最有意义的。  关于 eBPF 的故事已经在云原生世界中泛滥了一段时间，有时将其描述为自切片面包以来最伟大的事物，有时则嘲笑它是对现实世界的无用干扰。当然，现实要微妙得多，因此仔细研究一下 eBPF 能做什么和不能做什么似乎是有必要的——技术毕竟只是工具，使用的工具应该适合手头的任务。\n最近经常出现的一项特殊任务是服务网格所需的复杂的第 7 层处理。将其交给 eBPF 可能对服务网格来说是一个巨大的胜利，所以让我们仔细看看 eBPF 可能扮演的角色。\n究竟什么是 eBPF？ 让我们先把这个名字弄清楚：“eBPF”最初是“extended Berkeley Packet Filter”（扩展的伯克利包过滤器），尽管现在它 根本不代表任何东西。Berkeley 数据包过滤器可以追溯到近 30 年前：它是一种允许用户应用程序直接在操作系统内核中运行某些代码（可以肯定是经过严格审查和高度约束的代码）的技术。BPF 仅限于网络堆栈，但它仍然使一些惊人的事情成为可能：\n 典型的例子是，它可以使试验新型防火墙之类的东西变得更加容易。无需不断地重新编译内核模块，只需对 eBPF 代码进行编辑并重新加载。 同样，它可以为轻松开发一些非常强大的网络分析打开大门，包括那些不想在内核中运行的。例如，如果想使用机器学习对传入数据包进行分类，可以使用 BPF 抓取感兴趣的数据包并将它们交给运行 ML 模型的应用程序。  还有其他例子：这只是 BPF 实现的两件非常明显的事情 1 — eBPF 采用了相同的概念并将其扩展到网络以外的领域。但是所有这些讨论都提出了一个问题，即为什么这种事情首先需要特别注意。\n简短的回答是“隔离”。\n隔离 计算——尤其是云原生计算——在很大程度上依赖于硬件同时为多个实体做多项事情的能力，即使其中一些实体对其他实体怀有敌意。这是 竞争性多租户 ，我们通常使用可以调解对内存本身的访问的硬件进行管理。例如，在 Linux 中，操作系统为自己创建一个内存空间（内核空间），并为每个用户程序创建一个单独的空间（用户空间），尽管每个程序都有自己的空间但统称为用户空间。操作系统然后使用硬件来防止任何跨空间访问 2。 保持系统各部分之间的这种隔离对于安全性和可靠性都是非常关键的——基本上 所有 计算安全都依赖于它，事实上，云原生世界更加依赖它，也要保持内核容器之间的隔离。因此，内核开发人员共同花费了数千人年的时间来审查围绕这种隔离的每一次交互，并确保内核都能正确处理。这是一项棘手的、精细的、艰苦的工作，遗憾的是，在发现错误之前常常被忽视，而且它是操作系统实际所做工作的 重要 组成部分 3。\n这项工作如此棘手和精细的部分原因是内核和用户程序不能完全隔离：用户程序显然需要访问某些操作系统功能。从历史上看，这是 系统调用 的领域。\n系统调用 系统调用或 syscall 是操作系统内核向用户代码公开 API 的原始方式。对大量细节进行了修饰，用户代码将请求打包并将其交给内核。内核仔细检查以确保其遵循了所有规则，并且——如果一切看起来正常——内核将代表用户执行系统调用，并根据需要在内核空间和用户空间之间复制数据。关于系统调用的关键是：\n 内核控制着一切。用户代码可以提出请求，而不是要求。 检查、复制数据等需要时间。这使得系统调用比运行普通代码慢，无论是用户代码还是内核代码：是跨越边界的行为让执行变慢。随着时间的推移，事情变得越来越快，但是对于繁忙的系统来说对每个网络数据包进行系统调用是不可能的。  这就是 eBPF 的亮点：无需对每个网络数据包（或跟踪点和其他）进行系统调用，只需将一些用户代码直接放入内核！然后内核可以全速运行它，只有在真正需要时才将数据分发给用户空间。（最近对 Linux 中的用户/内核交互进行了相当多类似的思考，通常效果很好。[io_uring](https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/) 正是该领域的另一个例子。）\n当然，在内核中运行用户代码确实很危险，因此内核花费了大量精力来验证用户代码的实际用途。\neBPF 验证 当用户进程启动时，内核基本上会认为其没问题的并启动运行它。内核在它周围设置了围栏，并且会立即杀死任何试图破坏规则的用户进程，但是用户代码基本上被认为是有权执行的。\n对于 eBPF 代码，没有这样的礼遇。在内核本身中，保护性围栏基本上不存在，并且盲目地认为用户代码是可以安全运行的，这会为每个安全漏洞敞开大门（以及允许错误使整个机器崩溃）。相反，eBPF 代码只有在内核能够果断地证明它是安全的时才能运行。\n证明一个程序是安全的 非常 困难 4。为了使它更易于处理，内核极大地限制了 eBPF 程序可以做什么。例如：\n eBPF 程序不允许阻塞。 他们不允许有无限循环（事实上，直到 最近才允许他们有循环）。 它们不允许超过某个最大尺寸。 验证者必须能够评估所有可能的执行路径。  验证者完全是严苛的，并最终做出决定：它必须如此，以维持我们整个云原生世界所依赖的隔离保证。它还必须在声明程序不安全时报错：如果不能 完全 确定程序是安全的，它就会拒绝。不幸的是，有一些 eBPF 程序是安全的，但是验证者不够聪明，无法通过——如果你处于那个位置，你需要重写程序直到验证者可以接受，或者你将需要修补验证程序并构建自己的内核 5。\n最终结果是 eBPF 是一种 高度 受限的语言。这意味着虽然对每个传入的网络数据包进行简单检查等事情很容易，但在多个数据包之间缓冲数据等看似简单的事情却很难。在 eBPF 中实现 HTTP/2 或终止 TLS 根本不可能：它们太复杂了。\n最后，所有这些带来了问题：将 eBPF 的网络功能应用于服务网格会是什么样子。\neBPF 和服务网格 服务网格必须处理云原生网络的所有复杂性。例如，它们通常必须发起和终止 mTLS、重试失败的请求、透明地将连接从 HTTP/1 升级到 HTTP/2、基于工作负载身份执行访问策略、跨越集群边界发送流量等等。云原生世界还会有更多的复杂性。\n大多数服务网格使用 边车 模型来实现。网格将在自己的容器中运行的代理附加到每个应用程序 pod，并且代理拦截进出应用程序 pod 的网络流量，执行网格功能所需的任何操作。这意味着网格可以处理任何工作负载并且不需要更改应用程序，这对开发人员来说是一个相当大的胜利。这也是平台方面的胜利：他们不再需要依赖应用程序开发人员来实现 mTLS、重试、黄金指标 6 等，因为网格在整个集群中提供了所有这些以及更多。\n另一方面，就在不久前，部署所有这些代理的想法完全是疯狂的，人们仍然担心运行额外容器带来的开销。但是 Kubernetes 使部署变得容易，只要保持代理的轻量级和足够快，它就可以很好地工作。（当然，“轻量级和快速”是主观的。许多网格使用通用 Envoy 代理作为 sidecar；Linkerd 似乎是唯一使用专门构建的轻量级代理的。笔者注：还有 Flomesh 使用 可编程的轻量级代理 Pipy。）\n那么，一个明显的问题是，我们是否可以将 sidecar 中的功能下沉到 eBPF 中，以及这样做是否会有所帮助。在 OSI 第 3 层和第 4 层——IP、TCP 和 UDP——我们已经看到 eBPF 取得了一些明显的成功。例如，eBPF 可以使复杂的动态 IP 路由变得相当简单。它可以进行非常智能的数据包过滤，或进行复杂的监控，并且可以快速且高效地完成所有这些工作。在网格需要与这些层的功能交互的地方，eBPF 似乎肯定可以帮助网格实现。\n然而，OSI 第 7 层情况有所不同。eBPF 的执行环境受到如此严格的限制，以至于 HTTP 和 mTLS 级别的协议 远远 超出了它的能力，至少在今天是这样。鉴于 eBPF 不断发展，也许未来的某个版本可以管理这些协议，但值得记住的是，编写 eBPF 本身就非常困难，调试可能更加困难。许多第 7 层协议是复杂的野兽，在相对宽容的用户空间环境中表现得非常糟糕。即使在 eBPF 的受限环境中可以重写它们，但目前尚不清楚否实用。\n当然，我们可以做的是将 eBPF 与代理配对：将核心低级功能放在 eBPF 中，然后将其与用户空间代码配对以管理复杂的功能。这样我们就有可能在底层获得 eBPF 性能的优势，同时将真正令人讨厌的东西留在用户空间中。这实际上是当今每个现存的“eBPF 服务网格”所做的，尽管它通常没有被广泛宣传。\n这引发了新的问题：这样的代理应该在哪？\n主机级代理与边车 与其在 sidecar 模型中那样在每个应用程序 pod 上部署一个代理，不如考虑为每个主机（或者，用 Kubernetes 来说，每个节点）部署一个代理。它为管理 IP 路由的方式增加了一点复杂性，但乍一看似乎提供了一些经济优势，因为需要代理少了。\n然而，sidecar 比主机级代理有一些显著的好处。这是因为 sidecar 就像应用程序的一部分一样，而不是独立于应用程序：\n Sidecar 资源占用与应用程序负载成正比，因此如果应用程序没有做太多事情，sidecar 的资源占用将保持在较低水平 7。当应用程序承受大量负载时，Kubernetes 的所有现有机制（资源请求和限制、OOMKiller 等）都会完全按照习惯的方式工作。 如果 sidecar 发生故障，它只会影响一个 pod，并且现有的 Kubernetes 的机制会响应 pod 故障使其再次正常工作。 sidecar 操作与应用程序 pod 操作基本相同。例如，通过正常的 Kubernetes 滚动重启升级到新版本的 sidecar。 sidecar 与它的 pod 具有完全相同的安全边界：相同的安全上下文、相同的 IP 地址等。例如，它只需要为它的 pod 做 mTLS，这意味着它只需要那个单一 pod 的密钥数据。如果代理中存在错误，它只能泄漏该单个密钥。  对于主机级代理，所有这些事情都会消失。请记住，在 Kubernetes 中，集群调度程序决定将哪些 pod 调度到给定节点上，这意味着每个节点都可以获得一组随机的 pod。这意味着给定的代理将与应用程序 完全 解耦，这很重要：\n 实际上不可能推断单个代理的资源使用情况，因为它将由到应用程序 pod 随机子集的随机的流量子集决定。反过来，这意味着代理最终会因为一些难以理解的原因而失败，而网格团队将承担责任。 应用程序突然更容易受到 嘈杂邻居 的影响，因为给定主机上调度的每个 pod 的流量都必须流经单个代理。一个高流量的 pod 可能会完全消耗该节点的所有代理资源，让所有其他 pod 都饿死。代理可以尝试确保公平处理，但如果高流量 Pod 也消耗了所有节点的 CPU，这也会失败。 如果代理失败，它会影响应用程序 pod 的随机子集——并且该子集将不断变化。同样，尝试升级代理将影响类似随机的、不断变化的应用程序 pod 子集。任何故障或维护任务都会突然产生不可预知的副作用。 代理现在会跨越节点上调度的每个应用程序 pod 的安全边界，这比仅仅耦合到单个 pod 复杂得多。例如，mTLS 需要为每个调度的 pod 保存密钥，而不是混淆哪个密钥与哪个 pod 一起使用。代理中的任何错误都是更可怕的事情。  基本上，sidecar 使用容器模型来发挥其优势：内核和 Kubernetes 努力在容器级别强制执行隔离和公平，且正常。主机级代理超出了该模型，这意味着它们必须自己解决所有竞争性多租户问题。\n主机级代理确实有优势。首先，在 sidecar 世界中，从一个 Pod 到另一个 Pod 总是两次通过代理；在主机的世界中，有时它只有一个跳跃 8，这可以减少一点延迟。此外，最终可以运行更少的代理，如果代理在空闲时资源使用率很高，则可以节省资源消耗。然而，与运营和安全问题的成本相比，这些改进相当小，而且它们在很大程度上可以通过使用更小、更快、更简单的代理来缓解。\n我们是否还可以通过改进代理以更好地处理竞争性多租户来缓解这些问题？也许。这种方法有两个主要问题：\n 竞争多租户是一个安全问题，最好使用更小、更简单、更易于调试的代码来处理安全问题。添加大量代码以更好地处理竞争性多租户基本上与安全最佳实践截然相反。 即使安全问题可以完全解决，操作问题仍然存在。每当我们选择进行更复杂的操作时，我们都应该问为什么，以及谁受益。  总体而言，这些类型的代理更新可能需要大量工作 9，这引发了有关进行这项工作的价值的真正问题。\n把一切都绕回来，让我们回顾一下我们最初的问题：将服务网格功能下沉到 eBPF 会是什么样子？我们知道我们需要一个代理来维护我们需要的第 7 层功能，并且我们进一步知道 sidecar 代理可以在操作系统的隔离保护范围内工作，其中主机级的代理必须自己管理所有内容。这不是一个小区别：主机级代理的潜在性能优势根本不会超过额外的安全问题和操作复杂性，因此无论是否涉及 eBPF，我们都将 sidecar 作为最可行的选择。\n展望未来 显而易见，任何服务网格的首要任务都必须是用户的操作体验。我们可以在哪里使用 eBPF 来获得更高的性能和更低的资源使用，太棒了！但是我们需要注意不要在这个过程中牺牲用户体验。\neBPF 最终会能够覆盖服务网格的全部范围吗？不太可能。如上所述，非常不清楚在 eBPF 中实现所有需要的第 7 层处理是否可行，即使在某些时候它确实有可能。同样，可能还有一些其他机制可以将这些 L7 功能转移到内核中——不过，从历史上看，这方面并没有很大的推动力，也不清楚是什么真正使这种能力引人注目。（请记住，将功能移入内核意味着移除我们为确保用户空间安全而依赖的围栏。）\n那么，在可预见的未来，服务网格的最佳前进方向似乎是积极寻找依赖 eBPF 来提高性能的地方，但接受用户空间 sidecar 的需求代理，并加倍努力使代理尽可能小、快速和简单。\n脚注 关于作者 Flynn 是 Buoyant 的技术布道者，主要关注 Linkerd 服务网格、Kubernetes 和云原生开发。他还是 Emissary-ingress API 网关的原作者和维护者，并且在通信和安全方面的软件工程领域投入了数十年的时间。\n   或者，至少，要容易得多。 \u0026#x21a9;\u0026#xfe0e;\n  至少，不是没有节目之间的预先安排。这超出了本文的范围。 \u0026#x21a9;\u0026#xfe0e;\n  其余的大部分是调度。 \u0026#x21a9;\u0026#xfe0e;\n 事实上，在一般情况下是不可能的。如果你想清理你的 CS 课程作业，首先要解决的是停顿问题。 \u0026#x21a9;\u0026#xfe0e;\n 其中一件事可能比另一件事更容易。特别是如果您想让您的验证程序补丁被上游接受！ \u0026#x21a9;\u0026#xfe0e;\n 流量、延迟、错误和饱和度。 \u0026#x21a9;\u0026#xfe0e;\n 再次假设，一个足够轻的边车。 \u0026#x21a9;\u0026#xfe0e;\n 不过，有时它仍然是两个，所以这有点喜忧参半。 \u0026#x21a9;\u0026#xfe0e;\n 例如，有一个有趣的 twitter 帖子 说要为 Envoy 做到这一点有多难。 \u0026#x21a9;\u0026#xfe0e;\n   ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/10/31/pexelspixabay163323.jpg","permalink":"https://atbug.com/translate-ebpf-service-mesh/","tags":["Service Mesh","eBPF","sidecar"],"title":"【译】eBPF 和服务网格：还不能丢掉 Sidecar"},{"categories":["云原生"],"contents":"背景 在《SMI 与 Gateway API 的 GAMMA 倡议意味着什么？》的一文中，介绍过 Kubernetes Gateway API 处理东西向（网格）流量的可能性。让我们重新看下 Gateway API 中对流量定义（路由）的定义，以 HTTP 路由为例：\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: my-route namespace: gateway-api-example-ns2 spec: parentRefs: - kind: Gateway name: foo-gateway namespace: gateway-api-example-ns1 hostnames: - foo.example.com rules: - backendRefs: - name: foo-svc port: 8080 上面的 YAML 中，使用 parentRefs 字段将路由策略附加到了网关资源 foo-gateway 上。网关 foo-gateway 会根据请求头部的 HOST 来选择合适的 HTTPRoute，然后将流量代理到上游 Service foo-svc。简单来说，就是定义了“网关 -\u0026gt; 路由 -\u0026gt; 后端”的映射。\n通过对流量的定义以及处理流量的资源的选择，实现 7 层 HTTP 协议的反向代理和负载均衡。但在实际的场景下，简单的代理路由策略还并不够，可能还需要配合治理策略来提升服务的健壮性，比如限流、重试、超时、熔断、自定义健康检查等等。\n我们看下 Istio 的超时配置，首先使用了 reviews 进行流量的匹配，然后再根据路由的策略 uri 前缀 /review 进行路由匹配，将流量代理到目的地 v2 的 reviews。与此同时，该路由的超时为 0.5s。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: - uri: prefix: \u0026#34;/review\u0026#34; route: - destination: host: reviews subset: v2 timeout: 0.5s 在这段定义中耦合了流量匹配、路由匹配、后端指定、超时，如果需要配置重试呢，再加个 retries 字段。如此一来，灵活性大大折扣。\n目标 Kubernetes Gateway API 的 策略附件（Policy Attachment） 的设计目标：\n 策略和资源关联标准化 支持策略的默认值和覆盖值 不同资源上策略生效的优先级  为资源施加策略的时候，既要考虑通用性（标准化）又要考虑灵活性（策略值优先级）。\n这里说的资源可以是网关相关的任何资源，比如 GatewayClass、Gateway、路由（如 HTTPRoute、TCPRoute 、GRPCRoute）、Kubernetes 核心资源（如 Namespace、Service ）。后面几种资源还会分命名空间，比如是请求发起方（Gateway、服务消费方）或者请求接收方（服务提供方）的命名空间。\n这些资源的影响范围各有不同，我们暂且称为粒度，比如 GatewayClass 影响某类网关实现所有实例上的流量；Gateway 资源则特指进入某个网关实例的流量；Namespace 特指由该命名空间下的资源发出或者接收的流量；路由则特指具有某种特征的流量，并且路由的定义可以定义在发起方的命名空间，也可以定义在接收方的命名空间。\n可见虽然策略是附加在资源上，但作用的最终目标还是流量：南北向流量和东西向流量，分别对应入口策略和网格策略。\n入口策略 网格策略 设计 策略与资源关联标准化 策略与资源关联的设计引入了 Target Reference API 的概念，即每个策略上必须要有一个 targetRef 字段，通过该字段将资源与策略进行关联，完成解耦。\napiVersion: networking.example.net/v1alpha1 kind: TimeoutPolicy metadata: name: foo spec: default: connectionTimeout: 3 readTimeout: 5 targetRef: kind: HTTPRoute name: example targetRef 字段一次只能指定一个资源，但是每个策略配置配置不同粒度的资源。比如上面的重试策略，附加在同命名空间下名为 example 的 HTTPRoute 上。将所有具有该特征的流量，连接超时为 3，读取超时为 5。\ntargetRef 字段的结构固定的，由 Gateway API 的 PolicyTargetReference 进行定义，包含了资源的最最基础信息。\ntype PolicyTargetReference struct { Group Group `json:\u0026#34;group\u0026#34;` Kind Kind `json:\u0026#34;kind\u0026#34;` Name ObjectName `json:\u0026#34;name\u0026#34;` Namespace *Namespace `json:\u0026#34;namespace,omitempty\u0026#34;` } 默认值和覆盖值 默认值体现的是缺省状态下的策略设置，比如前面具有 HTTPRoute example 描述的特征的流量，通过 spec.default 配置了默认超时时间。\n下面的策略，则使用 spec.override 对所有进入 Gateway 资源 example（网关实例）的流量连接超时为 5。\n那最终哪个值会生效呢？我们可以这样理解，所有的覆盖值的优先级会高于默认值。也就是符合匹配策略条件的流量，会优先使用覆盖值。\napiVersion: networking.example.net/v1alpha1 kind: TimeoutPolicy metadata: name: foo spec: override: connectionTimeout: 5 targetRef: kind: Gateway name: example 策略的优先级 默认值和覆盖值的优先级已经有了，上面在介绍资源的时候提到了资源的级别，并且策略也可以附加到不通的资源上。比如下面的策略定义了读取超时的默认值以及连接超时的覆盖值，附加到了 Service example 上，实际作用的资源是后端（Backend）工作负载。\napiVersion: networking.example.net/v1alpha1 kind: TimeoutPolicy metadata: name: foo spec: override: connectionTimeout: 1 default: readTimeout: 10 targetRef: kind: Service name: example 流量的路径是 Gateway example -\u0026gt; HTTPRoute example -\u0026gt; Backend example，\n对于 覆盖值，优先级与资源的级别相同；而 默认值，则与资源的级别正好相反。因此，最终的生效设置是：\nconnectionTimeout: 5 # from gateway override readTimeout: 10 # from backend default 现状 目前 Policy Attachment 只是提供了概念的设计，还有很多方面暂未完善，而且具体的处理细节也依赖各个实现的设计。比如如何为外部资源（服务）配置策略、同一资源上的策略优先级、如何为多种资源配置相同的策略等等，同时 Policy Attachment 并未提供各种策略的 API。\n截止文章发出时，了解到的 Linkerd 在 1.12 中参考 Policy Attachment 的设计实现了 AuthorizationPolicy，并计划待 Gateway API 中的设计稳定后切换到 Gateway API 的 CRD。\n如果你了解有其他的 Policy Attachment 实现，可以在下面留言。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/10/30/squashed.jpg","permalink":"https://atbug.com/explore-k8s-gateway-api-policy-attachment/","tags":["Kubernetes","网关","Service Mesh"],"title":"一文搞懂 Kubernetes Gateway API 的 Policy Attachment"},{"categories":["笔记"],"contents":"之前介绍过一些 Ingress 使用，比如 Ingress SSL 透传、Ingress 的多租户。从 Demo 看起来是创建 Ingress 之后，就能从集群外访问服务了。实际上除了 Ingress 的作用以外，还有 Kubernetes Service 和负载均衡器（Load Balancer）参与（当 Service 类型为 LoadBalancer 时）。\n这篇文章就来介绍了 Kubernetes LoadBalancer Service 和两个比较典型的负载均衡器的工作原理。\nLoadBalancer Service Service 是 Kubernetes 中的资源类型，用来将一组 Pod 的应用作为网络服务公开。每个 Pod 都有自己的 IP，但是这个 IP 的生命周期与 Pod 生命周期一致，也就是说 Pod 销毁后这个 IP 也就无效了（也可能被分配给其他的 Pod 使用）。而 Service 的 IP（ClusterIP） 则是在创建之后便不会改变，Service 与 Pod 之前通过 userspace 代理、iptables 和 ipvs 代理 等手段关联。\nLoadBalancer 是 Service 四种类型中的一种，其他三种是 ClusterIP、NodePort、ExternalName。\nLoadBalancer 的工作需要搭配第三方的负载均衡器来完成。当我们安装 Ingress 控制器时，会创建一个类型为 LoadBalancer 的 Service。新创建的 Service 的 EXTERNAL-IP 状态是 pending，假如没有负载均衡器的话，会一直处于 pending 状态：\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fsm-ingress-pipy-controller LoadBalancer 10.43.188.208 \u0026lt;pending\u0026gt; 80:30508/TCP 8s 在 Demo 中用的是 K3s（默认提供了负载均衡器），这个 Service 会获得 EXTERNAL-IP，192.168.1.12 和 192.168.1.13 是集群中两个节点的 IP，当我们使用该 IP 地址访问时，流量会进入到 Ingress 控制器的 pod，然后路由到配置对应的 pod 中。\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) fsm-ingress-pipy-controller LoadBalancer 10.43.188.208 192.168.1.12,192.168.1.13 80:30508/TCP 47s 如何实现的呢？\nK3s 服务端的 controller 运行时会监听集群中 Service 的变更，如果 Service 类型是 LoadBalancer 便为其创建一个 Daemonset ，传入 Service 的 ClusterIP、Protocol、Port 等信息。剩下的工作就由这个 Daemonset 来完成了。\n而这个 Daemonset 使用的是 klipper-lb 的镜像。\nKlipper LB klipper-lb 是 K3s 的一个开源负载均衡器的实现，实现很简单：使用主机端口（pod.spec.containers.ports.hostPort 与 Service 的端口号相同）来接收流量，并使用 iptables 将接收的流量转发到 Service 的 ClusterIP。\nPod 启动时使用通过环境变量注入的 Service 的信息，创建 iptables 的 NAT 规则。\n代码量很少：\n#!/bin/sh set -e -x trap exit TERM INT for dest_ip in ${DEST_IPS} do if echo ${dest_ip} | grep -Eq \u0026#34;:\u0026#34; then if [ `cat /proc/sys/net/ipv6/conf/all/forwarding` != 1 ]; then exit 1 fi ip6tables -t nat -I PREROUTING ! -s ${dest_ip}/128 -p ${DEST_PROTO} --dport ${SRC_PORT} -j DNAT --to [${dest_ip}]:${DEST_PORT} ip6tables -t nat -I POSTROUTING -d ${dest_ip}/128 -p ${DEST_PROTO} -j MASQUERADE else if [ `cat /proc/sys/net/ipv4/ip_forward` != 1 ]; then exit 1 fi iptables -t nat -I PREROUTING ! -s ${dest_ip}/32 -p ${DEST_PROTO} --dport ${SRC_PORT} -j DNAT --to ${dest_ip}:${DEST_PORT} iptables -t nat -I POSTROUTING -d ${dest_ip}/32 -p ${DEST_PROTO} -j MASQUERADE fi done if [ ! -e /pause ]; then mkfifo /pause fi \u0026lt;/pause 那除了 klipper-lb 以外，今天介绍下另一种负载均衡器 metallb。\nMetal LB MetalLB 是裸机 Kubernetes 集群的负载均衡器实现，使用标准路由协议。\n 注意： MetalLB 目前还是 beta 阶段。\n  在 Kubernetes 集群中使用 MetalLB 作为 LoadBalancer（上）- Layer2 在 Kubernetes 集群中使用 MetalLB 作为 LoadBalancer（下）- BGP  关于 Metal LB，在上面的文章介绍过。其有两种工作模式，这里通过 BGP 的工作模式来进行说明。\n与 klipper-lb 不同的时，我们需要预先配置好可用的 IP 地址段。当创建 LoadBalancer 类型的 Service 时，metallb 会从地址池中为其分配 IP 地址。\n以 Daemonset 方式运行在各个节点的 speaker 组件会监听 Service 的变更，如果该 Service 已经分配了 IP 地址（service.status.loadBalancer.ingress），speaker 在完成一些列的校验工作后会向路由器声明该地址。\n路由接收到 speaker 的广播后，会更新路由表：使用 speaker 的节点进行该 IP 的路由。\n当请求该 IP 地址时，因为无法找到 IP 地址对应的 MAC 地址（没有 ARP 广播），便将请求转给路由器。路由器根据路由表中的记录进行路由，转发到对应的节点上。后续的流程，就是 iptables/kubeproxy 的工作了。\n可能你也看出来，如果目标 pod 不在该节点上 ，还会再有一次转发。这种情况可以通过设置 Service 的 externalTrafficPolicy 来避免。\n如果 service.sepc.externalTrafficPolicy 设置为 Local，只有 Service 的 Endpoints 所在的节点，才会参与广播和路由；否则，所有的节点都会参与广播和路由。\n总结 由于篇幅的原因，本文只介绍了两种不同负载均衡器的工作原理，没有深入探索其实现细节。不同云提供商的负载均衡器的实现可能各不相同，不同实现间也各有优缺点。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/09/30/redlighthouseg6a13100ad1280.jpg","permalink":"https://atbug.com/k8s-service-and-load-balancer/","tags":["Kubernetes","网络"],"title":"Kubernetes LoadBalancer Service 与负载均衡器"},{"categories":["教程"],"contents":"金丝雀发布是服务治理中的重要功能，在发布时可以可控地将部分流量导入新版本的服务中；其余的流量则由旧版本处理。发布过程中，可以逐步增加新版本服务的流量。通过测试，可以决定是回滚还是升级所有实例，停用旧版本。\n有了金丝雀发布，使用真实流量对服务进行测试，通过对流量的控制可以有效的降低服务发布的风险。\n本文将介绍如何将使用 Argo Rollouts 和服务网格 osm-edge 来进行应用的自动、可控的金丝雀发布，不涉及工作原理的分析。对工作原理有兴趣的同学可以留言，可以再做一篇原理的介绍。\nArgo Rollouts Argo Rollouts 包括一个 Kubernetes控制器 和一组 CRD，提供如蓝绿色、金丝雀、金丝雀分析、体验等高级部署功能和 Kubernetes 的渐进交付功能。\nArgo Rollouts 与 入口控制器 和服务网格集成，利用其流量管理能力，在发布期间逐步将流量转移到新版本。此外，Rollouts 可以查询和解析来自各种提供商的指标，以验证关键 KPI，并在更新期间推动自动推进或回滚。\nArgo Rollouts 支持服务网格标准 SMI（Service Mesh Interface） 的 TrafficSplit API，通过 TrafficSplit API 的使用来控制金丝雀发布时的流量控制。\n服务网格 osm-edge 服务网格是处理服务间网络通信的基础设施组件，旨在从平台层面提供可观性、安全以及可靠性特性，以进程外的方式提供原本由部分应用层逻辑承载的基础能力，真正实现与业务逻辑的分离。\nosm-edge 是面向云边一体的轻量化服务网格，采用实现了服务网格标准 SMI（Service Mesh Interface） 的 osm（Open Service Mesh） 作为控制平面，采用 Pipy 作为数据平面，采用 fsm 项目提供的 Ingress、Egress、Gateway API 多集群服务网格能力，具有高性能、低资源、简单、易用、易扩展、广泛兼容（支持 x86/arm64/龙芯/RISC-V）的特点。\n环境准备 K3s export INSTALL_K3S_VERSION=v1.23.8+k3s2 curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 安装服务网格 推荐使用 CLI 进行安装。\nsystem=$(uname -s | tr [:upper:] [:lower:]) arch=$(dpkg --print-architecture) release=v1.1.2 curl -L https://github.com/flomesh-io/osm-edge/releases/download/${release}/osm-edge-${release}-${system}-${arch}.tar.gz | tar -vxzf - ./${system}-${arch}/osm version cp ./${system}-${arch}/osm /usr/local/bin/ CLI 下载之后就可以通过下面的命令进行安装了，这里默认开启 宽松流量策略模式 并安装 Ingress。\nexport osm_namespace=osm-system export osm_mesh_name=osm osm install \\  --mesh-name \u0026#34;$osm_mesh_name\u0026#34; \\  --osm-namespace \u0026#34;$osm_namespace\u0026#34; \\  --set=osm.enablePermissiveTrafficPolicy=true \\  --set=fsm.enabled=true 确认 pod 正常启动并运行。\nkubectl get pods -n osm-system NAME READY STATUS RESTARTS AGE fsm-repo-d785b55d-r92r5 1/1 Running 0 2m20s osm-bootstrap-646497898f-cdjq4 1/1 Running 0 3m12s osm-controller-7bbdcf748b-jdhsw 2/2 Running 0 3m12s fsm-manager-7f9b665bd9-s8z6p 1/1 Running 0 3m12s fsm-bootstrap-57cb75d586-vvvzl 1/1 Running 0 3m01s osm-injector-86798c9ddb-gfqb4 1/1 Running 0 3m12s fsm-ingress-pipy-5bc7f4d6f6-7th6g 1/1 Running 0 3m12s fsm-cluster-connector-local-7464b77ffd-cphxf 1/1 Running 0 4m22s 安装 Argo Rollouts kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml Argo Rollouts 中默认使用 SMI TrafficSplit 的 v1alpha1 版本，通过下面的命令指定其使用 v1alpha2 的版本。\nkubectl patch deployment argo-rollouts -n argo-rollouts --type=json -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/args\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;--traffic-split-api-version=v1alpha2\u0026#34;]}]\u0026#39; 确认 pod 正常启动并运行。\nkubectl get pods -n argo-rollouts NAME READY STATUS RESTARTS AGE argo-rollouts-58b9bc98f7-cj79l 1/1 Running 0 1m5s 安装 kubectl argo 插件 使用 kubectl argo 插件可以通过命令行对发布进行操作。\n在 macOS 下，其他平台参考 官方安装文档。\nbrew install argoproj/tap/kubectl-argo-rollouts 通过下面的命令启动 Argo Rollouts Dashboard，在浏览器中打开 http://localhost:3100/rollouts 就可访问 Dashboard。\nkubectl argo rollouts dashboard 接下来就是部署示例应用。\n部署应用 示例应用使用常见的 bookstore 系统，包含下面几个组件。这里只对 bookstore 应用进行金丝雀发布，为了方便演示，除了  bookstore 以外的几个应用继续使用 Deployment 的方式部署，只有 bookstore 使用 Argo Rollouts 的 Rollout CRD。\n bookbuyer 是一个 HTTP 客户端，它发送请求给 bookstore。这个流量是 允许 的。 bookthief 是一个 HTTP 客户端，很像 bookbuyer，也会发送 HTTP 请求给 bookstore。这个流量应该被 阻止。 bookstore 是一个服务器，负责对 HTTP 请求给与响应。同时，该服务器也是一个客户端，发送请求给 bookwarehouse 服务。这个流量是被 允许 的。 bookwarehouse 是一个服务器，应该只对 bookstore 做出响应。bookbuyer 和 bookthief 都应该被其阻止。 mysql 是一个 MySQL 数据库，只有 bookwarehouse 可以访问。   这里舍弃了 bookthief 应用。\n 创建命名空间 创建命名空间 rollouts-demo ，并纳入到网格管理中。\nkubectl create namespace rollouts-demo kubectl config set-context --current --namespace rollouts-demo osm namespace add rollouts-demo 创建 Service kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: bookbuyer namespace: rollouts-demo labels: app: bookbuyer spec: ports: - port: 14001 name: bookbuyer-port selector: app: bookbuyer --- apiVersion: v1 kind: Service metadata: name: bookstore namespace: rollouts-demo labels: app: bookstore spec: ports: - port: 14001 name: bookstore-port selector: app: bookstore --- apiVersion: v1 kind: Service metadata: name: bookstore-v1 namespace: rollouts-demo labels: app: bookstore version: v1 spec: ports: - port: 14001 name: bookstore-port selector: app: bookstore --- apiVersion: v1 kind: Service metadata: name: bookstore-v2 namespace: rollouts-demo labels: app: bookstore version: v2 spec: ports: - port: 14001 name: bookstore-port selector: app: bookstore --- apiVersion: v1 kind: Service metadata: name: bookwarehouse namespace: rollouts-demo labels: app: bookwarehouse spec: ports: - port: 14001 name: bookwarehouse-port selector: app: bookwarehouse --- apiVersion: v1 kind: Service metadata: name: mysql namespace: rollouts-demo spec: ports: - port: 3306 targetPort: 3306 name: client appProtocol: tcp selector: app: mysql clusterIP: None EOF 部署 Deployment 应用 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: bookbuyer namespace: rollouts-demo spec: replicas: 1 selector: matchLabels: app: bookbuyer version: v1 template: metadata: labels: app: bookbuyer version: v1 spec: nodeSelector: kubernetes.io/os: linux containers: - name: bookbuyer image: flomesh/bookbuyer:latest imagePullPolicy: Always command: [\u0026#34;/bookbuyer\u0026#34;] env: - name: \u0026#34;BOOKSTORE_NAMESPACE\u0026#34; value: rollouts-demo - name: \u0026#34;BOOKSTORE_SVC\u0026#34; value: bookstore --- apiVersion: apps/v1 kind: Deployment metadata: name: bookwarehouse namespace: rollouts-demo spec: replicas: 1 selector: matchLabels: app: bookwarehouse template: metadata: labels: app: bookwarehouse version: v1 spec: nodeSelector: kubernetes.io/os: linux containers: - name: bookwarehouse image: flomesh/bookwarehouse:latest imagePullPolicy: Always command: [\u0026#34;/bookwarehouse\u0026#34;] --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: rollouts-demo spec: serviceName: mysql replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: nodeSelector: kubernetes.io/os: linux containers: - image: mariadb:10.7.4 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: mypassword - name: MYSQL_DATABASE value: booksdemo ports: - containerPort: 3306 name: mysql volumeMounts: - mountPath: /mysql-data name: data readinessProbe: tcpSocket: port: 3306 initialDelaySeconds: 15 periodSeconds: 10 volumes: - name: data emptyDir: {} volumeClaimTemplates: - metadata: name: data spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 250M EOF 部署 bookstore Rollout 我们为 bookstore 的金丝雀发布设置了三个阶段：10%、50%、90% 流量，每个阶段后都会进入暂停状态（也可以设置暂停时间、条件等等）。\n前面创建 Service 时，我们为 bookstore 创建了如下三个服务，这里正好会用到：\n 根服务 bookstore 稳定版服务 bookstore-v1 金丝雀版本 bookstore-v2  kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: bookstore namespace: rollouts-demo spec: replicas: 1 strategy: canary: canaryService: bookstore-v2 stableService: bookstore-v1 trafficRouting: smi: rootService: bookstore steps: - setWeight: 10 - pause: {} - setWeight: 50 - pause: {} - setWeight: 90 - pause: {} revisionHistoryLimit: 2 selector: matchLabels: app: bookstore template: metadata: labels: app: bookstore spec: nodeSelector: kubernetes.io/os: linux containers: - name: bookstore image: addozhang/bookstore-v1:latest imagePullPolicy: Always ports: - containerPort: 14001 name: web command: [\u0026#34;/bookstore\u0026#34;] args: [\u0026#34;--port\u0026#34;, \u0026#34;14001\u0026#34;] env: - name: BOOKWAREHOUSE_NAMESPACE value: rollouts-demo EOF 部署完成后，此时 bookstore-v1 的版本会运行，并不会部署新的版本，此时三个 Service 都会选择到稳定版本的 pod。\nkubectl get endpoints -n rollouts-demo -l app=bookstore NAME ENDPOINTS AGE bookstore-v1 10.42.1.34:14001 54s bookstore 10.42.1.34:14001 54s bookstore-v2 10.42.1.34:14001 54s 查看 TrafficSplit 的设定，访问 bookstore 的所有流量都会进入 bookstore-v1 的 endpoint 中：\nkubectl get trafficsplit bookstore -n rollouts-demo -o yaml apiVersion: split.smi-spec.io/v1alpha2 kind: TrafficSplit metadata: name: bookstore namespace: rollouts-demo spec: backends: - service: bookstore-v2 weight: 0 - service: bookstore-v1 weight: 100 service: bookstore 查看 Argo Rollouts Dashboard，可以看到刚刚创建的 Rollout。\n点开可以看到详细信息，包括当前的 revision，已经我们设置的发布步骤。\n创建 bookbuyer Ingress 为了方便查看运行效果，为 bookbuyer 创建 Ingress。然后就可以通过 http://ingress_host:80 来访问 bookbuyer 应用了，也可以通过 http://ingress_host:80/reset 来重置应用页面的计数器，方便灰度发布的效果确认。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: bookbuyer namespace: rollouts-demo spec: ingressClassName: pipy rules: - http: paths: - path: / pathType: Prefix backend: service: name: bookbuyer port: number: 14001 --- kind: IngressBackend apiVersion: policy.openservicemesh.io/v1alpha1 metadata: name: bookbuyer namespace: rollouts-demo spec: backends: - name: bookbuyer port: number: 14001 protocol: http sources: - kind: Service namespace: osm-system name: fsm-ingress-pipy-controller EOF 发布测试 笔者的 ingress IP 是 192.168.1.12，因此在浏览器中打开 http://192.168.1.12。此时，所有的流量都到了 bookstore-v1。\n执行应用升级 通过下面的命令，部署 bookstore v2 版本开始金丝雀发布。除了命令行，也可以在 Dashboard 的发布详情页面上修改容器的镜像。\nkubectl argo rollouts set image bookstore bookstore=addozhang/bookstore-v2:latest 然后通过下面的命令可以查看稳定版和金丝雀版本的实例状态。\nkubectl argo rollouts get rollout bookstore 重置 bookstore 应用页面的计数器后，可以发现 v1 和 v2 流量接近 9:1。\nDashboard 的发布详情页上，会显示 v2 版本的 revision，右侧 steps 列表中可以看到当前处于第一个 pause 阶段。\n推进到 50% 流量 验证完成后，就可以将发布推进到下一阶段。可以使用下面的命令，也可在 Dashboard 详情页上点击 PROMOTE 按钮。\nkubectl argo rollouts promote bookstore rollout \u0026#39;bookstore\u0026#39; promoted 还是先清空应用页面计数器，然后查看效果。\n推进到 100% 流量 接下来，可以继续推荐到下一阶段 90%。也可以通过下面的命令直接进入到最后的阶段 100%，在 Dashboard 详情页上点击 PROMOTE-FULL 也可达到同样的效果。\nkubectl argo rollouts promote bookstore --full rollout \u0026#39;bookstore\u0026#39; fully promoted 其他操作 在发布的任何一个阶段，可以通过命令或者页面上的操作回到上一个阶段，或者退出发布。\n#回到上一阶段 kubectl argo rollouts undo bookstore #退出发布 kubectl argo rollouts abort bookstore 总结 服务网格以进程外无侵入的方式为服务提供了丰富的治理功能，从平台层面提升系统的可观测性、安全以及可靠性。金丝雀发布是服务网格的主要应用场景之一，大大减低了应用发布带来的风险，将不稳定性限制在可控的范围内。\n同时 Argo Rollouts 也提供了丰富的设置，来控制发布的流程，满足不同的使用需求。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/09/21/pexelsjeremybishop12088976.jpg","permalink":"https://atbug.com/canary-release-via-argo-rollouts-and-service-mesh/","tags":["Service Mesh","CICD","SMI"],"title":"使用 Argo Rollouts 和服务网格实现自动可控的金丝雀发布"},{"categories":["笔记"],"contents":"最近正在读 《Solving the Bottom Turtle》  这本书，这篇是对部分内容的总结和思考，由于内容较多，会分几篇来发。\n这本书的副标题是：\n a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity. 通过通用身份以 SPIFFE 的方式在基础设施中建立信任。\n 大家记住其中的有几个关键词：通用身份、SPIFFE 的方式、基础设施、建立信任。\n背景 零信任是一种异常火热的安全模型，在之前翻译的文章中 《零信任对 Kubernetes 意味着什么》，对什么是零信任、为什么要实施零信任，做了初步的介绍。\n过去几十年流行的边界安全模型，已经不在适合如今分布式的微服务架构、复杂多样的系统环境以及不可避免的人为失误。从零信任的原则来看，通过安全边界防护的人、系统和网络流量是不可信的，因为“你可能不是你”，身份不可信。\n可能有人会问不是有用户名密码、证书么？这些都可能存在泄露，尤其是时间越长泄露的概率越高（在安全领域，只有 0 和 100，所有做不到 100% 的安全，都将其视作 0。这也是笔者对零信任的浅薄理解）。\n零信任模型的核心是重塑身份的分配和验证方式，身份是构建零信任模型的基石。\n身份 现实世界的身份 在说系统身份之前，先说下现实中的身份。大家都有身份证（Identity Card），生活中用到身份证的场景也越来越多，很多的服务都会查验身份证。为什么身份证就能验证持有人的身份，首先身份证是由政府机构颁发的，在颁发前会查验过申请的人信息（证明你就是你），这个充分可信的；身份证本身做了防伪处理，有特定的查验手段很难被伪造。这里有几个定义：\n 主体：人 身份：唯一 ID。实际上身份证号这个唯一 ID 就是个人的身份，而姓名只是代称。 身份证明文件：身份证，身份证号、姓名、性别、照片、有效期等等（申请时还会录入生物识别信息）。 颁发机构：政府机构。  再者身份证仅限在国内使用，只有在国内被信任，这个区域就是另一个定义“信任域”。\n出国就要使用另一种证件护照，护照是按照国际标准制作和颁发的，可以被其他国家接受，也就是可以信任域间互信。这些互信的信任域，就组成了信任联盟。\n这里一共提到了如下的定义：\n 主体 身份 身份证明文件 颁发机构 信任域 信任联盟  数字世界的身份 数字世界的身份也有与现实世界类似的定义。\n数字身份证明是数字世界的主体的身份证明文件，常见的有：X.509 证书、签名的 JWT 和 Kerberos 令牌等。数字身份证明可以使用密码技术进行验证。计算系统可以使用数字身份证明通过验证，就像人使用身份证和护照一样。\n为实现这些有一个非常流行的技术 Public Key Infrastructure (PKI) ，其定义了一系列的为创建、管理、分发、使用、存储、撤销数字证书和管理公钥加密所需要的角色、策略、硬件、软件和流程。\n接下来简单介绍下 X.509，有经验的同学可以直接跳过。\nX.509 概述 1988 年 X.509 作为 PKI 的标准首次发布，X.509 假设有一套严格的层次化的证书颁发机构（CA，Certificate Authority）。申请者要提供证书签名请求（CSR，Certificate Signing Request。申请者通过自己的私钥签名创建的，里面包含了申请者的身份信息，进行用于验真的公钥、请求证书的名称、以及 CA 可能要求的其他身份信息等）。然后 CA 对该 CSR 进行签名转换成合格的证书发回申请方。\nCA 将受信任的根证书（包含了公钥）发给所有成员，成员使用根证书中的公钥对其他成员的证书（数字身份证明）进行查验。\n证书的生命周期 前面讲了证书的颁发流程，每个证书在创建的时候都会设置有效截止日期，如果过期证书就无效了。\n证书在过期之前，如果申请方出了问题，证书变成不可信会被 CA 吊销。就像用户名密码一样，有效日期越长出问题的几率就越大。因此一个有效的办法就是 在成本可控的情况下，尽可能缩短证书的有效期。 然后通过更新操作，发放新的证书。\n数字身份的使用  认证：服务通过 X.509 证书或者 JWT 向其他服务提供身份信息。 保密性和完整性：保密性是指攻击者无法获取消息的内容；完整性指消息传输的过程中无法被更改。TLS 是广泛使用的协议，用来通过证书在不可信的网络连接上提供认证、保密性和消息完整性。TLS 的特性之一是可以作为双向的 TLS 认证（mutual TLS authentication）。 授权：认证完成之后，使用认证过的数字身份判断是否可以访问服务。 可观测性：唯一的身份可以帮助提升组织内部基础设施的可观测性。 计量：比如微服务架构中常见的限流，可以通过唯一身份标识来管理请求的限额。  说完基础的身份，接下来就是 SPIFFE 和 SPIRE 了。\nSPIFFE Secure Production Identity Framework for Everyone 的缩写 SPIFFE，是一个软件身份标识的开源标准（CNCF 下），为了以组织和平台无关的方式实现身份标识的互操作性，SPIFFE 定义了以全自动方式获取和验证加密身份所需的接口和文档。为构建分布式系统的通用身份的控制平面提供了标准的支撑。这个控制平面就是我们文章开头提到的“基础设施”。作为基础设施，需要是语言、技术栈无关的，并可以做到全自动。\n SPIFFE（适用于所有人的安全生产身份框架）以特制 X.509 证书的形式为现代生产环境中的每个工作负载提供安全身份。SPIFFE 消除了对应用程序级身份验证和复杂网络级 ACL 配置的需求。\n 实施 SPIFFE 有着重要的前提，假设零信任网络安全模型中：\n 网络通信是不可信的，可能被攻破 运行 SPIFFE 实现的组件的硬件及其运维人员是可靠的  概念 SPIFFE 框架包含了如下几部分：\n SPIFFE ID： SVID（SPIFFE Veriﬁable Identity Document） Workload API Trust Bundle SPIFFE 联盟  SPIFFE ID SPIFFE ID 使用 URI（统一资源标志符）格式的字符串，软件服务的唯一身份标识。由多个部分组成：\n 前缀 spiffe:// 信任域的名字，比如 example.com，还可以通过多级域名来表示环境、集群等信息 stagging.example.com、k8s-cluster.example.com 工作负载的名字或者身份标识，比如 /payment/mysql、/payment/web-fe、`/9eebccd2-12bf-40a6-b262-65fe0487d4  比如 spiffe://example.com/bizops/hr/taxrun/withholding\nSVID SVID 是可以对外提供服务身份标识加密的、可验证的证明文件。SVID 由机构签发，包含了单一 SPIFFE ID，以及服务属于信任域。\n目前 SVID 的格式有两种：X.509 和 JWT。\nX.509 SVID SPIFFE ID 位于扩展字段 SAN（Subject Alternative Name）中。作为 SVID，SAN 中只能有一个值（其他场景中，SAN 中可以有多个值）。\n建议尽可能使用 X.509 SVID 而不是 JWT-SVID，因为安全性更高，尤其是与 TLS 结合使用。\nJWT-SVID JWT-SVID 将 SPIFFE ID 编码在 标准的 JWT) 中。在应用层，JWT-SVID 被用作 bearer 令牌向对端提供身份标识。\n与 X.509 SVID 不同，JWT-SVID 存在 重放攻击 的风险，令牌会被未经授权的一方获取并冒用。\nSPIFFE 定义了三种机制来避免这种风险：\n JWT-SVID 必须通过安全通道传输 aud 声明必须与令牌针对方的严格字符串匹配 所有的 JWT-SVID 必须包含有效期  Workload API Workload API 是一个本地的、非联网的 API，最重要的是，这个 API 无需认证。工作负载无需调用 API 时无需提供任何信息，就像在操作系统中执行 whoami 操作系统会返回用户名一样，但 workload API 会返回更多的信息。工作负载可以访问其获取当前的身份证明文件、信任包及其他相关信息。\nSPIFFE 的实现可以通过创造性的方案（比如操作系统提供的特性）来对调用方（工作负载）进行辨认。\nWorkload API 作为 gRPC 服务器暴露提供服务，并使用双向流，服务器端的更新可以推送给工作负载。\nTrust Bundle SPIFFE 信任包，是包含了信任域公钥的文件。每种类型的 SVID 在信任包都有其特有的方式，比如 X.509 SVID 中 CA 证书中包含了公钥。\n信任包中不会包含除公钥以外的其他敏感内容，可以被随意传播。\nSPIFFE 联盟 SPIFFE 联盟有点类似前面讲的护照可以在其他国家来识别身份。\nSPIFFE 联盟是一种可以共享 SPIFFE 信任包的简单机制。通常我们希望允许不同信任域中的服务可以安全地进行网络通信。通信的双方检查对方提供提供的信任包，来决定是否进行通信。这是基于双方可以互相暴露或者共享信任包，这种机制被称为包端点（bundle endpoint）。\n包端点是个简单的、由 TLS 保护的 HTTP 服务。SPIFFE 的实现通过暴露该端点，使得包的内容可以被获取到。\nSPIRE SPIRE 是 SPIFFE 运行时环境（SPIFFE Runtime Environment）的首字母，是一个生产就绪、开源的（同在 CNCF 下） SPIFFE 实现。SPIRE 实现了 SPIFFE 五个定义。\nSPIRE 包含了两个主要组件：服务端和代理。服务端负责验证代理和生成 SVID，而代理负责提供 workload API，并与服务端进行通信。\n这两个组件的设计都支持插件，可以很容易地扩展支持不同的配置和平台。\nSPIRE 架构 服务端 SPIRE 服务端管理和颁发 SPIFFE 信任域中的所有身份标识，使用数据库（SQLite）来保存代理和工作负载的信息。\n服务端通过使用注册条目来获知其管理的工作负载，注册条目是为节点和工作负载分配 SPIFFE ID 的灵活规则。\n服务端提供了 API 和 CLI 两种访问方式。\n服务端为工作负载签发 SVID 时，可以使用其生成的自签发证书（默认），也可以通过上游证书颁发机构插件来配置使用上游证书颁发机构，来完成证书的签发。\n代理 代理只有一个非常重要的功能：提供 workload API。为了提供这个 API，还要支持工作负载身份标识的确定、调用并将自身安全地接入 SPIRE 服务端。\nSPIRE 代理会从 SPIRE 服务端接收 本地信任域 以及 会直接调用它（SIRE 代理）的工作负载 的信息。\n当在指定信任域增加新的工作负载时，会在 SPIRE 服务端创建和更新记录，该工作负载的信息会自动地传播到正确的代理。\n插件架构 前面提到服务端和代理都支持插件，通过插件来扩展适应新的 节点证明者、工作 负载证明者，以及上游机构。\nSIVD 管理 节点证明过程中会为 SPIRE 代理申请到身份标识，代理使用该标识来完成 SPIRE 服务端的认证。通过认证时候，从服务端获取被授权管理的所有工作负载的 SVID（保存在内存中）。\n证明 证明是使用可用信息作为证据确认工作负载身份的过程。在 SPIRE 中有两种证明方式：节点证明和工作负载证明。\n节点证明是断言描述节点属性（比如 AWS 自动扩展组的成员）；工作负载证明是断言描述工作负载的属性（比如使用的 Kubernetes Service Account，或者二进制文件的路径）。\n这些属性在 SPIRE 中统统称为选择器（selector）。SPIRE 提供了大量的开箱即用的选择器，比如支持逻辑、Kubernetes、AWS、GCP、AZure 等等的节点选择器；支持 Docker、Kubernetes、Unix 等的工作负载选择器。\nSPIRE 的插件架构设计，也支持更多选择器的扩展。\n节点证明 节点证明发生在代理第一次启动时，代理与服务端通信进行信息交换。代理将通过插件采集的信息上报到服务端，服务器端使用对等的插件进行信息的确认。节点证明成功后，节点会拿到身份证明，并使用身份证明与服务端进行后续的通信。\n工作负载证明 工作负载证明发生在工作负载与 SPIRE 代理的 workload API 建立连接时。证明的过程完全由 SPIRE 代理完成，代理利用操作系统特性来确认是哪个进程创建的连接。\n比如在 Linux 下通过系统调用来获取调用 socket 的对端进程 ID、用户标识和全局唯一标识。不同操作系统的内核元数据不同，拿到工作负载的 ID 之后，代理将 ID 提供给各个插件，从选择器中获取调用者的信息。\n每个插件都会对调用者进行检查。比如一个插件从系统内核中获信息，生成取用户、组等选择器；一个插件则会与 Kubernetes 交互，生成命名空间、service account 等选择器；另一个与 Docker daemon 进行交互，生成 Docker 镜像 ID、标签和容器环境变量的选择器。\n注册条目 SPIRE 通过注册条目获知工作负载的各种信息，比如工作负载是否应该或允许出现在环境中、应该运行的位置、SPIFFE ID 和其他信息。这些信息是通过 SPIRE API 创建和管理的。\n每个注册条目包含了 3 个核心属性：\n Parent ID：告诉 SPIRE 工作负载运行在哪里，进一步感知哪些代理有权获取其 SVID。 SPIFFE ID：工作负载可以使用的 SPIFFE ID 可以用来辨识工作负载的信息：也就是选择器从证明中获取到的  注册条目可以描述一组节点或者一个工作负载，通常后者通过 Parent ID 来引用后者。\n节点条目 描述一个节点或者一组节点的注册条目使用节点证明生成的选择器来分配 SPIFFE ID，在注册工作负载时可以引用该 ID。\n一个节点可以被多个节点条目的选择器证明，因此可以存在于多个组中。\nSPIRE 服务端一次可以加载多个节点证明器，而代理一次只能加载一个。\n注册条目的 Pareent ID 指向的是 SPIRE 服务端的 SPIFFE ID。\n工作负载条目 工作负载条目的 Parent ID 是节点（一个或者一组）的 SPIFFE ID。 表示其可以运行在哪个或哪组节点上。该节点上的 SPIRE 代理收到该工作负载的条目，包含了在颁发 SIVD 之前必须要证明的选择器。\n与节点证明过程不同，SPIRE 代理一次可以加载多个工作负载证明器插件。\nSPIFFE/SPIRE 应用概念威胁模型 安全边界 安全边界通俗地讲，就是从不安全的一方到安全的一方，并且都有其各自的证明方式。\nSPIFFE/SPIRE 中定义了 3 个安全边界：工作负载与代理之间、代理与服务端之间、不同信任域的服务端之间。\n工作负载和其他信任域中的服务端是完全不可信的，网络通信也是完全不可信的。\n工作负载与代理的安全边界 代理不会信任工作负载的任何输入，代理对工作负载的检查都是通过外部检查完成的。换句话说，所有值可以被工作负载修改的选择器，也是不安全的。\n代理与服务端的安全边界 代理比工作负载安全一些，但是安全性不如服务端。SPIRE 的一个明确设计目标是它应该在节点被攻破后存活。\n代理负责根据工作负载的行为来创建和管理标识，但是也有必要限制代理的能力在其职责范围内（最少权限原则）。\n代理在获取标识前，必须能有证明其对注册条目的所有权。\n在节点尚未证明自己获得标识之前，与服务端的通信使用单向 TLS；获得身份标识和有效的 SVID 之后，使用双向的 TLS。\n服务端之间的边界 SPIRE 服务端仅被信任在其直接管理的信任域内生成 SVID。\n各组件被攻破的影响 假如代理被攻破，会影响其管理的工作负载。如果工作负载跟代理是 1:1 部署时（参考 Kubernetes Pod 中的多 sidecar），只会影响一个工作负载。但是代理管理多个工作负载时，影响范围变大（参考 Kubernetes 中的 Daemonset，每个节点运行一个副本）。\nSPIRE 服务端是整个系统中最为敏感的组件，如果被攻破会影响到整个信任域，强烈建议将其部署到与要管理的工作负载不同的硬件上。\n概念威胁模型 vs 传统边界安全模型 这个概念威胁模型，实际上是将传统安全边界模型粒度进行细化，将隔离区缩小并进行自治。缩得越小，安全性越高，破防时影响的范围越小。\n总结 本篇主要介绍了零信任安全模型中身份定义，以及通用身份验证的标准和实现 SPIFFE/SPIRE。\n回到开头书的副标题中的几个关键词：通用身份、SPIFFE 的方式、基础设施、建立信任。SPIFFE 提供了覆盖了身份、身份证书的类型以及创建、管理和颁发方式的定义，贯彻了零信任的安全模型，为构建基础设施提供了标准框架。SPIRE 则提供了基础设施的实现，使用服务端 + 代理的分级策略采用概念威胁模型，尽可能地提升安全性和降低威胁带来的影响。\n标准和实现都有了，实际上落地实施也是一个复杂的过程。书中也提供了落地计划的设计方案以及分享落地的经验，后续会继续为大家解读。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/09/15/16632173289239.jpg","permalink":"https://atbug.com/what-is-spiffe-and-spire/","tags":["零信任","安全","Kubernetes"],"title":"零信任安全：SPIFFE 和 SPIRE 通用身份验证的标准和实现"},{"categories":["翻译"],"contents":"这篇是 Buoyant 的创始人 William Morgan 文章《What Does Zero Trust Mean for Kubernetes?》 的翻译，文章很好的解释了什么是零信任、为什么要实施零信任，以及服务网格如何以最小的代码实现零信任。\n零信任是营销炒作，还是新的机会，各位看官你怎么看？\n 要点\n 零信任是一种被大量炒作的安全模型，但尽管存在营销噪音，但它对于具有安全意识的组织具有一些具体而直接的价值。 零信任的核心是，将授权从“在边界验证一次”转变为“随时随地验证”。 为此，零信任要求我们重新思考身份的概念，并摆脱基于位置的身份，例如 IP 地址。 Kubernetes 采用者在网络层实现零信任时具有明显的优势，这要归功于基于 Sidecar 的服务网格，它提供无需更改应用程序就可实现的最细粒度的身份验证和授权。 虽然服务网格可以提供帮助，但 Kubernetes 安全性仍然是一个复杂而微妙的话题，需要从多个层次进行了解。  零信任是一种位于现代安全实践前沿的强大的安全模型。这也是一个容易引起轰动和炒作的术语，因此很难消除噪音。那么，究竟什么是零信任，对于 Kubernetes，它究竟意味着什么？在本文中，我们将从工程的角度探讨什么是零信任，并构建一个基本框架来理解它对 Kubernetes 运维和安全团队等的影响。\n介绍 如果你正在构建现代云软件，无论是采用 Kubernetes 还是其他软件，可能都听说过“零信任”一词。零信任的安全模式变得如此重要，以至于美国联邦政府已经注意到：白宫最近发布了一份联邦零信任战略的备忘录，要求所有美国联邦机构在年底前满足特定的零信任安全标准。2024财年；国防部创建了零信任参考架构；美国国家安全局发布了一份Kubernetes 强化指南，专门描述了 Kubernetes 中零信任安全的最佳实践。\n随着这种噪音，零信任无疑吸引了很多营销关注。但尽管有噪音，零信任不仅仅是一个空洞的术语——它代表了对未来安全的一些深刻和变革性的想法。那么具体来说，什么是零信任，为什么它突然变得如此重要？零信任对 Kubernetes 用户意味着什么？\n什么是零信任？ 正如所料，零信任从根本上讲是关于信任。它是解决安全核心问题之一的模型：是否允许 X 访问 Y？换句话说，我们是否相信 X 可以访问 Y？\n当然，零信任中的“零”有点夸张。要使软件正常工作，显然某些东西需要信任其他东西。因此，零信任并不是完全消除信任，而是将信任降低到最低限度（众所周知的最小特权原则）并确保它在每一点都得到执行。\n这听起来像是常识。但与技术中的许多新想法一样，理解零信任的最佳方法是了解它的反应。零信任摒弃了边界安全的观点。在边界安全模型中，在敏感组件周围实施“装甲”。例如，数据中心周围可能有一个防火墙，其任务是阻止问题流量和参与者进入。这种模型，有时被称为城堡策略，具有直观的意义：城堡的墙壁是为了将坏人拒之门外。如果你在城堡里，那么根据定义，你就是一个好人。\n零信任模型表明，边界安全已经不足。根据零信任原则，即使在安全边界内，仍必须将用户、系统和网络流量视为不受信任。国防部的参考架构很好地总结了这一点：\n“在安全边界之外或之内运行的任何参与者、系统、网络或服务都是不可信的。相反，我们必须验证任何试图建立访问权限的事物。从边界验证一次到对每个用户、设备、应用程序和交易的持续验证，这是我们如何保护基础设施、网络和数据的哲学的巨大范式转变。”\n当然，零信任并不意味着抛弃防火墙——纵深防御是任何安全策略的重要组成部分。这也不意味着我们可以忽略所有其他重要的安全组件，例如事件记录和供应链管理。零信任只要求我们将信任检查从“一次在边界”转移到“每次，无处不在”。\n然而，为了正确地做到这一点，我们需要重新考虑一些关于“信任”意味着什么以及我们如何捕捉它的基本假设。\n身份 零信任最直接的影响之一是它改变了我们思考和分配身份的方式，尤其是系统身份。\n在边界模型中，位置实际上就是身份。如果在防火墙内，那么是可信的；如果你在它之外，就不是。因此，基于边界的系统可以允许基于客户端 IP 地址等信息访问敏感系统。\n在零信任世界中，这已经不够了。IP 地址仅用于指示位置，因此不再足以确定是否可以访问特定资源。相反，我们需要另一种形式的身份：以某种内在方式与工作负载、用户或系统相关联。而且这种身份需要以某种方式进行验证，而这种方式本身并不需要信任网络。\n这是一个具有丰富含义的大要求。提供网络安全但依赖于 IP 地址等网络标识符（如 IPSec 或 Wireguard）的系统不足以实现零信任。\n策略 有了新的身份模型，我们现在需要一种方法来捕获每个身份的访问类型。在上面的边界方案中，通常授予一系列 IP 地址对敏感资源的完全访问权限。例如，我们可能会设置 IP 地址过滤，以确保仅允许防火墙内的 IP 地址访问敏感服务。在零信任的情况下，我们反而需要执行必要的最低访问级别。基于身份以及任何其他相关因素，应尽可能限制对资源的访问。\n虽然我们的应用程序代码可以自己做出这些授权决策，但我们通常会使用在应用程序之外指定的某种形式的策略来捕获它。拥有明确的策略允许我们在不修改应用程序代码的情况下审核和更改访问权限。\n为了实现我们的零信任目标，这些策略可能非常复杂。我们可能有一个策略，它将对服务的访问限制为只有那些需要访问它的服务调用方（即，在双方都使用工作负载身份）。我们可能会进一步细化，只允许访问该服务上的某些接口（HTTP 路由、gRPC 方法）。更进一步，根据请求的用户身份限制访问。在所有情况下，目标都是最低权限——只有在非常必要时才能访问系统和数据。\n执行 最后，零信任要求我们在最细粒度的级别上同时执行身份验证（确认身份）和授权（验证策略是否允许该操作）。每个授予数据或计算访问权限的系统都应该设置从外围到单个组件的安全边界。\n与策略类似，这种执行理想情况下是在整个堆栈中统一完成的。不是每个组件都使用自己的自定义执行代码，而是使用统一的执行层，统一之后方便审计，并将应用程序开发人员的关注点与运营和安全团队的关注点分离。\nKubernetes 零信任 我们必须从第一原则重新思考身份，以任意表达性策略的形式来将信任具象化，并将新的执行机制渗透到基础设施的各个层面。面对这些的要求，我们不可避免地经历短暂的恐慌。前面是不是提到需要在 2024 财年之前实现？\n好消息是，至少对于 Kubernetes 用户来说，采用零信任的某些方面要容易得多。尽管有缺陷和复杂性，Kubernetes 是一个具有明确范围、定义良好的安全模型和明确的扩展机制的平台。这使其成为零信任实施的成功领域。\n在 Kubernetes 中解决零信任网络的最直接方法之一是使用服务网格。服务网格利用了 Kubernetes 强大的“sidecar”概念，其中平台容器（译者注：此处指 sidecar 代理容器）可以在部署时以后期绑定操作功能的形式，与应用程序容器动态注入到一起，。\n服务网格使用这种 sidecar 方法在运行时将代理添加到应用程序 pod 中，并连接这些代理以处理所有传入和传出流量。这允许服务网格以与应用程序代码解耦的方式交付功能。应用程序和平台之间的关注点分离是服务网格主张的核心价值：当然，这些功能可以直接在应用程序中实现，但是通过解耦，我们允许安全团队和开发人员相互独立地迭代，同时仍然努力实现安全但功能齐全的应用程序的共同目标。\n由于服务网格处理进出应用程序之间的默认网络，因此它可以很好地处理零信任问题：\n 工作负载身份可以从 Kubernetes 中的 pod 身份而不是其 IP 地址中获取。 可以通过在双向 TLS 中包装连接来执行身份验证，这是 TLS 的一种变体，其使用加密信息在连接的两端进行验证。 授权策略可以用 Kubernetes 术语表示，例如，通过自定义资源定义 (CRD)，明确策略并并与应用程序解耦。 最重要的是，策略在跨技术栈的单个 pod 级别统一执行。每个 pod 都有自己的身份验证和授权，这意味着网络永远不受信任。  所有这些共同实现了我们的大部分零信任目标（至少对于 Kubernetes 集群而言！）。我们使用工作负载身份而不是网络身份；在最细粒度级别（pod）上执行，以及在技术栈中以一致且统一的方式应用身份验证和授权的，而无需更改应用程序。\n在基本框架内，不同的服务网格实现提供了不同的权衡。例如， Linkerd是一个开源、Cloud Native Computing Foundation 毕业的服务网格项目，它提供了一个以简单性为目标和重点的实现，直接从 Kubernetes ServiceAccounts 提取工作负载标识来达到“零配置”，默认开启双向 TLS。同样，Linkerd 的基于 Rust 的微代理提供了一个极简的零信任实现。\n当然，仅仅在集群中添加一个服务网格并不是万能的。安装后，必须完成定义、更新和评估授权策略的工作。集群运维人员必须小心确保所有新创建的 pod 都与它们的 sidecar 组件配对。当然，服务网格本身必须像集群上的任何软件一样进行维护、监控和迭代。然而，不管是不是灵丹妙药，服务网格确实提供了从集群中默认的未加密、未经身份验证的流量转变为具有强大工作负载身份和丰富授权系统的默认加密、经过身份验证的流量——这是朝着零信任迈出的一大步。\n总结 零信任是一种强大的安全模型，处于现代安全实践的前沿。如果可以消除围绕它的营销噪音，那么采用零信任有一些深刻而重要的好处。虽然零信任需要对身份等核心理念进行一些根本性的改变，但如果 Kubernetes 用户能够采用服务网格并从纯粹基于边界的网络安全转变为“对每个用户、设备、应用程序和交易的持续验证”，那么他们至少有很大的优势。\n关于作者 William Morgan William Morgan是 Buoyant 的联合创始人兼首席执行官，Linkerd 的创建者。在加入 Buoyant 之前，他是 Twitter 的一名基础架构工程师，在那里他帮助将 Twitter 从单体架构转变为微服务。他是 Powerset、Microsoft 和 Adap.tv 的软件工程师，以及 MITRE 的研究科学家。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/09/08/pexelsannashvets4672717.jpg","permalink":"https://atbug.com/translate-zero-trust-for-k8s/","tags":["零信任","Kubernetes","安全"],"title":"译：零信任对 Kubernetes 意味着什么"},{"categories":["容器"],"contents":"Docker 在刚刚发布的 Docker Desktop 4.12.0 中，加入了实验特性：进一步集成 containerd，使用 containerd 来管理和存储镜像。\n为什么说是“进一步集成”？这就要翻翻 Docker 和 containerd 的历史了。\ncontainerd 的诞生 containerd 最早出现在 Docker Engine 中，后来为了将 Docker Engine 做得更加轻量、快速和健壮，在 2016 年 Docker 将 containerd 从 daemon（dockerd） 中独立出来，并完成了与 daemon 的集成。独立出来的 containerd 全面支持 OCI（Open Container Initiative）资源的启动和生命周期的管理，也因此 containerd 可以支持 runc（前身是 Docker 中的 libcontainer，后来捐赠给 LF）以外的其他 OCI 实现。2017 年 Docker 将 containerd 捐献给 CNCF；2019 年 2 月，containerd 毕业。\ncontainerd 独立出来之后，发送到 Docker Engine 的请求：\n Docker daemon 完成镜像管理的操作（拉取、更新镜像） daemon 会为创建容器进行准备工作（创建 OCI bundles）：镜像的信息和运行时的信息。 daemon 调用 containerd 的 API。 收到请求的 containerd 不会直接去操作容器（不直接作为容器的父进程，防止 containerd 挂掉影响容器），而是先创建一个 container-shim 进程。 container-shim 调用 runc cli 来运行容器，并启动 Unix domain socket 暴露 API 提供给 containerd进行容器的管理。  随着 containerd 的不断演进，除了容器创建和容器声明周期管理以外，从 1.1 开始 containerd 加入了 CRI（Container Runtime Interface）的支持。\nCRI 在《源码解析 kubectl port-forward 工作原理》 中层提到 kubelet 会调用 rumtime service 的 gRPC 接口，除了用于 portforward 流的 stream server以外，其实还有实现 CRI 接口 RuntimeService 和 ImageService  的 RuntimeServiceServer 和 ImageServiceServer。\nRuntimeServiceServer 用于接收并处理容器及其生命周期相关的操作，而 ImageServiceServer 则是用来处理镜像相关的操作。containerd 提供了镜像拉取、删除、检查、存储等功能。\n既然 containerd 可以进行镜像的管理，而且 Docker 已经在使用，Docker 也没有必要自己继续维护一套相同的功能。\n切换到 containerd 的镜像管理 在 Docker Desktop 的设置中启动 containerd 管理镜像后，运行 docker info 会发现存储的驱动从原来的 overlay2 变成了 containerd 的 stargz。\n切换前：\n切换后：\n既然使用了 containerd 的 snapshotters 来管理存储（挂在容器的根文件系统），就可以支持多种 snapshotters，比如 stargz 的延迟拉取。\n此外，得益于 containerd 原生支持多平台镜像的存储，还是因为 snapshotters 的原因，可以使用 docker 来构建多平台的镜像了。\n#切换前 docker buildx build -t demo --no-cache --platform linux/amd64,linux/arm64 . [+] Building 0.0s (0/0) error: multiple platforms feature is currently not supported for docker driver. Please switch to a different driver (eg. \u0026#34;docker buildx create --use\u0026#34;) 切换后：\n图片来自 docker 官方博客\n总结 使用 containerd 作为 Docker 的镜像管理目前还处于实验性阶段，必可避免会存在问题，使用时请谨慎对待。\n随着 Docker Swarm 在容器编排之战中的落败，Kubernetes 的话语权也越来越强。在 Kubernetes 1.24.0 中移除了 docker shim 代码，看起来更像是 containerd 取代了 Docker 曾经的地位，而 Docker 也越来越名声不显。从趋势来看，Docker 未来将会完全集成 containerd。\n参考  Extending Docker’s Integration with containerd Docker containerd integration Learning Containers From The Bottom Up  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/09/05/pexelsjuliussilver7533312.jpg","permalink":"https://atbug.com/docker-manipulate-image-via-containerd/","tags":["Docker","Containerd"],"title":"Docker 向全面集成 containerd 又迈进一步"},{"categories":["云原生"],"contents":"kube-ovn 从名字不难看出其是一款云原生的网络产品，将 SDN 等能力带入云原生领域。让 ovn/ovs 的使用更容易，屏蔽了复杂度，降低了使用的难度，并与云原生进行了结合。\n 借助 OVS/OVN 在 SDN 领域成熟的能力，Kube-OVN 将网络虚拟化的丰富功能带入云原生领域。目前已支持子网管理， 静态 IP 分配，分布式/集中式网关，Underlay/Overlay 混合网络， VPC 多租户网络，跨集群互联网络，QoS 管理， 多网卡管理，ACL 网络控制，流量镜像，ARM 支持， Windows 支持等诸多功能。\n 安装 k3s 参考官方的准备工作文档，操作系统使用 Ubuntu 20.04 以及 k3s v1.23.8+k3s2。\n在安装之前确保 /etc/cni/net.d/ 目录内容为空，不为空则清空其下的所有文件。kube-ovn 本身通过实现 cni 来管理网络。\n在安装 k3s 需要禁用 k3s 默认的网络策略控制器和flannel 的后端（默认是 VXLAN）：\nexport INSTALL_K3S_VERSION=v1.23.8+k3s1 curl -sfL https://get.k3s.io | sh -s - --flannel-backend=none --disable-network-policy --disable=traefik --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 为了节省资源，我也禁用了 traefik Ingress 控制器。\n此时检查 pod 会发现都处于 Pending 状态，这是因为还没安装 CNI。\nkubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-6c79684f77-llvhk 0/1 Pending 0 11s kube-system metrics-server-7cd5fcb6b7-kxm5j 0/1 Pending 0 11s kube-system coredns-d76bd69b-jd6gm 0/1 Pending 0 11s 检查 node 提示 container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized。\n接下来就是安装 kube-ovn 了。\n安装 kube-ovn kube-ovn 的安装使用官方提供一键安装脚本：\nwget https://raw.githubusercontent.com/kubeovn/kube-ovn/release-1.10/dist/images/install.sh 查看脚本中的配置：\nREGISTRY=\u0026quot;kubeovn\u0026quot; # 镜像仓库地址 VERSION=\u0026quot;v1.10.6\u0026quot; # 镜像版本/Tag POD_CIDR=\u0026quot;10.16.0.0/16\u0026quot; # 默认子网 CIDR 不要和 SVC/NODE/JOIN CIDR 重叠 SVC_CIDR=\u0026quot;10.96.0.0/12\u0026quot; # 需要和 apiserver 的 service-cluster-ip-range 保持一致 JOIN_CIDR=\u0026quot;100.64.0.0/16\u0026quot; # Pod 和主机通信网络 CIDR，不要和 SVC/NODE/POD CIDR 重叠 LABEL=\u0026quot;node-role.kubernetes.io/master\u0026quot; # 部署 OVN DB 节点的标签 IFACE=\u0026quot;\u0026quot; # 容器网络所使用的的宿主机网卡名，如果为空则使用 Kubernetes 中的 Node IP 所在网卡 TUNNEL_TYPE=\u0026quot;geneve\u0026quot; # 隧道封装协议，可选 geneve, vxlan 或 stt，stt 需要单独编译 ovs 内核模块 k3s 的默认 POD 和 SVC CIDR 分别是：10.42.0.0/16 和 10.43.0.0/16，可以在安装时通过参数 --cluster-cidr 和 --service-cidr 分别进行设置。上面，安装 k3s 时使用了的默认配置，因此需要修改 install.sh 中的配置。\nPOD_CIDR=\u0026quot;10.42.0.0/16\u0026quot; POD_GATEWAY=\u0026quot;10.42.0.1\u0026quot; SVC_CIDR=\u0026quot;10.43.0.0/12\u0026quot; 修改之后，运行脚本安装：\nbash install.sh 确认所有 pod 启动并运行：\nkubectl get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system ovs-ovn-trqk5 1/1 Running 0 2m37s kube-system kube-ovn-monitor-5f8f5dbfc-plfrq 1/1 Running 0 108s kube-system kube-ovn-controller-67bbd54575-2n87d 1/1 Running 0 108s kube-system ovn-central-644fbb8467-xtrjc 1/1 Running 0 2m37s kube-system kube-ovn-cni-glcfw 1/1 Running 0 108s kube-system kube-ovn-pinger-2k72d 1/1 Running 0 71s kube-system local-path-provisioner-6c79684f77-jh2zh 1/1 Running 0 71s kube-system coredns-d76bd69b-w28rg 1/1 Running 0 37s kube-system metrics-server-7cd5fcb6b7-8jgdm 1/1 Running 0 36s 检查 kube-system 下的 DaemonSet 类型应用，运行在各个 node 上负责 ovs/ovn、CNI 和网络检查 ping。\nkubectl get ds -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ovs-ovn 1 1 1 1 1 kubernetes.io/os=linux 3m14s kube-ovn-cni 1 1 1 1 1 kubernetes.io/os=linux 2m25s kube-ovn-pinger 1 1 1 1 1 kubernetes.io/os=linux 2m25s 至此 kube-ovn 就安装完成了，下面简单体验下 kube-ovn 的功能。\n体验 子网 前面介绍 kube-ovn 安装时提到了子网，子网 是 kube-ovn 的核心概念。\n Kube-OVN 会以子网来组织 IP 和网络配置，每个 Namespace 可以归属于特定的子网， Namespace 下的 Pod 会自动从所属的子网中获取 IP 并共享子网的网络配置（CIDR，网关类型，访问控制，NAT控制等）。\n 我们部署一个 pod，可以看到 k3s 从 kube-ovn 的默认子网中分配 IP 地址。所有没有设置子网的 namespace 的 pod IP 地址都位于该网段。\nkubectl run pipy --image flomesh/pipy:latest -n default kubectl get po -o wide -n default NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pipy 1/1 Running 0 41s 10.42.0.10 ubuntu-dev1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 我们手动创建一个 namespace 和新的子网：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Namespace metadata: creationTimestamp: null name: another --- apiVersion: kubeovn.io/v1 kind: Subnet metadata: name: another-subnet spec: protocol: IPv4 cidrBlock: 10.66.0.0/16 excludeIps: - 10.66.0.1 gateway: 10.66.0.1 gatewayType: distributed natOutgoing: true namespaces: - another EOF 接下来验证一下，在 another namespace 下创建新的 pod：\nkubectl run curl --image rancher/curl --command sleep 1d -n another kubectl get po -o wide -n another NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES curl 1/1 Running 0 14s 10.66.0.11 ubuntu-dev1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以看到 pod curl 从新的子网中获取了 IP 地址，试着访问之前创建的 pod：\nkubectl exec -it curl -n another -- curl -i 10.42.0.10:8080 HTTP/1.1 200 OK content-length: 11 connection: keep-alive Hi, there! 不同的子网有什么用呢？试想这样的场景，有两个 namespace another 和 default。要禁止 annother 中的 pod 对 default 中pod 的访问，第一时间想到的是 Kubernetes 的 Network Policy，或者借助其他的应用，比如之前介绍的 使用 Cilium 增强 Kubernetes 网络安全（也是网络内核层的实现），或者服务网格的访问控制。\n但现在既然用 kube-ovn，可以使用 ovn 的 ACL 规则。\n子网 ACL 接下来，更新默认子网的设置，添加 ACL 规则：\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: kubeovn.io/v1 kind: Subnet metadata: name: ovn-default spec: cidrBlock: 10.42.0.0/16 default: true excludeIps: - 10.42.0.1 gateway: 10.42.0.1 gatewayType: distributed natOutgoing: true protocol: IPv4 provider: ovn vpc: ovn-cluster acls: - action: drop direction: from-lport match: ip4.src == 10.66.0.0/16 \u0026amp;\u0026amp; ip priority: 1002 EOF 这段 ACL 规则表示：丢弃（action: drop）来自（direction: from-lport）10.66.0.0/16 网段（match: ip4.src == 10.66.0.0/16 \u0026amp;\u0026amp; ip）的包，更多规则说明参考 ACL 规则文档。\n此时再次访问，但是这回加上连接超时，因为不想等太久：\nkubectl exec -it curl -n another -- curl -i --connect-timeout 5 10.42.0.10:8080 curl: (28) Connection timed out after 5000 milliseconds command terminated with exit code 28 another namespace 下的 pod 无法访问到 default 下的 pod，符合预期。\n总结 这篇仅仅展示了 kube-ovn 很基础的功能，基于 ovs/ovn 的强大能力，kube-ovn 还提供了其他更强大的功能，有兴趣的同学可以去参考官方的使用指南。\n貌似应该要好好看看 ovs/ovn 了。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/09/03/pexelseberhardgrossgasteiger673018.jpg","permalink":"https://atbug.com/run-kube-ovn-on-k3s/","tags":["Network","Kubernetes"],"title":"k3s 上的 kube-ovn 轻度体验"},{"categories":["云原生"],"contents":"8 月 24 日，发明服务网格的公司 Buoyant 发布了 Linkerd 2.12，这是时隔近一年的版本发布。不知道大家的对新版本期待如何，在我来看新版本中的功能对于一年的时间来说确实不算多，但是我想说的是 Linkerd 还是那个 Linkerd，还是秉持着一贯的 “Keep it simple” 的设计理念。\n新版本的内容不一一介绍，其中最主要的功能：per-route 策略，相比上个版本基于端口的策略，扩展到了 per-route。\n而我想说的特别之处也就是在 per-route 和策略上。\nper-route 和策略有何特别之处？ 在《SMI 与 Gateway API 的 GAMMA 倡议意味着什么？》 层有过猜想：未来服务网格的标准有可能全部或者部分以 Kubernetes 原生 API 的方式存在。\n“网关 API，之于集群是入口/出口网关，之于 Pod 是 inbound/outbound sidecar。”\n现在来看，Linkerd 先迈出了这一步。\nper-route 路由（routing）是通过网络将流量从源地址传输到目的地的操作。不同的流量有不同的操作（路由），流量路由的前提是流量的识别，通俗来讲就是请求的匹配。对服务网格中常见的 TCP 流量，有源地址、源端口、目的地址、目的端口等标识；对于 HTTP 流量，标识则更多：路径、报头、参数等等。\nLinkerd 通过一个 CRD ServiceProfile 提供了 per-route 的 指标、重试和超时设置，该 CRD 中可以配置匹配规则来进行流量的识别。在新的 per-route 策略中，则又引入了新的流量识别机制 HTTPRoute 。这个 HTTPRoute 实际上是来自 Kubernetes Gateway API，以镜像的方式维护在 policy.linkerd.io/v1alpha1 包中。\n至于为何要自己维护 HTTPRoute，在 Buoyant 的另一篇博客《Linkerd and the Gateway API》 中也给出了答案：目前的 Gateway API 虽易展现出成为标准的设计，但还不足以满足目前的需求。\n为何无法满足目前的需求，让我们继续来看策略。\n策略 2.12 带来的 per-route 策略是授权策略（Authorization Policy），通过 AuthorizationPolicy 及其他几个 CRD 来进行配置，与 HTTPRoute 一起维护在 policy.linkerd.io/v1alpha1 包中。从包名来看，policy.linkerd.io/v1alpha1 是考虑到了 Kubernetes Gateway API Policy Attachment的设计。\n图片来自 Kubernetes Gateway API 文档\n在服务网格的功能中有很多的策略：路由、认证/授权、超时、重试、熔断、限流、故障注入等等。而目前 Policy Attachment 还只是提供了概念设计，对这些策略没有任何具体的定义。因此 Linkerd 将其维护在 policy.linkerd.io/v1alpha1 包中，并乐观地相信有一天其可以被 Kubernetes 原生的 CRD 替代，且切换的成本微不足道。\n在此我也猜想，原本 ServiceProfile 中的策略也会被废弃，并转移到新的包中。因为 ServiceProfile 的设计本就不够优雅，将流量的标识与策略耦合（估计这也是 Linkerd 的 Keep it simple 执念，尽量少定义 CRD）。\n无独有偶 架构设计总是伴随着取舍，在结构化和可扩展性间进行着平衡。Linkerd 秉持 Keep it simple 的设计理念，尽量避免定义过多的 CRD，降低复杂度。不只是这次的 HTTPRoute，其流量拆分上使用了Service Mesh Interface（SMI）的 TrafficSplit API（Linkerd 支持 v1alpha2，最新为 v1alpha4）。\n简化设计，降低学习和维护的成本；降低资源需求，降低使用成本；提供最小功能集，“够用就好”，减少冗余功能带来的成本提升。\n服务网格面世 5 年来，用户逐渐意识到“轻、快、易用”对于服务网格的重要性。这条路上 Linkerd 并不是孤独的，国产的服务网格产品 Flomesh 开源的 osm-edge 也是同样的简化设计，轻量级高性能的 sidecar 代理，此外还有着：\n 开放的控制平面设计，通过扩展支持更多的服务发现。 可编程的sidecar 代理 Pipy，功能扩展更容易；除了作为服务网格代理，还可以独立使用。 基于 SMI 的实现。SMI 与 Gateway API 都是同样的流量标识与策略分离的设计，简单的对比可以看这篇。  总结 这篇文章是 Linkerd 2.12 发表之际的一些启发和感想，版本发布只是表面，更要探寻表面之外的深意。利用周末的时间来挖掘一下，写下这篇。\n写作的同时，有一个词浮出脑海：返璞归真。下一篇，目前只想到了标题《服务网格：少即是多，Less is more》。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/08/28/doorsg0b30f61201280.png","permalink":"https://atbug.com/thoughts-on-linkerd-release/","tags":["Service Mesh","Kubernetes"],"title":"另眼旁观  Linkerd 2.12 的发布：服务网格标准的曙光？"},{"categories":["源码解析"],"contents":"本文的源码基于 Kubernetes v1.24.0，容器运行时使用 Containerd 1.5，从源码来分析 kubectl port-forward 的工作原理。\n通过 port-forward 流程的分析，梳理出 kubectl -\u0026gt; api-server -\u0026gt; kubelet -\u0026gt; 容器运行时 的交互，了解 cri 的工作方式。\nkubectl 简单创建个 pod：\nkubectl run pipy --image flomesh/pipy:latest -n default 在执行 kubectl forward 时添加参数 -v 9 打印日志。\nkubectl port-forward pipy 8080 -v 9 ... I0807 21:45:58.457986 14495 round_trippers.go:466] curl -v -XPOST -H \u0026#34;User-Agent: kubectl/v1.24.3 (darwin/arm64) kubernetes/aef86a9\u0026#34; -H \u0026#34;X-Stream-Protocol-Version: portforward.k8s.io\u0026#34; \u0026#39;https://192.168.1.12:6443/api/v1/namespaces/default/pods/pipy/portforward\u0026#39; I0807 21:45:58.484013 14495 round_trippers.go:553] POST https://192.168.1.12:6443/api/v1/namespaces/default/pods/pipy/portforward 101 Switching Protocols in 26 milliseconds I0807 21:45:58.484029 14495 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 26 ms I0807 21:45:58.484035 14495 round_trippers.go:577] Response Headers: I0807 21:45:58.484040 14495 round_trippers.go:580] Upgrade: SPDY/3.1 I0807 21:45:58.484044 14495 round_trippers.go:580] X-Stream-Protocol-Version: portforward.k8s.io I0807 21:45:58.484047 14495 round_trippers.go:580] Date: Sun, 07 Aug 2022 13:45:58 GMT I0807 21:45:58.484051 14495 round_trippers.go:580] Connection: Upgrade Forwarding from 127.0.0.1:8080 -\u0026gt; 8080 Forwarding from [::1]:8080 -\u0026gt; 8080 从日志可以看到请求的地址为 /api/v1/namespaces/default/pods/pipy/portforward，其中 portforward 为 pod 资源的子资源。\n 这里使用的协议是 spdy。\n kubectl 此时会监听本地端口，同时使用 pod 子资源 portforward 的 url 创建到 api-server 的连接。\n当本地端口有连接接入时，kubectl 会不断地在两个连接间拷贝数据。\n参考源码：  staging/src/k8s.io/kubectl/pkg/cmd/portforward/portforward.go:389 staging/src/k8s.io/client-go/tools/portforward/portforward.go:242 staging/src/k8s.io/client-go/tools/portforward/portforward.go:330  api-server pod 的三个子资源 exec、attach 和 portforward，对这三个资源的操作都会代理有对应 node 的 kubetlet server 进行处理。\napi-server 在接收到访问 pod 子资源 portforward 的请求后，通过 pod 及其所在 node 的信息，获取访问该 node 上 kubelet server 的 url。\n然后将访问 pod 的 portforward 的请求，代理到 kubelet server。\n参考源码  pkg/registry/core/pod/rest/subresources.go:185  kubelet portforward 请求来到了 pod 所在节点的 kubelet server，在 kubelet server 中，有几个用于调试的 endpoint，portforward 便是其中之一：\n /run/{podNamespace}/{podID}/{containerName} /exec/{podNamespace}/{podID}/{containerName} /attach/{podNamespace}/{podID}/{containerName} /portforward/{podNamespace}/{podID} /containerLogs/{podNamespace}/{podID}/{containerName} /runningpods/  kubelet server 收到请求后，首先会通过 RuntimeServiceClient 发送 gRCP 请求到容器运行时的接口（/runtime.v1alpha2.RuntimeService/PortForward）获取容器运行时 streaming server 处理 pordforward 请求的 url。\n拿到 portforward streaming 的 url 之后，kubelet server 将请求代理到该 url。\n参考源码  pkg/kubelet/server/server.go:463 pkg/kubelet/server/server.go:873 pkg/kubelet/cri/streaming/portforward/portforward.go:46 pkg/kubelet/cri/streaming/server.go:111  cri 这里以 Containerd 为例。\nContainerd 在启动时会启动 runtime service 和 image service。前者是负责容器相关的操作，后者负责镜像相关的操作。\nkubelet 获取用于端口转发的 streaming url，就是调用了 runtime service 的 gRPC 接口完成的。\n除了两个 gRPC service 以外，还加载了一系列插件。这些插件中，其中有一个是 cri service。\ncri service 会启动 streaming server。这个 server 会响应 /exec、/attach 和 /portforward 的 stream 请求。\nportforward 支持两种操作系统 linux 和 windows：sandbox_portforward_linux.go 和 sandbox_portforward_windows.go。\n在 linux 上，在 pod 所在的 network namespace 中使用地址 localhost 创建到目标端口的连接。然后在 streaming server 的连接和该连接之间拷贝数据，完成数据的传递。\n在 windows 上，是通过 wincat.exe 使用地址 127.0.0.1 创建到目标端口的连接。\n参考源码  pkg/cri/streaming/server.go:149 pkg/cri/server/streaming.go:69 pkg/cri/server/service.go:138 pkg/cri/server/sandbox_portforward_linux.go:34  总结 结合源码分析对 port-foward 工作原理的梳理，相信对 cri 的工作方式也有了一定的了解。本文是以容器运行时 Containerd 为例，不同的容器运行时虽然实现了 cri，但是实现的细节上也会有所差异。\n比如在 port-forward 的实现上，Kubernetes v1.23.0 版本中的 docker shim（1.24 中被移除） 中，是使用nsenter 进入 pod 所在的 network namespace 中通过 socat 完成的端口转发。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/04/pexelscomputersourcecode.png","permalink":"https://atbug.com/how-kubectl-port-forward-works/","tags":["Kubernetes","Container","CRI"],"title":"源码解析 kubectl port-forward 工作原理"},{"categories":["云原生"],"contents":"就在上周 Gateway API 发布版本 0.5.0，其中几个最重要的 CRD Gateway、GatewayClass 以及 HTTPRoute 第一次升级到了 beta 版本。升级的详细内容这里不做详谈，我也想说说的是与版本一同发布的，也是很容易被忽略的社区合作倡议 GAMMA。\nGAMMA 是 Gateway API Mesh Management and Administration 的简写，这个计划是 Gateway API 子项目中的一个专用工作流。这个小组的目标是调研、设计和跟踪 Gateway API 资源、语义和其他与服务网格技术和用例相关的组件。此外，小组还努力倡导服务网格项目的 Gateway API 实现之间的一致性，无论使用何种技术栈或者代理。\n可能你会有疑问如此简单一个倡议，有什么特别？有两点：\n 规范的参与：这个倡议由 SIG Network（Gateway API 所在的 Kubernetes SIG）与服务网格社区共同发起，服务网格社区有来自 Cilium Service Mesh、Consul、Istio、Kuma、Linkerd、SMI、NGINX Service Mesh 和 Open Service Mesh 的代表。SMI 与其他几个不同，它是服务网格的规范 API，并非是实现，Gateway API 也一样。 面向东西向流量：小组的首个工作探索使用 Gateway API 处理东西向流量已经开始。东西向流量，也就是服务网格中服务到服务的流量。  可见，服务网格部分 API 的统一将迈进一步，且成为 Kubernetes 原生 API 的一员。当然现在还言之过早，还需看各个厂商的跟进程度。\n接下来，我们分别从规范，以及当前的实现两个角度进行分析。\n规范 SMI 与 Gateway API 一样都是规范，前者用于服务网格，而后者用于网关。但二者在流量规范的部分存在明显重叠（见下图中红色的部分），流量规范可以可以理解为流量标识（traffic identify）。流量标识作为流量管理的关键，将流量标识之后才能进行流量路由、分流、限流等操作。\n在 SMI issue 249 发起了讨论，关于 SMI 能否使用 Gateway API 的流量规范，将精力集中在服务网格的规范中。\n只能说 Gateway API 作为 Kubernetes 的原生 API，有着更强的说服力，其他服务网格厂商更容易接纳。\n已经对 SMI 和 Gateway API 有了解的同学，可以直接跳过规范的介绍。\n服务网格的规范 SMI SMI（Service Mesh Interface） 首次登场在 2019 年，并于 2020 年加入 CNCF。\n在开放服务网格 Open Service Mesh 如何开放一文中曾介绍过 SMI，有兴趣的可以浏览各个 API 的说明及示例。\n SMI 是服务网格的规范，重点关注在 Kubernetes 上运行的服务网格。它定义了一个可以由各种供应商使用的通用标准。允许最终用户的标准化和服务网格技术提供商的创新。SMI 实现了灵活性和互操作性，并涵盖了最常见的服务网格功能。\n  SMI 提供了 Kubernetes 服务网格的标准接口、常见服务网格场景的基本功能集、支持服务网格新功能的灵活性，以及服务网格技术生态的创新空间。\n  SMI 的目标是将概念与实现隔离开来，像软件开发过去一直做的那样，将复杂的东西分层抽象。\n SMI 规范的最新版本是 0.6.0，覆盖了常见的流量访问控制、遥测、管理等基础服务网格能力。与规范对应的是 API，各个网格供应商基于该 API 进行实现。\nKubernetes Gateway API Kubernetes Gateway API是由SIG-NETWORK 社区管理的开源项目，是一种规范，不提供实现。Gateway API 被认为是 Kubernetes Ingress API 的继任者，在 2019 年的Ingress 革命中首次被提出，并在 2020 年 11 月发布了第一个版本。\nGateway API 是一系列资源的集合，相比 Ingress API，Gateway API 最大的进步就是将流量规范独立成 API，大大提升了扩展性和灵活性。目前提供了 HTTPRoute、TCPRoute、UDPRoute 和 TLSRoute 规范来匹配特定协议的规则。\n流量规范独立之后，网关的实现、基础设施的管理以及流量路由的定义得到了解耦合，这得益于 Gateway API 面向角色的 API 设计：\n 厂商实现了Gateway API 并定义了自己 GatewayClass 类型，一系列的实现可供选择。 集群管理员安装 Gateway API 的实现，部署跨命名空间的共享网关实例，或者命名空间独享的网关实例。如下图，集群管理员部署了跨 store 和 site 命名空间的网关 foo。 开发人员创建 HTTPRoute 资源将流量路由到指定的后端服务。  现有的流量规范 这里主要讨论的是服务网格中所面对的东西向流量，部分代码片段来自探索使用 Gateway API 处理东西向流量。\n从代码片段不难看出，几乎各家都有各自特别的实现。\nSMI TrafficSplit SMI 的 TrafficSplit 定义了流量与后端服务（service）的映射关系（路由）。通过 service 的 ClusterIP 进行流量的匹配，然后进一步使用 HTTPRouteGroup 对 HTTP 流量进行匹配。并且，服务的映射限制在同一个命名空间下。\n比如下面示例将访问 service website 的全部 Firefox 流量路由到 website-v2 服务。\nSMI 的实现 osm 会穷举出 spec.service 上指定的 service 的所有 FQDN。\nkind: TrafficSplit metadata: name: ab-test spec: service: website matches: - kind: HTTPRouteGroup name: ab-test backends: - service: website-v1 weight: 0 - service: website-v2 weight: 100 --- kind: HTTPRouteGroup metadata: name: ab-test matches: - name: firefox-users headers: user-agent: \u0026#34;.*Firefox.*\u0026#34; Istio VirtualService VirtualService 中的流量匹配使用了 hosts，进一步通过一些基于协议的规则进行匹配。不同的是，协议规则的匹配不是通过独立的 CRD 来维护的，而是耦合在 VirtualService中。\nspec: hosts: - reviews.prod.svc.cluster.local # could also be just \u0026#34;reviews\u0026#34; http: - match: - uri: prefix: \u0026#34;/frontpage\u0026#34; route: - destination: host: frontpage.prod.svc.cluster.local Kuma TrafficRoute 从配置上可以看出流量的匹配是通过 service 的 ClusterIP 实现的。\nspec: sources: - match: kuma.io/service: backend_default_svc_80 destinations: - match: kuma.io/service: redis_default_svc_6379 conf: http: - match: … Cilium CiliumEnvoyConfig 与 Kuma 一样，通过 service 的 ClusterIP 进行流量匹配。但不同的是，提供了跨命名空间的流量路由。\nspec: services: - name: httpbin namespace: default listener: envoy-lb-listener backendServices: - name: echo namespace: default resources: … Consul ServiceRouter 使用 ServiceRouter 资源名（web）作为 service 名，然后通过其 ClusterIP 进行流量匹配。\napiVersion: consul.hashicorp.com/v1alpha1 kind: ServiceRouter metadata: name: web spec: routes: - match: http: pathPrefix: /admin destination: service: admin Linkerd ServiceProfile ServiceProfile 将名字与请求的 Host 头进行匹配。然后 routes 下挂不同的 endpoint 定义（如 /admin、/store ），在 endpoint 的粒度上进行 HTTP 协议属性的匹配、超时、重试等策略的配置。\napiVersion: linkerd.io/v1alpha2 kind: ServiceProfile metadata: name: webapp.my-service-namespace.svc.cluster.local spec: routes: … 未来的设计 在看未来的可能设计之前，先看下 Kubernetes Gateway API 如何完成流量匹配的：\napiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: my-route namespace: gateway-api-example-ns2 spec: parentRefs: - kind: Gateway name: foo-gateway namespace: gateway-api-example-ns1 hostnames: - foo.example.com rules: - backendRefs: - name: foo-svc port: 8080 从 HTTPRoute 的定义来看，存在着两个映射（这里的映射，也可以看做附着点）：\n 与网关实现（foo-gateway.gateway-api-example-ns1）的映射（通过 parentRefs 实现），也就是说将流量匹配规则写入到哪个网关实例。 与服务（foo-svc:8080）的映射（通过 hostnames 实现）。  通过 HTTPRoute 上的附着点，将通过网关实现进入的流量，与后端服务进行关联。先通过 spec.hostnames 进行流量的匹配，然后再由 spec.rules 中的匹配规则完成协议属性的匹配。HTTP 协议可以使用 method、path、参数、请求头进行匹配，完成 7 层的路由。\n东西向流量 在东西向流量的场景，需要实现：\n 与网格实现的映射，将流量匹配规则写入到某种网格的 sidecar 中。 与服务的映射，如何将进入到网格 sidecar 流量与目标服务（网格内的服务，或者网格外的服务）进行匹配。  因此需要一种设计或者 CRD 来承载如上的关系，截止发稿时目前在探索使用 Gateway API 处理东西向流量中已经有 5 种方式来实现，有兴趣的同学可以了解一下。\n总结 还是那句话，最终会使用何种方式，都为时尚早。\n但已知的是，目前社区也意识到服务网格是实现上的分裂，而未来服务网格的标准有可能全部或者部分以 Kubernetes 原生 API 的方式存在。如果是后者，我认为 SMI 会有走出来的可能。正如在开放服务网格 Open Service Mesh 如何开放中，谈及较多的标准与开放关乎着生态的发展、创新、扩展性和灵活性。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/07/22/pexelsananddandekar1532771.jpg","permalink":"https://atbug.com/why-smi-collaborating-in-gateway-api-gamma/","tags":["Kubernetes","Service Mesh"],"title":"SMI 与 Gateway API 的 GAMMA 倡议意味着什么？"},{"categories":["笔记"],"contents":"GitHub Actions 是一个功能强大、“免费” 的 CI（持续集成）工具。\n与之前介绍的 Tekton 类似，GitHub Actions 的核心也是 Pipeline as Code 也就是所谓的流水线即代码。二者不同的是，GitHub Actions 本身就是一个 CI 平台，用户可以使用代码来定义流水线并在平台上运行；而 Tekton 本身是一个用于构建 CI/CD 平台的开源框架。\nPipeline as Code，既然与代码扯上了关系。那流水线的定义就可繁可简了，完全看需求。小到一个 GitHub Pages，大到流程复杂的项目都可以使用 GitHub Actions 来构建。\n本篇文章不会介绍如何使用 GitHub Actions 的，如果还未用过的同学可以浏览下官方的文档。今天主要来分享下如何在 Kubernetes 上的自托管资源来执行流水线作业。\n0x01 背景 在介绍 GitHub Actions 的时候，免费带上了引号，为何？其作为一个 CI 工具，允许用户定义流水线并在平台上运行，需要消耗计算、存储、网络等资源，这些运行流水线的机器称为 Runner。GitHub 为不同类（等）型（级）的用户每月提供了不同的免费额度（额度用完后，每分钟 0.008 美元。），见下图。不同类型的主机，分钟数的消耗倍数也不同：Linux 为 1、macOS 为 10、Windows 为 2。\n拿免费用户来看，每月 2000 分钟看似也不少。比如笔者个人就是拿来构建下博客静态页面，以及几个简单的应用，每个月也用不了太多。但对于企业或者组织来说，尤其是当流水线的触发频繁（每次代码提交触发）、或者项目的单元测试耗时很长（bug 引起的或者项目本身的复杂度所致），积少成多也会变成一笔不小的开支。\n那有没有办法使用自己的资源来运行流水线呢？有。GitHub Actions Runner 分为两种：Github 托管的 Runner 和自托管的 Runner。我们可以将自己的资源作为自托管的 Runner 来运行流水线，而且还可以借助 Kubernetes 的能力来管理这些 Runner。\n同时，自托管 Runner 也适合那些对 CI 有更高要求的用户，比如更高性能、更多类型的计算资源等等。\n0x02 准备工作 Kubernetes 集群 我们使用 k3s 快速创建一个单节点的集群。\nexport INSTALL_K3S_VERSION=v1.22.11+k3s2 curl -sfL https://get.k3s.io | sh -s - --disable traefik --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 使用 Github Actions 构建的项目 这里使用之前做的一个 graalvm+maven 的基础镜像仓库来进行测试：https://github.com/addozhang/docker-graalvm-maven。\nGitHub Access Token 参考文档，创建 Access Token。\n注意：鉴于本演示的需要，分配完全的 repo 访问权限。\n0x03 创建 Runner GitHub Actions Runner 的程序源码是开源的，GitHub 托管的 Runner 也是使用该程序运行的。\nRunner 可以是某个仓库使用，也可以由组织下的所有仓库共享。鉴于这里演示用的项目，我们为上面提到的项目仓库创建 Runner。\nRunner 在启动时，会通过 GitHub API 将自己注册到 GitHub Actions；然后不断发送请求到 GitHub 查看是否分配了流水线作业，如有则会执行流水线作业；Runner 停止运行时，需要进行注销的操作。这里的一系列操作，都会用到上面创建的 Access Token。\n要在 Kubernetes 上运行，我们要将 Runner 应用以及上面的逻辑打成镜像，并以 Deployment 的方式部署到 Kubernetes 中。然后通过 workflow 作业 webhook 查看排队的作业数来决定是否增加或者减少 Deployment 的副本数。\n听起来就有些复杂，但确实是实现的基本方式。这里我们使用一个 Kubernetes controller actions-runner-controller/actions-runner-controller 来实现所有的流程。\nactions-runner-controller 提供了三种 CRD：\n Runner：可以理解为 Pod，该 Runner 只能执行一次作业。 RunnerDeployments：可以理解为 Deployment，可以设置要创建的 Runner 数量。Runner 在执行完作业后会销毁，然后 Controller 会创建新的 Runner 等待作业调度。 RunnerSets：可以理解为 StatefulSet，也是基于 StatefulSet 来创建 Pod（即 Runner），提供 StatefulSet 的特性。  更多用法，可以参考 actions-runner-controller 官方文档。\n部署 Cert Manager actions-runner-controller 的运行，需要使用 cert-mananger。通过下面的命令来部署：\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.yaml 检查 pod 是否成功启动：\nkubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-66b646d76-7b9fz 1/1 Running 0 18s cert-manager-cainjector-59dc9659c7-rzlrl 1/1 Running 0 18s cert-manager-webhook-7d8f555998-6zkqp 1/1 Running 0 18s 启动 Controller 接下来就是启动 controller，本文发布时的最新版本是 v0.25.0。使用下面的命令部署 controller：\nkubectl create -f https://github.com/actions-runner-controller/actions-runner-controller/releases/download/v0.25.0/actions-runner-controller.yaml  注：这里使用的 create 而非 apply。使用 apply 会报类似下面的错误： Error from server (Invalid): error when creating \u0026ldquo;https://github.com/actions-runner-controller/actions-runner-controller/releases/download/v0.25.0/actions-runner-controller.yaml\u0026quot;: CustomResourceDefinition.apiextensions.k8s.io \u0026ldquo;runnerdeployments.actions.summerwind.dev\u0026rdquo; is invalid: metadata.annotations: Too long: must have at most 262144 bytes\n 此时，pod 无法启动，还需要为其创建 Secret 来提供 GitHub Access Token：\nexport GITHUB_TOKEN=\u0026lt;TOKEN_HERE\u0026gt; kubectl create secret generic controller-manager \\  -n actions-runner-system \\  --from-literal=github_token=${GITHUB_TOKEN} 现在可以看到 pod 可以成功运行：\nkubectl get pods -n actions-runner-system NAME READY STATUS RESTARTS AGE controller-manager-58c598f64d-27xn6 2/2 Running 0 40s 创建 Runner 执行下面的命令，创建一个名为 building-runner 的 Runner CR。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: actions.summerwind.dev/v1alpha1 kind: Runner metadata: name: building-runner spec: repository: addozhang/docker-graalvm-maven env: [] EOF 此时，会发现 controller 创建了一个同名的 pod：\nkubectl get pods -l actions-runner=\u0026#34;\u0026#34; -n actions-runner-system NAME READY STATUS RESTARTS AGE building-runner 2/2 Running 0 16s 在仓库的 Settings/Actions/Runners 中可以看到同名的 Runner，处于 idle 状态。\n假如此时启动流水线作业，会发现作业并没有调度该 Runner 上。这是因为还没有将作业 Runner 的 label 设置为该 runner 的 label：self-hosted、Linux、 X64。\n修改流水线定义，将 runs-on 指定为 [self-hosted, linux, X64]：\n然后可以查看作业成功调度到该 runner 上运行：\n现在再去看 pod 的状态，其中的 runner 容器是 Completed：\nkubectl get pods -l actions-runner=\u0026#34;\u0026#34; -n actions-runner-system NAME READY STATUS RESTARTS AGE building-runner 1/2 Running 0 3m53s 此时，再去触发流水线执行，作业会一直等待可用的 runner 来执行：\nrunner 无法重复使用，怎么办？\n0x04 可重用 Runner RunnerDeployment 要派上用场了，前面提到可以将 RunnerDeployments 理解为 Deployment，可以设置要创建的 Runner 数量。Runner 在执行完作业后会销毁，然后 Controller 会创建新的 Runner 等待作业调度。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: actions.summerwind.dev/v1alpha1 kind: RunnerDeployment metadata: name: building-runner spec: template: spec: repository: addozhang/docker-graalvm-maven env: [] EOF 使用上面的命令创建 RunnerDeployment 之后，controller 会创建新的 pod（runner）并得到作业的调度；完成作业的执行后，pod building-runner-9k4cj-ml46v 被销毁，新的 pod building-runner-9k4cj-f8twz 被创建并等待作业的调度：\nkubectl get events --sort-by=\u0026#39;metadata.creationTimestamp\u0026#39; ... 3m7s Normal PodCreated runner/building-runner-9k4cj-ml46v Created pod \u0026#39;building-runner-9k4cj-ml46v\u0026#39; 3m6s Normal Scheduled pod/building-runner-9k4cj-ml46v Successfully assigned actions-runner-system/building-runner-9k4cj-ml46v to ubuntu-dev1 3m7s Normal RegistrationTokenUpdated runner/building-runner-9k4cj-ml46v Successfully update registration token 3m6s Normal Pulling pod/building-runner-9k4cj-ml46v Pulling image \u0026#34;summerwind/actions-runner:latest\u0026#34; 3m3s Normal Started pod/building-runner-9k4cj-ml46v Started container docker 3m3s Normal Pulled pod/building-runner-9k4cj-ml46v Successfully pulled image \u0026#34;summerwind/actions-runner:latest\u0026#34; in 3.06531539s 3m3s Normal Created pod/building-runner-9k4cj-ml46v Created container runner 3m3s Normal Started pod/building-runner-9k4cj-ml46v Started container runner 3m3s Normal Created pod/building-runner-9k4cj-ml46v Created container docker 3m3s Normal Pulled pod/building-runner-9k4cj-ml46v Container image \u0026#34;docker:dind\u0026#34; already present on machine 2m6s Normal RegistrationTokenUpdated runner/building-runner-9k4cj-f8twz Successfully update registration token 2m6s Normal PodCreated runner/building-runner-9k4cj-f8twz Created pod \u0026#39;building-runner-9k4cj-f8twz\u0026#39; 2m5s Normal Scheduled pod/building-runner-9k4cj-f8twz Successfully assigned actions-runner-system/building-runner-9k4cj-f8twz to ubuntu-dev1 2m5s Normal Pulling pod/building-runner-9k4cj-f8twz Pulling image \u0026#34;summerwind/actions-runner:latest\u0026#34; 2m5s Normal Killing pod/building-runner-9k4cj-ml46v Stopping container docker 2m3s Normal Pulled pod/building-runner-9k4cj-f8twz Successfully pulled image \u0026#34;summerwind/actions-runner:latest\u0026#34; in 2.50468863s 2m3s Normal Created pod/building-runner-9k4cj-f8twz Created container runner 2m3s Normal Started pod/building-runner-9k4cj-f8twz Started container runner 2m3s Normal Pulled pod/building-runner-9k4cj-f8twz Container image \u0026#34;docker:dind\u0026#34; already present on machine 2m3s Normal Created pod/building-runner-9k4cj-f8twz Created container docker 2m3s Normal Started pod/building-runner-9k4cj-f8twz Started container docker 前面创建 RunnerDeployment 的时候没有指定副本数，也就是 runner 的数量，controller 只创建了一个 Runner。假设这个 RunnerDeployment 是某个组织下多个项目共用的，多个项目同时执行作业会怎样？\n这里我重新运行 3 个已经完成的作业，模拟作业的并发：\ncurl -X POST -H \u0026#34;Accept: application/vnd.github+json\u0026#34; -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/addozhang/docker-graalvm-maven/actions/runs/2643982502/rerun curl -X POST -H \u0026#34;Accept: application/vnd.github+json\u0026#34; -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/addozhang/docker-graalvm-maven/actions/runs/2643982274/rerun curl -X POST -H \u0026#34;Accept: application/vnd.github+json\u0026#34; -H \u0026#34;Authorization: token ${GITHUB_TOKEN}\u0026#34; https://api.github.com/repos/addozhang/docker-graalvm-maven/actions/runs/2643982274/rerun 或者 push 几个空的提交到仓库（不推荐，会有 commit 历史。测试项目随意）：\ngit commit --allow-empty -m \u0026#34;trigger action\u0026#34; git push 去 Actions 列表会发现，只有一个作业在运行，其他两个都是 queued 的等待状态。前面的作业执行完成后，后一个作业才会被执行。\n不支持并发？既然是类似 Deployment 的方式运行 runner，那是否有类似 HPA（水平 pod 自动扩缩容）的功能？\n0x05 自动伸缩 actions-runner-controller 在 3 个 Runner 的 CRD 之外，还提供了类似 HPA 的 CRD HorizontalRunnerAutoscaler，简称 HRA。\nHRA 可以根据指标 PercentageRunnersBusy 或者 TotalNumberOfQueuedAndInProgressWorkflowRuns 来对 runner 进行扩缩容，或者基于 GitHub Events（webhook）来进行扩缩容。这两种都各有优缺点，前者指标是通过 GitHub API 轮训等待的作业数，实现简单，但时效性差；后者基于事件触发时效性更佳，但是实现复杂，需要对外暴露访问端点接收 GitHub Event。\n这里为了演示，使用基于指标的方式进行扩缩容。\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: actions.summerwind.dev/v1alpha1 kind: HorizontalRunnerAutoscaler metadata: name: building-runner-autoscaler spec: scaleDownDelaySecondsAfterScaleOut: 30 scaleTargetRef: name: building-runner minReplicas: 1 maxReplicas: 5 metrics: - type: PercentageRunnersBusy scaleUpThreshold: \u0026#39;0.75\u0026#39; scaleDownThreshold: \u0026#39;0.25\u0026#39; scaleUpFactor: \u0026#39;2\u0026#39; scaleDownFactor: \u0026#39;0.5\u0026#39; EOF 还是通过 API 触发作业模拟并发执行，由于轮训间隔比较长（默认 1 分钟），自动扩容需要等待一段时间：\nkubectl get pods -l actions-runner=\u0026#34;\u0026#34; -n actions-runner-system NAME READY STATUS RESTARTS AGE building-runner-k6jlc-dk7gc 2/2 Running 0 1m34s building-runner-k6jlc-pbwnz 2/2 Running 0 54s building-runner-k6jlc-nlptp 2/2 Running 0 54s 0x06总结 GitHub Actions 是个很强大的 CI 工具，结合自托管的 Runner 可以在付出较低成本的基础上有更好的体验。本文也只是从满足需求的出发，对 “Runner on Kubernetes” 进行了探索。对一个工具从会用到用好，还有很长的路要走。但满足当前需求，已是足矣。\n有兴趣的同学可以更进一步思考：\n 如何限制 Runner 运行时的资源占用 持久化如何实现，比如 Maven 构建时的本地库，Nodejs 的 node_mudules 如何避免重复下载 自动扩缩容能否更加高效  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/07/10/runnersonk8s.png","permalink":"https://atbug.com/run-github-actions-runners-on-kubernetes/","tags":["Kubernetes","CICD"],"title":"在 Kubernetes 上执行 GitHub Actions 流水线作业"},{"categories":["翻译"],"contents":"本文翻译自 Jack Roper  的文章 Kubernetes Best Practice。\n 译者：文章中作者从应用程序开发、治理和集群配置三个方面给出了一些 Kubernetes 的最佳实践，同时翻译过程中也加入了我过往的一些使用经验。有误的地方，也欢迎大家指正。\n  在这篇文章中，我将介绍一些使用 Kubernetes (K8s) 时的最佳实践。\n作为最流行的容器编排系统，K8s 是现代云工程师掌握的事实标准。众所周知，不管使用还是维护 K8s 复杂的系统，因此很好地掌握它应该做什么和不应该做什么，并知道什么是可能的，将是一个好的开局。\n这些建议包含 3 大类中的常见问题，即应用程序开发、治理和集群配置。\n最佳实践目录  使用命名空间 使用就绪和存活探针（译者注：还有启动探针） 使用自动缩放 使用资源请求和约束 使用 Deployment、DaemonSet、ReplicaSet 或者 StatefulSet 跨节点部署 Pod 使用多节点 使用基于角色的访问控制（RBAC） 在外部托管Kubernetes集群（使用云服务） 升级Kubernetes版本 监控集群资源和审计策略日志 使用版本控制系统 使用基于Git的工作流程（GitOps） 缩小容器的大小 用标签整理对象（译者注：或理解为资源） 使用网络策略 使用防火墙  使用命名空间 K8s 中的命名空间对于组织对象、在集群中创建逻辑分区以及安全方面的考虑至关重要。 默认情况下，K8s 集群中有 3 个命名空间，default、kube-public 和 kube-system。\nRBAC 可用于控制对特定命名空间的访问，以限制组的访问以控制可能发生的任何错误的爆炸半径，例如，一组开发人员可能只能访问名为 dev 的命名空间，并且无法访问 production 命名空间。 将不同团队限制在不同命名空间的能力对于避免重复工作或资源冲突可能很有价值。\n还可以针对命名空间配置 LimitRange 对象，以定义部署在命名空间中的容器的标准大小。ResourceQuotas 还可用于限制命名空间内所有容器的总资源消耗。可以对命名空间使用网络策略来限制 pod 之间的流量。\n使用就绪和存活探针 Readiness（就绪）和 Liveness（存活）探针本质上都是健康检查的类型。这些是在 K8s 中使用的另一个非常重要的概念。\n就绪探针确保仅当 pod 准备好为请求提供服务时，才会将对 pod 的请求定向到它。如果它还没有准备好，那么请求将被定向到其他地方。为每个容器定义就绪探针很重要，因为在 K8s 中没有为这些设置默认值。例如，如果一个 pod 需要 20 秒才能启动并且缺少就绪探测，那么在启动期间定向到该 pod 的任何流量都会导致失败。就绪探针应该是独立的，并且不考虑对其他服务的任何依赖，例如后端数据库或缓存服务。\n存活探针试应用程序是否正在运行以将其标记为健康。例如，可以测试 Web 应用程序的特定路径以确保其响应。如果不是，该 pod 将不会被标记为健康，并且探测失败将导致 kubelet 启动一个新的 pod，然后将再次对其进行测试。这种类型的探测用作恢复机制，以防进程无响应。\n译者：在 Kubernetes 1.18 引入了 StartUp（启动）探针。当容器启动时间较长（要花较长的时间完成初始化工作），此时应该使用启动探针。如果启动探针探测不成功，其他探针则不会生效。\n译者：还有一点尽量为 Pod 内的所有容器都定义探针。\n使用自动缩放 在适当的情况下，可以使用自动缩放来动态调整 pod 的数量（Pod 水平自动缩放器，HPA）、pod 消耗的资源量（Pod 垂直自动缩放器, VPA）或集群中的节点数量（集群自动缩放器，CA），具体取决于 对资源的需求。\nPod 水平自动扩缩器还可以根据 CPU 需求扩展复制控制器、副本集或有状态集。\n使用缩放也带来了一些挑战，例如不在容器的本地文件系统中存储持久数据，因为这会阻止水平自动缩放。相反，可以使用 PersistentVolume。\n当集群上存在高度可变的工作负载并且可能根据需求在不同时间需要不同数量的资源时，集群自动缩放器非常有用。 自动删除未使用的节点也是省钱的好方法！\n译者：更多自动缩放的内容，可以浏览我之前翻译的Kubernetes 的自动伸缩你用对了吗？；HPA 除了可以基于 CPU 指标伸缩，还可以基于内存，或者自定义指标，可以浏览Kubernetes HPA 基于 Prometheus 自定义指标的可控弹性伸缩。\n使用资源请求和约束 应设置资源请求和约束（可在容器中使用的最小和最大资源量）以避免容器在未分配所需资源的情况下启动，或集群用尽可用资源。\n在没有限制的情况下，Pod 可以使用比所需更多的资源，从而导致可用资源总量减少，这可能会导致集群上的其他应用程序出现问题。节点可能会崩溃，并且调度程序可能无法正确调度新的 pod。\n如果没有请求，无法为应用程序分配足够的资源，它可能会在尝试启动或执行异常时失败。\n资源请求和限制以毫核和兆字节为单位定义可用的 CPU 和内存。请注意，如果进程超出内存限制，则该进程将终止，因此在所有情况下都可能不适合设置此值。如果容器超过 CPU 限制，则进程会受到限制。\n使用 Deployment、DaemonSet、ReplicaSet 或者 StatefulSet 跨节点部署 Pod 永远不应该直接使用 Pod 运行。相反，为了提高容错性，Pod 应该始终作为 Deployment、DaemonSet、ReplicaSet 或 StatefulSet 的一部分。然后可以在部署中使用反亲和性规则跨节点部署 pod，以避免所有 pod 调度到同一个节点上运行，如果该节点发生故障可能会导致服务停止。\n使用多节点 如果想提升容错性，在单个节点上运行 K8s 并不是一个好主意。集群中应该使用多个节点，以便可以在它们之间分散工作负载。\n使用基于角色的访问控制（RBAC） 在 K8s 集群中使用 RBAC 对于正确保护系统至关重要。可以为用户、组和 service account 分配权限，以在特定命名空间（角色）或整个集群（ClusterRole）上执行允许的操作。每个角色可以有多个权限。要将定义的角色绑定到用户、组或 service account，使用 RoleBinding 或 ClusterRoleBinding 对象。\nRBAC 角色授予应设置为使用最小权限原则，即仅授予所需的权限。例如，管理员组可能有权访问所有资源，而运维人员组可能能够部署，但不能读取 secret。\n在外部托管Kubernetes集群（使用云服务） 在自己的硬件上托管 K8s 集群可能是一项复杂的工作。云服务将 K8s 集群作为平台即服务 (PaaS) 提供，例如 Azure 上的 AKS（Azure Kubernetes 服务）或 Amazon Web Services 上的 EKS（亚马逊弹性 Kubernetes 服务）。利用这一点意味着底层基础设施将由云服务商管理，并且可以更轻松地完成有关扩展集群的任务，例如添加和删除节点，而让工程师管理 K8s 集群上运行的内容本身。\n升级Kubernetes版本 除了引入新功能外，新的 K8s 版本还包括漏洞和安全修复，这使得在集群上运行最新版本的 K8s 非常重要。对旧版本的支持可能不如新版本好。\n然而，迁移到新版本时应谨慎对待，因为某些功能可能会被废弃，也可能会添加新功能。此外，在升级之前，应检查集群上运行的应用程序是否与较新的目标版本兼容。\n监控集群资源和审计策略日志 监控 K8s 控制平面中的组件对于控制资源消耗非常重要。控制平面是 K8s 的核心，这些组件保持系统运行，因此对于正确 K8s 操作至关重要。 Kubernetes API、kubelet、etcd、controller-manager、kube-proxy 和 kube-dns 组成了控制平面。\n控制平面组件可以以最常见的 K8s 监控工具 Prometheus 兼容的格式输出指标。\n应该使用自动监控工具而不是手动管理告警。\n可以在启动 kube-apiserver 时打开 K8s 中的审计日志记录，以便使用选择的工具进行更深入的调查。audit.log 将详细记录向 K8s API 发出的所有请求，并应定期检查集群上可能存在的任何问题。Kubernetes 集群默认策略在 audit-policy.yaml 文件中定义，可以根据需要进行修改。\nAzure Monitor 等日志聚合工具可用于将日志从 AKS 发送到日志分析工作区，以便将来使用 Kusto 查询进行审讯。在 AWS Cloudwatch 上可以使用。第三方工具还提供更深入的监控功能，例如 Dynatrace 和 Datadog。\n最后，应该为日志设置一个保留期，通常为 30-45 天左右。\n使用版本控制系统 K8s 配置文件应该在版本控制系统 (VCS) 中进行管理。这带来了很多好处，包括提高安全性、启用更改的审计跟踪，并将提高集群的稳定性。应为所做的任何更改设置审批，以便团队可以在将更改提交到主分支之前对其进行评审。\n使用基于Git的工作流程（GitOps） K8s 的成功部署需要考虑团队使用的工作流程。使用基于 git 的工作流可以通过使用 CI/CD（持续集成 / 持续交付）管道实现自动化，这将提高应用程序部署效率和速度。CI/CD 还将提供部署的审计跟踪。Git 应该是所有自动化的单一事实来源，并将实现对 K8s 集群的统一管理。还可以考虑使用专用的基础架构交付平台，例如 Spacelift，它最近引入了 Kubernetes 支持。\n缩小容器的大小 较小的映像大小将有助于加快构建和部署速度，并减少容器在 K8s 集群上消耗的资源量。应尽可能删除不必要的软件包，并应优先使用诸如 Alpine 之类的小型操作系统分发映像。较小的图像可以比较大的图像更快地拉取，并且消耗更少的存储空间。\n遵循这种方法还将提供安全优势，因为恶意行为者的潜在攻击媒介将会减少。\n用标签整理对象 K8s 标签是附加到对象用来组织集群资源的键值对。标签应该是有意义的元数据，提供一种机制来跟踪 K8s 系统中不同组件的交互方式。\nK8s 官方文档中推荐的 Pod 标签包括名称、实例、版本、组件、部分和管理者。\n标签也可以以类似于在云环境中对资源使用标签的方式使用，以跟踪与业务相关的事物，例如对象所有权和对象应属于的环境。\n此外，还建议使用标签来详细说明安全要求，包括机密性和合规性。\n使用网络策略 应该使用网络策略来限制 K8s 集群中对象之间的流量。默认情况下，所有容器都可以在网络中相互通信，如果恶意行为者获得对容器的访问权限，从而允许他们遍历集群中的对象，这会带来安全风险。网络策略可以在 IP 和端口级别控制流量，类似于云平台中的安全组的概念，以限制对资源的访问。通常，默认情况下应拒绝所有流量，然后应制定允许规则以允许所需流量。\n使用防火墙 除了使用网络策略来限制 K8s 集群上的内部流量外，还应该在 K8s 集群前面放置防火墙，以限制来自外部环境对 API server 的请求。IP 地址应列入白名单并限制开放端口。\n总结 在设计、运行和维护 Kubernetes 集群时遵循本文中列出的最佳实践将使你在现代应用程序之旅中走上成功之路！\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/07/04/16569411884953.jpg","permalink":"https://atbug.com/translate-kubernetes-best-practices/","tags":["Kubernetes"],"title":"译：Kubernetes 最佳实践"},{"categories":["翻译"],"contents":"本篇文章译自 SUNKU RANGANATH 的 4 Types of Edge Computing - Broadly Categorized。文章通过 往返终端设备和数据中心的延迟 来对边缘计算的类型进行大致的分类，通俗易懂，方便大家对边缘计算有个大概的了解。\n 边缘计算被认为是分布式计算的下一个前沿。 然而，并不是每个人都知道什么是边缘计算以及存在多种类型的边缘计算。\n简单地说，边缘计算通过将计算靠近最终用户，实现了过多设备（通常在 5G 中称为用户设备 UE）和电信数据中心核心之间的连接。\n促成边缘计算的关键因素是 UE 和需要高度分布式架构的计算服务器之间的往返通信所带来的延迟。\n了解各种类型边缘计算的一种简单方法是基于其靠近终端设备的程度以及与数据中心的往返延迟。 将延迟作为主要因素，边缘计算可以大致分为以下几类：\n IoT 边缘 本地边缘 接入边缘 网络边缘  IoT 边缘 这种边缘的延迟预期通常小于 1 毫秒。\n这几乎涵盖了任何连接到私有或公共网络（如互联网）的设备。该设备可以是能够处理数据的智能设备，例如移动电话或中继周围环境信息的简单传感器。\n包括但不限于零售亭、摄像头、工厂传感器、联网汽车、无人机、联网路灯、智能停车咪表、远程手术设备等。\n本地边缘 这种边缘的延迟预期通常小于 5 毫秒。\n人们需要一种方法来聚合来自物联网边缘的众多设备的数据，以便使用数据存储、处理、分析和响应相关请求。本地边缘通过帮助本地化数据处理并减少数据处理时间，在企业内部提供计算资源。此边缘的设备通常连接到网络边缘或数据中心以获取进一步的请求。\n大型企业、工业制造车间、大型零售业务等企业可以从这些类型的部署中受益，从而能够在接近其来源的地方处理数据，同时仍然拥有拥有必要硬件的所有权。\n通用客户端设备 (u-CPE) 是其中一个例子，它在本地部署的单个设备中提供防火墙、WAN 优化器、路由器的组合。\n网络要求通常使用软件定义的广域网 (SD-WAN) 来满足，它允许企业利用 MPLS 或 LTE 或宽带互联网服务的组合。\n5G 允许能够在本地建立小型基站，并在许可和非许可频谱中运行。\n接入边缘 这种边缘的延迟预期通常小于 10 - 40 毫秒。\n曾经可用作固定功能设备的传统无线电接入网络 (RAN) 现在已被分解为使用现成的服务器在软件中作为一组虚拟功能运行。RAN是将无线设备连接到电信运营商网络核心的关键点。\n虚拟 RAN (vRAN) 等概念以及 Open-RAN 和 O-RAN 等行业计划使界面能够管理这些虚拟化部署，类似于管理任何其他边缘设备。转向 RAN 功能的云原生实例化使用利用 DevOps 模型的持续集成 (CI)/持续部署 (CD) 结构来简化广泛的 RAN 部署的生命周期管理。\n部署和管理软件 RAN 功能所需的一组基础设施可以广义地称为接入边缘。\n网络边缘 这种边缘的延迟预期通常也小于 10 - 40 毫秒。\n来自多个 IoT 边缘、本地边缘和接入边缘的数据需要聚合，以满足特定区域的需求，然后才能连接到可以跨越大量区域的集中式数据中心。\n这种类型的部署可以广义地称为“网络边缘”或与服务提供商的核心数据中心相关的近边缘。\nNext Generation Central Office (NGCO) 架构旨在解决网络边缘的要求。这种边缘的另一个例子是有线固定接入边缘，它提供宽带服务，如 IPTV、VoIP、互联网服务等。\n随着固定移动融合的出现，由于架构融合，该边缘的各种设备类型之间的界限正在变窄。\n总结 边缘计算正以各种部署模型成为主流。上面分享的 4 种类型提供了一个空间或域，人们可以在其中操作并绘制接口和交互，以便能够跨边缘类型进行扩展。各种类型的边缘计算中使用的术语比上面更多，但是当谈到往返延迟时，它们很可能属于上述情况。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/05/30/pexelspetrganaj4072563.jpg","permalink":"https://atbug.com/translate-4-types-edge-computing-by-latency/","tags":["边缘计算"],"title":"译：边缘计算的 4 种类型（大致分类）"},{"categories":["云原生"],"contents":"在平时的工作中，不知道你有没有经常需要构建容器镜像进行测试，并且不一定是在构建环境中使用镜像。这时候就需要将镜像推送到镜像仓库做中转，然后在别处拉取并运行容器。久而久之，因为忘记清理镜像仓库中的“垃圾”镜像越来越多。\n当然，也可以使用类似 Harbor 这种带有自动清理功能镜像仓库。但只是作为临时镜像的中转，Harbor 这种未免太重了。\n今天要介绍的 ttl.sh 正适合处理这种场景。\nttl.sh ttl.sh 是一个匿名的临时镜像仓库，免费使用无需登录，并且已经开源。无需登录，镜像名称本身就提供了保密性，比如你可以使用 UUID 来作为镜像名称，使用同一个 UUID 来推送和拉取镜像。\n使用 ttl.sh 的使用格外简单，跟平时使用 Docker Hub 或者 Docker Registry 没差别，只是 tag 的需要注意一下。\n docker build 构建镜像时通过 tag 为镜像指定有效期，比如 ttl.sh/b0a2c1c3-5751-4474-9dfe-6a9e17dfb927:1h。有效期默认是 1 小时，最长是 24 小时。有效的 tag 可以是 5m、300s、4h、1d，如果超过 24 小时有效期会被设置为 24 小时；如果时间格式无效，有效期设置为默认的 1 小时； 使用 docker push 推送镜像； 使用 docker pull 拉取镜像。  比如：\n# macOS 下默认生成大写的 UUID，需要转成小写；Linux 下直接使用 uuidgen 即可 # docker 镜像不支持大写镜像名 $ IMAGE_NAME=$(uuidgen | tr \u0026#34;[:upper:]\u0026#34; \u0026#34;[:lower:]\u0026#34;) $ docker build -t ttl.sh/${IMAGE_NAME}:5m . $ docker push ttl.sh/${IMAGE_NAME}:5m 实现 ttl.sh 的源码开源在 GitHub，实现也不复杂。\nttl.sh 基于 Registry v2 的镜像仓库，利用 Registry 的 notification 功能，将镜像的 push event 发送给 Hooks web 服务。\nHooks 将 event 中的镜像信息解析并记录在 Redis 中，主要是记录镜像的过期时间；同时有个 Reaper 的定时任务定期从 Redis 获取镜像的信息，过期的镜像会调用 Registry 的 REST API 进行清理。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/05/29/shipgda90edbee1280.jpg","permalink":"https://atbug.com/store-ephemeral-image-in-ttl-registry/","tags":["Docker","镜像"],"title":"ttl.sh：临时 Docker 镜像的匿名仓库"},{"categories":["笔记"],"contents":"软件供应链是指进入软件中的所有内容及其来源，简单地可以理解成软件的依赖项。依赖项是软件运行时所需的重要内容，可以是代码、二进制文件或其他组件，也可以是这些组件的来源，比如存储库或者包管理器之类的。包括代码的已知漏洞、受支持的版本、许可证信息、作者、贡献时间，以及在整个过程中的行为和任何时候接触到它的任何内容，比如用于编译、分发软件的基础架构组件。\nCICD 流水线作为基础架构组件，承担着软件的构建、测试、分发和部署。作为供应链的一环，其也成为恶意攻击的目标。CICD 的行为信息可以作为安全审查的重要依据，比如构建打包的环境、操作流程、处理结果等等。\n今天介绍的 Tekton Chains，便是这样的行为收集工具。\nTekton Chains Chains 是一个 Kubernetes CRD 控制器，用于管理 Tekton 中的供应链安全。Chains 使 Tekton 能够在持续交付中，捕获 PipelineRun 和 TaskRun 运行的元数据用于安全审计，实现二进制溯源和可验证的构建，成为构成保证供应链安全的基础设施的一部分。\n当前 Chains（v0.9.0）提供如下功能：\n 使用用户提供的加密密钥，将 TaskRun 的结果（包含 TaskRun 本身和 OCI 镜像）镜像进行签名 支持类似 in-toto 的证明格式 使用多种加密密钥类型和服务（x509、KMS）进行签名 签名的存储支持多种实现  接下来的 Demo，我们还是继续使用之前在 Tekton Pipeline 实战 用的 Java 项目 tekton-test，在代码中同样还包含了 Pipeline 和 Task 的定义。\n为了支持 Chains，原有 task 增加了新的参数。\nDemo 环境准备 使用 k3d 快速搭建 k3s 的集群\nk3d cluster create tekton-test 安装 Teketon Pipelines 及 CLI\nkubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml brew install tektoncd-cli kubectl get po -n tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-webhook-7f5d9fc745-cr7th 1/1 Running 0 12m tekton-pipelines-controller-7c95d87d96-vvkgr 1/1 Running 0 12m 安装 tekton chains\nkubectl apply --filename https://storage.googleapis.com/tekton-releases/chains/latest/release.yaml kubectl get po -n tekton-chains NAME READY STATUS RESTARTS AGE tekton-chains-controller-7d9fb75899-l9d4j 1/1 Running 0 9m8s 安装流水线，这里还是使用 tekton-test 中的流水线定义，详细的说明查看Tekton Pipelines 实战 中的说明。\ncd tekton-test/tekton #创建sa kubectl apply -f serviceaccount.yaml # 创建 docker hub 凭证 secret，这里需要命令行工具 jq kubectl create secret docker-registry dockerhub --docker-server=https://index.docker.io/v1/ --docker-username=[USERNAME] --docker-password=[PASSWORD] --dry-run=client -o json | jq -r \u0026#39;.data.\u0026#34;.dockerconfigjson\u0026#34;\u0026#39; | base64 -d \u0026gt; /tmp/config.json \u0026amp;\u0026amp; kubectl create secret generic docker-config --from-file=/tmp/config.json -n tekton-pipelines \u0026amp;\u0026amp; rm -f /tmp/config.json #安装 git-clone task tkn hub install task git-clone #安装镜像构建 task kubectl apply -f tasks/source-to-image.yaml #安装部署 task kubectl apply -f tasks/deploy-to-k8s.yaml #安装 pipeline kubectl apply -f pipeline/build-pipeline.yaml #查看 task tkn t list NAME DESCRIPTION AGE git-clone These Tasks are Git... 2 minutes ago source-to-image 1 minute ago deploy-to-k8s 31 seconds ago #查看 pipeline tkn p list NAME AGE LAST RUN STARTED DURATION STATUS build-pipeline 35 seconds ago --- --- --- --- 配置 1. 添加认证凭证 Chains controller 需要使用进行：\n 在镜像签名完成后，向 OCI 镜像仓库发送签名 在使用密钥签名时，使用 Fulcio 获取签名证书  这里我们需要配置用于访问 Docker 镜像仓库的凭证，Chains controller 使用执行 PipelineRun 的 service account 将镜像签名推送到镜像仓库。\n这里直接使用前面安装 pipeline 时创建的 secret，并授权 service account 访问 secret。\nkubectl patch serviceaccount tekton-build \\  -p \u0026#34;{\\\u0026#34;imagePullSecrets\\\u0026#34;: [{\\\u0026#34;name\\\u0026#34;: \\\u0026#34;docker-config\\\u0026#34;}]}\u0026#34; -n tekton-pipelines 2. 签名密钥 为 Chains 生成并配置用于签名的签名密钥，支持下面的任何一种方式：\n x509 Cosign KMS EXPERIMENTAL: Keyless signing  我们使用 cosign （安装非常简单，在 macOS 上执行 brew install cosign）创建 secret，输入密码时直接回车，不设置密码。\ncosign generate-key-pair k8s://tekton-chains/signing-secrets Enter password for private key: Enter password for private key again: Successfully created secret signing-secrets in namespace tekton-chains Public key written to cosign.pub 下面就是创建的包含了密钥的 secret。\n3. 配置 Chains  artifacts.taskrun.format=in-toto artifacts.taskrun.storage=tekton artifacts.taskrun.signer=x509  kubectl patch configmap chains-config -n tekton-chains -p=\u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;artifacts.taskrun.format\u0026#34;: \u0026#34;in-toto\u0026#34;}}\u0026#39; kubectl patch configmap chains-config -n tekton-chains -p=\u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;artifacts.taskrun.storage\u0026#34;: \u0026#34;tekton\u0026#34;}}\u0026#39; kubectl patch configmap chains-config -n tekton-chains -p=\u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;artifacts.taskrun.signer\u0026#34;: \u0026#34;x509\u0026#34;}}\u0026#39; 验证 创建 PipelineRun 开始执行 Pipeline。\n#tekton-test 根目录 kubectl apply -f ./tekton/run/run.yaml pipelinerun.tekton.dev/generic-pipeline-run created 执行完成后，检查 TaskRun 的运行结果。\ntkn tr list NAME STARTED DURATION STATUS generic-pipeline-run-source-to-image 1 minute ago 1 minute Succeeded generic-pipeline-run-fetch-from-git 1 minute ago 11 seconds Succeeded kubectl get tr generic-pipeline-run-source-to-image -o json | jq -r .metadata.annotations { \u0026#34;chains.tekton.dev/cert-taskrun-5bf67bed-af59-4744-a6a7-487de359cbae\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;chains.tekton.dev/chain-taskrun-5bf67bed-af59-4744-a6a7-487de359cbae\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;chains.tekton.dev/payload-taskrun-5bf67bed-af59-4744-a6a7-487de359cbae\u0026#34;: \u0026#34;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\u0026#34;, \u0026#34;chains.tekton.dev/retries\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;chains.tekton.dev/signature-taskrun-5bf67bed-af59-4744-a6a7-487de359cbae\u0026#34;: \u0026#34;eyJwYXlsb2FkVHlwZSI6ImFwcGxpY2F0aW9uL3ZuZC5pbi10b3RvK2pzb24iLCJwYXlsb2FkIjoiZXlKZmRIbHdaU0k2SW1oMGRIQnpPaTh2YVc0dGRHOTBieTVwYnk5VGRHRjBaVzFsYm5RdmRqQXVNU0lzSW5CeVpXUnBZMkYwWlZSNWNHVWlPaUpvZEhSd2N6b3ZMM05zYzJFdVpHVjJMM0J5YjNabGJtRnVZMlV2ZGpBdU1pSXNJbk4xWW1wbFkzUWlPbHQ3SW01aGJXVWlPaUpwYm1SbGVDNWtiMk5yWlhJdWFXOHZZV1JrYjNwb1lXNW5MM1JsYTNSdmJpMTBaWE4wSWl3aVpHbG5aWE4wSWpwN0luTm9ZVEkxTmlJNkltRmpZbVE1T1dZNFltVXdNRE5sT0dJM1pUYzNOalkxWldVd1pUTXpNbVU1Tm1SaE1tTmlOVFJpTXpJM01UazRZV0ppTkRZM01XTTRaR0l4WlRrNU9HRWlmWDFkTENKd2NtVmthV05oZEdVaU9uc2lZblZwYkdSbGNpSTZleUpwWkNJNkltaDBkSEJ6T2k4dmRHVnJkRzl1TG1SbGRpOWphR0ZwYm5NdmRqSWlmU3dpWW5WcGJHUlVlWEJsSWpvaWFIUjBjSE02THk5MFpXdDBiMjR1WkdWMkwyRjBkR1Z6ZEdGMGFXOXVjeTlqYUdGcGJuTkFkaklpTENKcGJuWnZZMkYwYVc5dUlqcDdJbU52Ym1acFoxTnZkWEpqWlNJNmUzMHNJbkJoY21GdFpYUmxjbk1pT25zaVEwaEJTVTVUTFVkSlZGOURUMDFOU1ZRaU9pSjdjM1J5YVc1bklEUTJPVE14Wm1KalpqQXpNRE15TUdNek9UbG1aamxqTXpBMU1HUTJZamN6TVRJMU9XRmlabU1nVzExOUlpd2lRMGhCU1U1VExVZEpWRjlWVWt3aU9pSjdjM1J5YVc1bklHaDBkSEJ6T2k4dloybDBhSFZpTG1OdmJTOWhaR1J2ZW1oaGJtY3ZkR1ZyZEc5dUxYUmxjM1F1WjJsMElGdGRmU0lzSWtsTlFVZEZJam9pZTNOMGNtbHVaeUJoWkdSdmVtaGhibWN2ZEdWcmRHOXVMWFJsYzNRZ1cxMTlJaXdpYVcxaFoyVlVZV2NpT2lKc1lYUmxjM1FpTENKcGJXRm5aVlZ5YkNJNkludHpkSEpwYm1jZ1lXUmtiM3BvWVc1bkwzUmxhM1J2YmkxMFpYTjBJRnRkZlNJc0luQmhkR2hVYjBSdlkydGxja1pwYkdVaU9pSkViMk5yWlhKbWFXeGxJbjE5TENKaWRXbHNaRU52Ym1acFp5STZleUp6ZEdWd2N5STZXM3NpWlc1MGNubFFiMmx1ZENJNkltMTJiaUlzSW1GeVozVnRaVzUwY3lJNld5SmpiR1ZoYmlJc0ltbHVjM1JoYkd3aUxDSXRSSE5yYVhCVVpYTjBjeUpkTENKbGJuWnBjbTl1YldWdWRDSTZleUpqYjI1MFlXbHVaWElpT2lKdFlYWmxiaUlzSW1sdFlXZGxJam9pWkc5amEyVnlMbWx2TDJ4cFluSmhjbmt2YldGMlpXNUFjMmhoTWpVMk9qY3lPVEl5WVdKak9UVmtNemhsTURKbU56VXdZak0wT0RBd01qTTVaR013WlRKak1qazRaVGMwWW1aa1pEazNNREF4T0RNMk4yWXdaRGt5T0RGa05XTWlmU3dpWVc1dWIzUmhkR2x2Ym5NaU9tNTFiR3g5TEhzaVpXNTBjbmxRYjJsdWRDSTZJaTlyWVc1cGEyOHZaWGhsWTNWMGIzSWlMQ0poY21kMWJXVnVkSE1pT2xzaUxTMWtiMk5yWlhKbWFXeGxQU1FvY0dGeVlXMXpMbkJoZEdoVWIwUnZZMnRsY2tacGJHVXBJaXdpTFMxa1pYTjBhVzVoZEdsdmJqMGtLSEJoY21GdGN5NXBiV0ZuWlZWeWJDazZKQ2h3WVhKaGJYTXVhVzFoWjJWVVlXY3BJaXdpTFMxamIyNTBaWGgwUFNRb2QyOXlhM053WVdObGN5NXpiM1Z5WTJVdWNHRjBhQ2tpTENJdExXUnBaMlZ6ZEMxbWFXeGxQU1FvY21WemRXeDBjeTVKVFVGSFJWOUVTVWRGVTFRdWNHRjBhQ2tpWFN3aVpXNTJhWEp2Ym0xbGJuUWlPbnNpWTI5dWRHRnBibVZ5SWpvaVluVnBiR1F0WVc1a0xYQjFjMmdpTENKcGJXRm5aU0k2SW1kamNpNXBieTlyWVc1cGEyOHRjSEp2YW1WamRDOWxlR1ZqZFhSdmNrQnphR0V5TlRZNlptTmpZMlF5WVdJNVpqTTRPVEpsTXpObVl6ZG1NbVU1TlRCak9HVTBabU0yTmpWbE4yRTBZelkyWmpaaE9XUTNNR0l6TURCa04yRXlNVEF6TlRreVppSjlMQ0poYm01dmRHRjBhVzl1Y3lJNmJuVnNiSDBzZXlKbGJuUnllVkJ2YVc1MElqb2ljMlYwSUMxbFhHNWxZMmh2SUNRb2NHRnlZVzF6TGtsTlFVZEZLU0I4SUhSbFpTQWtLSEpsYzNWc2RITXVTVTFCUjBWZlZWSk1MbkJoZEdncElDQWdJQ0FnSUNCY2JpSXNJbUZ5WjNWdFpXNTBjeUk2Ym5Wc2JDd2laVzUyYVhKdmJtMWxiblFpT25zaVkyOXVkR0ZwYm1WeUlqb2lkM0pwZEdVdGRYSnNJaXdpYVcxaFoyVWlPaUprYjJOclpYSXVhVzh2YkdsaWNtRnllUzlpWVhOb1FITm9ZVEkxTmpwaU0yRmlaVFF5TlRVM01EWTJNVGhqTlRVd1pUaGtZalZsWXpBNE56VXpNamd6TXpOaE1UUmtZbVkyTmpObE5tWXhaVEppTmpnM05XWTBOVFV5TVdVMUluMHNJbUZ1Ym05MFlYUnBiMjV6SWpwdWRXeHNmVjE5TENKdFpYUmhaR0YwWVNJNmV5SmlkV2xzWkZOMFlYSjBaV1JQYmlJNklqSXdNakl0TURVdE1UUlVNRFU2TkRJNk1URmFJaXdpWW5WcGJHUkdhVzVwYzJobFpFOXVJam9pTWpBeU1pMHdOUzB4TkZRd05UbzBNam8wTWxvaUxDSmpiMjF3YkdWMFpXNWxjM01pT25zaWNHRnlZVzFsZEdWeWN5STZabUZzYzJVc0ltVnVkbWx5YjI1dFpXNTBJanBtWVd4elpTd2liV0YwWlhKcFlXeHpJanBtWVd4elpYMHNJbkpsY0hKdlpIVmphV0pzWlNJNlptRnNjMlY5TENKdFlYUmxjbWxoYkhNaU9sdDdJblZ5YVNJNkltZHBkQ3RvZEhSd2N6b3ZMMmRwZEdoMVlpNWpiMjB2WVdSa2IzcG9ZVzVuTDNSbGEzUnZiaTEwWlhOMExtZHBkQzVuYVhRaUxDSmthV2RsYzNRaU9uc2ljMmhoTVNJNklqUTJPVE14Wm1KalpqQXpNRE15TUdNek9UbG1aamxqTXpBMU1HUTJZamN6TVRJMU9XRmlabU1pZlgxZGZYMD0iLCJzaWduYXR1cmVzIjpbeyJrZXlpZCI6IlNIQTI1NjppRlk2RVhnWWt5enlabzgxU29MS1FPRy9CdjZyZnorZFYrMkZxNXFjY2NBIiwic2lnIjoiTUVVQ0lRREZ4bzE0bzVldDFRVkJKUklQK3liSlhmQ2VFTjJaK3ZFNWROQlpMQmhvSWdJZ0hwL3B4VDF0SDZleHF0d2x2UStML0prazhXVFMzbTJhL1BWN3AzeEdrTkE9In1dfQ==\u0026#34;, \u0026#34;kubectl.kubernetes.io/last-applied-configuration\u0026#34;: \u0026#34;{\\\u0026#34;apiVersion\\\u0026#34;:\\\u0026#34;tekton.dev/v1beta1\\\u0026#34;,\\\u0026#34;kind\\\u0026#34;:\\\u0026#34;PipelineRun\\\u0026#34;,\\\u0026#34;metadata\\\u0026#34;:{\\\u0026#34;annotations\\\u0026#34;:{},\\\u0026#34;generateName\\\u0026#34;:\\\u0026#34;generic-pr-\\\u0026#34;,\\\u0026#34;name\\\u0026#34;:\\\u0026#34;generic-pipeline-run\\\u0026#34;,\\\u0026#34;namespace\\\u0026#34;:\\\u0026#34;tekton-pipelines\\\u0026#34;},\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;params\\\u0026#34;:[{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;git-revision\\\u0026#34;,\\\u0026#34;value\\\u0026#34;:\\\u0026#34;main\\\u0026#34;},{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;git-url\\\u0026#34;,\\\u0026#34;value\\\u0026#34;:\\\u0026#34;https://github.com/addozhang/tekton-test.git\\\u0026#34;},{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;imageUrl\\\u0026#34;,\\\u0026#34;value\\\u0026#34;:\\\u0026#34;addozhang/tekton-test\\\u0026#34;},{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;imageTag\\\u0026#34;,\\\u0026#34;value\\\u0026#34;:\\\u0026#34;latest\\\u0026#34;}],\\\u0026#34;pipelineRef\\\u0026#34;:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;build-pipeline\\\u0026#34;},\\\u0026#34;serviceAccountName\\\u0026#34;:\\\u0026#34;tekton-build\\\u0026#34;,\\\u0026#34;workspaces\\\u0026#34;:[{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;git-source\\\u0026#34;,\\\u0026#34;volumeClaimTemplate\\\u0026#34;:{\\\u0026#34;spec\\\u0026#34;:{\\\u0026#34;accessModes\\\u0026#34;:[\\\u0026#34;ReadWriteOnce\\\u0026#34;],\\\u0026#34;resources\\\u0026#34;:{\\\u0026#34;requests\\\u0026#34;:{\\\u0026#34;storage\\\u0026#34;:\\\u0026#34;1Gi\\\u0026#34;}}}}},{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;docker-config\\\u0026#34;,\\\u0026#34;secret\\\u0026#34;:{\\\u0026#34;secretName\\\u0026#34;:\\\u0026#34;docker-config\\\u0026#34;}}]}}\\n\u0026#34;, \u0026#34;pipeline.tekton.dev/affinity-assistant\u0026#34;: \u0026#34;affinity-assistant-876633b0fe\u0026#34;, \u0026#34;pipeline.tekton.dev/release\u0026#34;: \u0026#34;422a468\u0026#34; } 将 payload 的内容解码可以看到 TaskRun 的输入、输出以及 Task 本身使用的镜像等信息。\n{ \u0026#34;_type\u0026#34;: \u0026#34;https://in-toto.io/Statement/v0.1\u0026#34;, \u0026#34;predicateType\u0026#34;: \u0026#34;https://slsa.dev/provenance/v0.2\u0026#34;, \u0026#34;subject\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;index.docker.io/addozhang/tekton-test\u0026#34;, \u0026#34;digest\u0026#34;: { \u0026#34;sha256\u0026#34;: \u0026#34;acbd99f8be003e8b7e77665ee0e332e96da2cb54b327198abb4671c8db1e998a\u0026#34; } } ], \u0026#34;predicate\u0026#34;: { \u0026#34;builder\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;https://tekton.dev/chains/v2\u0026#34; }, \u0026#34;buildType\u0026#34;: \u0026#34;https://tekton.dev/attestations/chains@v2\u0026#34;, \u0026#34;invocation\u0026#34;: { \u0026#34;configSource\u0026#34;: {}, \u0026#34;parameters\u0026#34;: { \u0026#34;CHAINS-GIT_COMMIT\u0026#34;: \u0026#34;{string 46931fbcf030320c399ff9c3050d6b731259abfc []}\u0026#34;, \u0026#34;CHAINS-GIT_URL\u0026#34;: \u0026#34;{string https://github.com/addozhang/tekton-test.git []}\u0026#34;, \u0026#34;IMAGE\u0026#34;: \u0026#34;{string addozhang/tekton-test []}\u0026#34;, \u0026#34;imageTag\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;{string addozhang/tekton-test []}\u0026#34;, \u0026#34;pathToDockerFile\u0026#34;: \u0026#34;Dockerfile\u0026#34; } }, \u0026#34;buildConfig\u0026#34;: { \u0026#34;steps\u0026#34;: [ { \u0026#34;entryPoint\u0026#34;: \u0026#34;mvn\u0026#34;, \u0026#34;arguments\u0026#34;: [ \u0026#34;clean\u0026#34;, \u0026#34;install\u0026#34;, \u0026#34;-DskipTests\u0026#34; ], \u0026#34;environment\u0026#34;: { \u0026#34;container\u0026#34;: \u0026#34;maven\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;docker.io/library/maven@sha256:72922abc95d38e02f750b34800239dc0e2c298e74bfdd970018367f0d9281d5c\u0026#34; }, \u0026#34;annotations\u0026#34;: null }, { \u0026#34;entryPoint\u0026#34;: \u0026#34;/kaniko/executor\u0026#34;, \u0026#34;arguments\u0026#34;: [ \u0026#34;--dockerfile=$(params.pathToDockerFile)\u0026#34;, \u0026#34;--destination=$(params.imageUrl):$(params.imageTag)\u0026#34;, \u0026#34;--context=$(workspaces.source.path)\u0026#34;, \u0026#34;--digest-file=$(results.IMAGE_DIGEST.path)\u0026#34; ], \u0026#34;environment\u0026#34;: { \u0026#34;container\u0026#34;: \u0026#34;build-and-push\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;gcr.io/kaniko-project/executor@sha256:fcccd2ab9f3892e33fc7f2e950c8e4fc665e7a4c66f6a9d70b300d7a2103592f\u0026#34; }, \u0026#34;annotations\u0026#34;: null }, { \u0026#34;entryPoint\u0026#34;: \u0026#34;set -e\\necho $(params.IMAGE) | tee $(results.IMAGE_URL.path) \\n\u0026#34;, \u0026#34;arguments\u0026#34;: null, \u0026#34;environment\u0026#34;: { \u0026#34;container\u0026#34;: \u0026#34;write-url\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;docker.io/library/bash@sha256:b3abe4255706618c550e8db5ec0875328333a14dbf663e6f1e2b6875f45521e5\u0026#34; }, \u0026#34;annotations\u0026#34;: null } ] }, \u0026#34;metadata\u0026#34;: { \u0026#34;buildStartedOn\u0026#34;: \u0026#34;2022-05-14T05:42:11Z\u0026#34;, \u0026#34;buildFinishedOn\u0026#34;: \u0026#34;2022-05-14T05:42:42Z\u0026#34;, \u0026#34;completeness\u0026#34;: { \u0026#34;parameters\u0026#34;: false, \u0026#34;environment\u0026#34;: false, \u0026#34;materials\u0026#34;: false }, \u0026#34;reproducible\u0026#34;: false }, \u0026#34;materials\u0026#34;: [ { \u0026#34;uri\u0026#34;: \u0026#34;git+https://github.com/addozhang/tekton-test.git.git\u0026#34;, \u0026#34;digest\u0026#34;: { \u0026#34;sha1\u0026#34;: \u0026#34;46931fbcf030320c399ff9c3050d6b731259abfc\u0026#34; } } ] } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/05/14/pexelsjoeykyber119562.jpg","permalink":"https://atbug.com/tekton-chains-secure-supply-chain/","tags":["CICD","Tekton","Security","云原生"],"title":"CICD 的供应链安全工具 Tekton Chains"},{"categories":["工具"],"contents":"本人算是个 Alfred 的重度依赖者了（上图是去年换了新电脑后的使用数据），Alfred 也算 Mac 上的第一款付费软件，买的 mega 版。安装的 workflow 估计有几十个，不过常用的估计有十几个吧。\n用了几年，一直都秉承着能不造轮子就不造的原则，基本也都能找到想要的 workflow。为什么突然要写个 workflow，这要从 macOS 12.3 移除了 Python2 说起。自从升级了 12.3，好多 workflow 都无法正常工作了，其中就有常用的 Safari 历史搜索。苦等了一段时间，也不见作者更新，不得已自己下场来写。正好五一假期有时间，顺便远离工作放空一下。\n搜索后看到了如何使用 Go 编写 workflow 这篇文章，就准备用 Go 来写。然后就有了我的第一个 workflow：Safari Toolkit。\n现在是实现了原 workflow 的所有功能，对 Safari 的访问历史进行搜索：基于 URL 和 Title，然后选择搜索结果在 Safari 中打开。\n后面计划抽时间再加上 tab 的搜索，不过目前意愿不强，Search Safari and Chrome Tabs 已经足够用了。\n刚兴趣的同学，可以从这里下载试用。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/05/03/20220503-at-213907.png","permalink":"https://atbug.com/create-an-alfred-workflow/","tags":["Aflred"],"title":"撸一个 Alfred Workflow"},{"categories":["笔记"],"contents":"目前我使用 Calibre 软件来管理电子书，电子书备份在 iCloud 中。但是每次推送电子书到 Kindle 都要打开电脑，感觉不够方便。如果能在手机端就可以操作岂不是更香。\n遂将目标瞄向了 calibre-web，这个类似 calibre 的 web 端。可以浏览、下载 Calibre 数据库中保存的书籍，也有 Docker 镜像提供。我有一个 x86 的虚拟机用来补全 m1 的短板，可以用来运行 calibre-web 容器。\n接下来就是数据的持久化了。众所周知 iCloud 一如 Apple 的系统一样封闭，无法在 Linux 上使用。想起 Google Drive 有 15GB 的免（bai）费（piao）空间，可以用来备份电子书。记得 Rclone 可以支持 Google Drive 的同步（网络问题不在讨论范围），虽然只有单向同步但正好满足我的需求。\n上面就是最终的理想设计了，这篇先进行虚拟机的安装实现本地文件到 Google Drive 的同步。\nRclone Rclone 是一个命令行程序，用于管理云存储上的文件。它是一个开源的软件，灵感来自 rsync。支持多种操作系统，以及 x86、arm 计算架构。\nRclone 支持的云存储有 60 多种，具体列表可以参考这里。\n动手 Google API client_id Rclone 访问 Google Drive，需要通过 Google API 来完成。首先我们要为创建 Rclone 创建一个 client_id，详细步骤参考这里。\n配置 Rclone 命令行中输入 rclone config，然后按照详细步骤一步步完成配置。\n由于在创建 client_id 时，应用类型选的是“桌面”应用。在配置 Rclone 时，需要在浏览器中打开 http://127.0.0.1:53682/auth 操作完成 Google Drive 的访问授权，这在虚拟机中无法完成，但可以使用代理的方式来解决。\n从 release 中下载最新版的 Pipy，然后创建一个脚本文件：\n#/tmp/proxy.js pipy() .listen(3682) .connect(\u0026#39;127.0.0.1:53682\u0026#39;) 然后，就可以使用命令 ./pipy /tmp/proxy.js 启动代理。然后我们通过虚拟机的 IP 地址和端口 3682，在浏览器中完成授权。授权后，有个回调的操作同样修改 IP 地址和端口来完成。\n其他的步骤就按照文档中的指示完成操作。然后执行命令 rclone ls remote，如果返回了 Google Drive 的文件列表，说明配置成功。\n通过 Rclone 的 sync 命令可以完成同步，比如将本地 /data/calibre-web 的内容同步到 Google Drive 的 calibre 目录中。\n$ rclone sync /data/calibre-web remote:calibre 自动同步 在每次修改文件内容后需要手动执行一次 sync 操作进行同步，很不方便。除了通过 crontab 在周期性的同步外，还可以通过监控文件操作来实现自动的同步。\n网上就有人提供了自动同步的脚本，使用 inotifywait 来监控文件的修改来进行同步。\n将脚本内容保存为 sync.sh，修改其中的几个参数即可。记得执行脚本前，需要先安装 inotify-toole 和 libnotify-bin。\n$ sudo apt install libnotify-bin inotify-tools #!/bin/bash ## One-way, immediate, continuous, recursive, directory synchronization ## to a remote Rclone URL. ( S3, SFTP, FTP, WebDAV, Dropbox, etc. ) ## Optional desktop notifications on sync events or errors. ## Useful only for syncing a SMALL number of files (\u0026lt; 8192). ## (See note in `man inotifywait` under `--recursive` about raising this limit.) ## Think use-case: Synchronize Traefik TLS certificates file (acme.json) ## Think use-case: Synchronize Keepass (.kdbx) database file immediately on save. ## Think use-case: Live edit source code and push to remote server ## This is NOT a backup tool!  ## It will not help you if you delete your files or if they become corrupted. ## Setup: Install `rclone` from package manager. ## Run: `rclone config` to setup the remote, including the full remote ## subdirectory path to sync to. ## MAKE SURE that the remote (sub)directory is EMPTY ## or else ALL CONTENTS WILL BE DELETED by rclone when it syncs. ## If unsure, add `--dry-run` to the RCLONE_CMD variable below,  ## to simulate what would be copied/deleted.  ## Copy this script any place, and make it executable: `chown +x sync.sh` ## Edit all the variables below, before running the script. ## Run: `./sync.sh systemd_setup` to create and enable systemd service. ## Run: `journalctl --user --unit rclone_sync.${RCLONE_REMOTE}` to view the logs. ## Edit the variables below, according to your own environment: # RCLONE_SYNC_PATH: The path to COPY FROM (files are not synced TO here): RCLONE_SYNC_PATH=\u0026#34;/data/calibre-web\u0026#34; # RCLONE_REMOTE: The rclone remote name to synchronize with. # Identical to one of the remote names listed via `rclone listremotes`. # Make sure to include the final `:` in the remote name, which # indicates to sync/delete from the same (sub)directory as defined in the URL. # (ALL CONTENTS of the remote are continuously DELETED # and replaced with the contents from RCLONE_SYNC_PATH) RCLONE_REMOTE=\u0026#34;remote:calibre\u0026#34; # RCLONE_CMD: The sync command and arguments: RCLONE_CMD=\u0026#34;rclone -v sync ${RCLONE_SYNC_PATH}${RCLONE_REMOTE}\u0026#34; # WATCH_EVENTS: The file events that inotifywait should watch for: WATCH_EVENTS=\u0026#34;modify,delete,create,move\u0026#34; # SYNC_DELAY: Wait this many seconds after an event, before synchronizing: SYNC_DELAY=5 # SYNC_INTERVAL: Wait this many seconds between forced synchronizations: SYNC_INTERVAL=3600 # NOTIFY_ENABLE: Enable Desktop notifications NOTIFY_ENABLE=true # SYNC_SCRIPT: dynamic reference to the current script path SYNC_SCRIPT=$(realpath $0) notify() { MESSAGE=$1 if test ${NOTIFY_ENABLE} = \u0026#34;true\u0026#34;; then notify-send \u0026#34;rclone ${RCLONE_REMOTE}\u0026#34; \u0026#34;${MESSAGE}\u0026#34; fi } rclone_sync() { set -x # Do initial sync immediately: notify \u0026#34;Startup\u0026#34; ${RCLONE_CMD} # Watch for file events and do continuous immediate syncing # and regular interval syncing: while [[ true ]] ; do inotifywait --recursive --timeout ${SYNC_INTERVAL} -e ${WATCH_EVENTS} \\ \t${RCLONE_SYNC_PATH} 2\u0026gt;/dev/null if [ $? -eq 0 ]; then # File change detected, sync the files after waiting a few seconds: sleep ${SYNC_DELAY} \u0026amp;\u0026amp; ${RCLONE_CMD} \u0026amp;\u0026amp; \\ \tnotify \u0026#34;Synchronized new file changes\u0026#34; elif [ $? -eq 1 ]; then # inotify error occured notify \u0026#34;inotifywait error exit code 1\u0026#34; sleep 10 elif [ $? -eq 2 ]; then # Do the sync now even though no changes were detected: ${RCLONE_CMD} fi done } systemd_setup() { set -x if loginctl show-user ${USER} | grep \u0026#34;Linger=no\u0026#34;; then echo \u0026#34;User account does not allow systemd Linger.\u0026#34; echo \u0026#34;To enable lingering, run as root: loginctl enable-linger $USER\u0026#34; echo \u0026#34;Then try running this command again.\u0026#34; exit 1 fi mkdir -p ${HOME}/.config/systemd/user SERVICE_FILE=${HOME}/.config/systemd/user/rclone_sync.${RCLONE_REMOTE}.service if test -f ${SERVICE_FILE}; then echo \u0026#34;Unit file already exists: ${SERVICE_FILE}- Not overwriting.\u0026#34; else cat \u0026lt;\u0026lt;EOF \u0026gt; ${SERVICE_FILE} [Unit] Description=rclone_sync ${RCLONE_REMOTE} [Service] ExecStart=${SYNC_SCRIPT} [Install] WantedBy=default.target EOF fi systemctl --user daemon-reload systemctl --user enable --now rclone_sync.${RCLONE_REMOTE} systemctl --user status rclone_sync.${RCLONE_REMOTE} echo \u0026#34;You can watch the logs with this command:\u0026#34; echo \u0026#34; journalctl --user --unit rclone_sync.${RCLONE_REMOTE}\u0026#34; } if test $# = 0; then rclone_sync else CMD=$1; shift; ${CMD} $@ fi ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/05/01/pexelsmarkusspiske330771.jpg","permalink":"https://atbug.com/sync-local-file-to-google-drive-with-rclone/","tags":null,"title":"使用 Rclone 同步文件到 Google Drive"},{"categories":["教程"],"contents":"这篇文章主要使用\u0026quot;核弹级\u0026quot;命令git-filter-branch, 对代码仓库进行整理. 这里的整理包括子目录重命名, 子目录独立为新项目(保留或者不保留原来的结构)等. 整理之后, 原来的提交信息都会完好的保存下来, 方便追溯.\n重要的事情说三遍：\n强烈建议第一次使用的时候做好备份, 切记!!! 强烈建议第一次使用的时候做好备份, 切记!!! 强烈建议第一次使用的时候做好备份, 切记!!!\n每处理完一个分支, 都需要执行如下操作, 并重新克隆项目并处理下一个分支\ngit remote remove origin git remote add origin xxxxxx.git git push -u origin BRANCH-NAME git push --tags 将子文件夹独立为新的仓库 git filter-branch --prune-empty --tag-name-filter cat --subdirectory-filter FOLDER-NAME BRANCH-NAME 演示:\n原结构: root ├── delete └── new └── test.sql 处理后: new └── test.sql 保留原来的目录结构, 清理掉其他的子目录, 也可将要保留的目录重命名. git filter-branch --prune-empty --tag-name-filter cat --tree-filter \u0026#39;ls | grep -v FOLDER-NAME | xargs rm -rf \u0026amp;\u0026amp; mv FOLDER-NAME NEW-FOLDER-NAME\u0026#39; BRANCH-NAME 演示:\n原结构: root ├── delete └── new └── test.sql 处理后: root └── new └── test.sql 将项目中的文件移入新建的子目录 #linux only git filter-branch --prune-empty --tag-name-filter cat --tree-filter \u0026#39;mkdir FOLDER-NAME \u0026amp;\u0026amp; (ls | grep -v FOLDER-NAME | xargs mv -t FOLDER-NAME) \u0026#39; BRANCH-NAME 演示:\n原结构: root ├── test1.sql └── test2.sql 处理后: root └── new ├── test1.sql └── test2.sql 保留项目的多个目录并重命名 git filter-branch --prune-empty --tag-name-filter cat --tree-filter \u0026#39;(ls | egrep -v \u0026#34;(FOLDER-NAME1|FOLDER-NAME2)\u0026#34; | xargs rm -rf) \u0026amp;\u0026amp; mv FOLDER-NAME1 NEW-FOLDER-NAME1 \u0026amp;\u0026amp; mv mv FOLDER-NAME2 NEW-FOLDER-NAME2\u0026#39; BRANCH-NAME 演示:\n原结构: root ├── old1 │ └── test1.sql └── old2 └── test2.sql 处理后: root ├── new1 │ └── test1.sql └── new2 └── test2.sql 迁出多层目录 git filter-branch --prune-empty --tag-name-filter cat --tree-filter \u0026#39;if [ -d \u0026#34;FOLDER-NAME/SUB-FOLDER-NAME\u0026#34; ]; then mv FOLDER-NAME/SUB-FOLDER-NAME . ;fi \u0026amp;\u0026amp; (ls | grep -v FOLDER-NAME | xargs rm -rf )\u0026#39; BRANCH-NAME 演示:\n原结构: root └── sub └── base └── base.sql 处理后: root └── base └── base.sql 修改author的信息, 如邮箱和名字 这个操作需要在各个分支上都运行一遍. 对commit的修改, 就改变revision id. 需要使用git push --force强制覆盖仓库, 因此tag信息也会被重写, 需要运行git push --tags --force\n#对author的信息进行修改: GIT_AUTHOR_NAME, GIT_COMMITTER_EMAIL git filter-branch --tag-name-filter cat --env-filter \u0026#39; if test \u0026#34;$GIT_AUTHOR_EMAIL\u0026#34; = \u0026#34;root@localhost\u0026#34; then GIT_AUTHOR_EMAIL=john@example.com fi if test \u0026#34;$GIT_COMMITTER_EMAIL\u0026#34; = \u0026#34;root@localhost\u0026#34; then GIT_COMMITTER_EMAIL=john@example.com fi \u0026#39; -- --all 参考:\n Splitting a subfolder out into a new repository Stack overflow  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/04/30/pexelstigerlily4483775.jpg","permalink":"https://atbug.com/split-git-repository-without-commit-lost/","tags":["Git"],"title":"Git代码仓库无损整理：目录拆分和重命名"},{"categories":["云原生"],"contents":"TL;DR 本文从服务网格发展现状、到 Open Service Mesh 源码，分析开放服务网格中的开放是什么以及如何开放。笔者总结其开放体现在以下几点：\n 资源提供者（Provider）接口和资源的重新封装：通过资源提供者接口抽象计算平台的资源，并封装成平台、代理无关的结构化类型，并由统一的接口 MeshCataloger 对外提供访问。虽然目前只有 Kubernetes 相关资源的 Provider 实现，但是通过抽象出的接口，可以兼容其他平台，比如虚拟机、物理机。 服务网格能力接口：对服务网格能力的抽象，定义服务网格的基础功能集。 代理控制面接口：这一层与反向代理，也就是 sidecar 的实现相关。实际上是反向代理所提供的接口，通过对接口的实现，加上 MeshCataloger 对资源的访问，生成并下发代理所需的配置。  背景 服务网格是什么 服务网格是在 2017年由 Buoyant 的 Willian Morgan 在 What’s a service mesh? And why do I need one? 给出了解释。\n服务网格是处理服务间网络通信的基础设施组件，旨在从平台层面提供可观性、安全以及可靠性特性，以进程外的方式提供原本由部分应用层逻辑承载的基础能力，真正实现与业务逻辑的分离。典型的实现是与应用程序一起部署的可扩展网络代理（通常称为 sidecar 模型），来代理服务间的网络通信，也是服务网格特性的接入点。这些代理就组成了服务网格的数据平面，并由控制平面进行统一的管理。\n图片来自 Pattern: Service Mesh\n服务网格现状 过去几年 Istio 有着成为服务网格事实标准的趋势，但时至今日，各种各样的服务网格产品如雨后春笋般层出不穷。CNCF 发布的 2021 Cloud Native survey results 中不难看出，这些网格产品也慢慢被市场所接受，并大有超越 Istio 的态势（部分地区）。\n打开 CNCF 的服务网格全景图，会发现有不少服务网格的产品。实际上还有很多产品没有列出，比如 HashiCorp Consul Connect、OpenShift Service Mesh、Nginx Service Mesh、Kong Mesh、SOFAMesh 等等，当然还有我司的 Flomesh。此外还有一些云厂商基于开源的网格产品。\n这些产品有些使用相同的数据平面，也有些不同，但控制平面各不相同。众多产品为大家提供了更多选择的同时，也由于各个产品间有着很强的隔离性，阻碍了生态系统的发展。这导致很难从一个实现切换到另一个，从控制面到到数据面，以及为其开发的管理后台都有重新开发的成本。此外，还有用户使用习惯的改变，无法做到透明无感知。\n针对这种诉求，按照“惯例”就是借助标准接口来进行抽象，提供实现的互通性。比如容器网络、运行时、存储有 CNI、CRI、CSI 接口，服务网格也有其抽象接口 Service Mesh Interface（简称SMI），以及其实现也就是今天的主角 Open Service Mesh（简称OSM）。\nSMI 与 OSM 简介 在 CNCF 的服务网格全景图中可以看到 SMI 和 OSM。\nSMI 是什么 SMI 是服务网格的规范，重点关注在 Kubernetes 上运行的服务网格。它定义了一个可以由各种供应商使用的通用标准。允许最终用户的标准化和服务网格技术提供商的创新。SMI 实现了灵活性和互操作性，并涵盖了最常见的服务网格功能。\nSMI 提供了 Kubernetes 服务网格的标准接口、常见服务网格场景的基本功能集、支持服务网格新功能的灵活性，以及服务网格技术生态的创新空间。\nSMI 的目标是将概念与实现隔离开来，像软件开发过去一直做的那样，将复杂的东西分层抽象。\nSMI 规范的最新版本是 0.6.0，覆盖了常见的流量访问控制、遥测、管理等基础服务网格能力。与规范对应的是 API，各个网格供应商基于该 API 进行实现。\nOSM 是什么 OSM 是一个轻量级可扩展的云原生服务网格，是简单、完整且独立的服务网格解决方案，它允许用户统一管理、保护并获得针对高度动态微服务环境的开箱即用的可观察性功能。OSM 运行在 Kubernetes 之上控制平面实现了 xDS API 并配置了 SMI API，为应用程序注入 Envoy sidecar 容器作为代理，通过 SMI Spec 来引用网格中的服务。\n虽然默认情况下使用 Envoy 作为数据平面，但是设计上提供接口的抽象，支持兼容 xDS 的代理，甚至其他的代理。\nSMI 规范 SMI 规范为常见服务网格能力提供了一套规范：\n 流量策略：跨服务应用身份和传输加密等策略。 流量遥测：捕获关键指标，如错误率和延迟。 流量管理：在不同服务之前转移流量。  流量策略 这个规范是用来指定应用的流量表现形式，并与访问控制策略结合来定义特定流量在服务网格中的行为。\n比如下面的定义中，/metrics 端点仅对外提供 GET 方式访问，比如 Prometheus。而其他端点，支持所有方式。\nkind: TCPRoute metadata: name: the-routes spec: matches: ports: - 8080 --- kind: HTTPRouteGroup metadata: name: the-routes spec: matches: - name: metrics pathRegex: \u0026#34;/metrics\u0026#34; methods: - GET - name: everything pathRegex: \u0026#34;.*\u0026#34; methods: [\u0026#34;*\u0026#34;] 下面的定义，则在前面的基础上，允许 Prometheus（使用 prometheus ServiceAccount 部署）访问所有 service-a ServiceAccount 的应用的 /metrics 端点。\nkind: TrafficTarget metadata: name: path-specific namespace: default spec: destination: kind: ServiceAccount name: service-a namespace: default rules: - kind: TCPRoute name: the-routes - kind: HTTPRouteGroup name: the-routes matches: - metrics sources: - kind: ServiceAccount name: prometheus namespace: default 流量管理 访问控制以外的流量管理，更多是体现在流量的拆分上。流量拆分 TrafficSplit 用于实现将流量按百分比拆分到同一应用程序的不同版本。\n下面的定义中，将来自Firefox 请求全都路由到 website 的 v2 版本，实现金丝雀发布。在实际场景中的操作流程，参考 SMI 的示例。\nkind: TrafficSplit metadata: name: ab-test spec: service: website matches: - kind: HTTPRouteGroup name: ab-test backends: - service: website-v1 weight: 0 - service: website-v2 weight: 100 --- kind: HTTPRouteGroup metadata: name: ab-test matches: - name: firefox-users headers: user-agent: \u0026#34;.*Firefox.*\u0026#34; 流量遥测 流量遥测还是处于很早期的版本 v1alpha1。\n该规范描述了一种资源，该资源为工具提供了一个通用集成点，这些工具可以通过使用与 HTTP 流量相关的指标而受益。对于即时指标它遵循 metrics.k8s.io 的模式，这些指标可被 CLI 工具、HPA 扩展或自动化金丝雀更新使用。\n比如下面定义了从 Pod foo-775b9cbd88-ntxsl 到 Pod baz-577db7d977-lsk2q 的延迟以及成功率指标。\nkind: TrafficMetrics # See ObjectReference v1 core for full spec resource: name: foo-775b9cbd88-ntxsl namespace: foobar kind: Pod edge: direction: to side: client resource: name: baz-577db7d977-lsk2q namespace: foobar kind: Pod timestamp: 2019-04-08T22:25:55Z window: 30s metrics: - name: p99_response_latency unit: seconds value: 10m - name: p90_response_latency unit: seconds value: 10m - name: p50_response_latency unit: seconds value: 10m - name: success_count value: 100 - name: failure_count value: 100 OSM 的设计 OSM 的控制层面包含了五个核心组件：\n Proxy Control Plane：代理控制面在操作服务网格中起着关键作用，所有以 sidecar 方式运行的代理，都会与其建立连接，并不断地接受配置的更新。这个组件实现反向代理所需的接口。目前 OSM 使用 Envoy 作为其默认的代理实现，因此这个组件实现了 Envoy 的 xDS API。 Certificate Manger：证书管理器组件为网格中的服务提供 TLS 证书，这些证书用于使用 mTLS 建立和加密服务之间的连接。 Endpoints Providers：端点提供者是与计算平台（Kubernetes 集群、云主机、本地机器）交互的一系列组件的统称。端点提供者将服务名解析为 IP 地址。 Mesh Specification：网格规范是现有SMI 规范组件的封装。该组件抽象了为 YAML 定义的特定存储。这个模块实际上是SMI Spec 的 Kubernetes informers的封装。 Mesh Catalog：是 OSM 的核心组件，它将其他组件的输出组装成新的结构。新的结构可以转换为代理配置并通过代理控制面分发给所有的代理。  这几个抽象的组件，都有接口与其一一对应，\n Proxy Control Plane：Envoy AggregatedDiscoveryServiceServer  Certificate Manger：certificate.Manager Endpoints Providers：endpoint.Provider Mesh Specification：smi.MeshSpec Mesh Catalog：catalog.MeshCataloger  源码分析 代理控制平面接口 基于 Evnoy 代理的服务网格实现，代理控制面都要实现 AggregatedDiscoveryServiceServer 接口。目前 OSM 实现的是 V3 版本的接口。深入到代码中，OSM 的 ads.Server 实现了 AggregatedDiscoveryServiceServer 接口。\n从 ads.Server 的定义中可以看到其他组件接口 catalog.MeshCataloger、certificate.Manager 的身影。\n// Server implements the Envoy xDS Aggregate Discovery Services type Server struct { catalog catalog.MeshCataloger proxyRegistry *registry.ProxyRegistry xdsHandlers map[envoy.TypeURI]func(catalog.MeshCataloger, *envoy.Proxy, *xds_discovery.DiscoveryRequest, configurator.Configurator, certificate.Manager, *registry.ProxyRegistry) ([]types.Resource, error) xdsLog map[certificate.CommonName]map[envoy.TypeURI][]time.Time xdsMapLogMutex sync.Mutex osmNamespace string cfg configurator.Configurator certManager certificate.Manager ready bool workqueues *workerpool.WorkerPool kubecontroller k8s.Controller // --- \t// SnapshotCache implementation structrues below \tcacheEnabled bool ch cachev3.SnapshotCache srv serverv3.Server // When snapshot cache is enabled, we (currently) don\u0026#39;t keep track of proxy information, however different \t// config versions have to be provided to the cache as we keep adding snapshots. The following map \t// tracks at which version we are at given a proxy UUID \tconfigVerMutex sync.Mutex configVersion map[string]uint64 msgBroker *messaging.Broker } 此处，不得不提一下 messaging.Broker，OSM 控制平面的“消息总线”。集群中，任何资源（K8s 原生资源、OSM 定义资源、SMI 资源）的变更，都会以事件的方式发布到消息总线。\n特定事件消息的订阅方在接收到事件后就会执行特定的逻辑。\n这里 ads.Server 在启动 grpc 服务后，便会订阅 ProxyUpdate 事件，收到事件会触发对应的逻辑：通过 catalog.MeshCataloger 的实现获取结构化的数据，进而转换成代理的配置并发送给代理。\n在 OSM 中 ProxyUpdate 事件是一些事件的集合，这些事件最终都会被**“当作”** ProxyUpdate 事件进行处理。\n网格目录接口 Mesh Catalog 的接口 catalog.MeshCataloger，定义了一些用于生成结构化数据的方法。这些结构化的数据，是在 K8s 原生资源、SMI 自定义资源的基础上的封装。作为底层资源和代理控制面的中间层，对上隔离了底层资源的实现，对下统一了资源的对外暴露形式。\n// MeshCataloger is the mechanism by which the Service Mesh controller discovers all Envoy proxies connected to the catalog. type MeshCataloger interface { // ListOutboundServicesForIdentity list the services the given service identity is allowed to initiate outbound connections to \tListOutboundServicesForIdentity(identity.ServiceIdentity) []service.MeshService // ListOutboundServicesForMulticlusterGateway lists the upstream services for the multicluster gateway \tListOutboundServicesForMulticlusterGateway() []service.MeshService // ListInboundServiceIdentities lists the downstream service identities that are allowed to connect to the given service identity \tListInboundServiceIdentities(identity.ServiceIdentity) []identity.ServiceIdentity // ListOutboundServiceIdentities lists the upstream service identities the given service identity are allowed to connect to \tListOutboundServiceIdentities(identity.ServiceIdentity) []identity.ServiceIdentity // ListServiceIdentitiesForService lists the service identities associated with the given service \tListServiceIdentitiesForService(service.MeshService) []identity.ServiceIdentity // ListAllowedUpstreamEndpointsForService returns the list of endpoints over which the downstream client identity \t// is allowed access the upstream service \tListAllowedUpstreamEndpointsForService(identity.ServiceIdentity, service.MeshService) []endpoint.Endpoint // GetIngressTrafficPolicy returns the ingress traffic policy for the given mesh service \tGetIngressTrafficPolicy(service.MeshService) (*trafficpolicy.IngressTrafficPolicy, error) // ListInboundTrafficTargetsWithRoutes returns a list traffic target objects composed of its routes for the given destination service identity \tListInboundTrafficTargetsWithRoutes(identity.ServiceIdentity) ([]trafficpolicy.TrafficTargetWithRoutes, error) // GetEgressTrafficPolicy returns the Egress traffic policy associated with the given service identity. \tGetEgressTrafficPolicy(identity.ServiceIdentity) (*trafficpolicy.EgressTrafficPolicy, error) // GetKubeController returns the kube controller instance handling the current cluster \tGetKubeController() k8s.Controller // GetOutboundMeshTrafficPolicy returns the outbound mesh traffic policy for the given downstream identity \tGetOutboundMeshTrafficPolicy(identity.ServiceIdentity) *trafficpolicy.OutboundMeshTrafficPolicy // GetInboundMeshTrafficPolicy returns the inbound mesh traffic policy for the given upstream identity and services \tGetInboundMeshTrafficPolicy(identity.ServiceIdentity, []service.MeshService) *trafficpolicy.InboundMeshTrafficPolicy } 接口的实现 catalog.MeshCatalog，也是通过几个接口获取底层的资源：\n endpoint.Provider：endpoint.Endpoint 抽象资源的提供者，在目前 OSM 版本中提供获取 Kubernetes Endpoint 的实现。 service.Provider：service.MeshService 抽象资源的提供者，目前同样是提供获取 Kubernetes Service 的实现。 smi.MeshSpec：故名思义针对是 SMI 自定义资源，直接使用 SMI 的定义，并没有重新抽象封装。 k8s.Controller：用于获取 Kubernetes 原生资源，比如 ServiceAcount、Namespace、Pods 等。 policy.Controller：用于获取 OSM 的 Egress 和 IngressBackend 资源。  // MeshCatalog is the struct for the service catalog type MeshCatalog struct { endpointsProviders []endpoint.Provider serviceProviders []service.Provider meshSpec smi.MeshSpec certManager certificate.Manager configurator configurator.Configurator // This is the kubernetes client that operates async caches to avoid issuing synchronous \t// calls through kubeClient and instead relies on background cache synchronization and local \t// lookups \tkubeController k8s.Controller // policyController implements the functionality related to the resources part of the policy.openrservicemesh.io \t// API group, such as egress. \tpolicyController policy.Controller } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/04/13/pexelsbradyknoll5409751.jpg","permalink":"https://atbug.com/how-open-presented-in-open-service-mesh/","tags":["Kubernetes","Service Mesh"],"title":"开放服务网格 Open Service Mesh 如何开放？"},{"categories":["教程"],"contents":"使用 Kubernetes 时，经常会遇到一些棘手的网络问题需要对 Pod 内的流量进行抓包分析。然而所使用的镜像一般不会带有 tcpdump 命令，过去常用的做法简单直接暴力：登录到节点所在节点，使用 root 账号进入容器，然后安装 tcpdump。抓到的包有时还需要拉回本地，使用 Wireshark 进行分析。而且整个过程非常繁琐，跨越几个环境。\n正好前几天也做了一次抓包问题排查，这次就介绍一下快速进行网络抓包的几种方法。\nTL;DR 几种方法各有优缺点，且都不建议在生产环境使用。假如必须使用，个人倾向于 kubectl debug 临时容器的方案，但这个方案也有不足。\n 使用额外容器：这种方案为了 Pod 添加一个额外的容器，使用了静态编译的 tcpdump 进行抓取，借助了多容器共享网络空间的特性，适合 distroless 容器。缺点是需要修改原来的 Pod，调式容器重启会引起 Pod 重启。 kubectl plugin ksniff：一个 kubectl 插件。支持特权和非特权容器，可以将捕获内容重定向到 wireshark 或者 tshark。非特权容器的实现会稍微复杂。 kubectl debug 临时容器：该方案对于 distroless 容器有很好的支持，临时容器退出后也不会导致 Pod 重启。缺点是 1.23 的版本临时容器才进入 beta 阶段；而且笔者在将捕获的数据重定向到本地的 Wireshark 时会报数据格式不支持的错误。  环境 使用 k3d 创建 k3s 集群，这里版本选择 1.23:\n$ k3d cluster create test --image rancher/k3s:v1.23.4-k3s1 抓包的对象使用 Pipy 运行的一个 echo 服务（返回请求的 body 内容）：\n$ kubectl run echo --image addozhang/echo-server --image-pull-policy IfNotPresent 为了方便访问，创建一个 NodePort Service：\n$ kubectl expose pod echo --name echo --port 8080 --type NodePort 挂载容器 在之前的文章我们介绍调试 distroless 容器的几种方法时曾用过修改 Pod 添加额外容器的方式，新的容器使用镜像 addozhang/static-dump 镜像。这个镜像中加入了静态编译的 tcpdump。\n修改后的 Pod：\napiVersion: v1 kind: Pod metadata: labels: run: echo name: echo spec: containers: - image: addozhang/echo-server imagePullPolicy: IfNotPresent name: echo resources: {} - image: addozhang/static-dump imagePullPolicy: IfNotPresent name: sniff command: [\u0026#39;sleep\u0026#39;, \u0026#39;1d\u0026#39;] dnsPolicy: ClusterFirst restartPolicy: Always status: {} 重新部署后，就可以使用下面命令将抓取网络包并重定向到本地的 Wireshark：\n$ kubectl exec -i echo -c sniff -- /static-tcpdump -i eth0 -U -w - | wireshark -k -i - kubectl plugin ksniff ksniff 是一个 kubectl 插件，利用 tcpdump 和 Wireshark 对 Pod 中的网络包实现远程抓取。使用这种方法既可以借助 Wireshark 的强大功能，又能降低对 Pod 的影响。\nksniff 的实现是上传一个静态编译的tcpdump 到 Pod 中，然后将 tcpdump 的输出重定向到本地的 Wireshark 进行调试。\n核心可以理解成 tcpdump -w - | wireshark -k -i -，与前面使用 debug 容器的方案类似。\n安装 通过 krew 安装：\n$ kubectl krew install sniff 或者下载发布包，手动安装：\n$ unzip ksniff.zip $ make install 特权模式容器 使用说明参考 ksniff 官方说明，这里我们只需要执行如下命令，默认就会重定向到 Wireshark，不需要显示地指定：\n$ kubectl sniff echo -n default -f \u0026#34;port 8080\u0026#34; 除了使用 Wireshark，可以使用其命令行模式的 tshark：\n$ kubectl sniff echo -n default -f \u0026#34;port 8080\u0026#34; -o - | tshark -r - 非特权模式容器 对于无特权的容器，就无法使用上面的方法了，会收到如下的错误提示：\nINFO[0000] command: \u0026#39;[/tmp/static-tcpdump -i any -U -w - port 8080]\u0026#39; executing successfully exitCode: \u0026#39;1\u0026#39;, stdErr :\u0026#39;static-tcpdump: any: You don\u0026#39;t have permission to capture on that device (socket: Operation not permitted) 不过，Ksniff 对此类容器也提供了支持。通过添加 -p 参数，ksniff 会创建一个新的可以访问节点上 Docker Daemon 的 pod，然后将容器附加到目标容器的网络命名空间，并执行报文捕获。\n注意，笔者使用的是 k3s 的环境，执行命令时需要通过参数指定 Docker Daemon 的 socket 地址 --socket /run/k3s/containerd/containerd.sock\n$ kubectl sniff echo -n default -f \u0026#34;port 8080\u0026#34; --socket /run/k3s/containerd/containerd.sock -p | wireshark -k -i - kubectl debug 临时容器 接下来也是之前介绍过的 kubectl debug ，也就是为 Pod 添加临时容器。\n同样我们可以通过这种方法对 Pod 的网络进行抓包，临时容器我们使用 addozhang/static-dump 镜像。\n$ kubectl debug -i echo --image addozhang/static-dump --target echo -- /static-tcpdump -i eth0 大家能发现这里将捕获的内容直接输出在标准输出中了，而不是重定向到本地的 Wireshark。\n原本临时容器应该是其中最接近完美的方案：不需上传任何文件目标容器、无需修改 Pod、无需重启、无需特权、支持 distroless 容器。然而，当尝试重定向到 Wireshark 或者 tshark 的时候，会遇到 Data written to the pipe is neither in a supported pcap format nor in pcapng format. 问题。\n最后经过一番折腾，也未能解决该问题。有解决了问题的朋友，也麻烦评论告知一下。感谢！\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/04/03/pexelslucafinardi7277575.jpg","permalink":"https://atbug.com/how-to-sniff-packet-in-kubernetes-pod/","tags":["Kubernetes","distroless"],"title":"如何在 Kubernetes Pod 内进行网络抓包"},{"categories":["笔记"],"contents":"这是一篇发表于 2016 年的论文，是 BeyondCorp 系列的第三篇。虽然过去多年，但是在流量的精细化控制方面仍然值得学习。\n BeyondCorp 是 Google 对零信任模型的实现。它建立在 Google 十年经验的基础上，并结合了社区的想法和最佳实践。通过将访问控制从网络边界转移到个人用户，BeyondCorp 几乎可以在任何位置进行安全工作，而无需传统的 VPN。\n The Access Proxy 详细描述了 BeyondCorp 前端基础设施的实现，介绍了在实现 Access Proxy（以下简称 AP）时的挑战与实践教训，并给出了最佳实践。\nAP 是在负载均衡反向代理的基础上增加了安全的处理，增加了验证、授权、集中日志记录，以及自服务配置。整个设计上，充分利用了对流量的精细化控制。\n认证 在传统代理的 TLS 外，引入认证功能。该功能的核心之一是请求方识别，验证用户身份。并支持一系列的身份验证选项。\n此外，认证还需要对后端服务“隐藏”身份验证凭证，也不会对后端服务自身的验证流程。\n多平台的认证 使用了一致的、不可复制的、唯一设备标识符和用于跟踪设备最新状态的存储，实现对设备的信任取代对网络的信任。\n对于不同的平台，充分利用各平台标识。比如 PC 和笔记本操作系统的设备证书，移动设备系统的设备标识。\n授权 授权的引入与认证功能一样，将基础功能与后端服务分离，解放了后端服务。但是由于复杂度限制，只能执行粗粒度的策略，还需与后端服务的细粒度策略结合使用。\n此外，还需解决代理与后端的双向认证问题，毕竟从ZTN（Zero Trust Network 零信任网络）的角度看内部网络同样不可信。通过双向认证确保数据（元数据，通常是请求头）不可欺骗的同时，还有额外的收益：通过增加元数据轻松地扩展 AP 的能力，新增的元数据也被后端接受并使用。\n实现中心化授权的前提是一个 ACL（Access Control List 访问控制表）引擎和用于表达 ACL 的 DSL（Domain Specific Language 领域特定语言）。\n集中日志记录 鉴于 AP 的定位非常适合进行数据的记录，用于事件响应和取证分析。可以记录请求头、响应编码以及用调试或重建访问决策和ACL 评估流程的元数据。\n运维扩展性 提供自服务配置降低团队介入的成本，前提是将配置结构化，同时实现配置的所有权管理。\n此外还提供响应的培训和充分的文档。\n其他 上面列出了 AP 提供的除代理以外的功能，还有一些最佳实践和经验分享，可以参考下面的脑图。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/03/20/pexelssorashimazaki5935792.jpg","permalink":"https://atbug.com/the-access-proxy-notes/","tags":["Proxy","Security"],"title":"《BeyondCorp Part III: The Access Proxy》解读"},{"categories":["云原生"],"contents":"在上一篇《在 Kubernetes 集群中使用 MetalLB 作为 LoadBalancer（上）- Layer2》中，我们使用 MetalLB 的 Layer2 模式作为 LoadBalancer 的实现，将 Kubernetes 集群中的服务暴露到集群外。\n还记得我们在 Configmap 中为 MetalLB 分配的 IP 地址池么？\napiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: |address-pools: - name: default protocol: layer2 addresses: - 192.168.1.30-192.168.1.49 这里分配的 192.168.1.30-192.168.1.49 IP 段正好是在笔者的家庭网络中，当我们用 192.168.1.30 可以成功访问服务。\n之前有提过 Layer2 的缺点时还漏了一点，除了故障转移过程中对可用性有影响且存在单点网络瓶颈，还有就是客户端需要与地址池位于同一个子网（假如将地址池改为 192.168.1.30-192.168.1.49，服务将无法访问）。不过在实验环境或者像笔者这样的 homelab 环境来说，前两个都不算是问题，后一个则在网络配置时稍微麻烦一些。\n虽然缺点很明显，但是 Layer2 模式有更强的通用性，不像 BGP 模式需要支持 BGP 的路由。但是这些都挡不住笔者的探（强）索（迫）欲（症），因为还有一个 OpenWrt 软路由运行在我的 Proxmox 虚拟机中。这个 OpenWrt 以软路由的方式，通过 192.168.1.2 对外提供路由服务，通过安装路由软件套件来支持 BGP。\n正式开始之前，先看下什么是 BPG 以及相关的术语。已经了解，或者觉得太抽象的同学可以直接跳过，待看完demo的再回头看。\n什么是 BGP BGP 是边界网关协议（Border Gateway Protocol）的缩写。\n 边界网关协议是互联网上一个核心的去中心化自治路由协议。它通过维护IP路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。\nBGP的邻居关系（或称通信对端/对等实体，peer）是通过人工配置实现的，对等实体之间通过TCP端口179建立会话交换数据。BGP路由器会周期地发送19字节的保持存活（keep-alive）消息来维护连接（默认周期为60秒）。在各种路由协议中，只有BGP使用TCP作为传输层协议。\n同一个AS自治系统中的两个或多个对等实体之间运行的BGP被称为iBGP（Internal/Interior BGP）。归属不同的AS的对等实体之间运行的BGP称为eBGP（External/Exterior BGP）。在AS边界上与其他AS交换信息的路由器被称作边界路由器（border/edge router），边界路由器之间互为eBGP对端。在Cisco IOS中，iBGP通告的路由距离为200，优先级比eBGP和任何内部网关协议（IGP）通告的路由都低。其他的路由器实现中，优先级顺序也是eBGP高于IGP，而IGP又高于iBGP。\niBGP和eBGP的区别主要在于转发路由信息的行为。例如，从eBGP peer获得的路由信息会分发给所有iBGP peer和eBGP peer，但从iBGP peer获得的路由信息仅会分发给所有eBGP peer。所有的iBGP peer之间需要全互联。\n 这里提到了三个名词：自治系统（AS）、内部网关协议（IGP）和外部网关协议（EGP）。\n自治系统 AS 我们看下来自维基百科的介绍：\n 自制系统（Autonomous system，缩写 AS），是指在互联网中，一个或多个实体管辖下的所有IP 网络和路由器的组合，它们对互联网执行共同的路由策略。自治系统编号都是16位长的整数，这最多能被分配给65536个自治系统。自治系统编号被分成两个范围。第一个范围是公开的ASN，从1到64511，它们可在互联网上使用；第二个范围是被称为私有编号的从64512到65535的那些，它们仅能在一个组织自己的网络内使用。\n 简单理解，电信、移动、联通都有自己的 AS 编号，且不只一个，有兴趣的可以查看维基百科中的中国互联网骨干网条目。\n除了互联网公开的 ASN 以外，私有的编号可以在内部使用。比如我可以我的家庭网络中使用私有编号创建几个 AS。\n内部路由协议 IGP 引用百科中的内容，不是本篇的重点因此不做过多介绍。\n 内部路由协议（Interior Gateway Protocol 缩写为 IGP）是指在一个自治系统（AS）内部所使用的一种路由协议。\n 外部网关协议 EGP 外部网关协议（Exterior Gateway Protocol，错写 EGP）是一个已经过时互联网路由协议。已由 BPG 取代。\nBPG 的由来 BPG 是为了替换 EGP 而创建的，而除了应用于 AS 外部，也可以应用在 AS 内部。因此又分为 EBGP 和 IBGP。\n说了这么多可能有些抽象，直接上 demo 吧。\nDemo 环境还是使用之前的，按照预先设想我们希望创建两个 AS：65000 和 65001。前者作为路由器和客户端所在的 AS，而后者是我们集群服务 LoadBalancer IP 所在的 AS。\n我们要先让 OpenWrt 支持 BGP。\nOpenWrt 支持 BGP 为了让 OpenWrt 支持 BGP，这里要用到路由软件套件 Quagga。Quagga 提供了 OSPFv2、OSPFv3、RIP v1 v2、RIPng 和 BGP-4 的实现。\nQuagga 架构由核心守护进程和 zebra 组成，后者作为底层 Unix 内核的抽象层，并通过 Unix 或者 TCP 向 Quagga 客户端提供 Zserv API。正是这些 Zserv 客户端实现了路由协议，并将路由的更新发送给 zebra 守护进程。当前 Zserv 的实现是：\nQuagga 的守护进程可以通过网络可访问的 CLI（简称 vty）进行配置。 CLI 遵循与其他路由软件类似的风格。还额外提供了一个工具 vtysh，充当了所有守护进程的聚合前端，允许在一个地方管理所有 Quagga 守护进程的所有功能。\n执行下面的命令即可完成安装：\n$ opkg update \u0026amp;\u0026amp; opkg install quagga quagga-zebra quagga-bgpd quagga-vtysh 成功安装之后，会自动启动并监听端口：\n$ netstat -lantp | grep -e \u0026#39;zebra\\|bgpd\u0026#39; tcp 0 0 0.0.0.0:2601 0.0.0.0:* LISTEN 2984/zebra tcp 0 0 0.0.0.0:2605 0.0.0.0:* LISTEN 3000/bgpd tcp 0 0 :::2601 :::* LISTEN 2984/zebra tcp 0 0 :::2605 :::* LISTEN 3000/bgpd 这里并没有看到 bpgd 用于接收路由信息而监听的 179 端口，这是因为该路由还没有分配 AS。不着急，让我们使用命令 vtysh进入 vty 进行配置：\n$ vtysh OpenWrt# conf t OpenWrt(config)# router bgp 65000 OpenWrt(config-router)# neighbor 192.168.1.5 remote-as 65001 OpenWrt(config-router)# neighbor 192.168.1.5 description ubuntu-dev1 OpenWrt(config-router)# neighbor 192.168.1.6 remote-as 65001 OpenWrt(config-router)# neighbor 192.168.1.6 description ubuntu-dev2 OpenWrt(config-router)# exit OpenWrt(config)# exit 在 vty 中使用 show ip bgp summary 命令查看：\nOpenWrt# show ip bgp summary BGP router identifier 192.168.1.2, local AS number 65000 RIB entries 0, using 0 bytes of memory Peers 2, using 18 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 192.168.1.5 4 65001 0 0 0 0 0 never Active 192.168.1.6 4 65001 0 0 0 0 0 never Active Total number of neighbors 2 Total num. Established sessions 0 Total num. of routes received 0 此时我们再去查看端口监听，就可以看到 bgpd 已经在监听 179 端口了：\n$ netstat -lantp | grep -e \u0026#39;zebra\\|bgpd\u0026#39; tcp 0 0 0.0.0.0:179 0.0.0.0:* LISTEN 3000/bgpd tcp 0 0 0.0.0.0:2601 0.0.0.0:* LISTEN 2984/zebra tcp 0 0 0.0.0.0:2605 0.0.0.0:* LISTEN 3000/bgpd tcp 0 0 :::179 :::* LISTEN 3000/bgpd tcp 0 0 :::2601 :::* LISTEN 2984/zebra tcp 0 0 :::2605 :::* LISTEN 3000/bgpd BGP 的路由设置好之后，就是 MetalLB 的部分了。\nMetalLB BGP 模式 我们更新一下 configmap：\napiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: |peers: - peer-address: 192.168.1.2 peer-asn: 65000 my-asn: 65001 address-pools: - name: default protocol: bgp addresses: - 192.168.0.30-192.168.0.49 更新之后，你会发现 Service 的 EXTERNAL-IP 并没有重新分配，MetalLB 的控制器并没有自动生效配置。我们删除控制器 pod 进行重启：\n$ kubectl delete po -n metallb-system -l app=metallb,component=controller pod \u0026#34;controller-66445f859d-vss2t\u0026#34; deleted 此时可以看到 Service 分配到了新的 IP：\n$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 25m nginx-lb LoadBalancer 10.43.188.185 192.168.0.30 8080:30381/TCP 21m nginx2-lb LoadBalancer 10.43.208.169 192.168.0.31 8080:32319/TCP 21m 检查 speaker POD 的日志，可以看到与 peer 192.168.1.2 之间的通信已经开始，并对外发布了 IP 地址的公告：\n{\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;configmap\u0026quot;:\u0026quot;metallb-system/config\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;peerAdded\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;peer configured, starting BGP session\u0026quot;,\u0026quot;peer\u0026quot;:\u0026quot;192.168.1.2\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.336335657Z\u0026quot;} {\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;configmap\u0026quot;:\u0026quot;metallb-system/config\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;configLoaded\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;config (re)loaded\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.336366122Z\u0026quot;} struct { Version uint8; ASN16 uint16; HoldTime uint16; RouterID uint32; OptsLen uint8 }{Version:0x4, ASN16:0xfde8, HoldTime:0xb4, RouterID:0xc0a80102, OptsLen:0x1e} {\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;sessionUp\u0026quot;,\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;localASN\u0026quot;:65001,\u0026quot;msg\u0026quot;:\u0026quot;BGP session established\u0026quot;,\u0026quot;peer\u0026quot;:\u0026quot;192.168.1.2:179\u0026quot;,\u0026quot;peerASN\u0026quot;:65000,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.337341549Z\u0026quot;} {\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;updatedAdvertisements\u0026quot;,\u0026quot;ips\u0026quot;:[\u0026quot;192.168.0.30\u0026quot;],\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;making advertisements using BGP\u0026quot;,\u0026quot;numAds\u0026quot;:1,\u0026quot;pool\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;protocol\u0026quot;:\u0026quot;bgp\u0026quot;,\u0026quot;service\u0026quot;:\u0026quot;default/nginx-lb\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.341939983Z\u0026quot;} {\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;serviceAnnounced\u0026quot;,\u0026quot;ips\u0026quot;:[\u0026quot;192.168.0.30\u0026quot;],\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;service has IP, announcing\u0026quot;,\u0026quot;pool\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;protocol\u0026quot;:\u0026quot;bgp\u0026quot;,\u0026quot;service\u0026quot;:\u0026quot;default/nginx-lb\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.341987657Z\u0026quot;} {\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;updatedAdvertisements\u0026quot;,\u0026quot;ips\u0026quot;:[\u0026quot;192.168.0.31\u0026quot;],\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;making advertisements using BGP\u0026quot;,\u0026quot;numAds\u0026quot;:1,\u0026quot;pool\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;protocol\u0026quot;:\u0026quot;bgp\u0026quot;,\u0026quot;service\u0026quot;:\u0026quot;default/nginx2-lb\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.342041554Z\u0026quot;} {\u0026quot;caller\u0026quot;:\u0026quot;level.go:63\u0026quot;,\u0026quot;event\u0026quot;:\u0026quot;serviceAnnounced\u0026quot;,\u0026quot;ips\u0026quot;:[\u0026quot;192.168.0.31\u0026quot;],\u0026quot;level\u0026quot;:\u0026quot;info\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;service has IP, announcing\u0026quot;,\u0026quot;pool\u0026quot;:\u0026quot;default\u0026quot;,\u0026quot;protocol\u0026quot;:\u0026quot;bgp\u0026quot;,\u0026quot;service\u0026quot;:\u0026quot;default/nginx2-lb\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2022-03-06T22:56:17.342056076Z\u0026quot;} 然后可以在 vty 中查看路由表：\nOpenWrt# show ip route Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, P - PIM, A - Babel, N - NHRP, \u0026gt; - selected route, * - FIB route K\u0026gt;* 0.0.0.0/0 via 192.168.1.1, br-lan C\u0026gt;* 127.0.0.0/8 is directly connected, lo B\u0026gt;* 192.168.0.30/32 [20/0] via 192.168.1.5, br-lan, 00:00:06 B\u0026gt;* 192.168.0.31/32 [20/0] via 192.168.1.5, br-lan, 00:00:06 C\u0026gt;* 192.168.1.0/24 is directly connected, br-lan 从表中我们可以找到 192.168.0.30/32 和 192.168.0.31/32 两条 BGP 的路由。\n测试 我们使用新的 IP 访问服务：\n$ curl -I 192.168.0.30:8080 HTTP/1.1 200 OK Server: nginx/1.21.6 Date: Sun, 06 Mar 2022 23:10:33 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 25 Jan 2022 15:03:52 GMT Connection: keep-alive ETag: \u0026#34;61f01158-267\u0026#34; Accept-Ranges: bytes 总结 至此，我们已经试过了 MetalLB 的两种模式：Layer2 有很强的通用性，不需要其他任何的依赖，但是缺点也明显；BGP 模式除了依赖支持 BGP 的路由，其他方面则没有任何限制，并且没有可用性的问题。\nBGP 应该是 LoadBalancer 的终极模式，但是 Layer2 也不是毫无用处。大家还是要看使用的场景来理性的选择，比如 homelab 中使用我会选择 Layer2 模式。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/03/07/pexelsarchiebinamira913215.jpg","permalink":"https://atbug.com/load-balancer-service-with-metallb-bgp-mode/","tags":["Network","Kubernetes","OpenWrt"],"title":"在 Kubernetes 集群中使用 MetalLB 作为 LoadBalancer（下）- BGP"},{"categories":["云原生"],"contents":"这是系列文章的上篇，下篇《在 Kubernetes 集群中使用 MetalLB 作为 LoadBalancer（下）- BGP》。\nTL;DR 网络方面的知识又多又杂，很多又是系统内核的部分。原本自己不是做网络方面的，系统内核知识也薄弱。但恰恰是这些陌生的内容满满的诱惑，加上现在的工作跟网络关联更多了，逮住机会就学习下。\n这篇以 Kubernetes LoadBalancer 为起点，使用 MetalLB 去实现集群的负载均衡器，在探究其工作原理的同时了解一些网络的知识。\n由于 MetalLB 的内容有点多，一步步来，今天这篇仅介绍其中简单又容易理解的部分，不出意外还会有下篇（太复杂，等我搞明白先 :D）。\nLoadBalancer 类型 Service 由于 Kubernets 中 Pod 的 IP 地址不固定，重启后 IP 会发生变化，无法作为通信的地址。Kubernets 提供了 Service 来解决这个问题，对外暴露。\nKubernetes 为一组 Pod 提供相同的 DNS 名和虚拟 IP，同时还提供了负载均衡的能力。这里 Pod 的分组通过给 Pod 打标签（Label ）来完成，定义 Service 时会声明标签选择器（selector）将 Service 与 这组 Pod 关联起来。\n根据使用场景的不同，Service 又分为 4 种类型：ClusterIP、NodePort、LoadBalancer 和 ExternalName，默认是 ClusterIP。这里不一一详细介绍，有兴趣的查看 Service 官方文档。\n除了今天的主角 LoadBalancer 外，其他 3 种都是比较常用的类型。LoadBalancer 官方的解释是：\n 使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。\n 看到“云提供商提供”几个字时往往望而却步，有时又需要 LoadBalancer 对外暴露服务做些验证工作（虽然除了 7 层的 Ingress 以外，还可以使用 NodePort 类型的 Service），而 Kubernetes 官方并没有提供实现。比如下面要介绍的 MetalLB 就是个不错的选择。\nMetalLB 介绍 MetalLB 是裸机 Kubernetes 集群的负载均衡器实现，使用标准路由协议。\n 注意： MetalLB 目前还是 beta 阶段。\n 前文提到 Kubernetes 官方并没有提供 LoadBalancer 的实现。各家云厂商有提供实现，但假如不是运行在这些云环境上，创建的 LoadBalancer Service 会一直处于 Pending 状态（见下文 Demo 部分）。\nMetalLB 提供了两个功能：\n 地址分配：当创建 LoadBalancer Service 时，MetalLB 会为其分配 IP 地址。这个 IP 地址是从预先配置的 IP 地址库获取的。同样，当 Service 删除后，已分配的 IP 地址会重新回到地址库。 对外广播：分配了 IP 地址之后，需要让集群外的网络知道这个地址的存在。MetalLB 使用了标准路由协议实现：ARP、NDP 或者 BGP。  广播的方式有两种，第一种是 Layer 2 模式，使用 ARP（ipv4）/NDP（ipv6） 协议；第二种是 BPG。\n今天主要介绍简单的 Layer 2 模式，顾名思义是 OSI 二层的实现。\n具体实现原理，看完 Demo 再做分析，等不及的同学请直接跳到最后。\n运行时 MetalLB 运行时有两种工作负载：\n Controler：Deployment，用于监听 Service 的变更，分配/回收 IP 地址。 Speaker：DaemonSet，对外广播 Service 的 IP 地址。  Demo 安装之前介绍下网络环境，Kubernetes 使用 K8s 安装在 Proxmox 的虚拟机上。\n安装 K3s 安装 K3s，这里需要通过 --disable servicelb 禁用 k3s 默认的 servicelb。\n 参考 K3s 文档，默认情况下 K3s 使用 Traefik ingress 控制器 和 Klipper Service 负载均衡器来对外暴露服务。\n curl -sfL https://get.k3s.io | sh -s - --disable traefik --disable servicelb --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 创建工作负载 使用 nginx 镜像，创建两个工作负载：\nkubectl create deploy nginx --image nginx:latest --port 80 -n default kubectl create deploy nginx2 --image nginx:latest --port 80 -n default 同时为两个 Deployment 创建 Service，这里类型选择 LoadBalancer：\nkubectl expose deployment nginx --name nginx-lb --port 8080 --target-port 80 --type LoadBalancer -n default kubectl expose deployment nginx2 --name nginx2-lb --port 8080 --target-port 80 --type LoadBalancer -n default 检查 Service 发现状态都是 Pending 的，这是因为安装 K3s 的时候我们禁用了 LoadBalancer 的实现：\nkubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 14m nginx-lb LoadBalancer 10.43.108.233 \u0026lt;pending\u0026gt; 8080:31655/TCP 35s nginx2-lb LoadBalancer 10.43.26.30 \u0026lt;pending\u0026gt; 8080:31274/TCP 16s 这时就需要 MetalLB 登场了。\n安装 MetalLB 使用官方提供 manifest 来安装，目前最新的版本是 0.12.1。此外，还可以其他安装方式供选择，比如 Helm、Kustomize 或者 MetalLB Operator。\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml kubectl get po -n metallb-system NAME READY STATUS RESTARTS AGE speaker-98t5t 1/1 Running 0 22s controller-66445f859d-gt9tn 1/1 Running 0 22s 此时再检查 LoadBalancer Service 的状态仍然是 Pending 的，嗯？因为，MetalLB 要为 Service 分配 IP 地址，但 IP 地址不是凭空来的，而是需要预先提供一个地址库。\n这里我们使用 Layer 2 模式，通过 Configmap 为其提供一个 IP 段：\napiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.1.30-192.168.1.49 此时再查看 Service 的状态，可以看到 MetalLB 为两个 Service 分配了 IP 地址 192.168.1.30、192.168.1.31：\nkubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 28m nginx-lb LoadBalancer 10.43.201.249 192.168.1.30 8080:30089/TCP 14m nginx2-lb LoadBalancer 10.43.152.236 192.168.1.31 8080:31878/TCP 14m 可以请求测试下：\ncurl -I 192.168.1.30:8080 HTTP/1.1 200 OK Server: nginx/1.21.6 Date: Wed, 02 Mar 2022 15:31:15 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 25 Jan 2022 15:03:52 GMT Connection: keep-alive ETag: \u0026#34;61f01158-267\u0026#34; Accept-Ranges: bytes curl -I 192.168.1.31:8080 HTTP/1.1 200 OK Server: nginx/1.21.6 Date: Wed, 02 Mar 2022 15:31:18 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 25 Jan 2022 15:03:52 GMT Connection: keep-alive ETag: \u0026#34;61f01158-267\u0026#34; Accept-Ranges: bytes macOS 本地使用 arp -a 查看 ARP 表可以找到这两个 IP 及 mac 地址，可以看出两个 IP 都绑定在同一个网卡上，此外还有虚拟机的 IP 地址。也就是说 3 个 IP 绑定在该虚拟机的 en0 上：\n而去虚拟机（节点）查看网卡（这里只能看到系统绑定的 IP）：\nLayer 2 工作原理 Layer 2 中的 Speaker 工作负载是 DeamonSet 类型，在每台节点上都调度一个 Pod。首先，几个 Pod 会先进行选举，选举出 Leader。Leader 获取所有 LoadBalancer 类型的 Service，将已分配的 IP 地址绑定到当前主机到网卡上。也就是说，所有 LoadBalancer 类型的 Service 的 IP 同一时间都是绑定在同一台节点的网卡上。\n当外部主机有请求要发往集群内的某个 Service，需要先确定目标主机网卡的 mac 地址（至于为什么，参考维基百科）。这是通过发送 ARP 请求，Leader 节点的会以其 mac 地址作为响应。外部主机会在本地 ARP 表中缓存下来，下次会直接从 ARP 表中获取。\n请求到达节点后，节点再通过 kube-proxy 将请求负载均衡目标 Pod。所以说，假如Service 是多 Pod 这里有可能会再跳去另一台主机。\n优缺点 优点很明显，实现起来简单（相对于另一种 BGP 模式下路由器要支持 BPG）。就像笔者的环境一样，只要保证 IP 地址库与集群是同一个网段即可。\n当然缺点更加明显了，Leader 节点的带宽会成为瓶颈；与此同时，可用性欠佳，故障转移需要 10 秒钟的时间（每个 speaker 进程有个 10s 的循环）。\n参考  地址解析协议 MetalLB 概念  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/03/03/pexelstirachardkumtanom347145.jpg","permalink":"https://atbug.com/load-balancer-service-with-metallb/","tags":["Network","Kubernetes"],"title":"在 Kubernetes 集群中使用 MetalLB 作为 LoadBalancer（上）- Layer2"},{"categories":["云原生"],"contents":"TL;DR 在本篇，我们分别使用了 Kubernetes 原生的网络策略和 Cilium 的网络策略实现了 Pod 网络层面的隔离。不同的是，前者只提供了基于 L3/4 的网络策略；后者支持 L3/4、L7 的网络策略。\n通过网络策略来提升网络安全，可以极大降低了实现和维护的成本，同时对系统几乎没有影响。\n尤其是基于 eBPF 技术的 Cilium，解决了内核扩展性不足的问题，从内核层面为工作负载提供安全可靠、可观测的网络连接。\n目录  TL;DR 目录 背景 示例应用 Kubernetes 网络策略  测试 思考   Cilium 网络策略  Cilium 简介 测试    背景 为什么说 Kubernetes 网络存在安全隐患？集群中的 Pod 默认是未隔离的，也就是 Pod 之间的网络是互通的，可以互相通信的。\n这里就会有问题，比如由于数据敏感服务 B 只允许特定的服务 A 才能访问，而服务 C 无法访问 B。要禁止服务 C 对服务 B 的访问，可以有几种方案：\n 在 SDK 中提供通用的解决方案，实现白名单的功能。首先请求要带有来源的标识，然后服务端可以接收规则设置放行特定标识的请求，拒绝其他的请求。 云原生的解决方案，使用服务网格的 RBAC、mTLS 功能。RBAC 实现原理与应用层的 SDK 方案类似，但是属于基础设施层的抽象通用方案；mTLS 则会更加复杂一些，在连接握手阶段进行身份验证，涉及证书的签发、验证等操作。  以上两种方案各有利弊：\n SDK 的方案实现简单，但是规模较大的系统会面临升级推广困难、多语言支持成本高等问题。 服务网格的方案是基础设施层的通用方案，天生支持多语言。但是对于未落地网格的用户来说，架构变化大，成本高。如果单纯为了解决安全问题，使用网格方案性价比又很低，且不说现有网格实现等落地难度大及后期的使用维护成本高。  继续向基础设施下层找方案，从网络层入手。Kubernetes 提供了的网络策略 NetworkPolicy，则可以实现“网络层面的隔离”。\n示例应用 在进一步演示 NetworkPolicy 的方案之前，先介绍用于演示的示例应用。我们使用 Cilium 在互动教程 Cilium getting started 中使用的“星球大战”场景。\n这里有三个应用，星战迷估计不会陌生：\n 死星 deathstar：在 80 端口提供 web 服务，有 2 个 副本，通过 Kubernetes Service 的负载均衡为帝国战机对外提供”登陆“服务。 钛战机 tiefighter：执行登陆请求。 X翼战机 xwing：执行登陆请求。   如图所示，我们使用了 Label 对三个应用进行了标识：org 和 class。在执行网络策略时，我们会使用这两个标签识别负载。\n # app.yaml --- apiVersion: v1 kind: Service metadata: name: deathstar labels: app.kubernetes.io/name: deathstar spec: type: ClusterIP ports: - port: 80 selector: org: empire class: deathstar --- apiVersion: apps/v1 kind: Deployment metadata: name: deathstar labels: app.kubernetes.io/name: deathstar spec: replicas: 2 selector: matchLabels: org: empire class: deathstar template: metadata: labels: org: empire class: deathstar app.kubernetes.io/name: deathstar spec: containers: - name: deathstar image: docker.io/cilium/starwars --- apiVersion: v1 kind: Pod metadata: name: tiefighter labels: org: empire class: tiefighter app.kubernetes.io/name: tiefighter spec: containers: - name: spaceship image: docker.io/tgraf/netperf --- apiVersion: v1 kind: Pod metadata: name: xwing labels: app.kubernetes.io/name: xwing org: alliance class: xwing spec: containers: - name: spaceship image: docker.io/tgraf/netperf Kubernetes 网络策略 可以通过官方文档获取更多详细信息，这里我们直接放出配置：\n# native/networkpolicy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: policy namespace: default spec: podSelector: matchLabels: org: empire class: deathstar policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: org: empire ports: - protocol: TCP port: 80  podSelector ：表示要应用网络策略的工作负载均衡，通过 label 选择到了 deathstar 的 2 个 Pod。 policyTypes ：表示流量的类型，可以是 Ingress 或 Egress 或两者兼具。这里使用 Ingress，表示对选择的 deathstar Pod 的入站流量执行规则。 ingress.from：表示流量的来源工作负载，也是使用 podSelector 和 Label 进行选择，这里选中了 org=empire 也就是所有“帝国的战机”。 ingress.ports：表示流量的进入端口，这里列出了 deathstar 的服务端口。  接下来，我们测试下。\n测试 先准备环境，我们使用 K3s 作为 Kubernetes 环境。但由于 K3s 默认的 CNI 插件 Flannel 不支持网络策略，我们需要换个插件，这里选择 Calico，即 K3s + Calico 的方案。\n先创建一个单节点的集群：\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\u0026#34;644\u0026#34; INSTALL_K3S_EXEC=\u0026#34;--flannel-backend=none --cluster-cidr=10.42.0.0/16 --disable-network-policy --disable=traefik\u0026#34; sh - 此时，所有的 Pod 都处于 Pending 状态，因为还需要安装 Calico：\nkubectl apply -f https://projectcalico.docs.tigera.io/manifests/calico.yaml 待 Calico 成功运行后，所有的 Pod 也会成功运行。\n接下来就是部署应用：\nkubectl apply -f app.yaml 执行策略前，执行下面的命令看看“战机能否登陆死星”：\nkubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed 从结果来看，两种 ”战机“（Pod 负载）都可以访问 deathstar 服务。\n此时执行网络策略：\nkubectl apply -f native/networkpolicy.yaml 再次尝试”登陆“，xwing 的登陆请求会停在那（需要使用 ctrl+c 退出，或者请求时加上 --connect-timeout 2）。\n思考 使用 Kubernetes 网络策略实现了我们想要的，从网络层面为服务增加了白名单的功能，这种方案没有改造成本，对系统也几乎无影响。\nCilium 还没出场就结束了？我们继续看：\n有时我们的服务会对外暴露一些管理端点，由系统调用执行一些管理上的操作，比如热更新、重启等。这些端点是不允许普通服务来调用，否则会造成严重的后果。\n比如示例中，tiefighter 访问了 deathstar 的管理端点 /exhaust-port：\nkubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port Panic: deathstar exploded goroutine 1 [running]: main.HandleGarbage(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa) /code/src/github.com/empire/deathstar/ temp/main.go:9 +0x64 main.main() /code/src/github.com/empire/deathstar/ temp/main.go:5 +0x85 出现了 Panic 错误，检查 Pod 你会发现 dealthstar 挂了。\nKubernetes 的网络策略仅能工作在 L3/4 层，对 L7 层就无能为力了。\n还是要请出 Cilium。\nCilium 网络策略 由于 Cilium 涉及了 Linux 内核、网络等众多知识点，要讲清实现原理篇幅极大。故这里仅摘取了官网的介绍，后期希望有时间再写一篇关于实现的。\nCilium 简介  Cilium 是一个开源软件，用于提供、保护和观察容器工作负载（云原生）之间的网络连接，由革命性的内核技术 eBPF 推动。\n eBPF 是什么？\n Linux 内核一直是实现监控/可观测性、网络和安全功能的理想地方。 不过很多情况下这并非易事，因为这些工作需要修改内核源码或加载内核模块， 最终实现形式是在已有的层层抽象之上叠加新的抽象。 eBPF 是一项革命性技术，它能在内核中运行沙箱程序（sandbox programs）， 而无需修改内核源码或者加载内核模块。\n  将 Linux 内核变成可编程之后，就能基于现有的（而非增加新的）抽象层来打造更加智能、 功能更加丰富的基础设施软件，而不会增加系统的复杂度，也不会牺牲执行效率和安全性。\n 我们来看下 Cilium 的网络策略：\n# cilium/networkpolicy-L4.yaml apiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;rule1\u0026#34; spec: description: \u0026#34;L7 policy to restrict access to specific HTTP call\u0026#34; endpointSelector: matchLabels: org: empire class: deathstar ingress: - fromEndpoints: - matchLabels: org: empire toPorts: - ports: - port: \u0026#34;80\u0026#34; protocol: TCP 与 Kubernetes 的原生网络策略差异不大，参考前面的介绍也都看懂，我们直接进入测试。\n测试 由于 Cilium 本身就实现了 CNI，所以之前的集群就不能用了，先卸载集群：\nk3s-uninstall.sh # ！！！切记要清理之前的 cni 插件 sudo rm -rf /etc/cni/net.d 还是使用同样的命令创建单节点的集群：\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\u0026#34;644\u0026#34; INSTALL_K3S_EXEC=\u0026#34;--flannel-backend=none --cluster-cidr=10.42.0.0/16 --disable-network-policy --disable=traefik\u0026#34; sh - # cilium 会使用该变量 export KUBECONFIG=/etc/rancher/k3s/k3s.yaml 接下来安装 Cilium CLI：\ncurl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} cilium version cilium-cli: v0.10.2 compiled with go1.17.6 on linux/amd64 cilium image (default): v1.11.1 cilium image (stable): v1.11.1 cilium image (running): unknown. Unable to obtain cilium version, no cilium pods found in namespace \u0026#34;kube-system\u0026#34; 安装 Cilium 到集群：\ncilium install 待 Cilium 成功运行：\ncilium status /¯¯\\  /¯¯\\__/¯¯\\  Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\  Hubble: disabled \\__/¯¯\\__/ ClusterMesh: disabled \\__/ Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 DaemonSet cilium Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 1 cilium-operator Running: 1 Cluster Pods: 3/3 managed by Cilium Image versions cilium-operator quay.io/cilium/operator-generic:v1.11.1@sha256:977240a4783c7be821e215ead515da3093a10f4a7baea9f803511a2c2b44a235: 1 cilium quay.io/cilium/cilium:v1.11.1@sha256:251ff274acf22fd2067b29a31e9fda94253d2961c061577203621583d7e85bd2: 1 部署应用：\nkubectl apply -f app.yaml 待应用启动后测试服务调用：\nkubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed 执行 L4 网络策略：\nkubectl apply -f cilium/networkpolicy-L4.yaml 再次尝试“登陆”死星，xwing 战机同样无法登陆，说明 L4 层的规则生效。\n我们再尝试 L7 层的规则：\n# cilium/networkpolicy-L7.yaml apiVersion: \u0026#34;cilium.io/v2\u0026#34; kind: CiliumNetworkPolicy metadata: name: \u0026#34;rule1\u0026#34; spec: description: \u0026#34;L7 policy to restrict access to specific HTTP call\u0026#34; endpointSelector: matchLabels: org: empire class: deathstar ingress: - fromEndpoints: - matchLabels: org: empire toPorts: - ports: - port: \u0026#34;80\u0026#34; protocol: TCP rules: http: - method: \u0026#34;POST\u0026#34; path: \u0026#34;/v1/request-landing\u0026#34; 执行规则：\nkubectl apply -f cilium/networkpolicy-L7.yaml 这回，使用 tiefighter 调用死星的管理接口：\nkubectl exec tiefighter -- curl -s -XPUT deathstar.default.svc.cluster.local/v1/exhaust-port Access denied # 登陆接口工作正常 kubectl exec tiefighter -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing Ship landed 这回返回了 Access denied，说明 L7 层的规则生效了。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/02/13/pexelstimamiroshnichenko5380664.jpg","permalink":"https://atbug.com/enhance-kubernetes-network-security-with-cilium/","tags":["Cilium","Kubernetes","eBPF"],"title":"使用 Cilium 增强 Kubernetes 网络安全"},{"categories":["云原生"],"contents":"TL;DR 本文介绍并安装体验了极简 Kubernetes 发行版，也顺便分析学习下编译的流程。\n背景 k8e 本意为 kuber easy，是一个 Kubernetes 的极简发行版，意图让云原生落地部署 Kubernetes 更轻松。k8e 是基于另一个发行版 k3s ，经过裁剪（去掉了 Edge/IoT 相关功能、traefik等）、扩展（加入 ingress、sidecar 实现、cilium等）而来。\nk8e 具有以下特性：\n 单二进制文件，集成了 k8s 的各种组件、containerd、runc、kubectl、nerdctl 等 使用 cilium 作为 cni 的实现，方便 eBPF 的快速落地 支持基于 Pipy 的 ingress、sidecar proxy，实现应用流量一站式管理 只维护一个 k8s 版本，目前是 1.21 按照私有云的经验增加、优化代码  得益于这些特性，k8e 非常适合CI、开发和企业级部署，单机版的集群适合技术验证环境。\n安装测试 可以从 GitHub 上下载对应版本的二进制文件，也可以自己手动编译（后面对编译的流程进行了简单的分析）。\nsudo k8e check-config sudo k8e server \u0026amp; # Kubeconfig is written to /etc/k8e/k8e.yaml export KUBECONFIG=/etc/k8e/k8e.yaml # On a different node run the below. NODE_TOKEN comes from # /var/lib/k8e/server/node-token on your server sudo k8e agent --server https://myserver:6443 --token ${NODE_TOKEN} # query all node from k8s cluster sudo k8e kubectl get nodes 因为没有提供默认的 cni 实现，此时 pod 都处于 Pending 状态。需要手动安装 cilium：\n通过 k8e check-config 可以找到 builddata 目录：/var/lib/k8e/data/5a7ced03412504a18bf3f49cbee5dafca7187d86ef8fdaa789448d53d7fbb823\n/var/lib/k8e/data/5a7ced03412504a18bf3f49cbee5dafca7187d86ef8fdaa789448d53d7fbb823/bin/cilium install cilium 安装成功后 Pod 成功运行，可以查看 cilium 状态：\n/var/lib/k8e/data/5a7ced03412504a18bf3f49cbee5dafca7187d86ef8fdaa789448d53d7fbb823/bin/cilium status /¯¯\\  /¯¯\\__/¯¯\\  Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\  Hubble: disabled \\__/¯¯\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 1, Ready: 1/1, Available: 1/1 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 1 cilium-operator Running: 1 Cluster Pods: 3/3 managed by Cilium Image versions cilium quay.io/cilium/cilium:v1.10.5: 1 cilium-operator quay.io/cilium/operator-generic:v1.10.5: 1 或者执行 cilium connectivity test 进行检查网络。\n上面是作为开发验证环境的部署，若要部署标准 Kubernetes，可以参考官方文档。\n编译流程分析 k8e 的编译部署简单的两条命令就能完成：\nmake generate make 执行 make generate 下载内置的几个工具：runc、nerdctl、cilium，保存在 bin 目录中。\n.DEFAULT_GOAL 是 ci，执行 make 则会执行 target ci 。\nDapper 在 makefile 的第一行是 TARGETS := $(shell ls hack | grep -v \\\\.sh | grep -v package-airgap| grep -v clean) 定义了变量 TARGETS，然后 $(TARGETS): .dapper 为几个 target 指定前置条件。其中就有 ci，因此在执行 ci 之前会先执行 target .dapper。\ntarget .dapper 用于下载 rancher dapper。dapper 是 Docker 的构建封装器，执行时会使用位于源码根目录 Dockerfile.dapper 构建镜像，并以此作为代码构建环境。\n在 Dockerfile.dapper 中，会准备构建环境，以及几个需要关注的设置：\n WORKDIR：/go/src/github.com/xiaods/k8e/，也就是容器的工作目录。dapper 启动时会将源码内容拷贝到容器中 ENTRYPOINT：[\u0026quot;./hack/entry.sh\u0026quot;]，容器启动的 entrypoint。 CMD：[\u0026quot;ci\u0026quot;]，容器运行时的默认 CMD ENV  DAPPER_SOURCE：/go/src/github.com/xiaods/k8e/ DAPPER_OUTPUT：./bin ./dist ./build/out，构建完成后，会执行 docker cp ${DAPPER_SOURCE}/${DAPPER_OUTPUT} . 将容器中的内容拷贝到容器外。 \u0026hellip;    dapper 下载完成后会执行 ci 的命令：./.dapper ci，根据 Dockerfile.dapper 的配置，启动后会执行 ./hack/entry.sh ci。\nCI CI 的流程与 k3s 的流程相比精简了很多。在 ./hack/ci 中，会依次执行：\n ./hack/validate：代码格式化、校验 ./hack/build：代码编译 ./hack/package：将前面下载、编译的 bin/ 目录下的二进制一同打包到同一个二进制文件中（使用 go-bindata）。 ./hack/binary_size_check.sh：二进制文件大小的检查，是否超过 81 M（k3s 的是64M，由于加入了 cilium 等 cli，体积会有增加）。  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/02/12/pexelsstanswinnen6465964.jpg","permalink":"https://atbug.com/explore-simple-kubernetes-distribution/","tags":["Kubernetes","Docker"],"title":"探秘 k8e：极简 Kubernetes 发行版"},{"categories":["笔记"],"contents":"SDKMAN 是一款管理多版本 SDK 的工具，可以实现在多个版本间的快速切换。安装和使用非常简单：\ncurl -s \u0026#34;https://get.sdkman.io\u0026#34; | bash sdk install java x.y.z 但在 m1（Apple Silicon）的 mac 上，不知道注意到没，列出的可选 Java SDK 少了很多。\nsdk list java ================================================================================ Available Java Versions for macOS ARM 64bit ================================================================================ Vendor | Use | Version | Dist | Status | Identifier -------------------------------------------------------------------------------- Corretto | | 17.0.2.8.1 | amzn | | 17.0.2.8.1-amzn | | 17.0.1.12.1 | amzn | | 17.0.1.12.1-amzn | | 17.0.0.35.2 | amzn | | 17.0.0.35.2-amzn Java.net | | 19.ea.5 | open | | 19.ea.5-open | | 19.ea.1.lm | open | | 19.ea.1.lm-open | | 18.ea.31 | open | | 18.ea.31-open | | 17.0.2 | open | | 17.0.2-open | | 17.0.1 | open | | 17.0.1-open Liberica | | 17.0.2.fx | librca | | 17.0.2.fx-librca | | 17.0.2 | librca | | 17.0.2-librca | | 17.0.1.fx | librca | | 17.0.1.fx-librca | \u0026gt;\u0026gt;\u0026gt; | 17.0.1 | librca | installed | 17.0.1-librca | | 11.0.14 | librca | | 11.0.14-librca | | 11.0.13 | librca | | 11.0.13-librca | | 8.0.322 | librca | | 8.0.322-librca | | 8.0.312 | librca | installed | 8.0.312-librca Microsoft | | 17.0.1 | ms | | 17.0.1-ms Oracle | | 17.0.2 | oracle | | 17.0.2-oracle | | 17.0.1 | oracle | | 17.0.1-oracle SapMachine | | 17.0.2 | sapmchn | | 17.0.2-sapmchn | | 17.0.1 | sapmchn | | 17.0.1-sapmchn Temurin | | 17.0.1 | tem | | 17.0.1-tem Zulu | | 17.0.2 | zulu | | 17.0.2-zulu | | 17.0.2.fx | zulu | | 17.0.2.fx-zulu | | 17.0.1 | zulu | | 17.0.1-zulu | | 17.0.1.fx | zulu | | 17.0.1.fx-zulu | | 11.0.14 | zulu | | 11.0.14-zulu | | 11.0.13 | zulu | | 11.0.13-zulu | | 8.0.322 | zulu | | 8.0.322-zulu | | 8.0.312 | zulu | | 8.0.312-zulu ================================================================================ 仔细看的话，第一行 “Available Java Versions for macOS ARM 64bit” 说明只列出了支持 arm64 的版本，并且列表里也没有 Graalvm。\n如果要安装 Graalvm，一种方法是参考官方安装文档，稍微复杂并且多版本的支持会麻烦一些；另一种方法是继续使用 sdkman。\n打开 ~/.sdkman/etc/config 可以看到 rosetta2 的兼容选项 sdkman_rosetta2_compatible，将值从 false 改为 true：\nsdkman_auto_answer=false sdkman_auto_complete=true sdkman_auto_env=false sdkman_beta_channel=false sdkman_colour_enable=true sdkman_curl_connect_timeout=7 sdkman_curl_max_time=10 sdkman_debug_mode=false sdkman_insecure_ssl=false sdkman_rosetta2_compatible=true sdkman_selfupdate_enable=true 关闭并重新打开命令行窗口，或者执行 source ~/.zshrc （这里是 zsh，其他 shell 也是类似的方式）。\n重新执行 sdk list java，就可以看到这次的列表就很全了，想要的 GraalVM 就在结果中：\nsdk list java ================================================================================ Available Java Versions for macOS 64bit ================================================================================ Vendor | Use | Version | Dist | Status | Identifier -------------------------------------------------------------------------------- Corretto | | 17.0.2.8.1 | amzn | | 17.0.2.8.1-amzn | | 17.0.1.12.1 | amzn | | 17.0.1.12.1-amzn | | 17.0.0.35.2 | amzn | | 17.0.0.35.2-amzn | | 11.0.14.9.1 | amzn | | 11.0.14.9.1-amzn | | 11.0.13.8.1 | amzn | | 11.0.13.8.1-amzn | | 8.322.06.1 | amzn | | 8.322.06.1-amzn | | 8.312.07.1 | amzn | | 8.312.07.1-amzn GraalVM | | 22.0.0.2.r17 | grl | | 22.0.0.2.r17-grl | | 22.0.0.2.r11 | grl | | 22.0.0.2.r11-grl | | 21.3.1.r17 | grl | | 21.3.1.r17-grl | | 21.3.1.r11 | grl | | 21.3.1.r11-grl | | 21.3.0.r17 | grl | | 21.3.0.r17-grl | | 21.3.0.r11 | grl | | 21.3.0.r11-grl | | 21.2.0.r16 | grl | | 21.2.0.r16-grl | | 21.2.0.r11 | grl | | 21.2.0.r11-grl | | 20.3.5.r11 | grl | | 20.3.5.r11-grl | | 20.3.4.r11 | grl | | 20.3.4.r11-grl | | 20.3.3.r11 | grl | | 20.3.3.r11-grl | | 19.3.6.r11 | grl | | 19.3.6.r11-grl Java.net | | 19.ea.5 | open | | 19.ea.5-open | | 19.ea.1.lm | open | | 19.ea.1.lm-open | | 18.ea.31 | open | | 18.ea.31-open | | 17.ea.3.pma | open | | 17.ea.3.pma-open | | 21.2.0.r16 | grl | | 21.2.0.r16-grl | | 21.2.0.r11 | grl | | 21.2.0.r11-grl | | 20.3.5.r11 | grl | | 20.3.5.r11-grl | | 20.3.4.r11 | grl | | 20.3.4.r11-grl | | 20.3.3.r11 | grl | | 20.3.3.r11-grl | | 19.3.6.r11 | grl | | 19.3.6.r11-grl Java.net | | 19.ea.5 | open | | 19.ea.5-open | | 19.ea.1.lm | open | | 19.ea.1.lm-open | | 18.ea.31 | open | | 18.ea.31-open | | 17.ea.3.pma | open | | 17.ea.3.pma-open | | 17.0.2 | open | | 17.0.2-open | | 17.0.1 | open | | 17.0.1-open | | 11.0.2 | open | | 11.0.2-open Liberica | | 17.0.2.fx | librca | | 17.0.2.fx-librca | | 17.0.2 | librca | | 17.0.2-librca | | 17.0.1.fx | librca | | 17.0.1.fx-librca | \u0026gt;\u0026gt;\u0026gt; | 17.0.1 | librca | installed | 17.0.1-librca | | 11.0.14.fx | librca | | 11.0.14.fx-librca | | 11.0.14 | librca | | 11.0.14-librca | | 11.0.13.fx | librca | | 11.0.13.fx-librca | | 11.0.13 | librca | | 11.0.13-librca | | 8.0.322.fx | librca | | 8.0.322.fx-librca | | 8.0.322 | librca | | 8.0.322-librca | | 8.0.312.fx | librca | | 8.0.312.fx-librca | | 8.0.312 | librca | installed | 8.0.312-librca Liberica NIK | | 21.3.0.r17 | nik | | 21.3.0.r17-nik | | 21.3.0.r11 | nik | | 21.3.0.r11-nik | | 21.2 | nik | | 21.2-nik Microsoft | | 17.0.1 | ms | | 17.0.1-ms | | 11.0.13 | ms | | 11.0.13-ms Oracle | | 17.0.2 | oracle | | 17.0.2-oracle | | 17.0.1 | oracle | | 17.0.1-oracle SapMachine | | 17.0.2 | sapmchn | | 17.0.2-sapmchn | | 17.0.1 | sapmchn | | 17.0.1-sapmchn | | 11.0.14 | sapmchn | | 11.0.14-sapmchn | | 11.0.13 | sapmchn | | 11.0.13-sapmchn Semeru | | 17.0.1 | sem | | 17.0.1-sem | | 11.0.13 | sem | | 11.0.13-sem | | 8.0.312 | sem | | 8.0.312-sem Temurin | | 17.0.1 | tem | | 17.0.1-tem | | 11.0.14 | tem | | 11.0.14-tem | | 11.0.13 | tem | | 11.0.13-tem | | 8.0.322 | tem | | 8.0.322-tem | | 8.0.312 | tem | | 8.0.312-tem Trava | | 11.0.9 | trava | | 11.0.9-trava | | 8.0.232 | trava | | 8.0.232-trava Zulu | | 17.0.2 | zulu | | 17.0.2-zulu | | 17.0.2.fx | zulu | | 17.0.2.fx-zulu | | 17.0.1 | zulu | | 17.0.1-zulu | | 17.0.1.fx | zulu | | 17.0.1.fx-zulu | | 11.0.14 | zulu | | 11.0.14-zulu | | 11.0.14.fx | zulu | | 11.0.14.fx-zulu | | 11.0.13 | zulu | | 11.0.13-zulu | | 11.0.13.fx | zulu | | 11.0.13.fx-zulu | | 8.0.322 | zulu | | 8.0.322-zulu | | 8.0.322.fx | zulu | | 8.0.322.fx-zulu | | 8.0.312 | zulu | | 8.0.312-zulu | | 8.0.312.fx | zulu | | 8.0.312.fx-zulu | | 7.0.332 | zulu | | 7.0.332-zulu | | 7.0.322 | zulu | | 7.0.322-zulu ================================================================================ ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-andrea-piacquadio-3822859.jpg","permalink":"https://atbug.com/install-graalvm-on-m1-mac-with-sdkman/","tags":["macOS","Java","Graalvm"],"title":"使用 sdkman 在 M1 Mac 上 安装 graalvm jdk"},{"categories":["翻译","云原生"],"contents":"译者注：\n这篇文章很全面的罗列出了 Kubernetes 中涉及的网络知识，从 Linux 内核的网络内容，到容器、Kubernetes，一一进行了详细的说明。\n​文章篇幅有点长，不得不说，网络是很复杂很麻烦的一层，但恰恰这层多年来变化不大。希望翻译的内容对大家能有所帮助，有误的地方，也欢迎大家指正。\n本文翻译获得 Learnk8s 的授权，原文 Tracing the path of network traffic in Kubernetes 作者 Kristijan Mitevski。\n TL;DR： 本文将代理了解 Kubernetes 集群内外的数据流转。从最初的 Web 请求开始，一直到托管应用程序的容器。\n目录  目录 Kubernetes 网络要求 Linux 网络命名空间如何在 pod 中工作 Pause 容器创建 Pod 中的网络命名空间 为 Pod 分配了 IP 地址 检查集群中 pod 到 pod 的流量 Pod 命名空间连接到以太网桥接器 跟踪同一节点上 pod 间的流量 跟踪不同节点上 pod 间的通信  位运算的工作原理   容器网络接口 - CNI 检查 pod 到服务的流量 使用 Netfilter 和 Iptables 拦截和重写流量 检查服务的响应 回顾  Kubernetes 网络要求 在深入了解 Kubernetes 中的数据流转之前，让我们先澄清下 Kubernetes 网络的要求。\nKubernetes 网络模型定义了一套基本规则：\n 集群中的 pod 应该能够与任何其他 pod 自由通信，而无需使用网络地址转换（NAT）。 在不使用 NAT 的情况下，集群节点上运行的任意程序都应该能够与同一节点上的任意 pod 通信。 每个 pod 都有自己的 IP 地址（IP-per Pod），其他 pod 都可以使用同一个地址进行访问。  这些要求不会将实现限制在单一方案上。\n相反，他们概括了集群网络的特性。\n在满足这些限制时，必须解决如下挑战：\n 如何保证同一 pod 中的容器间的访问就像在同一主机上一样？ Pod 能否访问集群中的其他 pod？ Pod 能否访问服务（service）？以及服务可以负载均衡请求吗？ Pod 可以接收来自集群外的流量吗？  本文将专注于前三点，从 pod 内部网络或者容器间的通信说起。\nLinux 网络命名空间如何在 pod 中工作 我们想象下，有一个承载应用程序的主容器和另一个与它一起运行的容器。\n在示例 Pod 中有一个 Nginx 容器和 busybox 容器：\napiVersion: v1 kind: Pod metadata: name: multi-container-pod spec: containers: - name: container-1 image: busybox command: [\u0026#39;/bin/sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;sleep 1d\u0026#39;] - name: container-2 image: nginx 在部署时，会出现如下情况：\n Pod 在节点上得到自己的网络命名空间。 Pod 分配到一个 IP 地址，两个容器间共享端口。 两个容器共享同一个网络命名空间，在本地互相可见。  网络配置在后台很快完成。\n然后，我们退后一步，是这理解为什么上面是容器运行所必须的。\n在 Linux 中，网络命名空间是独立的、隔离的逻辑空间。\n可以将网络命名空间看成将物理网络接口分割成更小的独立部分。\n每部分都可以单独配置，并使用自己的网络规则和资源。\n这些可以包括防火墙规则、接口（虚拟或物理）、路由和其他所有与网络相关的内容。\n  物理接口持有根命名空间。\n  可以使用 Linux 网络命名空间创建隔离的网络。每个网络都是独立的，除非进行配置否则不会与其他命名空间通信。\n  物理接口必须处理最后的所有真实数据包，因此所有的虚拟接口都是从中创建的。\n网络命名空间可以通过 ip-netns 管理工具 来管理，可以使用 ip netns list 列出主机上的命名空间。\n 请注意，创建的网络命名空间将会出现在 /var/run/netns 目录下，但 Docker 并没有遵循这一点。\n 例如，下面是 Kubernetes 节点的命名空间：\n$ ip netns list cni-0f226515-e28b-df13-9f16-dd79456825ac (id: 3) cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd (id: 4) cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e (id: 2) cni-7619c818-5b66-5d45-91c1-1c516f559291 (id: 1) cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8 (id: 0)  注意 cni- 前缀意味着命名空间的创建由 CNI 来完成。\n 当创建 pod 并分配给节点时，CNI 会：\n 为其创建网络命名空间。 分配 IP 地址。 将容器连接到网络。  如果 pod 像上面的示例一样包含多个容器，则所有容器都被置于同一个命名空间中。\n  创建 pod 时，CNI 为容器创建网络命名空间\n  然后分配 IP 地址\n  最后将容器连接到网络的其余部分\n  那么当列出节点上的容器时会看到什么？\n可以 SSH 到 Kubernetes 节点来查看命名空间：\n$ lsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND 4026531992 net 171 1 root unassigned /run/docker/netns/default /sbin/init noembed norestore 4026532286 net 2 4808 65535 0 /run/docker/netns/56c020051c3b /pause 4026532414 net 5 5489 65535 1 /run/docker/netns/7db647b9b187 /pause lsns 命令会列出主机上所有的命名空间。\n 记住 Linux 中有多种命名空间类型。\n Nginx 容器在哪？\n那么 pause 容器又是什么？\nPause 容器创建 Pod 中的网络命名空间 从节点上的所有进程中找出 Nginx 容器：\n$ lsns NS TYPE NPROCS PID USER COMMAND # truncated output 4026532414 net 5 5489 65535 /pause 4026532513 mnt 1 5599 root sleep 1d 4026532514 uts 1 5599 root sleep 1d 4026532515 pid 1 5599 root sleep 1d 4026532516 mnt 3 5777 root nginx: master process nginx -g daemon off; 4026532517 uts 3 5777 root nginx: master process nginx -g daemon off; 4026532518 pid 3 5777 root nginx: master process nginx -g daemon off; 该容器出现在了挂在（mount mnt）、Unix 分时系统（Unix time-sharing uts）和 PID（pid）命名空间中，但是并不在网络命名空间（net）中。\n不幸的是，lsns 只显示了每个进程最低的 PID，不过可以根据进程 ID 进一步过滤。\n可以通过以下内容检索Nginx 容器的所有命名空间：\n$ sudo lsns -p 5777 NS TYPE NPROCS PID USER COMMAND 4026531835 cgroup 178 1 root /sbin/init noembed norestore 4026531837 user 178 1 root /sbin/init noembed norestore 4026532411 ipc 5 5489 65535 /pause 4026532414 net 5 5489 65535 /pause 4026532516 mnt 3 5777 root nginx: master process nginx -g daemon off; 4026532517 uts 3 5777 root nginx: master process nginx -g daemon off; 4026532518 pid 3 5777 root nginx: master process nginx -g daemon off; pause 进程再次出现，这次它劫持了网络命名空间。\n那是什么？\n集群中的每个 pod 都有一个在后台运行的隐藏容器，被称为 pause。\n列出节点上的所有容器并过滤出 pause 容器：\n$ docker ps | grep pause fa9666c1d9c6 k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_kube-dns-599484b884-sv2js… 44218e010aeb k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_blackbox-exporter-55c457d… 5fb4b5942c66 k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_kube-dns-599484b884-cq99x… 8007db79dcf2 k8s.gcr.io/pause:3.4.1 \u0026#34;/pause\u0026#34; k8s_POD_konnectivity-agent-84f87c… 将看到对于节点分配到的每个 pod，都有一个匹配的 pause 容器。\n该 pause 容器负责创建和维持网络命名空间。\n它包含的代码极少，部署后立即进入睡眠状态。\n然而，它在 Kubernetes 生态中的首当其冲，发挥着至关重要的作用。。\n  创建 pod 时，CNI 会创建一个带有睡眠容器的网络命名空间\n  Pod 中的所有容器都会加入到它创建的网络命名空间中\n  此时 CNI 分配 IP 地址并将容器连接到网络\n  进入睡眠状态的容器有什么用？\n要了解它的实用性，我们可以想象下如示例一样有两个容器的 pod，但没有 pause 容器。\n容器启动，CNI：\n 为 Nginx 容器创建一个网络命名空间。 把 busybox 容器加入到前面创建的网络命名空间中。 为 pod 分配 IP 地址。 将容器连接到网络。  假如 Nginx 容器崩溃了会发生什么？\nCNI 将不得不再次完成所有流程，两个容器的网络都会中断。\n由于 sleep 容器不太可能有任何 bug，因此创建网络命名空间通常是一个更保险、更健壮的选择。\n如果 pod 中的一个容器崩溃，其余的仍可以处理网络请求。\n为 Pod 分配了 IP 地址 前面提到 pod 和所有容器获得了同样的 IP。\n这是怎么配置的？\n在 pod 网络命名空间中，创建一个接口并分配 IP 地址。\n我们来验证下。\n首先，找到 pod 的 IP 地址：\n$ kubectl get pod multi-container-pod -o jsonpath={.status.podIP} 10.244.4.40 接下来，找到相关的网络命名空间。\n由于网络命名空间是从物理接口创建的，需要访问集群节点。\n 如果你运行的是 minikube，可以通过 minikube ssh 访问节点。如果在云提供商中运行，应该有某种方法通过 SSH 访问节点。\n 进入后，可以找到创建的最新的网络命名空间：\n$ ls -lt /var/run/netns total 0 -r--r--r-- 1 root root 0 Sep 25 13:34 cni-0f226515-e28b-df13-9f16-dd79456825ac -r--r--r-- 1 root root 0 Sep 24 09:39 cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd -r--r--r-- 1 root root 0 Sep 24 09:39 cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e -r--r--r-- 1 root root 0 Sep 24 09:39 cni-7619c818-5b66-5d45-91c1-1c516f559291 -r--r--r-- 1 root root 0 Sep 24 09:39 cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8 本示例中，它是 cni-0f226515-e28b-df13-9f16-dd79456825ac。此时，可以在该命名空间总执行 exec 命令：\n$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip a # output truncated 3: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.244.4.40/32 brd 10.244.4.40 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::14a4:f8ff:fe4f:5677/64 scope link valid_lft forever preferred_lft forever 10.244.4.40 就是 pod 的 IP 地址。\n通过查找 @if12 中的 12 找到网络接口。\n$ ip link | grep -A1 ^12 12: vethweplb3f36a0@if16: mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default link/ether 72:1c:73:d9:d9:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 1 还可以验证 Nginx 容器是否从该命名空间中监听 HTTP 流量：\n$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac netstat -lnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 692698/nginx: master tcp6 0 0 :::80 :::* LISTEN 692698/nginx: master  如果无法通过 SSH 访问集群的节点，可以试试 kubectl exec 进入到 busybox 容器，然后使用 ip 和 netstat 命令。\n 太棒了！\n现在我们已经介绍了容器间的通信，接下来看看 Pod 与 Pod 直接如何建立通信。\n检查集群中 pod 到 pod 的流量 当说起 pod 间通信时，会有两种可能：\n Pod 流量流向同一节点上的 pod。 Pod 流量流量另一个节点上的 pod。  为了使整个设置正常工作，我们需要之前讨论过的虚拟接口和以太网桥接。\n在继续之前，我们先讨论下他们的功能以及为什么他们时必需的。\n要完成 pod 与其他 pod 的通信，它必须先访问节点的根命名空间。\n这是使用连接 pod 和根命名空间的虚拟以太网对来实现的。\n这些虚拟接口设备（veth 中的 v）连接并充当两个命名空间间的隧道。\n使用此 veth 设备，将一端连接到 pod 的命名空间，另一端连接到根命名空间。\n这些 CNI 可以替你做，也可以手动操作：\n$ ip link add veth1 netns pod-namespace type veth peer veth2 netns root 现在 pod 的命名空间有了可以访问根命名空间的隧道。\n节点上每个新建的 pod 都会设置如下所示的 veth 对。\n创建接口对时其中一部分。\n其他的就是为以太网设备分配地址，并创建默认路由。\n来看下如何在 pod 的命名空间中设置 veth1 接口：\n$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip addr add 10.244.4.40/24 dev veth1 $ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip link set veth1 up $ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip route add default via 10.244.4.40 在节点侧，我们创建另一个 veth2 对：\n$ ip addr add 169.254.132.141/16 dev veth2 $ ip link set veth2 up 可以像以前一样检查现有的 veth 对。\n在 pod 的命名空间中，检查 eth0 接口的后缀。\n$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip link show type veth 3: eth0@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0 这种情况下可以使用 grep -A1 ^12 进行查找（或者滚动到目标所在）：\n$ ip link show type veth # output truncated 12: cali97e50e215bd@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-0f226515-e28b-df13-9f16-dd79456825ac  也可以使用 ip -n cni-0f226515-e28b-df13-9f16-dd79456825ac link show type veth 命令。\n 注意 3: eth0@if12 和 12: cali97e50e215bd@if3 接口上的符号。\n在 pod 命名空间中，eth0 接口连接到根命名空间中编号为 12 的接口。因此是 @if12。\n在 veth 对的另一端，根命名空间连接到 pod 命名空间的 3 号接口。\n接下来是连接 veth 对两端的桥接器（bridge）。\nPod 命名空间连接到以太网桥接器 桥接器将位于根命名空间中的虚拟接口的每一端“绑定”。\n该桥接器将允许流量在虚拟对之间流动，并通过公共根命名空间。\n理论时间。\n以太网桥接器位于OSI 网络模型的第二层。\n可以将桥接器看作一个虚拟交换机，接受来自不同命名空间和接口的连接。\n以太网桥接器允许连接同一个节点上的多个可用网络。\n因此，可以使用该设置连接两个接口：从 pod 命名空间的 veth 连接到同一节点上另一个 pod 的 veth。\n我们继续看下以太网桥接器和 veth 对的作用。\n跟踪同一节点上 pod 间的流量 假设同一个节点上有两个 pod，Pod-A 想向 Pod-B 发送消息。\n  由于目标不是同命名空间的容器，Pod-A 向其默认接口 eth0 发送数据包。这个接口与 veth 对的一端绑定，作为隧道。因此数据包将被转发到节点的根命名空间。\n  以太网桥接器作为虚拟交换机，必须以某种方式将目标 pod IP（Pod-B）解析为其 MAC 地址。\n  轮到ARP 协议上场了。当帧到达桥接器时，会向所有连接的设备发送 ARP 广播。桥接器喊道谁有 Pod-B 的 IP 地址。\n  收到带有连接 Pod-B 接口的 MAC 地址的回复，然后此信息存储在桥接器 ARP 缓存（查找表）中。\n  IP 和 MAC 地址的映射存储完成后，桥接器在表中查找，并将数据包转发到正确的短点。数据包到达根命名空间中 Pod- B 的 veth，然后从那快速到达 Pod-B 命名空间内的 eth0 接口。\n  有了这个，Pod-A 和 Pod-B 之间的通信取得了成功。\n跟踪不同节点上 pod 间的通信 对于需要跨不同节点通信的 pod，需要额外的通信跳转。\n  前几个步骤保持不变，直到数据包到达根命名空间并需要发送到 Pod- B。\n  当目标地址不在本地网络中，数据包将被转发到本节点的默认网关。节点上退出或默认网关通常位于 eth0 接口上 \u0026ndash; 将节点连接到网络的物理接口。\n  这次并不会发生 ARP 解析，因为源和目标 IP 在不同网络上。\n检查使用位运算（Bitwise）操作完成。\n当目标 IP 不在当前网络上时，数据包将被转发到节点的默认网关。\n位运算的工作原理 在确定数据包的转发位置时，源节点必须执行位运算。\n这也被称为与操作。\n作为复习，位与操作产生如下结果：\n0 AND 0 = 0 0 AND 1 = 0 1 AND 0 = 0 1 AND 1 = 1 除了 1 与 1 以外的都是 false。\n如果源节点的 IP 为 192.168.1.1，子网掩码为 /24，目标 IP 为 172.16.1.1/16，则按位与操作将确认他们不在同一网络上。\n这意味着目标 IP 与数据包的源在不同的网络上，因此数据包将在默认网关中转发。\n数学时间。\n我们必须从二进制文件中的 32 位地址执行与操作开始。\n先找出源和目标 IP 的网络。\n| Type | Binary | Converted | | ---------------- | ----------------------------------- | ------------------ | | Src. IP Address | 11000000.10101000.00000001.00000001 | 192.168.1.1 | | Src. Subnet Mask | 11111111.11111111.11111111.00000000 | 255.255.255.0(/24) | | Src. Network | 11000000.10101000.00000001.00000000 | 192.168.1.0 | | | | | | Dst. IP Address | 10101100.00010000.00000001.00000001 | 172.16.1.1 | | Dst. Subnet Mask | 11111111.11111111.00000000.00000000 | 255.255.0.0(/16) | | Dst. Network | 10101100.00010000.00000000.00000000 | 172.16.0.0 | 位运算操作后，需要将目标 IP 与数据包源节点的子网进行比较。\n| Type | Binary | Converted | | ---------------- | ----------------------------------- | ------------------ | | Dst. IP Address | 10101100.00010000.00000001.00000001 | 172.16.1.1 | | Src. Subnet Mask | 11111111.11111111.11111111.00000000 | 255.255.255.0(/24) | | Network Result | 10101100.00010000.00000001.00000000 | 172.16.1.0 进行位比较后，ARP 会检查其查询表来查找默认网关的 MAC 地址。\n如果有条目，将立即转发数据包。\n否则，先进行广播以确定网关的 MAC 地址。\n  数据包现在路由到另一个节点的默认接口，我们叫它 Node-B。\n  以相反的顺序。数据包现在位与 Node-B 的根命名空间，并到达桥接器，这里会进行 ARP 解析。\n  收到带有连接 Pod-B 的接口 MAC地址的回复。\n  这次桥接器通过 Pod-B 的 veth 设备将帧转发，并到达 Pod-B 自己的命名空间。\n  此时应该已经熟悉了 pod 之间的流量如何流转，接下来再探索下 CNI 如何创建上述内容。\n容器网络接口 - CNI 容器网络接口（CNI）关注当前节点的网络。\n可以将 CNI 看作网络插件在解决 Kubernetes 某些 需求时要遵循的一套规则。\n然而，它不仅仅与 Kubernetes 或者特定网络插件关联。\n可以使用如下 CNI：\n Calico Cilium Flannel Weave Net 其他网络插件  他们都实现相同的 CNI 标准。\n如果没有 CNI，你需要手动完成如下操作：\n 创建 pod（容器）的网络命名空间 创建接口 创建 veth 对 设置命名空间网络 设置静态路由 配置以太网桥接器 分配 IP 地址 创建 NAT 规则  还有太多其他需要手动完成的工作。\n更不用说删除或重新启动 pod 时删除或调整上述所有内容了。\nCNI 必须支持四个不同的操作：\n ADD - 将容器添加到网络 DEL - 从网络中删除容器 CHECK - 如果容器的网络出现问题，则返回错误 VERSION - 显示插件的版本  让我们在实践中看看它是如何工作的。\n当 pod 分配到特定节点时，kubelet 本身不会初始化网络。\n相反，它将任务交给了 CNI。\n然后，它指定了配置，并以 JSON 格式将其发送给 CNI 插件。\n可以在节点的 /etc/cni/net.d 目录中，找到当前 CNI 的配置文件：\n$ cat 10-calico.conflist { \u0026#34;name\u0026#34;: \u0026#34;k8s-pod-network\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;calico\u0026#34;, \u0026#34;datastore_type\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;mtu\u0026#34;: 0, \u0026#34;nodename_file_optional\u0026#34;: false, \u0026#34;log_level\u0026#34;: \u0026#34;Info\u0026#34;, \u0026#34;log_file_path\u0026#34;: \u0026#34;/var/log/calico/cni/cni.log\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;calico-ipam\u0026#34;, \u0026#34;assign_ipv4\u0026#34; : \u0026#34;true\u0026#34;, \u0026#34;assign_ipv6\u0026#34; : \u0026#34;false\u0026#34;}, \u0026#34;container_settings\u0026#34;: { \u0026#34;allow_ip_forwarding\u0026#34;: false }, \u0026#34;policy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;k8s\u0026#34; }, \u0026#34;kubernetes\u0026#34;: { \u0026#34;k8s_api_root\u0026#34;:\u0026#34;https://10.96.0.1:443\u0026#34;, \u0026#34;kubeconfig\u0026#34;: \u0026#34;/etc/cni/net.d/calico-kubeconfig\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} }, {\u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;snat\u0026#34;: true, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true}} ] } 每个插件都使用不同类型的配置来设置网络。\n例如，Calico 使用 BGP 路由协议配对的第 3 层网络来连接 pod。\nCilium 在第 3 到 7 层使用 eBPF 配置覆盖网络。\n与 Calico 一起，Cilium 支持设置网络策略来限制流量。\n那该如何选择呢？\n这取决于。\nCNI 主要有两组。\n第一组中，可以找到使用基本网络设置（也称为扁平网络）的CNI，并将集群 IP 池 中的IP 地址分配给 pod。\n这可能会因为快速用尽可用的 IP 地址而成为负担。\n相反，另一种方法是使用覆盖网络。\n简而言之，覆盖网络是主（底层）网络之上的辅助网络。\n覆盖网络的工作原理是封装来自底层网络的所有数据包，这些数据包指向另一个节点上的 pod。\n覆盖网络的一项流行技术是 VXLAN，它允许在 L3 网络上隧道传输 L2 域。\n那么哪种更好？\n没有唯一的答案，通常取决于你的需求。\n你是在构建一个拥有数万个节点的大集群吗？\n可能覆盖网络更好。\n你是否在意更简单的设置和在嵌套网络中不失去检查网络流量的能力。\n扁平网络更适合你。\n现在已经讨论了 CNI，让我们继续探索 Pod 到服务（service）的通信是如何完成的。\n检查 pod 到服务的流量 由于 Kubernetes 环境下 pod 的动态特性，分配给 pod 的 IP 地址不是静态的。\n这些 IP 地址是短暂的，每次创建或者删除 pod 时都会发生变化。\n服务解决了这个问题，为连接到一组 pod 提供了稳定的机制。\n默认情况下，在 Kubernetes 中创建服务时，会为其预定并分配虚拟 IP 。\n使用选择器将服务于目标 pod 进行管理。\n当删除 pod 并添加新 pod 时会发生什么？\n该服务的虚拟 IP 保持不变。\n然而，无需敢于，流量将到达新创建的 pod。\n换句话说，Kubernetes 中的服务类似于负载均衡器。\n但他们时如何工作的？\n使用 Netfilter 和 Iptables 拦截和重写流量 Kubernetes 中的服务基于两个 Linux 内核组件：\n Netfilter Iptables  Netfilter 是一个框架，允许配置数据包过滤、创建 NAT或端口翻译规则，并管理网络中的流量。\n此外，它还屏蔽和阻止不请自来的连接访问服务。\n另一方面，Iptables 是一个用户空间程序，允许你配置 Linux 内核防火墙的 IP 数据包过滤器规则。\niptables 使用不同的 Netfilter 模块实现。\n你可以使用 iptables CLI 实时更改过滤规则，并将其插入 netfilters 的挂点。\n过滤器组织在不同的表中，其中包含处理网络流量数据包的链。\n每个协议都使用不同的内核模块和程序。\n 当提到 iptables 时，通常说的是 IPV4。对于 IPV6 的规则，CLI 是 ip6tables。\n Iptables 有五种类型的链，每种链都直接映射到 Netfilter 钩子。\n从 iptables 角度看是：\n PRE_ROUTING INPUT FORWARD OUTPUT POST_ROUTING  对应映射到 Netfilter 钩子：\n NF_IP_PRE_ROUTING NF_IP_LOCAL_IN NF_IP_FORWARD NF_IP_LOCAL_OUT NF_IP_POST_ROUTING  当数据包到达时，根据所处的阶段，会“出发” Netfilter 钩子，该钩子应用特定的 iptables 过滤。\n哎呀，看起来很复杂！\n不过不需要担心。\n这就是为什么我们使用 Kubernetes，上面的所有内容都是通过使用服务来抽象的，一个简单的 YAML 定义就可以自动完成这些规则的设置。\n如果对这些 iptables 规则感兴趣，可以登陆到节点并运行：\niptables-save 也可以使用可视化工具浏览节点上的 iptables 链。\n记住，可能会有数百条规则。想象下手动创建的难度。\n我们已经解释了相同和不同节点上的 pod 间如何通信。\n在 Pod-to-Service 中，通信的前半部分保持不变。\n当 Pod-A 发出请求时，希望到达 Pod-B（这种情况下，Pod-B 位与服务之后），转移的过程中会发生其他变化。\n原始请求从 Pod-A 命名空间中的 eth0 接口出来。\n从那里穿过 veth 对，到达根命名空间的以太网桥。\n一旦到达桥接器，数据包立即通过默认网关转发。\n与 Pod-to-Pod 部分一样，主机进行位比较，由于服务的 vIP 不是节点 CIDR 的一部分，数据包将立即通过默认网关转发出去。\n如果查找表中尚没有默认网关的 MAC 地址，则会进行相同的 ARP 解析。\n现在魔法发生了。\n在数据包经过节点的路由处理之前，Netfilter 钩子 NF_IP_PRE_ROUTING 被触发并应用一条 iptables 规则。规则进行了 DNAT 转换，重写了 POD-A 数据包的目标 IP 地址。\n原来服务 vIP 地址被重写称 POD-B 的IP 地址。\n从那里，路由就像 Pod-to-Pod 直接通信一样。\n然而，在所有这些通信之间，使用了第三个功能。\n这个功能被称为 conntrack，或连接跟踪。\nConntrack 将数据包与连接关联起来，并在 Pod-B 发送回响应时跟踪其来源。\nNAT 严重依赖 contrack 工作。\n如果没有连接跟踪，它将不知道将包含响应的数据包发送回哪里。\n使用 conntrack 时，数据包的返回路径可以轻松设置相同的源或目标 NAT 更改。\n另一半使用相反的顺序执行。\nPod-B 接收并处理了请求，现在将数据发送回 Pod-A。\n此时会发生什么？\n检查服务的响应 现在 Pod-B 发送响应，将其 IP 地址设置为源地址，Pod-A IP 地址设置为目标地址。\n  当数据包到达 Pod-A 所在节点的接口时，就会发生另一个 NAT\n  这次，使用 conntrack 更改源 IP 地址，iptables 规则执行 SNAT 将 Pod-B IP 地址替换为原始服务的 VIP 地址。\n  从 Pod-A 来看像是服务发回的响应，而不是 Pod-B。\n  其他部分都一样；一旦 SNAT 完成，数据包到达根命名空间中的以太网桥接器，并通过 veth 对转发到 Pod-A。\n回顾 让我们来总结下你在本文中学到的东西：\n 容器如何在本地或 pod 内通信。 当 pod 位于相同和不同的节点上时，Pod-to- Pod 如何通信。 Pod-to-Service - 当 pod 向 Kubernetes 服务背后的 pod 发送流量时。 Kubernetes 网络工具箱中有效通信所需的命名空间、veth、iptables、链、Netfilter、CNI、覆盖网络以及所有其他内容。  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-vlada-karpovich-4450071.jpg","permalink":"https://atbug.com/tracing-path-of-kubernetes-network-packets/","tags":["Container","Kubernetes"],"title":"追踪 Kubernetes 中的网络流量"},{"categories":["云原生"],"contents":"在《Kubernetes 的自动伸缩你用对了吗？》 一文中详细说明了如何使用 Kubernetes 的自动伸缩。在 Kubernetes 中弹性伸缩主要有三种：HPA、VPA、CA。本文不再详细说明，有兴趣的可以看那篇文章。这里主要来说下 Pod 水平缩放 HPA。\n随着 Kubernetes v1.23 的发布，HPA 的 API 来到了稳定版 autoscaling/v2：\n 基于自定义指标的伸缩 基于多项指标的伸缩 可配置的伸缩行为  从最初的 v1 版本 HPA 只支持 CPU、内存利用率的伸缩，到后来的自定义指标、聚合层 API 的支持，到了 v1.18 版本又加入了配置伸缩行为的支持，HPA 也越来越好用、可靠。\n依靠 CPU 或者内存指标的扩容并非使用所有系统，看起来也没那么可靠。对大部分的 web 后端系统来说，基于 RPS（每秒请求数）的弹性伸缩来处理突发的流量则会更加靠谱。\nPrometheus 也是当下流行开源监控系统，通过 Prometheus 可以获取到系统的实时流量负载指标，今天我们就来尝试下基于 Prometheus 的自定义指标进行弹性伸缩。\n注：目前 HPA 的缩容0 （scale to 0），则需要在 feature gate 打开 alpha 版本的 HPAScaleToZero 以及配置一个对象或者外部指标。即使是打开了，从 0 到 1 的扩容需要调度、IP 分配、镜像拉取等过程，存在一定的开销。如果降低这部分开销，这里先卖个关子，后续的文章进行补充。\n文章中使用的所有代码都可以从这里下载。\n整体架构 HPA 要获取 Prometheus 的指标数据，这里引入 Prometheus Adapter 组件。Prometheus Adapter 实现了 resource metrics、custom metrics 和 external metrics APIs API，支持 autoscaling/v2 的 HPA。\n获取到指标数据后，根据预定义的规则对工作负载的示例数进行调整。\n环境搭建 K3s 我们使用最新 1.23 版本的 K3s 作为 Kubernetes 环境。\nexport INSTALL_K3S_VERSION=v1.23.1+k3s2 curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --write-kubeconfig ~/.kube/config 示例应用 我们准备一个简单的 web 应用，可以记录请求次数并通过 /metrics 端点输出 Prometheus 格式的指标 http_requests_total。\nfunc main() { metrics := prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;http_requests_total\u0026#34;, Help: \u0026#34;Number of total http requests\u0026#34;, }, []string{\u0026#34;status\u0026#34;}, ) prometheus.MustRegister(metrics) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { path := r.URL.Path statusCode := 200 switch path { case \u0026#34;/metrics\u0026#34;: promhttp.Handler().ServeHTTP(w, r) default: w.WriteHeader(statusCode) w.Write([]byte(\u0026#34;Hello World!\u0026#34;)) } metrics.WithLabelValues(strconv.Itoa(statusCode)).Inc() }) http.ListenAndServe(\u0026#34;:3000\u0026#34;, nil) } 将应用部署到集群：\nkubectl apply -f kubernetes/sample-httpserver-deployment.yaml Prometheus 使用 Helm 安装 Prometheus，先添加 prometheus 的 chart 仓库：\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts 这里的测试只需要用到 prometheus-server，安装时禁用其他组件。同时为了演示效果的实效性，将指标的拉取间隔设置为 10s。\n# install prometheus with some components disabled # set scrape interval to 10s helm install prometheus prometheus-community/prometheus -n default --set alertmanager.enabled=false,pushgateway.enabled=false,nodeExporter.enabled=false,kubeStateMetrics.enabled=false,server.global.scrape_interval=10s 通过端口转发，可以在浏览器中访问 web 页面。\n# port forward kubectl port-forward svc/prometheus-server 9090:80 -n prometheus 这里查询 Pod 的 RPS 使用 sum(rate(http_requests_total[30s])) by (pod) 语句查询：\nPrometheus Adapter 同样使用 Helm 安装 Produmetheus Adapter，这里要进行额外的配置。\nhelm install prometheus-adapter prometheus-community/prometheus-adapter -n default -f kubernetes/values-adapter.yaml 除了要配置 Prometheus server 的访问方式外，还要配置自定义指标的计算规则，告诉 adapter 如何从 Prometheus 获取指标并计算出我们需要的指标：\nrules: default: false custom: - seriesQuery: '{__name__=~\u0026quot;^http_requests.*_total$\u0026quot;,container!=\u0026quot;POD\u0026quot;,namespace!=\u0026quot;\u0026quot;,pod!=\u0026quot;\u0026quot;}' resources: overrides: namespace: { resource: \u0026quot;namespace\u0026quot; } pod: { resource: \u0026quot;pod\u0026quot; } name: matches: \u0026quot;(.*)_total\u0026quot; as: \u0026quot;${1}_qps\u0026quot; metricsQuery: sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[30s])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) 可以参考详细的 Adapter 配置。\n待 promethues-adapter pod 成功运行后，可以执行 custom.metrics.k8s.io 请求：\nkubectl get --raw \u0026#39;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_qps\u0026#39; | jq . { \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_qps\u0026#34; }, \u0026#34;items\u0026#34;: [ { \u0026#34;describedObject\u0026#34;: { \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sample-httpserver-64c495844f-b58pl\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34; }, \u0026#34;metricName\u0026#34;: \u0026#34;http_requests_qps\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2022-01-18T03:32:51Z\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;100m\u0026#34;, \u0026#34;selector\u0026#34;: null } ] } 注意：这里的 value: 100m，值的后缀“m” 标识 milli-requests per seconds，所以这里的 100m 的意思是 0.1/s 每秒0.1 个请求。\nHPA 最后就是 HPA 的配置了：\n 最小最大的副本数分别设置 1、10 为了测试效果的实效性，设置扩缩容的行为 behavior 指定指标 http_requests_qps、类型 Pods 以及目标值 50000m：表示平均每个 pod 的 RPS 50 。比如以 300 的 RPS 访问，副本数就是 300/50=6 。  kind: HorizontalPodAutoscaler apiVersion: autoscaling/v2 metadata: name: sample-httpserver spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: sample-httpserver minReplicas: 1 maxReplicas: 10 behavior: scaleDown: stabilizationWindowSeconds: 30 policies: - type: Percent value: 100 periodSeconds: 15 scaleUp: stabilizationWindowSeconds: 0 policies: - type: Percent value: 100 periodSeconds: 15 metrics: - type: Pods pods: metric: name: http_requests_qps target: type: AverageValue averageValue: 50000m 测试 测试工具选用 vegeta，因为其可以指定 RPS。\n先为应用创建 NodePort service：\nkubectl expose deploy sample-httpserver --name sample-httpserver-host --type NodePort --target-port 3000 kubectl get svc sample-httpserver-host NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE sample-httpserver-host NodePort 10.43.66.206 \u0026lt;none\u0026gt; 3000:31617/TCP 12h 分别使用 240、120、40 的 RPS 发起请求：\n# 240 echo \u0026#34;GET http://192.168.1.92:31617\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 240 | vegeta report # 120 echo \u0026#34;GET http://192.168.1.92:31617\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 120 | vegeta report # 40 echo \u0026#34;GET http://192.168.1.92:31617\u0026#34; | vegeta attack -duration 60s -connections 10 -rate 40 | vegeta report 从 Prometheus 的 web 界面上观察请求量与示例数的变化：\nkubectl describe hpa sample-httpserver Warning: autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler Name: sample-httpserver Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; CreationTimestamp: Mon, 17 Jan 2022 23:18:46 +0800 Reference: Deployment/sample-httpserver Metrics: ( current / target ) \u0026#34;http_requests_qps\u0026#34; on pods: 100m / 50 Min replicas: 1 Max replicas: 10 Behavior: Scale Up: Stabilization Window: 0 seconds Select Policy: Max Policies: - Type: Percent Value: 100 Period: 15 seconds Scale Down: Stabilization Window: 30 seconds Select Policy: Max Policies: - Type: Percent Value: 100 Period: 15 seconds Deployment pods: 1 current / 1 desired Conditions: Type Status Reason Message ---- ------ ------ ------- AbleToScale True ReadyForNewScale recommended size matches current size ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests_qps ScalingLimited False DesiredWithinRange the desired count is within the acceptable range Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulRescale 25m horizontal-pod-autoscaler New size: 6; reason: pods metric http_requests_qps above target Normal SuccessfulRescale 19m horizontal-pod-autoscaler New size: 4; reason: All metrics below target Normal SuccessfulRescale 12m (x2 over 9h) horizontal-pod-autoscaler New size: 4; reason: pods metric http_requests_qps above target Normal SuccessfulRescale 11m horizontal-pod-autoscaler New size: 5; reason: pods metric http_requests_qps above target Normal SuccessfulRescale 9m40s (x2 over 12m) horizontal-pod-autoscaler New size: 2; reason: pods metric http_requests_qps above target Normal SuccessfulRescale 9m24s (x4 over 10h) horizontal-pod-autoscaler New size: 3; reason: pods metric http_requests_qps above target Normal SuccessfulRescale 7m54s (x3 over 9h) horizontal-pod-autoscaler New size: 2; reason: All metrics below target Normal SuccessfulRescale 7m39s (x4 over 9h) horizontal-pod-autoscaler New size: 1; reason: All metrics below target 总结 基于自定义指标比如每秒请求量进行应用的水平扩容相比 CPU/内存 作为依据更加靠谱，适用于大部分的 web 系统。在突发流量时可以进行快速扩容，通过对伸缩行为的控制，可以减少副本数的抖动。Promeheus 作为流行应用的监控系统，在 Adapter 和 Aggregate API 的支持下，可以作为伸缩的指标。\n目前 HPA 的 scale to 0 还在 alpha 的阶段，还需要关注副本从 0 到 N 的实效性。如果最小副本数大于0 ，对某些服务来说又会占用资源。接下来，我们会为尝试解决 0 到 N 的性能，以及资源占用的问题。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-milada-vigerova-5984860.jpg","permalink":"https://atbug.com/kubernetes-pod-autoscale-on-prometheus-metrics/","tags":["Kubernetes","Prometheus","云原生","DevOps"],"title":"Kubernetes HPA 基于 Prometheus 自定义指标的可控弹性伸缩"},{"categories":["翻译"],"contents":"本文翻译自 Vivian Hu 的 《eBPF and Wasm: Exploring the Future of the Service Mesh Data Plane》。\n 在 2021 年 12 月 2 日，Cilium 项目宣布了 Cilium Service Mesh 项目的测试版。在 2020 年 8 月 Google Cloud 宣布基于 eBPF 的 Google Kubernetes 服务（GKS）的数据平面 V2 的一年后，Cilium Service Mesh 带来了 “无边车服务网格”（sidecarless service mesh）的想法。它扩展了 Cilium eBPF 产品来处理服务网格中的大部分边车代理功能，包括 7 层路由和负载均衡、TLS 终止、访问策略、健康检查、日志和跟踪，以及内置的 Kubernetes Ingress。\nCilium 的创始人 Isovalent 在一篇名为 “How eBPF will solve Service Mesh - Goodbye Sidecars” 的文章中解释了 eBPF 如何替代边车代理。\n 它将我们从边车模型中解放处理，并允许我们将现有的代理技术集成到现有的内核命名空间概念中，使其成为日常使用的优雅容器抽象的一部分。\n 简而言之，eBPF 承诺会解决服务网格中的重要痛点 \u0026ndash; 当有许多细粒度微服务时的性能损耗。然而，使用 eBPF 替换边车代理这种新颖的想法，也存在着争议。\n服务网格中的数据平面是指管理数据流量如何路由和服务之间的流转的基础设施服务。目前，这是通过使用服务代理实现的。这种设计模式也通常被称为边车模式。边车允许其附加的微服务透明地向服务网格中的其他组件发送和接收请求。\n边车通常包含了 7 层代理，比如 Envoy、Linkerd 或者 MOSN。该代理处理流量的路由、负载均衡、健康检查、身份验证、授权、加密、日志、跟踪和统计数据收集。边车还可以包含一个基于 SDK 的应用程序框架（比如 Dapr）以提供网络代理以外的应用程序服务。此类服务的示例还包含了服务注册、服务发现、资源绑定、基于名称的服务调用、状态管理、actor 框架 和密钥存储。\n边车代理通常在 Kubernetes Pod 或者容器中运行。微服务应用也同样运行在容器内，并通过网络接口连接到边车。然而，这些容器化应用程序的一个重要问题就是资源消耗。边车服务随着微服务的数量几何级增长。当应用程序有数百个互联和负载均衡的微服务时，开销变得难以接受。服务网格代理商开始了性能上的竞争。正如 Infoq 之前报道的那样，Linkerd 将其 Go 写的代理用 Rust 重写了，并获得了显著的性能提升。\n并不奇怪，现有的服务网格提供商不相信 eBPF 是解决所有问题的圣杯。Solo.io 的 Idit Levine 等人针对 Cilium 的这篇公告撰写了一篇文章 “eBPF for Service Mesh? Yes, But Envoy Proxy is Here to Stay”。\n 在 Solo.io，我们认为 eBPF 是优化服务网格的很好的方式，并将 Envoy 代理视为数据平面的基石。\n Solo.io 作者提出的观点是：边车代理现在所做的不仅仅是简单的网络流量管理。当今的服务网格部署中有着复杂的需求，远超过 eBPF 支持的有限编程模型，其不符合图灵完备性且受到内核安全的诸多限制。Cilium eBPF 产品可以处理边车代理许多但并不是全部的任务。此外，Solo.io 的作者还指出，eBPF 的节点级代理与传统的 Pod 级边车代理相比缺乏灵活性，反而增加了整体开销。eBPF 缺陷在开发人员必须编写并部署到服务网格代理中的流量路由、负载均衡和授权的特定应用程序逻辑中体现得尤为明显。\nTerate.io 的开发人员在回应名为 The Debate in the Community about Istio and Service Mesh 的 Cilium 公告中也提出了类似的观点。他们指出，当前边车代理的性能是合理的，开源社区也已经有了进一步提高性能的方法。与此同时，开发人员很难在 eBPF 等新颖但图灵不完整的技术中构建应用程序特定的数据平面逻辑。\n Istio 架构稳定且生产就绪，生态系统正在发展期。\n eBPF 的许多问题与其是一种内核技术分不开，必定收到安全限制。有没有一种方法可以在不使用空间技术降低性能的情况下将复杂的应用程序特定的代理逻辑集成到数据平面中？事实证明，WebAssembly（Wasm）可能会是个选择。Wasm 运行时可以以近似原生性能安全地隔离和执行用户空间代码。\nEnvoy Proxy 率先使用 Wasm 作为扩展机制对数据平面的编程。开发人员可以用C、C++、Rust、AssemblyScript、Swift 和 TinyGO 等语言编写应用程序特定的代理逻辑，并将模块编译为 Wasm。通过proxy-Wasm 标准，代理可以在例如 Wasmtime 和 WasmEdge 的高性能运行时执行这些 Wasm 插件。目前，Envoy 代理、Istio 代理、MOSN 和 OpenResty 支持 proxy-Wasm。\n此外，Wasm 可以充当通用应用程序容器。它在服务网格数据平面上的应用不仅限于边车代理。附加到边车的微服务也可以运行在轻量级 Wasm 运行时中。WasmEdge WebAssembly 运行时是一个安全、轻量级、快速、便携式和多语言运行时，Kubernetes 可以直接将其作为容器进行管理。到 2012 年 12 月，WasmEdge 贡献者社区验证了基于 WasEdge 的微服务可以与 Dapr 和 Linkerd 边车一同工作，作为带有 guest 操作系统和完整软件对战的重量级 Linux 容器的替代品。与 Linux 容器应用程序相比，WebAssembly 微服务消耗了 1% 的资源，冷启动时间也只用了 1%。\neBPF 和 Wasm 是服务网格应用的新方向，以便在数据平面上实现高性能。他们仍然是新兴技术，但可能会成为微服务生态系统 Linux 容器的替代品或者补充。\n关于作者：\nVivian Hu：Vivian 是亚洲的开源爱好者和开发人员倡导者，Second State 的产品经理。她非常关心通过更好的工具、文档和教程来改善开发人员体验和生产力。Vivian 在 WebAssembly.today 上为 WebAssembly、Rust 和无服务器编写每周时事通讯。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-mikhail-nilov-7709020.jpg","permalink":"https://atbug.com/ebpf-wasm-service-mesh/","tags":["Service Mesh","eBPF"],"title":"eBPF 和 Wasm：探索服务网格数据平面的未来"},{"categories":["笔记"],"contents":"自从用上 m1 的电脑，本地开发环境偶尔会遇到兼容性的问题。比如之前尝试用 Colima 在虚拟机中运行容器运行时和 Kubernetes，其实际使用的还是 aarch64 虚拟机，实际使用还是会有些差异。\n手上有台之前用的黑苹果小主机，吃灰几个月了，实属浪费。正好元旦假期，有时间折腾一下。\nCPU: Intel 8700 6C12T MEM: 64G DDR4 DISK: 1T SSD 折腾的目的：\n 将平台虚拟化 提供多套实验环境 快速创建销毁实验环境 体验基础设施即代码 IaaS  主要用到的工具：\n 虚拟化工具 Proxmox VE Terraform：开源的基础设施即代码工具 terraform-provider-proxmox：Terraform Proxmox Provider，通过 Proxmox VE 的 REST API 在创建虚拟机。  安装 Proxmox 虚拟化工具 从官网 下载 ISO 镜像，写入到 U 盘中。macOS上推荐使用 balenaEtcher 写盘。\n电脑上插入 U 盘并从 U 盘启动，按照步骤一步步完成设置。\n官方的 wiki 安装步骤很详细。\n安装完成之后就可以创建虚拟机了，可以用命令行 qm create 或者 https://localhost:8006 Web UI来创建。\n这样毕竟还是有点麻烦，每次都要执行很多操作。虽说可以编写脚本，但是通用型不够好。因此我们选择 Terraform，实现基础设施即代码。\n创建 Ubuntu Cloud-Init Template 这里选用 Cloud-Init 的方式，从 cloud-init template 来克隆虚拟机。cloud-init 的虚拟机可以完成一些高级定制的初始化工作，有兴趣的参考 Cloud Init 文档。\n登陆到 Proxmox VE 宿主机，使用 Ubuntu 20.04 cloud init image 来创建模板，从官网下载：\nwget https://cloud-images.ubuntu.com/releases/focal/release/ubuntu-20.04-server-cloudimg-amd64.img 执行下面的命令创建一个虚拟机：\nqm create 9000 --name \u0026#34;ubuntu-2004-cloudinit-template\u0026#34; --memory 1024 --cores 1 --net0 virtio,bridge=vmbr0 qm importdisk 9000 ubuntu-20.04-server-cloudimg-amd64.img local-lvm qm set 9000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9000-disk-0 qm set 9000 --boot c --bootdisk scsi0 qm set 9000 --ide2 local-lvm:cloudinit qm set 9000 --serial0 socket --vga serial0 qm set 9000 --agent enabled=1 将刚创建好的虚拟机转换成模板：\nqm template 9000 模板与普通的虚拟机会有些许的不同，使用模板我们可以快速创建虚拟机。这里我们不会用 UI来创建。\n创建 Proxmox 用户及 API Token 使用 Proxmox VE 的 REST API 需要权限校验，有用户名密码或者 API Token 两种方式。我们选用后者，登陆到 Proxmox 宿主机，执行如下命令创建角色、用户以及 API Token：\npveum role add TerraformProv -privs \u0026#34;VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Audit\u0026#34; pveum user add terraform-prov@pve pveum aclmod / -user terraform-prov@pve -role TerraformProv pveum user token add terraform-prov@pve terraform-token --privsep=0 ┌──────────────┬──────────────────────────────────────┐ │ key │ value │ ╞══════════════╪══════════════════════════════════════╡ │ full-tokenid │ terraform-prov@pve!terraform-token │ ├──────────────┼──────────────────────────────────────┤ │ info │ {\u0026#34;privsep\u0026#34;:\u0026#34;0\u0026#34;} │ ├──────────────┼──────────────────────────────────────┤ │ value │ 9748c040-a283-4c72-a48b-9ce784778eed │ └──────────────┴──────────────────────────────────────┘ 这里我们会用到 token 的full-tokenid 和 value。\nTerraform 有了 token 和 cloud-init 模板后，就是定义虚拟机了。\n安装最新版本的 terraform。\nbrew install terraform 在空目录中创建 ubuntu.tf 文件，并按步骤进行配置：\n配置要使用的 provider： terraform { required_providers { proxmox = { source = \u0026#34;telmate/proxmox\u0026#34; } } } 配置 provider 需要提供 pm_api_url、pm_api_token_id 和 pm_api_token_secret：\nprovider \u0026#34;proxmox\u0026#34; { pm_tls_insecure = true pm_api_url = \u0026#34;https://192.168.1.4:8006/api2/json\u0026#34; pm_api_token_id = \u0026#34;terraform-prov@pve!terraform-token\u0026#34; pm_api_token_secret = \u0026#34;9748c040-a283-4c72-a48b-9ce784778eed\u0026#34; } 配置虚拟机资源 可以参考provider 的配置说明：\nresource \u0026#34;proxmox_vm_qemu\u0026#34; \u0026#34;proxmox-ubuntu\u0026#34; { count = 1 name = \u0026#34;ubuntu-${count.index + 1}\u0026#34; desc = \u0026#34;Ubuntu develop environment\u0026#34; # 节点名 target_node = \u0026#34;pve\u0026#34; # cloud-init template clone = \u0026#34;ubuntu-2004-cloudinit-template\u0026#34; # 关机 guest agent agent = 0 os_type = \u0026#34;ubuntu\u0026#34; onboot = true # CPU cores = 4 sockets = 1 cpu = \u0026#34;host\u0026#34; # 内存 memory = 16384 scsihw = \u0026#34;virtio-scsi-pci\u0026#34; bootdisk = \u0026#34;scsi0\u0026#34; # 硬盘设置，因计算的方式 101580M 代替 100G disk { slot = 0 size = \u0026#34;101580M\u0026#34; type = \u0026#34;scsi\u0026#34; storage = \u0026#34;local-lvm\u0026#34; iothread = 1 } # 网络 network { model = \u0026#34;virtio\u0026#34; bridge = \u0026#34;vmbr0\u0026#34; } lifecycle { ignore_changes = [ network, ] } # 记住这里要使用IP CIDR。因为只创建一个虚拟机，虚拟机的 IP 是 192.168.1.91。如果要创建多个虚拟机的话，IP 将会是 .91、.92、.93 。 ipconfig0 = \u0026#34;ip=192.168.1.9${count.index + 1}/24,gw=192.168.1.2\u0026#34; # 用户名和 SSH key ciuser = \u0026#34;addo\u0026#34; sshkeys = \u0026lt;\u0026lt;EOF  SSH KEYS HERE EOF } 创建虚拟机 第一次需要先执行 init 命令进行初始化：\nterraform init 可以使用 terraform fmt 和 terraform validate 对配置文件进行格式化和校验。\n然后执行 terraform apply 并输入 yes 开始创建虚拟机，\nproxmox_vm_qemu.proxmox-ubuntu[0]: Creating... proxmox_vm_qemu.proxmox-ubuntu[0]: Still creating... [10s elapsed] proxmox_vm_qemu.proxmox-ubuntu[0]: Still creating... [20s elapsed] proxmox_vm_qemu.proxmox-ubuntu[0]: Still creating... [30s elapsed] proxmox_vm_qemu.proxmox-ubuntu[0]: Still creating... [40s elapsed] proxmox_vm_qemu.proxmox-ubuntu[0]: Creation complete after 42s [id=pve/qemu/100] 这样虚拟机就创建成功了，使用前面配置的私钥和 IP 地址就可以 ssh 到虚拟机中。\n销毁虚拟机 虚拟机的销毁也很简单，执行 terraform destory 并输入 yes 即可。\n总结 有了 Terraform 和 Proxmox VE 后，就可以愉快的使用干净的实验环境了。但是干净到一些开发中常用软件都没有，使用起来也不方便。\n后续考虑通过 cloud-init 来对虚拟机进行高级定制，比如容器环境和 K3s 等等。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-tatiana-syrikova-3933258.jpg","permalink":"https://atbug.com/deploy-vm-on-proxmox-with-terraform/","tags":["IaaS","Tool"],"title":"快速搭建实验环境：使用 Terraform 部署 Proxmox 虚拟机"},{"categories":["生活"],"contents":"没错，这是一篇 2021 年的盘点。\n2021 年马上结束，翻看幕布，看到了年初给自己定下的目标。记性不好，好像把这个事情给忘记了。\n工作 工作这么多年，已然跨过的 35 岁这个坎。没有想象中的困难，唯有一路的坚持。\n职业生涯 自己一直坚持走技术路线，不愿走上管理这条路。就像有些事情，不是不会也不是不能，而是不愿。可能是自己性格使然，也唯有技术一条路可走，所以只有坚持，尤其是自己也喜欢搞搞技术。\n过去的自己埋头写代码、学习，鲜有与外界的沟通。正好借着去年工作的事情走出去，参与社区。见识到了技术人的简单直接，也结交到了志同道合的朋友。\n正职 年初太太跟我说，本命年少折腾，工作稳定点就行，不求其他。但还是在年中的时候换了工作，感谢上家公司的各种经历让自己成长很多，与志同道合的人一起做些有趣的事情。\n有幸加入到现在的公司，认识到一群更简单的人：简单的上司、简单的同事，简单地做着不简单的事情。短短几个月，自己学习了很多。这也正是我想要的：有机会不断地学习和挑战。\n现在的公司是远程办公的模式，自己掌控工作的时间，与家人相处的时间也多了（虽然很多时间也是在工作）。刚开始还会有些新鲜，慢慢地工作和生活的界限越来越模糊。不过既然意识到问题，可以慢慢的调整。\n学习 书看的不多，仅仅几本。但收益良多，比如《微习惯》、《深度工作》、《飞轮效应》还是比较推荐。准备有时间重读《深度工作》，配合现在的远程工作感觉会更适合实施。\n技术方面则是开拓知识面，关注一些技术博客、网站。尤其是 3 月底开始决定好好更新公众号。其实之前断断续续有在写博客，平时工作也都记笔记的习惯。本来想的好，将笔记好好整理一下发在公众号上。\n但实际上手才会发现，有时写一篇文章不管是翻译，还是技术探索类的都会用上几个小时的时间。比如画个复杂点的图，也好耗费几十分钟一个小时。几个小时的成块时间，对于工作的人来说太过宝贵。\n到今天应该有发过 50 多篇（其中 8、9 月份刚接触新工作，写的确实少），也算是没有打脸，如今也有了1000 多的关注。虽然写了这么多，但是回头看下，有点繁杂，不够系统。\n还有顺手完成了 CKA 的考试。\n生活 我将生活放到了最后，不是说不重视生活，而是希望生活平平淡淡就好。\n家人在工作和生活上总是给予我莫大的支持，感谢他们的付出。\n电子产品虽然也有在玩，但是玩的少了。基本就是买买买、卖卖卖的节奏，没有太多的时间投入在上面。\n也因为换工作的间隙，才有了北疆之行，有幸领略了北疆的风光。希望有机会再走伊昭和独库。\n临近年底，也下定决心了结一件心事，半年后再看。\n总结 想想过去一年发生的种种，有事与愿违、有意料之外、还有柳暗花明。\n这让我想起阿甘正传里那一段：人生就像一盒各式各样的巧克力，你永远不知道下一块将会是哪种。\n人生最难得可以做自己喜欢的事情，追求自己喜欢的事物。虽力有未逮，但仍可为之而努力。\n“君之蜜糖，吾之砒霜”，选择自己喜欢就好。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/farewell-2021/","tags":["生活"],"title":"再见，2021"},{"categories":["笔记"],"contents":"Colima 是一个以最小化设置来在MacOS上运行容器运行时和 Kubernetes 的工具。支持 m1（文末讨论），同样也支持 Linux。\nColima 的名字取自 Container on Lima。Lima 是一个虚拟机工具，可以实现自动的文件共享、端口转发以及 containerd。\nColima 实际上是通过 Lima 启动了名为 colima 的虚拟机，使用虚拟机中的 containerd 作为容器运行时。\n使用 Colima 的使用很简单，执行下面的命令就可以创建虚拟机，默认是 Docker 的运行时。\n初次运行需要下载虚拟机镜像创建虚拟机，耗时因网络情况有所差异。之后，启动虚拟机就只需要 30s 左右的时间。\ncolima start INFO[0000] starting colima INFO[0000] creating and starting ... context=vm INFO[0119] provisioning ... context=docker INFO[0119] provisioning in VM ... context=docker INFO[0133] restarting VM to complete setup ... context=docker INFO[0133] stopping ... context=vm INFO[0136] starting ... context=vm INFO[0158] starting ... context=docker INFO[0159] done 此时，在宿主机上就可以使用 Docker 相关的命令了：\ndocker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES docker pull busybox docker images REPOSITORY TAG IMAGE ID CREATED SIZE busybox latest b34806a1af7a 2 weeks ago 1.41MB 也可以使用 Lima 的命令行 limact工具查看虚拟机的情况：\nlimactl list NAME STATUS SSH ARCH CPUS MEMORY DISK DIR colima Running 127.0.0.1:64505 aarch64 2 2GiB 60GiB /Users/addo/.lima/colima 查看操作系统信息：\nuname -a Darwin Addos-Macbook-Pro.local 21.2.0 Darwin Kernel Version 21.2.0: Sun Nov 28 20:28:41 PST 2021; root:xnu-8019.61.5~1/RELEASE_ARM64_T6000 arm64 limactl shell colima uname -a Linux lima-colima 5.13.0-22-generic #22-Ubuntu SMP Fri Nov 5 13:22:27 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux 或者使用 Colima 的 ssh 命令进入虚拟机：\n# on host colima ssh # in vm uname -a Linux lima-colima 5.13.0-22-generic #22-Ubuntu SMP Fri Nov 5 13:22:27 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux 其他运行时 也可以在创建的时候通过 --runtime containerd 参数指定使用 Containerd 作为运行时。此时就需要使用 colima nerdctl 来使用 nerdctl 与 Containerd 进行交互。\ncolima start --runtime containerd 同样，还可以创建一个 k3s 作为 Kubernetes 运行时：\ncolima start --with-kubernetes Demo 我们尝试启动一个 nginx 容器：\ndocker run --rm -d --name nginx -p 8080:80 nginx:latest docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20d6c56e038b nginx:latest \u0026#34;/docker-entrypoint.…\u0026#34; 9 seconds ago Up 8 seconds 0.0.0.0:8080-\u0026gt;80/tcp, :::8080-\u0026gt;80/tcp nginx Colima 会自动配置端口转发：\ncurl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.21.4 Date: Sun, 26 Dec 2021 04:17:22 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 02 Nov 2021 14:49:22 GMT Connection: keep-alive ETag: \u0026#34;61814ff2-267\u0026#34; Accept-Ranges: bytes 虚拟机配置 Colima 启动的虚拟机默认是 2CPU、2GiB 内存 和 60GiB 存储。可以在创建时通过 --cpu 、--memory 和 --disk 来分配更多资源。\ncolima start --cpu 4 --memory 16 也可以修改当前虚拟机的配置：\ncolima stop colima start --cpu 4 --memory 16 同类工具比较 其实有不少类似的工具，比如 kind、k3d 和 minikube 三种都是用来创建 Kubernetes 环境。我个人此前用的 k3d 就比较多。\n对于 Docker 容器环境，这三个其实都没有提供。minikube 的虚拟机中也有容器运行时，但是无法单纯安装 Docker 环境。\n对于 Kubernetes 环境来说，这几种都适合，相比 Colima 来说还支持创建多个集群（当前 Colima 最新版本是 0.2.2，多集群的支持也在开发中。估计 0.3.0 会提供，毕竟创建多个虚拟机就能实现）。但使用 Colima 的话，Kubernetes 和 Docker 可以共享镜像（本地镜像）和运行时。\n不足 多集群的支持 前面提到，目前还不支持创建多个 Kubernetes 集群，估计 0.3.0 会提供。\nm1 的支持 这里还是要说下 m1，我现在主要用 m1 的电脑，本地的容器运行时用的 Docker Desktop。\n前面我们有留意到虚拟机使用的是 aarch64 架构系统，对于某些不支持 arm64 的镜像还是无法运行。毕竟 Lima 是原生支持 m1，而不是使用 Rosetta 转译的 Docker Desktop。\n有兴趣的同学可以尝试用 Rosetta 转译 Lima。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/containers-runtime-on-macos-with-colima/","tags":["Kubernetes","Docker","macOS"],"title":"Colima：MacOS 上的极简容器运行时和 Kubernetes（支持 m1）"},{"categories":["翻译"],"contents":"译者注： 本文篇幅较长，有助于了解 FaaS 和 OpenFaaS。作者分别从开发人员和运维人员的视角来了解 OpenFaaS，对了解新的技术是个很好的方式。\n本文翻译自 Ivan Velichko 的 OpenFaaS - Run Containerized Functions On Your Own Terms。\n 长期以来，无服务器（serverless） 对我来说无非就是 AWS Lambda 的代名词。Lambda 提供了一种方便的途径，可以将任意代码附加到平台事件（云实例的状态变更、DynamoDB 记录的更新或新的 SNS 消息）中。但是，我时不时会想到某个逻辑，但其又没大到足以有自己的服务，同时有不适合任何现有服务的范围。因此，我经常将其放入函数中，以便日后使用 CLI 命令或者 HTTP 调用来调用它。\n几年前，我来开了 AWS，自那以后，我一直怀念部署无服务器功能的便利性。因此，当我得知 OpenFaaS 项目时惊喜万分。它将在 Kubernetes 集群上部署函数变得简单，甚至仅需要 Containerd 就可以部署到虚拟机上。\n有兴趣？那么继续！\n无服务器与 FaaS 无服务器 已成为一个流行词，目前其实际含义扔不够清晰。\n许多现代平台被视为 无服务器 平台。在 AWS Fargate 或 GCP Cloud Run 上部署容器化服务？无服务器！在 Heroku 上运行应用程序？也可能是无服务器的。\n同时，我更喜欢将 FaaS 视为一种具体的设计模式。按照 FaaS 范式，可以部署代码片段（响应某些外部时间执行的函数）。这些函数与事件驱动程序中的回调类似，但是是运行在其他人的的服务器上。由于操作的是函数而不是服务器，顾名思义 FaaS 是无服务器的。\nsource\nOpenFaaS 项目旨在将 Kubernetes 集群或者独立的虚拟机等低级基础设施转化为管理无服务器函数的高级平台。\n站在开发人员的角度，这样一个平台看起来是真的无服务器的 \u0026ndash; 你只需要知道特定的 CLI/UI/API 来处理 函数 抽象。但站在运维的角度，需要了解 OpenFaaS 如何使用 服务器 来运行这些函数。\n就我而言，我经常既是开发又是运维，下面我将尝试从二者展开说明。然而，我认为在评估 UX 时，我们应该明确区分它们。\n开发人员眼中的 OpenFaaS OpenFaaS 创建于 2016 年，现在网上也有大量的教程。这里不会重复介绍，但可以通过以下链接了解：\n How to deploy OpenFaaS Create Functions Build Functions Writing a Node.js function - step-by-step guide  相反，我将描述我所理解的 OpenFaaS。我希望有助于一些需要评估该技术是否解决其问题的人，以及那些希望更有效地使用该技术的人。\n函数运行时 在进入正式编码之前，有必要了解下其未来的执行环境（又名运行时）。或者，简单说：\n 如何启动函数 如何组织 I/O 操作 如何重置 / 终止函数 如何隔离函数和调用  OpenFaaS 自带多个运行时模式，这些模式针对不同的场景定制。因此，不同的场景下上述问题的答案会略有不同。\nOpenFaaS 函数在容器中运行，并且每个容器必须遵守简单的约定 ：它作为监听在预设端口（默认为 8080）上的 HTTP 服务器，临时存储并且是无状态的。\n然而，OpenFaaS 通过 函数 watchdog（译者注：watchdog 不做翻译）模式避免了用户编写此类服务器。函数 watchdog 是一种轻量级 HTTP 服务器，可以感知如何执行实际函数业务逻辑。因此，安装在容器中的所有内容加上作为入口点的 watchdog，就构成了函数的运行时环境。\n经典 watchdog 从最简单的开始，或者又是被称为经典 watchdog：\n这种模式下，watchdog 启动了监听在 8080 端口的轻量级 HTTP 服务器，每个进来的请求都会：\n 读取请求头和请求体 fork 或者 exec 包含实际函数的可执行文件 将请求头和请求体写入到函数进程的 stdin 等待函数进程的退出（或者超市） 读取函数进程的 stdout 和 stderr 在 HTTP 响应中将去读的字节发送回调用方  上述逻辑类似于传统的 通用网关接口（CGI）。一方面，每次函数调用都启动单独的进程看起来不够高效，而另一方面，它确实超级方便，因为 任何使用 stdio 流进行 I/O 处理的程序（包括最喜欢的 CLI 工具）都可以部署为 OpenFaaS 函数。\n提起隔离，我们有必要区分下函数和调用：\n OpenFaaS 中的不同函数始终分布在不同的容器中 一个函数可以有一个或多个容器 —— 取决于缩放选项 同一函数的独立调用可能会最终进入同一个容器 同一函数的独立调用将始终使用不同的进程进行  反向代理 watchdog 注意：使用 OpenFaaS 官方术语，本节讨论在 HTTP 模式下运行的 of-watchdog。但我个人认为称之为反向代理 watchdog 更加形象。\n如果 经典 运行时类似于 CGI，那么这个运行时模式类似于后来的 FastCGI。运行时希望在 watchdog 后面有一个长期运行的 HTTP 服务器，而不是每次函数调用时创建新的进程。这本质上是 将 watchdog 组件变成反向代理：\n当容器启动时，反向代理 watchdog 也会创建一个监听在 8080 端口的轻量级 HTTP 服务器。然而，与 经典 watchdog 不同的是反向代理watchdog 只创建一次函数的进程，并将其当成（长期运行的）上游服务器。然后，函数调用转变成到该上游的 HTTP 请求。\n然而，反向代理模式并不为了取代经典模式。经典模式的强项在于其函数的编写非常简单。这也是没有 HTTP 服务器的代码的唯一选择。比如使用 Cobol、bash 或者 PowerShell 脚本等等编写的函数。\n何时该使用反向代理运行时模式：\n 函数需要在两次调用之间保持状态：  缓存 持久连接（例如，保持从函数到数据库的连接打开） 有状态函数 🥴   每个函数启动一个进程可能开销很大，为每个调用带来了延迟 你想运行一个（微）服务作为函数 🤔   根据 OpenFaaS 的创建者 Alex Ellis 的解释，FaaS，特别是 OpenFaaS，可以被视为在不依赖服务器抽象的情况下 部署微服务的简化方式。即 FaaS 是无服务器架构的规范示例。\n 因此，使用反向代理的方式，函数可以被看作是部署微服务的固执的方式。方便、快速、简单。但使用有状态函数时，要留意由于多个调用可能在同一个进程中结束而导致的警告：\n 在一个进程中结束的并发调用可能会触发代码中的竞争条件（例如，一个带有全局变量的 Go 函数，而全局变量没有锁的保护）。 在一个进程中结束的后续调用可能会导致交叉调用数据泄露（当然，就像传统微服务一样）。 由于该进程在两次调用之间被复用，因此代码中的任何内存泄漏都不会被缓解。  其他运行时模式 经典运行时模式在将函数结果发送回调用方之前缓冲了函数的整个响应。但如果响应的大小超出了容器的内存怎么办？OpenFaaS 提供了响应流另一种运行时模式，该模式仍然为每个调用创建进程，但添加了。\n另一个又去的场景是从函数中提供静态文件服务。OpenFaaS 也有解决方案。\n这可能是所有的内置运行时模式。但如果仍未满足需求，OpenFaaS 是一个开源项目！看下现有的watchdog（1 \u0026amp; 2），简洁明了。因此，可以随时提交 PR 或者 issue，让整个社区因你的贡献收益。\n编写函数 此时，我们已经了解函数如何在配备了函数 watchdog 的容器中运行。那么最小的函数是什么样子的？\n下面的示例将简单的 shell 脚本封装到 OpenFaaS 函数中：\n######################################################### WARNING: Not for Production - No Security Hardening! ########################################################## This FROM is just to get the watchdog executable.FROMghcr.io/openfaas/classic-watchdog:0.2.0 as watchdog# FROM this line the actual runtime definion starts.FROMalpine:latest# Mandatory step - put the watchdog.COPY --from=watchdog /fwatchdog /usr/bin/fwatchdog# Optionally - install extra packages, libs, tools, etc.# Function\u0026#39;s payload - script echoing its STDIN, a bit transformed.RUN echo \u0026#39;#!/bin/sh\u0026#39; \u0026gt; /echo.shRUN echo \u0026#39;cat | rev | tr \u0026#34;[:lower:]\u0026#34; \u0026#34;[:upper:]\u0026#34;\u0026#39; \u0026gt;\u0026gt; /echo.shRUN chmod +x /echo.sh# Point the watchdog to the actual thingy to run.ENV fprocess=\u0026#34;/echo.sh\u0026#34;# Start the watchdog server.CMD [\u0026#34;fwatchdog\u0026#34;]当构建、部署和调用时，上面的函数作为 回显服务器，倒转并大写其输入。\n稍微高级点的例子：一个 Node.js Hello World 脚本作为函数：\n######################################################### WARNING: Not for Production - No Security Hardening! #########################################################FROMghcr.io/openfaas/classic-watchdog:0.2.0 as watchdogFROMnode:17-alpineCOPY --from=watchdog /fwatchdog /usr/bin/fwatchdogRUN echo \u0026#39;console.log(\u0026#34;Hello World!\u0026#34;)\u0026#39; \u0026gt; index.jsENV fprocess=\u0026#34;node index.js\u0026#34;CMD [\u0026#34;fwatchdog\u0026#34;]因此，要编写一个简单的函数，只需要在 Dockerfile 中加入：\n 实际脚本（或可执行文件） 它的所有依赖项——软件包、操作系统库等 首选的 watchdog  然后将 watchdog 指向该脚本（或可执行文件），并将 watchdog 作为入口。有点酷，因为：\n 可以完全控制函数未来的运行时 可以部署任何可以在容器中作为函数运行的东西  但上述方法有个明显的缺点 \u0026ndash; 一个生产就绪的 Dockerfile 可能有上百行。如果我只想运行一个简单的 Node.js/Python 脚本或者一个小的 Go 程序作为函数，要怎么处理 Dockerfile？就不能有一个占位符来粘贴代码片段？\n功能模板 OpenFaaS 的美妙之处在于，我们可以两者兼而有之 —— 使用 Dockerfile 进行低级修补或目标语言编写高级脚本！得益于丰富的功能模板库！\n$ faas-cli template store list NAME SOURCE DESCRIPTION csharp openfaas Classic C# template dockerfile openfaas Classic Dockerfile template go openfaas Classic Golang template java8 openfaas Java 8 template ... node14 openfaas HTTP-based Node 14 template node12 openfaas HTTP-based Node 12 template node openfaas Classic NodeJS 8 template php7 openfaas Classic PHP 7 template python openfaas Classic Python 2.7 template python3 openfaas Classic Python 3.6 template ... python3-flask openfaas Python 3.7 Flask template python3-http openfaas Python 3.7 with Flask and HTTP ... golang-http openfaas Golang HTTP template ... 上述功能模板由 OpenFaaS 作者和社区精心只做。典型的模板附带一个复杂的 Dockerfile，指向虚拟处理程序函数。当引导新函数时，通过 faas-cli new 命令来使用这些模板。例如：\n$ faas-cli new --lang python my-fn Folder: my-fn created. Function created in folder: my-fn Stack file written: my-fn.yml $ cat my-fn/handler.py def handle(req): \u0026#34;\u0026#34;\u0026#34; PUT YOUR BUSINESS LOGIC HERE \u0026#34;\u0026#34;\u0026#34; return req 因此，对于模板，编写函数的工作可以归结为简单地将业务逻辑放入响应的处理程序文件中。\n使用模板时，了解使用那种 watchdog 和 模式 很重要：\n 使用经典的类似 CGI 的 watchdog，处理程序通常被编写为接受和返回纯字符串的函数（例如：python3、php7） 在 HTTP 模式下，使用 of-watchdog时，处理程序看起来更像 HTTP 处理程序接受请求并返回响应结构（例如：python3-http，node17）。  函数商店 你的最佳函数是什么？对，你不需要写。OpenFaaS 接受这种想法，并带来了函数商店（经过社区测试并根据过往经验选择的 OpenFaaS 函数精选索引）。\n该商店包含一些有趣的函数，可以一键部署到现有的 OpenFaaS 中：\n$ faas-cli store list FUNCTION DESCRIPTION NodeInfo Get info about the machine that you\u0026#39;r... alpine An Alpine Linux shell, set the \u0026#34;fproc... env Print the environment variables prese... sleep Simulate a 2s duration or pass an X-S... shasum Generate a shasum for the given input Figlet Generate ASCII logos with the figlet CLI curl Use curl for network diagnostics, pas... SentimentAnalysis Python function provides a rating on ... hey HTTP load generator, ApacheBench (ab)... nslookup Query the nameserver for the IP addre... SSL/TLS cert info Returns SSL/TLS certificate informati... Colorization Turn black and white photos to color ... Inception This is a forked version of the work ... Have I Been Pwned The Have I Been Pwned function lets y... Face Detection with Pigo Detect faces in images using the Pigo... Tesseract OCR This function brings OCR - Optical Ch... Dockerhub Stats Golang function gives the count of re... QR Code Generator - Go QR Code generator using Go Nmap Security Scanner Tool for network discovery and securi... ASCII Cows Generate a random ASCII cow YouTube Video Downloader Download YouTube videos as a function OpenFaaS Text-to-Speech Generate an MP3 of text using Google\u0026#39;... Docker Image Manifest Query Query an image on the Docker Hub for ... face-detect with OpenCV Detect faces in images. Send a URL as... Face blur by Endre Simo Blur out faces detected in JPEGs. Inv... Left-Pad left-pad on OpenFaaS normalisecolor Automatically fix white-balance in ph... mememachine Turn any image into a meme. Business Strategy Generator Generates a Business Strategy (using ... Image EXIF Reader Reads EXIF information from an URL or... Open NSFW Model Score images for NSFW (nudity) content. Identicon Generator Create an identicon from a provided s... 这些函数实际上是存储在 Docker Hub 或者 Quay 等公共库的容器镜像，可以自由复用。\n场景示例：\n 使用 env 函数调试函数接收的HTTP标头 使用 curl 函数从 OpenFaaS 部署内部测试连接 从运行多个副本的函数中使用 hey 来增加负载  函数的构建和部署 由于函数是在容器中运行的，因此需要有人为这些容器构建镜像。无论你喜不喜欢，这都是开发人员的事情。OpenFaaS 提供了方便的 faas-cli build 命令，但没有服务器端构建。因此，要么需要（在安装 Docker 的机器上）手动运行 faas-cli build，要么使用 CI/CD 完成。\n接下来，构建好的镜像需要通过 faas-cli push 到仓库。显然，这种仓库也应该可以从 OpenFaaS 服务器端访问。否则，使 用faas-cli deploy 部署函数时会失败。\n开发人员的工作流程如下：\n调用函数 函数部署后，可以通过向 $API_HOST:$API_PORT/function/\u0026lt;fn-name\u0026gt; 端点发送 GET、POST、PUT 或者 DELET HTTP 请求来调用它。常见的调用方式有：\n 各种钩子（webhook） faas-cli invoke event connectors！  前两个选项相当简单。使用函数作为作为 webhook 处理器（GitHub、IFTTT 等）很方便，每个函数开发人员都已经安装了 faas-cli，因此可以成为日常脚本编写的组成部分。\n那什么是事件连接器？ 在本文开头是我对 AWS Lambda 与 AWS 平台事件紧密集成的温暖回忆。请记住，可以在响应新 SQS/SNS 消息、新的 Kinesis 记录、EC2 实例生命周期等事件时调用 Lambda。OpenFaaS 函数是否存在类似的东西呢？\n显然，OpenFaaS 无法开箱即用地与任何生态系统集成。然而，它提供了一种名为事件连接器模式的通用解决方案。\n官方支持的连接器：\n Cron connector MQTT connector NATS connector Kafka connector (需要专业版订阅)  OpenFaaS 还提供了很小的连接器-sdk库来简化连接器的开发。\n运维眼中的 OpenFaaS 开发眼中的 OpenFaaS 是个黑盒，提供简单的 API 来部署和调用函数。然而，作为运维可能会从了解 一点 OpenFaaS 内部原理中受益。\nOpenFaaS 通用架构 OpenFaaS 有一个简单但强大的架构，允许使用不同的基础设施作为后端。如果已经有了 Kubernetes 集群，可以通过在上面部署 OpenFaaS 轻松将其变成 FaaS 解决方案。但是如果旧的虚拟（或物理）机，仍然可以在上面安装 OpenFaaS，并获得差不多功能的更小的 FaaS 解决方案。\n上面的架构中唯一面向用户的组件是 API 网关。OpenFaaS 的 API 网关：\n 暴露 API 来管理和调用函数 提供内置的 UI 来管理函数 处理函数的自动缩放 预计后面会有兼容的 OpenFaaS 提供商  因此，当开发人员运行 faas-cli deploy、faas-cli list 或使用 curl $API_URL/function/foobar 调用函数等内容时，请求将发送到上述的 API 网关。\n上图中的另一个重要组成部分是 faas-provider。它不是一个具体的组件，而更像是接口。任何实现（非常简洁）的提供商 API 的软件都可以成为提供商。OpenFaaS 提供商：\n 管理功能（部署、列表、缩放、删除） 调用函数 暴露一些系统信息  两个最注明的提供商是 faas-netes（Kubernetes 上的 OpenFaaS）和 faasd（Containerd 上的 OpenFaaS）。下面，将介绍他们的实现。\nKubernetes 上的 OpenFaaS（faas-nets） 当部署在 Kubernetes 上时，OpenFaaS 利用了该平台开箱即用的强大原语。\n关键要点：\n API 网关成为标准（部署+服务）对。因此，可以随心所欲地扩展它。也可以随心所欲地把它暴露出来 每个函数也成为（部署+服务）对。可能不会直接处理函数，但对于 faas-netes，缩放变得就像调整相应的副本数一样简单 高可用性和开箱即用的水平缩放 - 同一功能的 pod 可以（而且应该）跨多个集群节点运行。 Kubernetes 作为一个数据库工作；例如，当运行 faas-cli list 等命令来获取当前部署的函数列表时，faas-netes 只会将其转换为相应的 Kubernetes API 查询  Containerd 上的 OpenFaaS（faasd） 对于没有使用 Kubernetes 集群的人来说，OpenFaaS 提供了名为 faasd 的替代轻量级提供商。它可以安装在（虚拟或物理）服务器上，，并利用 containerd 来管理容器。正如我之前写的那样，容器是一个在 Docker 和 Kubernetes 下使用的较低级别的容器管理器。结合 CNI 插件，它成为编写容器调度器的构建组件，OpenFaaS 的 faasd 是个很好的讲究案例：\n关键要点：\n 被设计为在 IoT 设备上或 VM 中运行 使用 containerd 的原生 pause （通过cgroup freezers）和超快速函数冷启动快速扩展到零 它可以在每台服务器上运行比 faas-netes 多十倍的函数，并且可以有效地使用更便宜的硬件，包括树莓派 containerd 和 faasd 作为 systemd 服务进行管理，因此会自动处理日志、重启等 没有 Kubernetes DNS，但 faasd 确保 DNS 在函数之间共享以简化互操作 containerd 扮演着数据库的角色（比如 faas-cli list 变成了类似 ctr container list的操作 ），所以如果服务器挂了，所有状态就会丢失，每个函数都需要重新部署 没有开箱即用的高可用性或水平扩展（参见 issue/225）  总结 对你所使用的软件有个好的心智模型是是有益的，它可以提高开发效率，防止单例场景的发生，并简化了故障排查。\n以 OpenFaaS 为例，区分开发人员和运维人员对系统的看法可能是个很好的思路。从开发人员角度来看，这是一个简单而强大的无服务器解决方案，主要关注 FaaS 场景。该解决方案由一个用于管理和调用函数的简洁 API、一个涵盖开发人员工作流的命令行工具以及一个函数模板库组成。无服务器函数以不同的运行时模式（类 CGI、反向代理）在容器中运行，并提供不同的隔离和状态保障。\n从运维人员的角度来看，OpenFaaS 是一个具有灵活架构的模块化系统，可以部署在不同类型的基础设施之上：从树莓派到裸机或虚拟机、以及成熟的 Kubernetes、OpenShift 或 Docker Swarm 集群。当然，每种选择都有其优缺点，需要详细评估取舍。但即使现有选项都不合适，简单的 faas-provider 抽象允许开发自己的后端来运行无服务器功能。\n上述内容主要集中在 OpenFaaS 基础知识上。但是 OpenFaaS 也有一些高级功能。通过以下链接进一步了解：\n 使用 NATS 消息传递系统的异步函数调用 使用 Prometheus 和 AlertManager 自动缩放功能 函数调用限制 使用 docker-compose 通过 faasd 运行有状态的工作负载  资源  OpenFaaS 官方文档 一堆清晰的 OpenFaaS 用例 使用 OpenFaaS 将任何 CLI 转换为函数 faasd 介绍、动机、主要用例 📖面向其他人的无服务器- 尽管有通用名称，但它是 faasd 的一个非常详细的指南   作者介绍： Ivan Velichko Software Engineer at heart, SRE at day job, Tech Storyteller at night.\n ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/openfaas-case-study-zh/","tags":["Serverless","Kubernetes","Container","云原生"],"title":"OpenFaaS - 以自己的方式运行容器化函数"},{"categories":["云原生"],"contents":"距离上个版本 用 Pipy 实现 OPA，已经过去快半年了。当初使用Pipy 实现了可信镜像仓库的检查，那时的版本实现起来会稍微复杂，从策略仓库到证书创建到Admission Webhook 的创建都需要大量的人工操作，配置和逻辑也还是耦合在一起。\n这个版本安装和使用起来会更加简单。\n当初我用“不务正业”来形容 Pipy 实现准入控制，等看完这篇文章，欢迎留言说说你的看法。\n架构 还是继续上次的场景，在 Pod 创建时对 Pod 使用的镜像所在仓库进行检查，以及检查镜像的 tag 是否合法。\n这里借助 Pipy Repo 的能力，将代表策略的脚本和配置交由 Repo 进行管理；Pipy 实例实时从 Pipy Repo 同步策略，并进行动态加载。\n同时 Pipy Repo 对外提供 REST API 来管理策略，对策略的修改更容易。也方便与企业现有管理后台进行对接。\n下面就开始部署验证，这里所使用的所有代码都已提交到GitHub 仓库：https://github.com/flomesh-io/demo-policy-as-code。\n运行 git clone https://github.com/flomesh-io/demo-policy-as-code.git cd demo-policy-as-code 准备 环境 使用 Kubernetes 发行版 K3s 作为集群环境，集群的搭建不做过多说明。我用 k3d：\nk3d cluster create policy-as-code -p \u0026#34;6060:30060@server:0\u0026#34; 注：K3d 是在容器中运行 K3s，这里做了将容器的 30060 端口映射到本地的 6060 端口，后面会详细解释。\n部署策略服务器 执行下面的命令部署策略服务器 Repo：\nkubectl apply -f repo/pipy-repo.yaml 确保 Pod 正常运行：\nkubectl get po -n pipy NAME READY STATUS RESTARTS AGE pipy-repo-697bbd9f4b-94pld 1/1 Running 0 10s 发布策略 要使用的策略（脚本和配置）位于 ./repo/scripts 目录中。前面提到 Repo 提供了 REST API 来管理 codebase（策略）。\n这里提供了脚本 init-codebase.sh，通过 curl 命令将策略发布到策略服务器。\n./init-codebase.sh 部署策略引擎 这个版本中，使用 helm chart 完成证书的创建、服务的部署以及 Admission WebHook 的注册。\nhelm install policy-as-code ./policy-as-code -n default 确保 Pod 正常运行：\nkubectl get po -n pipy NAME READY STATUS RESTARTS AGE pipy-repo-697bbd9f4b-94pld 1/1 Running 0 2m policy-as-code-5867f9cdb9-9vwks 1/1 Running 0 8s 测试 在 ./test 目录中有三个 yaml 文件用于测试。\n非法的镜像仓库：\nkubectl apply -f test/bad.yaml Error from server (192.168.64.1:5000/hello-world:linux repo not start with any repo [docker.io, k8s.gcr.io]): error when creating \u0026#34;test/bad.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.pipy.flomesh-io.cn\u0026#34; denied the request: 192.168.64.1:5000/hello-world:linux repo not start with any repo [docker.io, k8s.gcr.io] 非法的镜像 tag：\nkubectl apply -f test/bad2.yaml Error from server (docker.io/library/hello-world:latest tag end with :latest): error when creating \u0026#34;test/bad2.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.pipy.flomesh-io.cn\u0026#34; denied the request: docker.io/library/hello-world:latest tag end with :latest 合法的镜像\nkubectl apply -f test/ok.yaml pod/hello-world-success created 就这么结束了？当然没有，我们还要对策略进行动态的调整。\n继续下面的测试之前，执行 kubectl delete -f test/ok.yaml 清理刚才创建的 Pod。\n修改策略 修改 ./repo/scripts/config.json文件，清空 invalidTagSuffixes 数组中的内容。\n{ \u0026#34;validRepoPrefixes\u0026#34;: [ \u0026#34;docker.io\u0026#34;, \u0026#34;k8s.gcr.io\u0026#34; ], \u0026#34;invalidTagSuffixes\u0026#34;: [] } **注意：**这里需要执行脚本 ./init-codebase.sh 更新策略。\n此时，再次尝试 apply test/bad2.yaml。你会发现，这次 Pod 创建成功了。\nkubectl apply -f test/bad2.yaml pod/hello-world-bad-tag created 我们没有修改任何逻辑代码，仅仅修改了配置就完成了对 Pod 镜像检查逻辑的调整。可能有人会问，命令行太麻烦调试不方便，有没有更直观的方式？\n答案是：有！\n图形用户界面 Pipy Repo 提供了图形用户界面，方便脚本的开发和调试。详细信息可以参考快速入门 Pipy Repo，了解图形用户界面的使用。\n还记得开头的地方我们为 K3d 容器做了端口映射：6060=\u0026gt;30060，细心的你也可能发现我们为 Pipy Repo 创建了 NodePort Servce，node port 端口为 30060。\n此时，本地启动一个 Pipy：\n#k3d pipy http://localhost:6060 --admin-port=6061 #主机直接部署 k3s 请使用这条命令 pipy http://localhost:30060 --admin-port=6061 在浏览器中打开 http://localhost:6060，你会看到：\n点击下面我们创建的 codebase /image-verify，在左侧文件目录中可以找到我们修改后的 config.json:\n在编辑器中编辑，改回原来的配置，然后点击 2 和 3 两个按钮\n再次测试 #清理 kubectl delete -f test/bad2.yaml kubectl apply -f test/bad2.yaml Error from server (docker.io/library/hello-world:latest tag end with :latest): error when creating \u0026#34;test/bad2.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.pipy.flomesh-io.cn\u0026#34; denied the request: docker.io/library/hello-world:latest tag end with :latest 可以看到，修改的结果体现在错误信息里了。\n总结 Open Policy Agent (OPA) 为策略引擎带来了新的天地，但是使用 Rego 语言编写策略为使用带来了门槛。Pipy 以其流量编程、易扩展的特性结合 Repo 的 codebase 管理，可以轻松实现策略即代码。同时，资源消耗更少，性能更高（代理场景的特点）。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/policy-as-code-with-pipy/","tags":["Kubernetes","OPA","Pipy","云原生"],"title":"策略即代码：为了 OpenPolicyAgent 学 Rego？试试 Javascript"},{"categories":["云原生"],"contents":"随着 IT 技术的发展，AI、区块链和大数据等技术提升了对应用毫秒级扩展的需求，开发人员也面临着的功能快速推出的压力。混合云是新常态，数字化转型是保持竞争力的必要条件，虚拟化成为这些挑战的基本技术。\n在虚拟化的世界，有两个词耳熟能详：虚拟机和容器。前者是对硬件的虚拟化，后者则更像是操作系统的虚拟化。两者都提供了沙箱的能力：虚拟机通过硬件级抽象提供，而容器则使用公共内核提供进程级的隔离。有很多人将容器看成是“轻量化的虚拟机”，通常情况下我们认为容器是安全的，那到底是不是跟我们想象的一样？\n容器：轻量化的虚拟机？ 容器是打包、共享和部署应用的现代化方式，帮助企业实现快速、标准、灵活地完成服务交互。容器化是建立在 Linux 的命名空间（namespace）和控制组（cgroup） 的设计之上。\n命名空间创建一个几乎隔离的用户空间，并为应用提供专用的系统资源，如文件系统、网络堆栈、进程ID和用户ID。随着用户命名空间的引入，内核版本 3.8 提供了对容器功能的支持：Mount（mnt）、进程 ID（pid）、Network（net）、进程间通信（ipc）、UTS、用户 ID（user）6 个命名空间（如今已达 8 个，后续加入了 cgroup 和 time 命名空间）。\ncgroup 则实施对应用的资源限制、优先级、记账和控制。cgroup可以控制 CPU、内存、设备和网络等资源。\n同时使用 namespace 和 cgroup 使得我们可以在一台主机上安全地运行多个应用，并且每个应用都位于隔离的环境中。\n虚拟机提供更强大的隔离 虽然容器很棒，足够轻量级。但通过上面的描述，同一个主机上的多个容器其实是共享同一个操作系统内核，只是做到了操作系统级的虚拟化。虽然命名空间提供了高度的隔离，但仍然有容器可以访问的资源，这些资源并没有提供命名空间。这些资源是主机上所有容器共有的，比如内核 Keyring、/proc、系统时间、内核模块、硬件。\n我们都知道没有 100% 安全的软件，容器化的应用也一样，从应用源码到依赖库到容器 base 镜像，甚至容器引擎本身都可能存在安全漏洞。发生容器逃逸的风险远高于虚拟机，黑客可以利用这些逃逸漏洞，操作容器的外部资源也就是宿主机上的资源。除了漏洞，有时使用的不当也会带来安全风险，比如为容器分配了过高的权限（CAP_SYS_ADMIN 功能、特权权限），都可能导致容器逃逸。\n而虚拟机依靠硬件级的虚拟化，实现的硬件隔离比命名空间隔离提供了更强大的安全边界。与容器相比，虚拟机提供了更高程度的隔离，只因其有自己的内核。\n由此可见，容器并不是真正的“沙盒”，也并不是轻量化的虚拟机。有没有可能为容器增加一个更安全的边界，尽可能的与主机操作系统隔离，做到类似虚拟机的强隔离，使其成为真正的“沙盒”？\n沙盒化容器 答案是有，就是沙盒容器。这种容器就像虚拟机一样有自己的内核，这层内核成为用户空间内核。这层内核要保持容器的轻量级，使用现代编程技术编写，本身非常轻，仅用于作为容器和主机之间的强隔离层。\n并且还要支持 OCI 和 CRI 规范，可以与 Docker 和 Kubernetes 等容器工具很好的集成。\n这里简单介绍下 gVisor 和 Kata Containers。\ngVisor gVisor 是使用 Go 编写的应用内核，实现了 Linux 操作系统的大部分接口。其包含了一个叫做 runsc 的 OCI 运行时，提供了应用和宿主机内核间的隔离层。runsc 也实现了与 Docker 和 Kubernetes 的集成，可以很容易的运行沙盒容器。\ngVisor 为每个容器提供了独立的操作系统内核。应用与 gVisor 内核提供的虚拟环境进行交互，不是直接访问宿主机的内核。gVisor 还限制和管理文件和网络操作，确保容器化应用和主机操作系统之间有两个隔离层。通过减少和限制应用与主机内核的交互，尽可能减小攻击者绕过容器隔离机制的攻击面。\n与大部分内核不同，gVisor 不需要固定的物理资源；相反，其利用现有的主机内核功能，并作为一个正常进程运行。换句话说，gVisor 以 Linux 的方式实现了 Linux。\ngVisor 沙盒由多个进程组成，这些进程共同构成了可以运行一个或多个容器的环境。\n每个沙盒都有其独立的实例：\n Sentry：运行容器的内核，拦截并响应应用的系统调用。  沙盒中的每个容器都有其独立的实例：\n Gofer：提供容器文件系统的访问。  Kata Containers Kata Containers 与容器一样轻量级且快，并与容器管理层集成\u0026ndash; 包括 Docker 和 Kubernetes 等流行的容器编排工具 \u0026ndash; 同时还提供了与虚拟机一样的安全。\nKata Containers 与 OCI、容器运行时接口（CRI）和容器网络接口（CNI）完全集成。它支持各种类型的网络模型（例如，passthrough、MacVTap、桥接、tc 镜像）和可配置的访客内核，以便需要特殊网络模型或内核版本的应用都可以在上面运行。上图显示了 Kata VM 中的容器如何与现有编排平台交互。\nKata 在主机上有一个 kata 运行时来启动和配置新容器。对于 Kata VM 中的每个容器，主机上都有一个相应的 Kata Shim。Kata Shim 接收来自客户端（例如 docker 或 kubectl）的 API 请求，并通过 VSock 将请求转发给 Kata VM 内的代理。Kata 容器进一步进行了几项优化，以减少 VM 启动时间。\nKata Containers 由两个开源项目合并而来：Intel 的 Clear containers 和 Hyper runV。前者注重性能（引导时间小于 100ms）和安全；而后者通过支持不同的 CPU 架构和管理系统，将技术无关放在首位。Kata Containers 可以说集二者之大成。\n与传统的容器相比，Kata Container 做到了虚拟机的隔离，集虚拟机的安全性和容器的性能于一身。\n总结 与普通容器相比，沙盒容器提供了更强的隔离性，这种强隔离提供了更高的安全性。同时这类容器技术支持 OCI 和 CRI 规范，可以与现有的容器工具以及 Kubernetes 很好的集成。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/sandboxed-container/","tags":["Docker","Container"],"title":"沙盒化容器：是容器还是虚拟机"},{"categories":["笔记"],"contents":"此文是前段时间笔记的整理，之前自己对这方面的关注不够，因此做下记录。\n 有太多的文章介绍如何运行容器，然而如何停止容器的文章相对少很多。\n根据运行的应用类型，应用的停止过程非常重要。如果应用要写文件，停止前要保证正确刷新数据并关闭文件；如果是 HTTP 服务，要确保停止前处理所有未完成的请求。\n信号 信号是 Linux 内核与进程以及进程间通信的一种方式。针对每个信号进程都有个默认的动作，不过进程可以通过定义信号处理程序来覆盖默认的动作，除了 SIGSTOP 和 SIGKILL。二者都不能被捕获或重写，前者用来将进程暂停在当前状态，而后者则是从内核层面立即杀掉进程。\n有两个比较重要的进程 SIGTERM 和 SIGKILL。SIGTERM 是优雅地关闭命令，SIGKILL 则是暴力的关闭命令。比如 Docker，容器会先收到 SIGTERM 信号，10s 后会收到 SIGKILL 信号。\n还有很多其他的信号，只是限定于特定的上下文。\n中断 硬件的中断就像操作系统的信号。通常发生在硬件想要向操作系统注册事件时。操作系统必须立即停止运行，并处理中断。\n比较常见的中断例子就是键盘中断，比如按下 ctrl+z 或者 ctrl+c。Linux 将其分别转换成 SIGTSTP 和 SIGINT。硬件中断过去通常用来处理键盘和鼠标输入，但如今被用作操作系统软件驱动层面的信号轮训。\nDocker 前面说了这么多终于来到 Docker，容器的独特之处在于通常只运行一个进程。即使是单进程，容器内 PID 为 1 的进程也具有 init 系统的特殊规则和职责。\nPID 1 在 Linux 中非常重要，通常是 init 进程。通常进程在收到 SIGTERM 信号后，假如不对信号进程处理，会快速退出。但 PID 1 的进程收到 SIGTERM 之后假如不对信号进行处理则什么都不会做。\n容器内 PID 1 通常有两种情况： shell 进程 PID 为 1 和你的进程 PID 为 1。分别对应着 shell 和 exec 格式的命令。\nshell 格式 Dockerfile 有个特点，就是如果不使用 JSON 格式 来指定容器命令，会通过 shell 以 fork 的形式来执行命令，也就是 /bin/sh -c。\n docker run（宿主机上）  /bin/sh -c（PID 1，容器内）  /loop.sh （PID 2，容器内）      这种格式的命令特点是不会向业务进程发送信号。比如发送给 shell 的 SIGTERM 信号不会转发给子进程，而是等待子进程的退出。唯一杀死容器的方式就是发送 SIGKILL 信号，或者碰巧子进程自己崩溃。\n所以应该尽量避免使用这种方式，\nexec 格式 这个就是 Dockerfile 的推荐语法了，你的进程会立即启动并作为容器的初始化进程，然后就有了下面的进程树：\n docker run（宿主机上）  /loop.sh（PID 1，容器内）    说了这么多，很多人觉得不够直观。我们会用示例应用来进行说明，但在这之前简单说下如何发送信号来停止容器。\n发送信号 有几种方式来停止容器。\ndocker stop 默认情况下 docker stop 命令会向容器发送 SIGTERM 信号，然后等待 10s，如果容器没停止再发送 SIGKILL 信号。\n在 Dockerfile 中，可以通过 STOPSIGNAL 指令来设置默认的退出信号，比如 STOPSIGNAL SIGKILL 将退出信号设置为 SIGKILL。或者在 docker run 是通过 --stop-signal 参数来覆盖镜像中的 STOPSIGNAL 设置。\ndocker kill 默认情况下 docker kill 会直接杀死容器，不给容器任何机会进行优雅停止，这里发出的就是 SIGKILL 信号。\n当然 docker kill 可以通过 --signal 来指定要发送的信号，类似 Linux 的 kill 命令：\ndocker kill ----signal=SIGTERM foo docker rm -f 通常情况下 docker rm 用来删除已经停止的容器，但是加上 --force（简写 -f）会强制删除正在运行的容器。同样，也不会给容器任何优化停止的机会。\n信号处理 我们使用一个简单的应用对 shell 和 exec 两种格式做下对比。在这个应用中，对 SIGTERM 进行处理：收到信号后退出。\n#!/usr/bin/env sh trap \u0026#39;exit 0\u0026#39; SIGTERM while true; do :; done 接下来我们使用两种不同格式的 CMD 来构建镜像。\nshell 格式 使用下面的 Dockerfile 来构建镜像 term。\nFROM alpine:3.15.0 COPY loop.sh / CMD /loop.sh 执行下面的命令构建镜像、启动容器、停止容器。\ndocker build -t term . docker run --name term -d term docker stop term 此时你会发现容器并没有立刻停止，而是大约 10s 之后才被停止。可以通过命令查看容器的退出状态：\ndocker inspect -f \u0026#39;{{.State.ExitCode}}\u0026#39; term 137 137 = 128 + 9 说明容器的退出信号是 SIGKILL。\nexec 格式 调整下 Dockerfile，将 CMD 修改为推荐的 JSON 格式：\nFROM ubuntu:trusty COPY loop.sh / CMD [\u0026quot;/loop.sh\u0026quot;] 执行下面的命令构建镜像、启动容器、停止容器。（需要先执行 docker rm term 删除之前停止的容器）\ndocker build -t term . docker run --name term -d term docker stop term 此时容器会立刻退出。查看容器的退出状态：\ndocker inspect -f \u0026#39;{{.State.ExitCode}}\u0026#39; term 0 总结 docker rm -f 和 docker kill 干掉容器很容器，但是为了实现容器的优雅退出，应该使用 docker stop 命令，同时 Dockerfile 中应尽量避免使用 shell 格式设置 ENTRYPONT 或者 CMD。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/gracefully-stopping-docker-containers-with-correct-command/","tags":["Docker"],"title":"从 Docker 的信号机制看容器的优雅停止"},{"categories":["笔记"],"contents":"将应用部署到 Kubernetes 时通常会使用 Deployment、Service、Ingress，整个应用从部署到正常运行，经历的流程很长。从 kubectl apply YAML 文件，经过 apiserver、controller manager、scheduler、kubelet、以及 CRI、CNI 等众多组件的协同工作。\n漫长的“行程”，Pod 也经历各种正常和不正常的状态变化，即使正常运行也会出现服务无法访问的问题。对于刚开始在 Kubernetes 平台开展工作的同学来说，故障的排查确实棘手。之前工作的时候，经常要协助排查各种问题。去年在 Learnk8s 上看到了关于 Deployment 故障排查的视图，我还参考做了当时整个平台的故障排查视图，包括了从项目源码、CICD 流水线、部署整个流程的故障排查参考。\n现在 Learnk8s 的 Deployment 排查指南更新了，也有了中文版本。\n 年中翻译 Learnk8s 的文章《Kubernetes 的自动伸缩你用对了吗？》 时，与 Daniele Polencic 沟通时被问及是否能翻译故障排查的可视化指南。\n年中的时候就翻译完了，今天电报上被告知文章 A visual guide on troubleshooting Kubernetes deployments已更新，排查视图较上一版有了部分的调整。\n原文：https://learnk8s.io/troubleshooting-deployments\n中文版PDF：https://learnk8s.io/a/a-visual-guide-on-troubleshooting-kubernetes-deployments/troubleshooting-kubernetes.zh_cn.v2.pdf\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/troubleshooting-kubernetes-deployment-zh-v2/","tags":["Kubernetes","云原生"],"title":"Kubernetes Deployment 的故障排查可视化指南（2021 中文版）"},{"categories":["笔记"],"contents":"最近换上了 MacBook Pro 2021，也慢慢将工作转到新的电脑上。结束了一年多的黑白配，之前工作主力机是我的黑苹果，配置以及 OpenCore 的引导放在这里了。\n为了稳定性，系统一直停留在了 10.15.4。新的电脑拿到手就是 12.0.1，之前也就在我另一台 2016 款的 macbook pro 上用过几个周的 12.0.0。\n新的系统加上新的架构，不免有有些 bug，今天就说下我所遇到的两个比较棘手的。\n1. 安全相关  EXC_BAD_ACCESS (SIGKILL (Code Signature Invalid))\n 公司的核心产品是用 c++ 开发的，编译之后我都习惯性的放到 /usr/local/bin 目录中。在新系统中，就出现了上面的错误（从控制台获取），运行的时候进程直接被 kill。\n网上查了下，说与 kernel cache 有关，重启可解决。\n This was caused by kernel caching of previously signed binaries and my replacing those binaries with newly compiled binaries which weren\u0026rsquo;t part of a signed package.\n  By deleting the existing binaries, rebooting the Mac to clear the kernel cache, and then recopying the new binaries into place, I sorted the issue.\n 但是每次都要重启有点麻烦，通过与旧电脑中目录对比，最后将二进制文件直接复制到 /opt/homebrew/bin 解决，这个目录的 ownership 不是 root 的。\n如果问题还会出现，可以做一个软链接将二进制文件链接到 /opt/homebrew/bin 下。\n2. Clang 版本 这个问题也是排查了版本，还是使用公司产品的时候遇到的。当处理的 JSON 中包含数组，比如 {\u0026quot;a\u0026quot;: []}，应用启动失败。从控制台来看，错误码是 EXC_BAD_ACCESS (SIGSEGV)，报错的堆栈正好是在处理 JSON array。\n有问题的版本：\nApple clang version 13.0.0 (clang-1300.0.29.3) Target: arm64-apple-darwin21.1.0 Thread model: posix InstalledDir: /Library/Developer/CommandLineTools/usr/bin 正常的版本：\nApple clang version 12.0.0 (clang-1200.0.32.29) Target: arm64-apple-darwin20.1.0 Thread model: posix InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin 最后在构建的时候，将 CMAKE_BUILD_TYPE 设置为 Debug 临时解决。Debug 模式下编译器不做任何优化。\n期待系统升级能够修复。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/bug-with-m1-pro-and-monterey/","tags":["macOS"],"title":"Monterey 12.0.1 上的 bug"},{"categories":["笔记"],"contents":"TL;DR 本文内容：\n 介绍 distroless 镜像、作用以及简单的使用 如何针对 distroless 容器的进行调试 临时容器(v.1.18+)的使用  Distroless 镜像 Distroless 容器，顾名思义使用 Distroless 镜像作为基础镜像运行的容器。\n \u0026ldquo;Distroless\u0026rdquo; 镜像只包含了你的应用程序以及其运行时所需要的依赖。不包含你能在标准 Linxu 发行版里的可以找到的包管理器、shells 或者其他程序。\n GoogleContainerTools/distroless 针对不同语言提供了 distroless 镜像：\n gcr.io/distroless/static-debian11 gcr.io/distroless/base-debian11 gcr.io/distroless/java-debian11 gcr.io/distroless/cc-debian11 gcr.io/distroless/nodejs-debian11 gcr.io/distroless/python3-debian11  Distroless 镜像有什么用？ 那些可能是构建镜像时需要的，但大部分并不是运行时需要的。这也是为什么上篇文章介绍 Buildpacks 时说的一个 builder 的 stack 镜像包含构建时基础镜像和运行时基础镜像，这样可以做到镜像的最小化。\n其实控制体积并不是 distroless 镜像的主要作用。将运行时容器中的内容限制为应用程序所需的依赖，此外不应该安装任何东西。这种方式可能极大的提升容器的安全性，也是 distroless 镜像的最重要作用。\n这里并不会再深入探究 distroless 镜像，而是如何调试 distroless 容器\n没有了包管理器，镜像构建完成后就不能再使用类似 apt、yum 的包管理工具；没有了 shell，容器运行后无法再进入容器。\n“就像一个没有任何门的房间，也无法安装门。” Distroless 镜像在提升容器安全性的同时，也为调试增加了难度。\n使用 distroless 镜像 写个很简单的 golang 应用：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func defaultHandler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello world!\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, defaultHandler) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } 比如使用 gcr.io/distroless/base-debian11 作为 golang 应用的基础镜像：\nFROMgolang:1.12 as build-envWORKDIR/go/src/appCOPY . /go/src/appRUN go get -d -v ./...RUN go build -o /go/bin/appFROMgcr.io/distroless/base-debian11COPY --from=build-env /go/bin/app /CMD [\u0026#34;/app\u0026#34;]使用镜像创建 deployment\n$ kubectl create deploy golang-distroless --image addozhang/golang-distroless-example:latest $ kubectl get po NAME READY STATUS RESTARTS AGE golang-distroless-784bb4875-srmmr 1/1 Running 0 3m2s 尝试进入容器：\n$ kubectl exec -it golang-distroless-784bb4875-srmmr -- sh error: Internal error occurred: error executing command in container: failed to exec in container: failed to start exec \u0026#34;b76e800eafa85d39f909f39fcee4a4ba9fc2f37d5f674aa6620690b8e2939203\u0026#34;: OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \u0026#34;sh\u0026#34;: executable file not found in $PATH: unknown 如何调试 Distroless 容器 1. 使用 distroless debug 镜像 GoogleContainerTools 为每个 distroless 镜像都提供了 debug tag，适合在开发阶段进行调试。如何使用？替换容器的 base 镜像：\nFROMgolang:1.12 as build-envWORKDIR/go/src/appCOPY . /go/src/appRUN go get -d -v ./...RUN go build -o /go/bin/appFROMgcr.io/distroless/base-debian11:debug # use debug tag hereCOPY --from=build-env /go/bin/app /CMD [\u0026#34;/app\u0026#34;]重新构建镜像并部署，得益于debug镜像中提供了 busybox shell 让我们可以 exec 到容器中。\n2. debug 容器与共享进程命名空间 同一个 pod 中可以运行多个容器，通过设置 pod.spec.shareProcessNamespace 为 true，来让同一个 Pod 中的多容器共享同一个进程命名空间。\n Share a single process namespace between all of the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set. Optional: Default to false.\n 添加一个使用 ubuntu 镜像的 debug 容器，这里为了测试（后面解释）我们为原容器添加 securityContext.runAsUser: 1000，模拟两个容器使用不同的 UID 运行：\napiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: golang-distroless name: golang-distroless spec: replicas: 1 selector: matchLabels: app: golang-distroless strategy: {} template: metadata: creationTimestamp: null labels: app: golang-distroless spec: shareProcessNamespace: true containers: - image: addozhang/golang-distroless-example:latest name: golang-distroless-example securityContext: runAsUser: 1000 resources: {} - image: ubuntu name: debug args: [\u0026#39;sleep\u0026#39;, \u0026#39;1d\u0026#39;] securityContext: capabilities: add: - SYS_PTRACE resources: {} status: {} 更新 deployment 之后：\n$ kubectl get po NAME READY STATUS RESTARTS AGE golang-distroless-85c4896c45-rkjwn 2/2 Running 0 3m12s $ kubectl get po -o json | jq -r \u0026#39;.items[].spec.containers[].name\u0026#39; golang-distroless-example debug 然后通过 debug 容器来进入到 pod 中：\n$ kubectl exec -it golang-distroless-85c4896c45-rkjwn -c debug -- sh 然后在容器中执行：\n$ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 14:54 ? 00:00:00 /pause # infra 容器 1000 7 0 0 14:54 ? 00:00:00 /app # 原容器，UID 为 1000 root 19 0 0 14:55 ? 00:00:00 sleep 1d # debug 容器 root 25 0 0 14:55 pts/0 00:00:00 sh root 32 25 0 14:55 pts/0 00:00:00 ps -ef 尝试访问 进程 7 的进程空间：\n$ cat /proc/7/environ $ cat: /proc/7/environ: Permission denied 我们需要为 debug 容器加上：\nsecurityContext: capabilities: add: - SYS_PTRACE 之后再访问就正常了：\n$ cat /proc/7/environ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=golang-distroless-58b6c5f455-v9zkvSSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crtKUBERNETES_PORT_443_TCP=tcp://10.43.0.1:443KUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP_ADDR=10.43.0.1KUBERNETES_SERVICE_HOST=10.43.0.1KUBERNETES_SERVICE_PORT=443KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_PORT=tcp://10.43.0.1:443HOME=/root 同样我们也可以访问进程的文件系统：\n$ cd /proc/7/root $ ls app bin boot dev etc home lib lib64 proc root run sbin sys tmp usr var 无需修改容器的基础镜像，使用 pod.spec.shareProcessNamespace: true 配合安全配置中增加 SYS_PTRACE 特性，为 debug 容器赋予完整的 shell 访问来调试应用。但是修改 YAML 和安全配置只适合在测试环境使用，到了生产环境这些都是不允许的。\n我们就需要用到 kubectl debug 了。\n3. Kubectl debug 针对不同的资源 kubectl debug 可以进行不同操作：\n 负载：创建一个正在运行的 Pod 的拷贝，并可以修改部分属性。比如在拷贝中使用新版本的tag。 负载：为运行中的 Pod 增加一个临时容器（下面介绍），使用临时容器中的工具调试，无需重启 Pod。 节点：在节点上创建一个 Pod 运行在节点的 host 命名空间，可以访问节点的文件系统。  3.1 临时容器 从 Kubernetes 1.18 之后开始，可以使用 kubectl 为运行的 pod 添加一个临时容器。这个命令还处于 alpha 阶段，因此需要在“feature gate”中打开。\n在使用 k3d 创建 k3s 集群时，打开 EphemeralContainers feature：\n$ k3d cluster create test --k3s-arg \u0026#34;--kube-apiserver-arg=feature-gates=EphemeralContainers=true\u0026#34;@ 然后创建临时容器，创建完成后会直接进入容器：\n$ kubectl debug golang-distroless-85c4896c45-rkjwn -it --image=ubuntu --image-pull-policy=IfNotPresent #临时容器 shell $ apt update \u0026amp;\u0026amp; apt install -y curl $ curl localhost:8080 Hello world! 值得注意的是，临时容器无法与原容器共享进程命名空间：\n$ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 02:59 pts/0 00:00:00 bash root 3042 1 0 03:02 pts/0 00:00:00 ps -ef 可以通过添加参数 --target=[container] 来将临时容器挂接到目标容器。这里与 pod.spec.shareProcessNamespace 并不同，进程号为 1 的进程是目标容器的进程，而后者的进程是 infra 容器的进程 /pause：\n$ kubectl debug golang-distroless-85c4896c45-rkjwn -it --image=ubuntu --image-pull-policy=IfNotPresent --target=golang-distroless-example 注意：目前的版本还不支持删除临时容器，参考 issue，支持的版本：\n3.2 拷贝 Pod 并添加容器 除了添加临时容器以外，另一种方式就是创建一个 Pod 的拷贝，并添加一个容器。注意这里的是普通容器，不是临时容器。 注意这里加上了 --share-processes\n$ kubectl debug golang-distroless-85c4896c45-rkjwn -it --image=ubuntu --image-pull-policy=IfNotPresent --share-processes --copy-to=golang-distroless-debug 注意这里加上了 --share-processes，会自动加上 pod.spec.shareProcessNamespace=true：\n$ kubectl get po golang-distroless-debug -o jsonpath=\u0026#39;{.spec.shareProcessNamespace}\u0026#39; true 注意：使用 kubectl debug 调试，并不能为 pod 自动加上 SYS_PTRACE 安全特性，这就意味着如果容器使用的 UID 不一致，就无法访问进程空间。 截止发文，计划在 1.23 中支持。\n总结 目前上面所有的都不适合在生产环境使用，无法在不修改 Pod 定义的情况下进行调试。\n期望 Kubernetes 1.23 版本之后 debug 功能添加 SYS_PTRACE 的支持。到时候，再尝试一下。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/debug-distroless-container-on-kubernetes/","tags":["Kubernetes","Container"],"title":"Kubernetes 上调试 distroless 容器"},{"categories":["笔记"],"contents":"过去的工作中，我们使用微服务、容器化以及服务编排构建了技术平台。为了提升开发团队的研发效率，我们同时还提供了 CICD 平台，用来将代码快速的部署到 Openshift（企业级的 Kubernetes） 集群。\n部署的第一步就是应用程序的容器化，持续集成的交付物从以往的 jar 包、webpack 等变成了容器镜像。容器化将软件代码和所需的所有组件（库、框架、运行环境）打包到一起，进而可以在任何环境任何基础架构上一致地运行，并与其他应用“隔离”。\n我们的代码需要从源码到编译到最终可运行的镜像，甚至部署，这一切在 CICD 的流水线中完成。最初，我们在每个代码仓库中都加入了三个文件，也通过项目生成器（类似 Spring Initializer）在新项目中注入：\n Jenkinsfile.groovy：用来定义 Jenkins 的 Pipeline，针对不同的语言还会有多种版本 Manifest YAML：用于定义 Kubernetes 资源，也就是工作负载及其运行的相关描述 Dockerfile：用于构建对象  这个三个文件也需要在工作中不断的演进，起初项目较少（十几个）的时候我们基础团队还可以去各个代码仓库去维护升级。随着项目爆发式的增长，维护的成本越来越高。我们对 CICD 平台进行了迭代，将“Jenkinsfile.groovy”和 “manifest YAML”从项目中移出，变更较少的 Dockerfile 就保留了下来。\n随着平台的演进，我们需要考虑将这唯一的“钉子户” Dockerfile 与代码解耦，必要的时候也需要对 Dockerfile 进行升级。因此调研了一下 buildpacks，就有了今天的这篇文章。\n什么是 Dockerfile Docker 通过读取 Dockerfile 中的说明自动构建镜像。Dockerfile 是一个文本文件，包含了由 Docker 可以执行用于构建镜像的指令。我们拿之前用于测试 Tekton 的 Java 项目的 Dockerfile 为例：\nFROMopenjdk:8-jdk-alpineRUN mkdir /appWORKDIR/appCOPY target/*.jar /app/app.jarENTRYPOINT [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java -Xmx128m -Xms64m -jar app.jar\u0026#34;]镜像分层 你可能会听过 Docker 镜像包含了多个层。每个层与 Dockerfile 中的每个命令对应，比如 RUN、COPY、ADD。某些特定的指令会创建一个新的层，在镜像构建过程中，假如某些层没有发生变化，就会从缓存中获取。\n在下面的 Buildpack 中也同样通过镜像分层和 cache 来加速镜像的构建。\n什么是 Buildpack BuildPack 是一个程序，它能将源代码转换成容器镜像的并可以在任意云环境中运行。通常 buildpack 封装了单一语言的生态工具链。适用于 Java、Ruby、Go、NodeJs、Python 等。\nBuilder 是什么？ 一些 buildpacks 按顺序组合之后就是 builder，除了 buildpacks， builder 中还加入了 生命周期 和 stack 容器镜像。\nstack 容器镜像由两个镜像组成：用于运行 buildpack 的镜像 build image，以及构建应用镜像的基础镜像 run image。如上图，就是 builder 中的运行环境。\nBuildpack 的工作方式 每个 buildpack 运行时都包含了两个阶段：\n1. 检测阶段 通过检查源代码中的某些特定文件/数据，来判断当前 buildpack 是否适用。如果适用，就会进入构建阶段；否则就会退出。比如：\n Java maven 的 buildpack 会检查源码中是否有 pom.xml Python 的 buildpack 会检查源码中是否有 requirements.txt 或者 setup.py 文件 Node buildpack 会查找 package-lock.json 文件。  2. 构建阶段 在构建阶段会进行如下操作：\n 设置构建环境和运行时环境 下载依赖并编译源码（假如需要的话） 设置正确的 entrypoint 和启动脚本。  比如：\n Java maven buildpack 在检查到有 pom.xml 文件之后，会执行 mvn clean install -DskipTests Python buildpack 检查到有 requrements.txt 之后，会执行 pip install -r requrements.txt Node build pack 检查到有 package-lock.json 后执行 npm install  BuildPack 上手 那到底如何在没有 Dockerfile 的情况下使用 builderpack 构建镜像的。看了上面这些，大家基本上也都能了解到这个核心就在 buildpack 的编写和使用的。\n其实现在有很多开源的 buildpack 可以用，没有特定定制的情况下无需自己手动编写。比如下面的几个大厂开源并维护的 Buildpacks：\n Heroku Buildpacks Google Buildpacks Paketo  但是正式详细介绍开源的 buildpacks 之前，我们还是通过自己创建 buildpack 的方式来深入了解 Buildpacks 的工作方式。测试项目呢，我们还是用测试 Tekton 的 Java 项目。\n下面所有的内容都提交到了 Github 上，可以访问：https://github.com/addozhang/buildpacks-sample 获取相关代码。\n最终的目录buildpacks-sample 结构如下：\n├── builders │ └── builder.toml ├── buildpacks │ └── buildpack-maven │ ├── bin │ │ ├── build │ │ └── detect │ └── buildpack.toml └── stacks ├── build │ └── Dockerfile ├── build.sh └── run └── Dockerfile 创建 buildpack pack buildpack new examples/maven \\  --api 0.5 \\  --path buildpack-maven \\  --version 0.0.1 \\  --stacks io.buildpacks.samples.stacks.bionic 看下生成的 buildpack-maven 目录：\nbuildpack-maven ├── bin │ ├── build │ └── detect └── buildpack.toml 各个文件中都是默认的初试数据，并没有什么用处。需要添加些内容：\nbin/detect：\n#!/usr/bin/env bash  if [[ ! -f pom.xml ]]; then exit 100 fi plan_path=$2 cat \u0026gt;\u0026gt; \u0026#34;${plan_path}\u0026#34; \u0026lt;\u0026lt;EOL [[provides]] name = \u0026#34;jdk\u0026#34; [[requires]] name = \u0026#34;jdk\u0026#34; EOL bin/build：\n#!/usr/bin/env bash  set -euo pipefail layers_dir=\u0026#34;$1\u0026#34; env_dir=\u0026#34;$2/env\u0026#34; plan_path=\u0026#34;$3\u0026#34; m2_layer_dir=\u0026#34;${layers_dir}/maven_m2\u0026#34; if [[ ! -d ${m2_layer_dir} ]]; then mkdir -p ${m2_layer_dir} echo \u0026#34;cache = true\u0026#34; \u0026gt; ${m2_layer_dir}.toml fi ln -s ${m2_layer_dir} $HOME/.m2 echo \u0026#34;---\u0026gt; Running Maven\u0026#34; mvn clean install -B -DskipTests target_dir=\u0026#34;target\u0026#34; for jar_file in $(find \u0026#34;$target_dir\u0026#34; -maxdepth 1 -name \u0026#34;*.jar\u0026#34; -type f); do cat \u0026gt;\u0026gt; \u0026#34;${layers_dir}/launch.toml\u0026#34; \u0026lt;\u0026lt;EOL [[processes]] type = \u0026#34;web\u0026#34; command = \u0026#34;java -jar ${jar_file}\u0026#34; EOL break; done buildpack.toml：\napi = \u0026#34;0.5\u0026#34; [buildpack] id = \u0026#34;examples/maven\u0026#34; version = \u0026#34;0.0.1\u0026#34; [[stacks]] id = \u0026#34;com.atbug.buildpacks.example.stacks.maven\u0026#34; 创建 stack 构建 Maven 项目，首选需要 Java 和 Maven 的环境，我们使用 maven:3.5.4-jdk-8-slim 作为 build image 的 base 镜像。应用的运行时需要 Java 环境即可，因此使用 openjdk:8-jdk-slim作为 run image 的 base 镜像。\n在 stacks 目录中分别创建 build 和 run 两个目录：\nbuild/Dockerfile\nFROMmaven:3.5.4-jdk-8-slimARG cnb_uid=1000 ARG cnb_gid=1000 ARG stack_idENV CNB_STACK_ID=${stack_id}LABEL io.buildpacks.stack.id=${stack_id}ENV CNB_USER_ID=${cnb_uid}ENV CNB_GROUP_ID=${cnb_gid}# Install packages that we want to make available at both build and run timeRUN apt-get update \u0026amp;\u0026amp; \\  apt-get install -y xz-utils ca-certificates \u0026amp;\u0026amp; \\  rm -rf /var/lib/apt/lists/*# Create user and groupRUN groupadd cnb --gid ${cnb_gid} \u0026amp;\u0026amp; \\  useradd --uid ${cnb_uid} --gid ${cnb_gid} -m -s /bin/bash cnbUSER${CNB_USER_ID}:${CNB_GROUP_ID}run/Dockerfile\nFROMopenjdk:8-jdk-slimARG stack_idARG cnb_uid=1000 ARG cnb_gid=1000 LABEL io.buildpacks.stack.id=\u0026#34;${stack_id}\u0026#34;USER${cnb_uid}:${cnb_gid}然后使用如下命令构建出两个镜像：\nexport STACK_ID=com.atbug.buildpacks.example.stacks.maven docker build --build-arg stack_id=${STACK_ID} -t addozhang/samples-buildpacks-stack-build:latest ./build docker build --build-arg stack_id=${STACK_ID} -t addozhang/samples-buildpacks-stack-run:latest ./run 创建 Builder 有了 buildpack 和 stack 之后就是创建 Builder 了，首先创建 builder.toml 文件，并添加如下内容：\n[[buildpacks]] id = \u0026#34;examples/maven\u0026#34; version = \u0026#34;0.0.1\u0026#34; uri = \u0026#34;../buildpacks/buildpack-maven\u0026#34; [[order]] [[order.group]] id = \u0026#34;examples/maven\u0026#34; version = \u0026#34;0.0.1\u0026#34; [stack] id = \u0026#34;com.atbug.buildpacks.example.stacks.maven\u0026#34; run-image = \u0026#34;addozhang/samples-buildpacks-stack-run:latest\u0026#34; build-image = \u0026#34;addozhang/samples-buildpacks-stack-build:latest\u0026#34; 然后执行命令，注意这里我们使用了 --pull-policy if-not-present 参数，就不需要将 stack 的两个镜像推送到镜像仓库了：\npack builder create example-builder:latest --config ./builder.toml --pull-policy if-not-present 测试 有了 builder 之后，我们就可以使用创建好的 builder 来构建镜像了。\n这里同样加上了 --pull-policy if-not-present  参数来使用本地的 builder 镜像：\n# 目录 buildpacks-sample 与 tekton-test 同级，并在 buildpacks-sample 中执行如下命令 pack build addozhang/tekton-test --builder example-builder:latest --pull-policy if-not-present --path ../tekton-test 如果看到类似如下内容，就说明镜像构建成功了（第一次构建镜像由于需要下载 maven 依赖耗时可能会比较久，后续就会很快，可以执行两次验证下）：\n... ===\u0026gt; EXPORTING [exporter] Adding 1/1 app layer(s) [exporter] Reusing layer \u0026#39;launcher\u0026#39; [exporter] Reusing layer \u0026#39;config\u0026#39; [exporter] Reusing layer \u0026#39;process-types\u0026#39; [exporter] Adding label \u0026#39;io.buildpacks.lifecycle.metadata\u0026#39; [exporter] Adding label \u0026#39;io.buildpacks.build.metadata\u0026#39; [exporter] Adding label \u0026#39;io.buildpacks.project.metadata\u0026#39; [exporter] Setting default process type \u0026#39;web\u0026#39; [exporter] Saving addozhang/tekton-test... [exporter] *** Images (0d5ac1158bc0): [exporter] addozhang/tekton-test [exporter] Adding cache layer \u0026#39;examples/maven:maven_m2\u0026#39; Successfully built image addozhang/tekton-test 启动容器，会看到 spring boot 应用正常启动：\ndocker run --rm addozhang/tekton-test:latest . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\  \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.3.RELEASE) ... 总结 其实现在有很多开源的 buildpack 可以用，没有特定定制的情况下无需自己手动编写。比如下面的几个大厂开源并维护的 Buildpacks：\n Heroku Buildpacks Google Buildpacks Paketo  上面几个 buildpacks 库内容比较全面，实现上会有些许不同。比如 Heroku 的执行阶段使用 Shell 脚本，而 Paketo 使用 Golang。后者的扩展性较强，由 Cloud Foundry 基金会支持，并拥有由 VMware 赞助的全职核心开发团队。这些小型模块化的 buildpack，可以通过组合扩展使用不同的场景。\n当然还是那句话，自己上手写一个会更容易理解 Buildpack 的工作方式。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/build-docker-image-without-dockerfile/","tags":["Docker"],"title":"无需 Dockerfile 的镜像构建：BuildPack vs Dockerfile"},{"categories":["翻译","云原生"],"contents":"译者：\n作为一个曾经在制造业企业的基础架构团队任职，为支持公司的“互联网基因”和“数字化转型”落地了云原生基础设施平台，并在尝试采用服务网格未成的我来说，看到这篇文章深有感触。尤其是文中所说的“人少，问题多，需要快速输出价值”，直戳到了痛处。有限的人手有限的时间，我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上，要想很好的运行复杂的系统是非常困难的。\n服务网格是一个新的基础设施层，可以承载很多的功能，未来还会有更大的想象空间和光明的未来。\n以上的种种原因，也促使我后来选择进入一家提供服务网格的产品企业，也希望服务网格可以被更简单的使用。\n“道阻且长，行则将至！”\n本文翻译自 Chris Campbell 的 How Unnecessary Complexity Gave the Service Mesh a Bad Name\n 关键要点  采用服务网格有巨大的价值，但必须以轻量级的方式进行，以避免不必要的复杂性。 在实施服务网时，要采取务实的方法，与技术的核心功能保持一致，并小心干扰（译者：注意力的分散）。 服务网格的一些核心特性包括标准化监控、自动加密和身份识别、智能路由、可靠的重试和网络可扩展性。 服务网格可以提供强大的功能，但这些功能会分散本应对核心优势的关注，并且这些功能也不是实施服务网格的主要原因。 在初始实施服务网格时没有必要去关注那些明显会分散注意力的功能，比如复杂的控制平面、多集群支持、Envoy、WASM 和 A/B 测试。   服务网格是 Kubernetes 世界中的一个热门话题，但许多潜在的采用者已经有些失望了。服务网格的落地受到压倒性的复杂性和看似无穷无尽的供应商解决方案的限制。在我亲自浏览了这个领域之后，我发现采用服务网格具有巨大的价值，但它必须以轻量级的方式完成，以避免不必要的复杂性。尽管普遍存在幻灭感，但服务网格的未来依然光明。\n在工作中学习 我进入服务网格的世界始于我在一家老牌的财富 500 强技术公司担任云计算架构师的角色。在开始我们的服务网格之旅时，我身边有许多强大的工程师，但大多数人几乎没有云计算开发经验。我们的组织诞生于云计算之前，完全实现云计算的价值需要时间。我们的传统业务线主要集中在技术栈的硬件元素上，云计算的决策最初是由为运送硬件或为该硬件提供固件和驱动程序而开发的流程驱动的。\n随着该组织经历其“数字化转型”，它越来越依赖于提供高质量的软件服务，并逐渐开发出更好的方法。但作为云计算架构师，我仍在为优先考虑硬件的业务流程，以及具有不同技能、流程和信念的工程团队导航。随着时间的推移，我和我的团队在将 .NET 应用程序迁移到 Linux、采用 Docker、迁移到 AWS 以及与之相关的最佳实践（如持续集成、自动化部署、不可变基础设施、基础设施即代码、监控等）方面变得熟练并成功。但挑战依然存在。\n在此期间，我们开始将我们的应用程序拆分为一组微服务。起初，这是一个缓慢的转变，但最终这种方法流行起来，开发人员开始更喜欢构建新的服务而不是添加到现有服务。我们这些基础设施团队的人把这看作是一种成功。唯一的问题是与网络相关的问题数量激增，开发人员正在向我们寻求答案，而我们还没有准备好有效地应对这种冲击。\n服务网格的援救 我第一次听说服务网格是在 2015 年，当时我正在研究服务发现工具并寻找与 Consul 集成的简单方法。我喜欢将应用程序职责卸载到“sidecar”容器的想法，并找到了一些可以帮助做到这一点的工具。大约在这个时候，Docker 有一个叫做“链接”的功能，让你可以将两个应用程序放在一个共享的网络空间中，这样它们就可以通过 localhost 进行通信。此功能提供了类似于我们现在在 Kubernetes pod 中所拥有的体验：两个独立构建的服务可以在部署时进行组合以实现一些附加功能。\n我总是抓住机会用简单的方案来解决大问题，因此这些新功能的力量立即打动了我。虽然这个工具是为了与 Consul 集成而构建的，但实际上，它可以做任何你想做的事情。这是我们拥有的基础设施层，可以用来一次为所有人解决问题。\n这方面的一个具体例子出现在我们采用过程的早期。当时，我们正致力于跨不同服务的日志标准化输出。通过采用服务网格和这种新的设计模式，我们能够将我们的人的问题——让开发人员标准化他们的日志——换成技术问题——将所有流量传递给可以为他们记录日志的代理。这是我们团队向前迈出的重要一步。\n我们对服务网格的实现非常务实，并且与该技术的核心功能非常吻合。然而，大部分营销炒作都集中在不太需要的边缘案例上，在评估服务网格是否适合你时，能够识别这些干扰是很重要的。\n核心功能 服务网格可以提供的核心功能分为四个关键责任领域：可观察性、安全性、连接性和可靠性。这些功能包括：\n标准化监控 我们取得的最大胜利之一，也是最容易采用的，是标准化监控。它的运营成本非常低，可以适应你使用的任何监控系统。它使组织能够捕获所有 HTTP 或 gRPC 指标，并以标准方式在整个系统中存储它们。这控制了复杂性并减轻了应用程序团队的负担，他们不再需要实现 Prometheus 指标端点或标准化日志格式。它还使用户能够公正地了解其应用程序的黄金信号。\n自动加密和身份识别 证书管理很难做好。如果一个组织还没有在这方面进行投入，他们应该找到一个网格来为他们做这件事。证书管理需要维护具有巨大安全隐患的复杂基础设施代码。相比之下，网格将能够与编排系统集成，以了解工作负载的身份，在需要时可以用来执行策略。这允许提供与 Calico 或 Cilium 等功能强大的 CNI 提供的安全态势相当或更好的安全态势。\n智能路由 智能路由是另一个特性，它使网格能够在发送请求时“做正确的事”。场景包括：\n 使用延迟加权算法优化流量 拓扑感知路由以提高性能并降低成本 根据请求成功的可能性使请求超时 与编排系统集成以进行 IP 解析，而不是依赖 DNS 传输升级，例如 HTTP 到 HTTP/2  这些功能可能不会让普通人感到兴奋，但随着时间的推移，它们从根本上增加了价值\n可靠的重试 在分布式系统中重试请求可能很麻烦，但是它几乎总是需要实现的。分布式系统通常会将一个客户端请求转换为更多下游请求，这意味着“尾巴”场景的可能性会大大增加，例如发生异常失败的请求。对此最简单的缓解措施是重试失败的请求。\n困难来自于避免“重试风暴”或“重试 DDoS”，即当处于降级状态的系统触发重试、随着重试增加而增加负载并进一步降低性能时。天真的实现不会考虑这种情况，因为它可能需要与缓存或其他通信系统集成以了解是否值得执行重试。服务网格可以通过对整个系统允许的重试总数进行限制来实现这一点。网格还可以在这些重试发生时报告这些重试，可能会在你的用户注意到系统降级之前提醒你。\n网络可扩展性 也许服务网格的最佳属性是它的可扩展性。它提供了额外的适应性层，以应对 IT 下一步投入的任何事情。Sidecar 代理的设计模式是另一个令人兴奋和强大的功能，即使它有时会被过度宣传和过度设计来做用户和技术人员还没有准备好的事情。虽然社区在等着看哪个服务网格“生出”，这反映了之前过度炒作的编排战争，但未来我们将不可避免地看到更多专门构建的网格，并且可能会有更多的最终用户构建自己的控制平面和代理以满足他们的场景。\n服务网格干扰 平台或基础设施控制层的价值怎么强调都不为过。然而，在服务网格世界中，我了解到入门的一个主要的挑战是，服务网格解决的核心问题通常甚至不是大多数服务网格项目交流的焦点！\n相反，来自服务网格项目的大部分交流都围绕着听起来很强大或令人兴奋但最终会让人分心的功能。这包括：\n强（复）大（杂）的控制平面 要很好地运行复杂的软件是非常困难的。这就是为什么如此多的组织使用云计算来使用完全托管的服务来减轻这一点的原因。那么为什么服务网格项目会让我们负责操作如此复杂的系统呢？系统的复杂性不是资产，而是负债，但大多数项目都在吹捧它们的功能集和可配置性。\n多集群支持 多集群现在是一个热门话题。最终，大多数团队将运行多个 Kubernetes 集群。但是多集群的主要痛点是你的 Kubernetes 管理的网络被切分。服务网格有助于解决这个 Kubernetes 横向扩展问题，但它最终并没有带来任何新的东西。是的，多集群支持是必要的，但它对服务网格的承诺被过度宣传了。\nEnvoy Envoy 是一个很棒的工具，但它被作为某种标准介绍，这是有问题的。Envoy 是众多开箱即用的代理之一，你可以将其作为服务网格平台的基础。但是 Envoy 并没有什么内在的特别之处，使其成为正确的选择。采用 Envoy 会给你的组织带来一系列重要问题，包括：\n 运行时成本和性能（所有这些过滤器加起来！） 计算资源需求以及如何随负载扩展 如何调试错误或意外行为 你的网格如何与 Envoy 交互以及配置生命周期是什么 运作成熟的时间（这可能比你预期的要长）  服务网格中代理的选择应该是一个实现细节，而不是产品要求。\nWASM 我是 Web Assembly (WASM) 的忠实拥趸，已经成功地使用它在 Blazor 中构建前端应用程序。然而，WASM 作为定制服务网格代理行为的工具，让你处于获得一个全新的软件生命周期开销的境地，这与你现有的软件生命周期完全正交！如果你的组织还没有准备好构建、测试、部署、维护、监控、回滚和版本代码（影响通过其系统运行的每个请求），那么你还没有准备好使用 WASM。\nA/B 测试 直到为时已晚，我才意识到 A/B 测试实际上是一个应用程序级别的问题。在基础设施层提供原语来实现它是很好的，但是没有简单的方法来完全自动化大多数组织需要的 A/B 测试水平。通常，应用程序需要定义独特的指标来定义测试的积极信号。如果组织想要在服务网格级别投入 A/B 测试，那么解决方案需要支持以下内容：\n 对部署和回滚的精细控制，因为它可能同时进行多个不同的“测试” 能够捕获系统知道的自定义指标并可以根据这些指标做出决策 根据请求的特征暴露对流量方向的控制，其中可能包括解析整个请求正文  这需要实现很多，没有哪个服务网格是开箱即用的。最终，我们的组织选择了网格之外的特征标记解决方案，其以最小的努力取得了巨大的成功。\n我们在哪里结束 最终，我们面临的挑战并不是服务网格独有的。我们工作的组织有一系列限制条件，要求我们对解决的问题以及如何解决问题采取务实的态度。我们面临的问题包括：\n 一个拥有大量不同技能的开发人员的大型组织 云计算和 SaaS 能力普遍不成熟 为非云计算软件优化的流程 碎片化的软件工程方法和信念 有限的资源 激进的截止日期  简而言之，我们人少，问题多，需要快速输出价值。我们必须支持主要不是 Web 或云计算的开发者，我们需要扩大规模以支持有不同方法和流程的大型工程组织来做云计算。我们需要将大部分精力集中在解决成熟度曲线较低的基本问题上。\n最后，当面对我们自己的服务网格决策时，我们决定建立在 Linkerd 服务网格上，因为它最符合我们的优先事项：低运营成本（计算和人力）、低认知开销、支持性社区以及透明的管理——同时满足我们的功能要求和预算。在 Linkerd 指导委员会工作了一段时间后（他们喜欢诚实的反馈和社区参与），我了解到它与我自己的工程原则有多么的契合。Linkerd 最近在 CNCF 达到毕业状态，这是一个漫长的过程，强调了该项目的成熟及其广泛采用。\n关于作者 Chris Campbell 担任软件工程师和架构师已有十多年，与多个团队和组织合作落地云原生技术和最佳实践。他在与业务领导者合作采用加速业务的软件交付策略和与工程团队合作交付可扩展的云基础架构之间分配时间。他对提高开发人员生产力和体验的技术最感兴趣。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/service-mesh-unnecessary-complexity/","tags":["Service Mesh","云原生"],"title":"低复杂度 - 服务网格的下一站"},{"categories":["笔记"],"contents":"前几天有朋友在问如何在某云上拉取 Tekton 的镜像，这种情况其实比较普遍不只是某云。工作中经常要用到过某些靠运气才能拉取到的镜像，这对工作来说真是极度的不友好。\n因此也萌生了个想法，维护一个后网络友好的仓库镜像，在 Pod 创建时将镜像仓库切换到自维护的仓库，从自维护的仓库拉取镜像。\n前几天体验了极狐Gitlab 的容器镜像库，便是为这个想法做的准备。当然其他的云厂商也有提供针对个人版的免费镜像仓库和企业版仓库。\n正好 Pipy 作为策略引擎，非常适合实现这种策略的执行。\n实现思路 Admission Webhook Kubernetes 动态准备控制 的 MutatingWebhookConfiguration 可以 hook Pod 的创建或者更新，然后调用目标服务对 Pod 资源对象进行 patch 操作。\n策略引擎 Pipy 作为应用的核心，也就是 MutatingWebhookConfiguration 的目标服务，以策略引擎的角色完成策略的执行。\nPipy 支持从文件或者 HTTP 地址加载脚本，这里为了便于策略的更新，使用了后者。\n对于从 HTTP 地址加载脚本，HTTP 地址返回内容的第一行会作为 Pipy 的主脚本，Pipy 启动时会加载主脚本，其他的文件也会被缓存到内存中。\n#地址 http://localhost:6080/repo/registry-mirror/ $ curl http://localhost:6080/repo/registry-mirror/ /main.js /config.json Pipy 会每隔 5s 检查脚本和配置文件的 etag（就是文件的最后更新时间），假如与当前文件的 etag 不一致，则会缓存并重新加载。\n利用 Pipy 的这个特性，便可以策略和配置的准实时更新。\n策略 对于策略的部分，我们将其逻辑和配置进行了分离。配置部分，配置了需要进行替换的镜像的前缀，以及替换成的内容；而逻辑，这是对 MutatingWebhookConfiguration 的 AdmissionReview 的对象进行检查。\n配置：\n{ \u0026#34;registries\u0026#34;: { \u0026#34;gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd\u0026#34;: \u0026#34;registry.gitlab.cn/flomesh/registry-mirror/tekton-pipeline\u0026#34; } } 比如说，对于镜像 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1，将 gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd 替换成 registry.gitlab.cn/addozhang/registry-mirror/tekton-pipeline。\nDemo 本文使用所有的源码都已上传到了 github。\n脚本服务器 既然选用了 HTTP 方式加载 Pipy 的脚本，那就需要实现一个脚本服务器。实现的方式有两种：使用脚本实现脚本服务器和使用 Pipy 内置的 Codebase。\n使用脚本实现脚本服务器 根据需求定义两种路由：\n /repo/registry-mirror/：返回脚本和配置的文件列表 /repo/registry-mirror/[File Name]：返回对应的文件的内容，同时需要在响应头添加 etag，值是文件的更新时间  具体脚本如下：\n#repo.js pipy({ _serveFile: (req, type, filename) =\u0026gt; ( filename = req.head.path.substring(22), os.stat(filename) ? ( new Message( { bodiless: req.head.method === \u0026#39;HEAD\u0026#39;, headers: { \u0026#39;etag\u0026#39;: os.stat(filename)?.mtime | 0, \u0026#39;content-type\u0026#39;: type, }, }, req.head.method === \u0026#39;HEAD\u0026#39; ? null : os.readFile(filename), ) ) : ( new Message({ status: 404 }, `file ${filename}not found`) ) ), _router: new algo.URLRouter({ \u0026#39;/repo/registry-mirror/\u0026#39;: () =\u0026gt; new Message(\u0026#39;/main.js\\n/config.json\u0026#39;), \u0026#39;/repo/registry-mirror/*\u0026#39;: req =\u0026gt; _serveFile(req, \u0026#39;text/plain\u0026#39;) }), }) .listen(6080) .serveHTTP( req =\u0026gt; _router.find(req.head.path)(req) )% $ pipy repo.js 2021-10-05 21:40:25 [info] [config] 2021-10-05 21:40:25 [info] [config] Module /repo.js 2021-10-05 21:40:25 [info] [config] =============== 2021-10-05 21:40:25 [info] [config] 2021-10-05 21:40:25 [info] [config] [Listen on :::6080] 2021-10-05 21:40:25 [info] [config] -----\u0026gt;| 2021-10-05 21:40:25 [info] [config] | 2021-10-05 21:40:25 [info] [config] serveHTTP 2021-10-05 21:40:25 [info] [config] | 2021-10-05 21:40:25 [info] [config] \u0026lt;-----| 2021-10-05 21:40:25 [info] [config] 2021-10-05 21:40:25 [info] [listener] Listening on port 6080 at :: 检查路由：\n$ curl http://localhost:6080/repo/registry-mirror/ /main.js /config.json $ curl http://localhost:6080/repo/registry-mirror/main.js #省略 main.js 的内容 $ curl http://localhost:6080/repo/registry-mirror/config.json #省略 config.json 的内容 使用 Pipy 内置的 Codebase 在最新发布的 Pipy 内置了一个 Codebase，大家可以理解成脚本仓库，但是比单纯的仓库功能更加强大（后面会有文档介绍该特性）。\n目前版本的 Codebase 还未支持持久化的存储，数据都是保存在内存中。后续会提供 KV store 或者 git 类型的持久化支持。\n启动 Pipy 的 Codebase很简单：\n$ pipy 2021-10-05 21:49:08 [info] [codebase] Starting codebase service... 2021-10-05 21:49:08 [info] [listener] Listening on port 6060 at :: 对于新的 Codebase 控制台的使用，这里不做过多的介绍，直接使用 REST API 完成脚本的写入：\n#创建 registry-mirror codebase，会自动创建一个空的 main.js $ curl -X POST http://localhost:6060/api/v1/repo/registry-mirror #更新 main.js $ curl -X POST \u0026#39;http://localhost:6060/api/v1/repo/registry-mirror/main.js\u0026#39; --data-binary \u0026#39;@scripts/main.js\u0026#39; #创建 config.json $ curl -X POST \u0026#39;http://localhost:6060/api/v1/repo/registry-mirror/config.json\u0026#39; --data-binary \u0026#39;@scripts/config.json\u0026#39; #检查 codebase 的版本 $ curl -s http://localhost:6060/api/v1/repo/registry-mirror | jq -r .version #更新版本 $ curl -X POST \u0026#39;http://localhost:6060/api/v1/repo/registry-mirror\u0026#39; --data-raw \u0026#39;{\u0026#34;version\u0026#34;:2}\u0026#39; 安装 进入到项目的根目录中，执行：\n$ helm install registry-mirror ./registry-mirror -n default NAME: registry-mirror LAST DEPLOYED: Tue Oct 5 22:19:26 2021 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None 查看 webhook：\n$ kubectl get mutatingwebhookconfigurations NAME WEBHOOKS AGE registry-mirror-webhook 1 2m6s 检查 pod 的启动日志：\n$ kubectl logs -n pipy -l app=pipy 2021-10-05 14:19:28 [info] [codebase] GET http://192.168.1.101:6060/repo/registry-mirror/ -\u0026gt; 21 bytes 2021-10-05 14:19:28 [info] [codebase] GET /repo/registry-mirror/main.js -\u0026gt; 2213 bytes 2021-10-05 14:19:28 [info] [codebase] GET /repo/registry-mirror/config.json -\u0026gt; 149 bytes 2021-10-05 14:19:28 [info] [config] 2021-10-05 14:19:28 [info] [config] Module /main.js 2021-10-05 14:19:28 [info] [config] =============== 2021-10-05 14:19:28 [info] [config] 2021-10-05 14:19:28 [info] [config] [Listen on :::6443] 2021-10-05 14:19:28 [info] [config] -----\u0026gt;| 2021-10-05 14:19:28 [info] [config] | 2021-10-05 14:19:28 [info] [config] acceptTLS 2021-10-05 14:19:28 [info] [config] | 2021-10-05 14:19:28 [info] [config] |--\u0026gt; [tls-offloaded] 2021-10-05 14:19:28 [info] [config] decodeHTTPRequest 2021-10-05 14:19:28 [info] [config] replaceMessage 2021-10-05 14:19:28 [info] [config] encodeHTTPResponse --\u0026gt;| 2021-10-05 14:19:28 [info] [config] | 2021-10-05 14:19:28 [info] [config] \u0026lt;---------------------------------| 2021-10-05 14:19:28 [info] [config] 2021-10-05 14:19:28 [info] [listener] Listening on port 6443 at :: 测试 在上一篇中我已经推送了 Tekton 的两个镜像到容器镜像库中，因此这里直接安装 tekton 进行测试。\n$ kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/previous/v0.28.1/release.yaml $ kubectl get pod -n tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-75974fbfb8-f62dv 1/1 Running 0 7m36s tekton-pipelines-webhook-6cc478f7ff-mm5l9 1/1 Running 0 7m36s 检查结果：\n$ kubectl get pod -o json -n tekton-pipelines -l app=tekton-pipelines-controller | jq -r \u0026#39;.items[].spec.containers[].image\u0026#39; registry.gitlab.cn/flomesh/registry-mirror/tekton-pipeline/controller:v0.28.1 $ kubectl get pod -o json -n tekton-pipelines -l app=tekton-pipelines-webhook | jq -r \u0026#39;.items[].spec.containers[].image\u0026#39; registry.gitlab.cn/flomesh/registry-mirror/tekton-pipeline/webhook:v0.28.1 从上面的结果可以看到结果是符合预期的。\n总结 整个实现的策略部分加上配置，只有 70 多行的代码。并且实现了逻辑与配置的分离之后，后续的配置也都可以做到实时的更新而无需修改任何逻辑代码，更无需重新部署。\n但是目前的实现，是需要手动把镜像推送的自维护的镜像仓库中。实际上理想的情况是检查自维护的仓库中是否存在镜像（比如通过 REST API），如果未发现镜像，先把镜像拉取到本地，tag 后再推送到自维护的仓库。不过这种操作，还是需要网络的畅通。当然也尝试过通过 REST API 触发 CICD Pipeline 的执行拉取镜像并 tag，但是极狐Gitlab 是部署在某云的环境上，同样也受困于网络问题。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/kubernetes-images-swapper/","tags":["Kubernetes","DevOps","Container"],"title":"自动替换 Kubernetes 镜像"},{"categories":["笔记"],"contents":"感谢极狐团队为 GitLab（SaaS）本地化的努力，同时也感谢小马哥提供的内测资格。\n最近突然想到了个点子，需要使用一个私有的镜像仓库。极狐GitLab 有提供容器镜像库，正好和 CICD 一起做个轻度体验。\n容器镜像库 Container Registry 文档介绍在这里，目前还是英文。（应该本地化的工作量很大，文档还没翻译。）\n容器镜像库可以作为独立镜像仓库使用（为什么要这么用，卖个关子下篇文章见），就是使用 docker 命令将构建好的镜像推送到 容器镜像库。\n当然也可以同 CICD 流水线结合使用，后文也会介绍。\n独立使用 本地登录 Container Registry 有两种验证方式：\n 使用用户名和密码 开启了双重身份验证，可以使用访问个人访问令牌  其实，不管是否开始双重验证，都建议使用访问令牌。\ndocker login registry.gitlab.cn #根据提示输入用户名和密码或者令牌 image 的名字最多有三层，即 registry.example.com/[namespace] 之后的内容最多有 3 层。比如下面的 image 名字 myproject/my/image\nregistry.example.com/mynamespace/myproject/my/image:rc1 其次 image 名字的第一层必须是镜像名，如上面的 myproject。\n尝试将 tekton 的镜像推送上去：\ndocker tag gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/controller:v0.28.1 registry.gitlab.cn/addozhang/registry-mirror/tekton-pipeline/controller:v0.28.1 docker push registry.gitlab.cn/addozhang/registry-mirror/tekton-pipeline/controller:v0.28.1 请忽略发布时间，原镜像的 Created 字段就有问题。\n同样可以使用 REST API 进行访问：\ncurl --location --request GET \u0026#39;https://gitlab.cn/api/v4/projects/addozhang%2Fregistry-mirror/registry/repositories/155/tags\u0026#39; \\ --header \u0026#39;PRIVATE-TOKEN: TOKEN_HERE\u0026#39; [{\u0026#34;name\u0026#34;:\u0026#34;v0.28.1\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;addozhang/registry-mirror/tekton-pipeline/controller:v0.28.1\u0026#34;,\u0026#34;location\u0026#34;:\u0026#34;registry.gitlab.cn/addozhang/registry-mirror/tekton-pipeline/controller:v0.28.1\u0026#34;}] 使用 CICD 构建和推送 见下文。\nCICD 我将之前 github 的使用的测试 tekton 的项目镜像到了这里，并添加了一个 .gitlab-ci.yml 的流水线定义文件。\n有了官方的文档，以及参考官方提供各种的模板，流水线的定义上手很快。\n整个流水线包含了两个 stage：Java 代码的编译打包和镜像的构建。\n如上图，最新的一次使用了 cache 功能将 .m2/repository 缓存；而前两次使用了缓存（这里的构建耗时差异很大，不知道是不是因为晚上资源比较少？）。Java 项目会将依赖包保存在本地库中，使用 cache 功能可以提升构建的效率。\n流水线 DAG 使用 needs 可以控制同 stage 下作业的构建顺序，否则同 stage 下作业的执行是并行的。同时有了 needs 还可以构建出 DAG，前提是最少需要 3 个作业，因此我又加了一个作业。\ncache: paths: - .m2/repository variables: MAVEN_OPTS: \u0026#34;-Dhttps.protocols=TLSv1.2 -Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository -Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=WARN -Dorg.slf4j.simpleLogger.showDateTime=true -Djava.awt.headless=true\u0026#34; stages: - build - image - post-build maven-build: image: maven:3-jdk-8 stage: build artifacts: paths: - target/*.jar script: - mvn install -DskipTests docker-build: image: docker:19.03.12 stage: image needs: - maven-build dependencies: - maven-build services: - docker:19.03.12-dind variables: IMAGE_TAG: $CI_REGISTRY_IMAGE:latest script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY - docker build -t $IMAGE_TAG . - docker push $IMAGE_TAG done: image: busybox:latest stage: post-build needs: - docker-build script: - echo \u0026#34;All Done!\u0026#34; 感觉图有点简陋，后期应该会优化。\n作业依赖 前面的流水线定义中，为了传递 maven 构建的 jar，使用了 artifacts 和 dependencies 进行了传递。\n难道是我理解错了？鼠标悬停并没有显示做依赖的作业。\n流水线触发 除了 push 代码触发，还可以创建触发器通过 Web API 进行触发。\ncurl -X POST \\  -F token=TOKEN_HERE \\  -F ref=main \\  https://gitlab.cn/api/v4/projects/9766/trigger/pipeline {\u0026#34;id\u0026#34;:19252,\u0026#34;project_id\u0026#34;:9766,\u0026#34;sha\u0026#34;:\u0026#34;5dde144d584b76fe6d3b63a4a9beb789762d1a2d\u0026#34;,\u0026#34;ref\u0026#34;:\u0026#34;main\u0026#34;,\u0026#34;status\u0026#34;:\u0026#34;created\u0026#34;,\u0026#34;created_at\u0026#34;:\u0026#34;2021-10-01T07:37:42.806+08:00\u0026#34;,\u0026#34;updated_at\u0026#34;:\u0026#34;2021-10-01T07:37:42.806+08:00\u0026#34;,\u0026#34;web_url\u0026#34;:\u0026#34;https://gitlab.cn/addozhang/tekton-test/-/pipelines/19252\u0026#34;,\u0026#34;before_sha\u0026#34;:\u0026#34;0000000000000000000000000000000000000000\u0026#34;,\u0026#34;tag\u0026#34;:false,\u0026#34;yaml_errors\u0026#34;:null,\u0026#34;user\u0026#34;:{\u0026#34;id\u0026#34;:432,\u0026#34;name\u0026#34;:\u0026#34;addozhang\u0026#34;,\u0026#34;username\u0026#34;:\u0026#34;addozhang\u0026#34;,\u0026#34;state\u0026#34;:\u0026#34;active\u0026#34;,\u0026#34;avatar_url\u0026#34;:null,\u0026#34;web_url\u0026#34;:\u0026#34;https://gitlab.cn/addozhang\u0026#34;},\u0026#34;started_at\u0026#34;:null,\u0026#34;finished_at\u0026#34;:null,\u0026#34;committed_at\u0026#34;:null,\u0026#34;duration\u0026#34;:null,\u0026#34;queued_duration\u0026#34;:null,\u0026#34;coverage\u0026#34;:null,\u0026#34;detailed_status\u0026#34;:{\u0026#34;icon\u0026#34;:\u0026#34;status_created\u0026#34;,\u0026#34;text\u0026#34;:\u0026#34;created\u0026#34;,\u0026#34;label\u0026#34;:\u0026#34;created\u0026#34;,\u0026#34;group\u0026#34;:\u0026#34;created\u0026#34;,\u0026#34;tooltip\u0026#34;:\u0026#34;created\u0026#34;,\u0026#34;has_details\u0026#34;:true,\u0026#34;details_path\u0026#34;:\u0026#34;/addozhang/tekton-test/-/pipelines/19252\u0026#34;,\u0026#34;illustration\u0026#34;:null,\u0026#34;favicon\u0026#34;:\u0026#34;/assets/ci_favicons/favicon_status_created-4b975aa976d24e5a3ea7cd9a5713e6ce2cd9afd08b910415e96675de35f64955.png\u0026#34;}} 总结 由于之前任职的公司内部也有用 Gitlab，也有过 Github Action 和 Tektoncd 的使用经验，所以体验下来并还没有任何阻碍。这也得益于文档的完善，以及极狐团队的努力，希望极狐可以做得更好。\n文中使用 registry-mirror 做了仓库名，大家也能猜到点什么，敬请关注一下篇。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/jihu-gitlab-experience/","tags":["Container","DevOps"],"title":"极狐GitLab SaaS 内测轻度体验"},{"categories":["翻译"],"contents":"译者点评：\n最近听了很多资深的人士关于开源，以及商业化的分析。开源与商业化，听起来就是一对矛盾的所在，似乎大家都在尝试做其二者的平衡。是先有开源，还是先有商业化？俗话说“谈钱不伤感情”，近几年背靠开源的创业公司如雨后春笋般涌现，即使是开发人员也是需要生活的。\n容器神话 Docker 曾经无比风光，盛极一时。即使这样一个备受瞩目，大获风投的热捧的独角兽也未能免俗，并付出了不小的代价。\n今天这篇文章讲述了 Docker 这家公司从诞生到巅峰到没落，这一路上所做的抉择，并最终做了开源与商业的分离，再一次从开源踏上找寻商业化之路。这些都是值得我们参考和思考的，不管是已经开源或者准备从事开源的。\n这篇文章翻译自How Docker broke in half  这家改变游戏规则的容器公司是其昔日的外衣。作为云时代最热门的企业技术业务之一的它到底发生了什么？\n Docker 并没有发明容器——将计算机代码打包成紧凑单元的方法，可以轻松地从笔记本电脑移植到服务器——但它确实通过创建一套通用的开源工具和可重用的镜像使其成为主流，这使所有开发人员只需构建一次软件即可在任何地方运行。\nDocker 使开发人员能够轻松地将他们的代码“容器化”并将其从一个系统移动到另一个系统，迅速将其确立为行业标准，颠覆了在虚拟机 (VM) 上部署应用程序的主要方式，并使 Docker 成为新一代最快被采用的企业技术之一。\n今天，Docker 仍然活着，但它只是它可能成为的公司的一小部分，从未成功地将这种技术创新转化为可持续的商业模式，最终导致其企业业务于 2019 年 11 月出售给 Mirantis。InfoWorld 采访了十几位前任和现任 Docker 员工、开源贡献者、客户和行业分析师，了解 Docker 如何分崩离析的故事。\nDocker 诞生了 2008 年由 Solomon Hykes 在巴黎创立的 DotCloud，这个后来成为 Docker 的公司最初被设计为供开发人员轻松构建和发布他们的应用程序的平台即服务 (PaaS)。\n在 2010 年夏天一起搬到硅谷参加著名的 Y Combinator 计划之前，Hykes 很快就加入了他的朋友兼程序员同事 Sebastien Pahl。已经被拒绝一次的 Hykes 和 Pahl 重新申请，Pahl 的父亲在他们面试前几周把去旧金山的机票钱放在他们面前。唉，这对夫妇再次被拒绝，直到 YC 校友 James Lindenbaum，一家名为 Heroku 的竞争公司的创始人，出面为他们担保。\n我们所知道的 Docker 于 2013 年 3 月在 PyCon 上由 Hykes 首次演示，他解释说开发人员一直要求访问支持 DotCloud 平台的底层技术。他在那次谈话中说：“我们一直认为能够说“是”会很酷，这是我们的底层部分，现在你可以和我们一起做 Linux 容器，做任何你想做的事，去构建你的平台，这就是我们正在做的，”。\nDocker 首席执行官 Ben Golub 2013 年至 2017 年之间，告诉 InfoWorld，“这听起来很老套，但 Solomon 和我谈论的是预发布，我们可以看到所有集装箱船进入奥克兰港，我们正在谈论集装箱在航运界的价值。事实上，从世界的一侧运送汽车比将应用程序从一个服务器带到另一个服务器更容易，这似乎是一个需要解决的问题。”\nDocker 开源项目迅速崛起，吸引了成千上万的用户，与微软、AWS 和 IBM 等公司建立了备受瞩目的合作伙伴关系，并获得了满满一车的风险投资，包括 Benchmark 的 Peter Fenton 和 Trinity Ventures 的 Dan Scholnick 的早期投资。调整后的公司更名为 Docker，并从 Benchmark、Coatue Management、Goldman Sachs 和 Greylock Partners 等公司筹集了近 3 亿美元。然而，与许多基于开源软件的公司一样，它很难找到一种可盈利的商业模式，而这些投资者从未得到他们的大笔回报。。\nRedMonk 分析师 James Governor 说，“Solomon 建立了过去 20 年来最引人注目的技术之一，并且在将观点打包并使其对大量开发人员非常有价值的业务中。Docker 是否做出了错误的决定？显然是的，但风投都疯了，他们向他们投入的钱意味着他们一定觉得他们可以做任何事情，这是有问题的。”\n快进到 2021 年，这个故事的简版是，备受欢迎的开源容器编排工具 Kubernetes 通过取代其主要利润来源：一个名为 Docker Swarm 的企业版容器编排工具，吃掉了 Docker（业务）的午餐. 然而，真实的故事要复杂得多。\n开源商业化很难 巨额的风险投资、快速增长的竞争格局以及云行业巨头都想分一杯羹的阴影，将这家年轻的公司带入了一个犹如压力锅的运营环境。\n“有一种说法是\u0026rsquo;大象打架，草被践踏'，我们很清楚这不仅是针对 Docker，还有云供应商的相互竞争。他们都想把我们拉向不同的方向。既要保持我们的价值观和根基，又要建立一个企业，这个根本就是个困局”Golub 说。\n这位前 CEO 指出，随着 Docker 的发展，所有这些因素都造成了“自然的紧张关系”。Golub 说：“我们希望建立伟大的社区并通过开发者产品获利，同时还希望建立一个伟大的运营商产品，让客户能够大规模构建和部署容器。这就是我们的愿景，很快我们意识到我们必须迅速扩大规模，而且没有太多时间来平衡社区和成为一家商业企业\u0026hellip;\u0026hellip;在一家初创公司，你每天要做出 100 个决定，你希望 80 个是对的。”\n2014 年左右，Docker 开始认真考虑将其在容器领域的领先地位货币化的商业战略，当时该公司将部分风险投资资金用于 2014 年 Koality 的收购和 2015 年的 Tutum 的收购，同时还推出了自己的企业支持计划的第一次迭代。\n这些投资催生了像 Docker Hub 这样的产品——你可以认为它有点像 Docker 镜像的 GitHub（现在也存在）—— 以及最终的 Docker 企业版。但这些产品都没有真正受到企业客户的欢迎，他们通常乐于与更成熟的合作伙伴合作，或者构建而不是购买解决方案，尽管 Docker 努力生产客户真正想要的一系列产品。\n今年夏天在法国度假时，Hykes 告诉 InfoWorld：“我们从未发布过出色的商业产品，原因是我们没有集中注意力。我们试图做每件事的一点点。维持开发者社区的增长并构建一个伟大的商业产品已经够难的了，更不用说三四个了，而且基本不可能同时做到，但这就是我们试图做的，我们花了大量的钱来做这些事。 ”\nDockerDocker 业务发展和技术联盟的前副总裁、最早的员工之一 Nick Stinemates 说：“在开源之外出现了零技术交付，根本无法交付商业软件。”\n事后看来，Hykes 认为 Docker 应该花更少的时间来运送产品，而应该花更多的时间倾听客户的意见。Hykes 说：“我本来不会急于扩大商业产品的规模，而是投入更多资金从我们的社区收集见解，并建立一个致力于了解他们的商业需求的团队。我们在 2014 年有一个转折点，当时我们觉得我们等不及了，但我认为我们等待的时间比我们意识到的要多。”\n其他人认为 Docker 过早地免费提供了太多东西。今年早些时候，谷歌的 Kelsey Hightower 告诉 Increment 杂志： “他们免费推出了一些东西，这就是本垒打。他们解决了整个问题并达到了这个问题的天花板：创建一个映像、构建它、将它存储在某个地方、然后运行它。还需要做什么？”\nHykes 不同意这种说法。他说：“我认为这是错误的，一般来说，核心开源产品创造了巨大的增长，这首先创造了商业化的机会。许多公司成功地将 Docker 商业化，但 Docker 没有。有很多东西可以商业化，只是 Docker 未能将其商业化。”\n例如，红帽和 Pivotal（现在是 VMware 的一部分）都是 Docker 的早期合作伙伴，将 Docker 容器集成到他们的商业 PaaS 产品（分别是 OpenShift 和 Cloud Foundry）中，并为开源项目做出了贡献。\nStinemates 说：“如果我是慷慨的，红帽公司早期的贡献让 Solomon 有点失控了。Solomon 烧掉了很多桥梁，Hacker News 上有关于他与反对者争吵的帖子。企业合作伙伴不可能与 Solomon 一起拥有这些。”\n今天，Hykes 说他犯了混淆“社区与生态系统”的错误。红帽特别“不是社区的一部分，他们从来没有为 Docker 的成功而生根，”他说。“我们的错误是非常希望他们成为社区的一部分。回想起来，我们永远不会从这种伙伴关系中受益。”\n因此，旅游科技公司 Amadeus 等早期客户在 2015 年转向红帽，以填补他们认为 Docker 留下的企业级空白。Amadeus 的云平台负责人 Edouard Hubin 通过电子邮件告诉 InfoWorld：“我们直接从使用 [Docker 的] 开源版本的先驱模式转变为与红帽建立强大的合作伙伴关系，他们为我们提供容器技术的支持。容器化是远离虚拟化的技术变革的第一步。真正的游戏规则变革者是容器编排解决方案。显然，Docker 输给了 Kubernetes，这对他们来说是一个非常困难的局面。”\nKubernetes 的决定 Docker 会后悔之前做的一系列决定，因为它拒绝真正接受 Kubernetes 作为首选的新兴容器编排工具 —— Kubernetes 允许客户大规模、一致地运行容器队列——而不是短视地推进自己专有的 Docker Swarm 编排器（RIP）。\nDocker 最早也是服务时间最长的员工之一 Jérôme Petazzoni 说：“最大的错误是错过了 Kubernetes。我们处于集体思想泡沫中，我们在内部认为 Kubernetes 太复杂了，而 Swarm 会成功得多。没有意识到这一点是我们的集体失败。”\n事实是，Docker 在 2014 年有机会与谷歌的 Kubernetes 团队密切合作，并在此过程中可能拥有整个容器生态系统。Stinemates 说：“我们本可以让 Kubernetes 成为 GitHub 上 Docker 旗帜下的一流 Docker 项目。事后看来，Swarm 上市太晚是一个重大错误。”\n据在场的多名人士称，谷歌旧金山办公室的早期讨论是技术性的和紧张的，因为双方对如何进行容器编排持不同的意见。\nKubernetes 联合创始人、现任 VMware 副总裁 Craig McLuckie 表示，他提出将 Kubernetes 捐赠给 Docker，但双方未能达成协议。他告诉 InfoWorld：“那里有一个相互傲慢的因素，从他们那里我们不了解开发人员的经验，但相互的感觉是这些年轻的新贵真的不了解分布式系统管理。”其他人则表示讨论更为非正式，并且侧重于容器技术的联合开发。无论哪种方式，团队从来没有意见一致并最终分道扬镳，谷歌在 2014 年夏天推出了 Kubernetes。\nHykes 对 Google 向 Docker 提供 Kubernetes 项目的所有权提出异议，称他们“有机会像其他人一样成为生态系统的一部分”。\nHykes 承认当时 Docker 和 Google 团队之间处于紧张关系。Hykes 说：“有那么一刻，自负占了上风。谷歌的很多聪明和有经验的人都被 Docker 的完全局外人蒙蔽了双眼。我们没有在谷歌工作，我们没有去斯坦福，我们没有计算机科学博士学位。有些人觉得这是他们的事，所以有一场自我之战。结果并不是 Docker 和 Kubernetes 团队之间的良好合作，而此时合作确实有意义。”\nStinemates 说：“一方面是基本的自我，另一方面是与 [Kubernetes 联合创始人] Joe Beda、Brendan Burns 和 Craig McLuckie 的紧张关系——他们对服务级别 API 的需求有强烈的看法，而从简单的角度来看 Docker 在技术上对单个 API 有自己的看法，这意味着我们无法达成一致。”\nHykes 承认，当时 Docker 面临着为想要扩展容器使用规模的客户寻找编排解决方案的压力，但当时Kubernetes 将成为该解决方案并不明确。Hykes 说：“Kubernetes 太早了，而且是几十个中的一个，我们并没有神奇地猜测它会占据主导地位，甚至不清楚谷歌对它的承诺。我问我们的工程师和架构师该怎么做，他们建议我们继续使用 Swarm。”\n甚至 McLuckie 也承认他“不知道 Kubernetes 会变成 Kubernetes。回顾历史很容易认为这是一个糟糕的选择。”\n不管它失败了，Kubernetes 最终赢得了容器编排之战，其余的成为软件行业的匆匆过客。\n451 Research 的分析师 Jay Lyman 说：“Kubernetes 来了，并抢走了所有的风头。它代表了谷歌在开发和开源方面对容器的使用，这在很多方面都超过了对 Docker 的关注。[Docker] 将 Docker Swarm 视为他们通过软件获利的方式。如果他们可以回去，他们可能会从一开始就与 Kubernetes 更紧密地集成。他们过于专注于独自行动。”\nMcLuckie 说：“我最深切的遗憾之一是我们没有找到谈判的方法。Docker 提供了一些非凡的体验，而 Kubernetes 提供的东西，从体验的角度来看，并不那么引人注目。” 或者，正如 Docker 联合创始人 Sebastien Pahl 指出的那样：“简单并没有获胜。我喜欢 Kubernetes，但它不适合普通人。”\n高层的紧张气氛 在 2015 年以 10 亿美元的“独角兽”估值完成 9500 万美元的大型 D 轮融资之后，Docker 最终达到了炒作周期的巅峰。\nSteinmates 说：“这设定了非常高的期望，并暴露了我们作为一家公司将面临的一些基本问题。我认为 Ben [Golub，首席执行官] 对公司的想法与 Solomon 不同，两人没有意见一致应该不是什么秘密。董事会大量掺和努力让创始人开心，并给 CEO 足够的回旋余地，使公司取得成功。如果由 Solomon 决定，我们会坚持以社区为导向的路线来创造病毒式传播。如果由 Ben 决定，我们会更早地转向业务方面。这种紧张局势导致我们对两者都采取了半途而废的方式。”\n这种方法有效地催生了两个Docker：Docker 社区版（面向开发人员的广受欢迎的命令行工具和开源项目）和 Docker 企业版（面向希望大规模采用容器的企业客户的商业工具套件）。不幸的是，公司的动作太慢了，无法正式进行拆分并相应地分配资源。\nGolub 承认他们“应该比实际更早地拆分业务”，而 Hykes 同意 Docker “从未找到连接公司这两部分的方法”。\n到了 2018 年，裂缝开始显现，因为该公司努力在日益不满的开源社区、强大的合作伙伴和要求在生产中运行容器的企业客户之间找到可行的道路。\n不久之后，Hykes 于 2018 年 3 月离开了他在公司的日常角色，他在一篇博客文章中指出，“作为创始人，我当然有复杂的情绪。当你创建一家公司时，你的工作是确保它有一天可以在没有你的情况下取得成功。然后最终有一天会到来，庆祝活动可能是苦乐参半。对于创始人来说，放弃一生的工作绝非易事。”\n今天回想起来，Hykes 更简单。“我意识到我不属于这家公司。留下对我来说没有任何意义，所以我离开了\u0026hellip;\u0026hellip;我多半是个应该继续担任首席执行官或离开的不快乐创始人。”\nDocker 一分为二 面对日益严重的资金问题，Docker 轮换了新任 CEO，Golub 于 2017 年 5 月让位给前 SAP 执行官 Steve Singh，然后 Singh 于 2019 年 6 月让位给前 Hortonworks 首席执行官 Rob Bearden。\n最终要接受批评的是 Bearden。上任后不久，Docker 于 2019 年 11 月将其企业部分业务出售给 Mirantis，Docker Enterprise 并入 Mirantis Kubernetes Engine。\nBearden 当时在一份新闻稿中说：“在与管理团队和董事会进行彻底分析后，我们确定 Docker 有两个截然不同的业务：一个是活跃的开发者业务，另一个是成长中的企业业务。我们还发现产品和财务模型大不相同。”\nDocker 如今在哪里？ 有了原始投资者 Insight Venture Partners 和 Benchmark Capital 的 3500 万美元现金注入，剩下的 Docker 由原始 Docker Engine 容器运行时、Docker Hub 镜像存储库和 Docker 桌面应用程序支撑，在 7 年公司资深人士 Scott Johnston 的领导下活了下来。\nJohnston 告诉 InfoWorld，他正试图通过“像激光一样重新关注开发人员的需求”，让公司回归本源。“我们认为该公司比以往任何时候都更强大，因为三点：以客户为中心、统一的市场定位和生态系统友好的商业模式。”\n上周 Docker 宣布更改 Docker 软件的许可条款。很快，为大公司工作的 Docker Desktop 专业用户将不得不注册付费订阅才能继续使用该应用程序。\nJohnston 决心不重蹈覆辙，专注于为公司的核心软件开发人员受众提供价值。他说：“我们的野心更大，因为开发人员生态系统要面向的是世界上的每个开发人员，而不仅仅是那些与我们的运行时一致的开发人员。”\nJohnston 认为“Docker 2.0”的增长机会在于为安全、已验证的镜像构建新的开发人员工具和可信内容，以及新兴计算模型（如无服务器、机器学习和物联网 以容器技术为基础的工作负载）背后的持续动力。\n同时，Docker 仍然是行业标准的容器运行时，如今 Docker Desktop 安装在 330 万台机器上。此外，在 Stack Overflow 的 2021 年开发者调查中，49% 的受访者表示他们经常使用该工具。\n尽管如此，人们仍然对可能发生的事情深感失望。Stinemates 说：“如果我想要轻率，我会问今天 Docker 是否存在。从职业角度来看，这很可悲。我仍在寻找一家像 Docker 一样令人兴奋和充满活力的公司，并能创造出火花。”\nHykes 说：“可以公平地说，Docker 未能实现其商业化的潜力……到目前为止。我很高兴 Docker 在这么多年之后有再次有实现商业化的机会。这是对基础项目和品牌的证明。”\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/how-docker-broke-in-half/","tags":["OpenSource","Docker"],"title":"容器神话 Docker 是如何一分为二的"},{"categories":["笔记"],"contents":"为什么要在 arm64 平台上部署 Kubernetes，而且还是鲲鹏 920 的架构。说来话长 。。。 此处省略5000 字。\n介绍下系统信息；\n 架构：鲲鹏 920(Kunpeng920) OS：openEuler 20.03 (LTS-SP1) CPU：4c 内存：16G 硬盘：若干  整个过程虽然参考了鲲鹏论坛的帖子，不过还是颇费周折。\nTL;DR 整个过程中要注意 arm64 平台上安装 Kubernetes 及网络组件，需要使用 arm64 版本的镜像。\n环境配置 1.关闭 selinux #临时关闭 setenforce 0 #永久关闭 SELINUX=disabled vim /etc/sysconfig/selinux 2. 关闭swap分区 #临时关闭 swapoff -a #永久关闭 注释 swap 行 vim /etc/fstab 3. 关闭防火墙 systemctl stop firewalld ssystemctl disable firewalld 4. 网络配置 对iptables内部的nf-call需要打开的内生的桥接功能\nvim /etc/sysctl.d/k8s.conf 修改如下内容：\nnet.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm_swappiness=0 修改完成后执行：\nmodprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf 5. 添加 Kubernetes 源 在文件 /etc/yum.repos.d/openEuler.repo 中追加如下内容：\n[kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-aarch64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 安装配置 iSula yum install -y iSulad 修改 iSula 配置，打开文件 /etc/isulad/daemon.json，按照下面的部分：\n{ \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;docker.io\u0026#34; ], \u0026#34;insecure-registries\u0026#34;: [ \u0026#34;rnd-dockerhub.huawei.com\u0026#34; ], \u0026#34;pod-sandbox-image\u0026#34;: \u0026#34;k8s.gcr.io/pause:3.2\u0026#34;, // 按照对应 Kubernetes 版本进行修改，后面会有说明 \u0026#34;network-plugin\u0026#34;: \u0026#34;cni\u0026#34;, \u0026#34;cni-bin-dir\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cni-conf-dir\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;hosts\u0026#34;: [ \u0026#34;unix:///var/run/isulad.sock\u0026#34; ] } 修改之后重启 isulad\nsystemctl restart isulad systemctl enable isulad Kubernetes 部署 1. 安装 kubelet、kubeadm、kubectl yum install -y kubelet-1.20.0 kubeadm-1.20.0 kubectl-1.20.0 2. 准备镜像 由于某种未知的网络问提，会导致拉取 k8s.gcr.io 的镜像失败。需要提前下载好。\n通过 kubeadm config images list --kubernetes-version 1.20.0 命令，获取初始化所需的镜像。这里需要注意通过 --kubernetes-version 参数指定版本号，否则 kubeadm 是打印出最高的 1.20.x 版本的初始化镜像（比如，1.20.x 的最高版本是 1.20.4）。\nk8s.gcr.io/kube-apiserver:v1.20.0 k8s.gcr.io/kube-controller-manager:v1.20.0 k8s.gcr.io/kube-scheduler:v1.20.0 k8s.gcr.io/kube-proxy:v1.20.0 k8s.gcr.io/pause:3.2 k8s.gcr.io/etcd:3.4.13-0 k8s.gcr.io/coredns:1.7. 对应的 arm64 版本镜像为：\nk8s.gcr.io/kube-apiserver-arm64:v1.20.0 k8s.gcr.io/kube-controller-manager-arm64:v1.20.0 k8s.gcr.io/kube-scheduler-arm64:v1.20.0 k8s.gcr.io/kube-proxy-arm64:v1.20.0 k8s.gcr.io/pause-arm64:3.2 k8s.gcr.io/etcd-arm64:3.4.2-0 #支持 arm64 的 3.4.x 的最高版本 k8s.gcr.io/coredns:1.7.0 #无需特别的 arm64 版本 凭“运气”下载好镜像后，再通过 isula tag 命令修改成我们需要的：\nisula tag k8s.gcr.io/kube-apiserver-arm64:v1.20.0 k8s.gcr.io/kube-apiserver:v1.20.0 isula tag k8s.gcr.io/kube-controller-manager-arm64:v1.20.0 k8s.gcr.io/kube-controller-manager:v1.20.0 isula tag k8s.gcr.io/kube-scheduler-arm64:v1.20.0 k8s.gcr.io/kube-scheduler:v1.20.0 isula tag k8s.gcr.io/kube-proxy-arm64:v1.20.0 k8s.gcr.io/kube-proxy:v1.20.0 isula tag k8s.gcr.io/pause-arm64:3.2 k8s.gcr.io/pause:3.2 isula tag k8s.gcr.io/etcd-arm64:3.4.2-0 k8s.gcr.io/etcd:3.4.13-0 isula tag k8s.gcr.io/coredns:1.7.0 k8s.gcr.io/coredns:1.7.0 3. 初始化 master 节点 注意需要指定 --cri-socket 参数使用 isulad 的 API。\nkubeadm init --kubernetes-version v1.20.0 --cri-socket=/var/run/isulad.sock --pod-network-cidr=10.244.0.0/16 安装成功的话，会看到如下的内容\nYour Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 12.0.0.9:6443 --token 0110xl.lqzlegbduz2qkdhr \\ \t--discovery-token-ca-cert-hash sha256:42b13f5924a01128aac0d6e7b2487af990bc82701f233c8a6a4790187ea064af 4. 配置集群环境 然后根据上面的输出进行配置\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=/etc/kubernetes/admin.conf 5. 向集群添加 Node 节点 重复前面的步骤：环境配置、安装配置 iSula 以及 Kubernetes 部署 的 1 和 2。\n同样使用上面输出的命令，再加上 --cri-socket 参数：\nkubeadm join 12.0.0.9:6443 --token 0110xl.lqzlegbduz2qkdhr \\ \t--discovery-token-ca-cert-hash \\ \t--cri-socket=/var/run/isulad.sock 配置网络插件 完成 master 节点初始化并配置完集群环境后就可以执行 kubectl 的命令了。\nkubectl get nodes NAME STATUS ROLES AGE VERSION host-12-0-0-9 NotReady control-plane,master 178m v1.20.0 看下节点，发现节点是 NotReady 的状态，这是因为网络插件还没安装。如果此时通过命令 journalctl -uf kubelet 查看 kubelet 的日志，会看到日志提示网络插件没有 ready。\nkubelet.go:2160] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:iSulad: network plugin is not ready: cni config uninitialized 还记得 isulad 的配置么？\n\u0026quot;network-plugin\u0026quot;: \u0026quot;cni\u0026quot;, \u0026quot;cni-bin-dir\u0026quot;: \u0026quot;\u0026quot;, //使用默认 /opt/cni/bin \u0026quot;cni-conf-dir\u0026quot;: \u0026quot;\u0026quot;, //使用默认 /etc/cni/net.d 实际上两个目录都是空的内容，如果目录不存在，先创建：\nmkdir -p /opt/cni/bin mkdir -p /etc/cni/net.d 这里使用 calico 做为网络插件，先下载 manifest。\nwget https://docs.projectcalico.org/v3.14/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 因为是 arm64 的硬件，同样需要使用对应 arm64 版本的镜像，先查看要用哪些镜像：\ngrep \u0026#39;image:\u0026#39; calico.yaml | uniq image: calico/cni:v3.14.2 image: calico/pod2daemon-flexvol:v3.14.2 image: calico/node:v3.14.2 image: calico/kube-controllers:v3.14.2 对应的 arm64 版本，操作步骤参考上面，不再赘述。\ncalico/cni:v3.14.2-arm64 calico/pod2daemon-flexvol:v3.14.2-arm64 calico/node:v3.14.2-arm64 calico/kube-controllers:v3.14.2-arm64 搞定镜像之后执行：\nkubectl apply -f calico.yaml 之后就可以看到节点变成了 Ready 状态。\n测试 通常都是用 nginx 的镜像创建 pod 进行测试，但是 nginx 并没有 arm64 的版本，这里就用 docker 官方提供的 hello-world 镜像。没错，支持 arm64。\nkubectl run hello-world --image hello-world:latest --restart=Never kubectl logs hello-world --previous 可以看到\nHello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026quot;hello-world\u0026quot; image from the Docker Hub. (arm64v8) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ 总结 至此，我们就完成了在鲲鹏平台上基于 openEuler + iSula 部署 Kubernetes 的工作。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/setup-kubernetes-running-with-isulad-on-openeuler/","tags":["Kubernetes","Linux"],"title":"ARM64 平台基于 openEuler + iSula 环境部署 Kubernetes"},{"categories":["云原生"],"contents":"写在最前 和上一篇《使用 Flomesh 强化 Spring Cloud 服务治理》一样，这次同样是在无代码侵入的情况下对 Dubbo 服务治理的提升。\n更多治理场景陆续添加中，有兴趣的可关注 https://github.com/flomesh-io/service-mesh-dubbo-demo。\n开源的 Pipy 作为 Flomesh 的核心，得益于其轻量及灵活性可以通过编程的方式轻松快速的支持多中平台的服务发现机制，比如 Eureka、Consul、Nacos 等。\n概览 细节 环境搭建 搭建 Kubernetes 环境，可以选择 kubeadm 进行集群搭建。也可以选择 minikube、k3s、Kind 等，本文使用 k3s。\n使用 k3d 安装 k3s。k3d 将在 Docker 容器中运行 k3s，因此需要保证已经安装了 Docker。\n$ k3d cluster create dubbo-demo -p \u0026#34;80:80@loadbalancer\u0026#34; --k3s-server-arg \u0026#39;--no-deploy=traefik\u0026#39; 安装 Flomesh 从仓库 https://github.com/flomesh-io/service-mesh-dubbo-demo 克隆代码。进入到 release目录。\n所有 Flomesh 组件以及用于 demo 的 yamls 文件都位于这个目录中。\n$ kubectl apply -f artifacts/cert-manager-v1.3.1.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created 注意: 要保证 cert-manager 命名空间中所有的 pod 都正常运行：\n$ kubectl get pod -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-cainjector-59f76f7fff-ggmdm 1/1 Running 0 32s cert-manager-59f6c76f4b-r2h5r 1/1 Running 0 32s cert-manager-webhook-56fdcbb848-sdnxb 1/1 Running 0 32s 安装 Pipy Operator $ kubectl apply -f artifacts/pipy-operator.yaml 执行完命令后会看到类似的结果：\nnamespace/flomesh created customresourcedefinition.apiextensions.k8s.io/proxies.flomesh.io created customresourcedefinition.apiextensions.k8s.io/proxyprofiles.flomesh.io created serviceaccount/operator-manager created role.rbac.authorization.k8s.io/leader-election-role created clusterrole.rbac.authorization.k8s.io/manager-role created clusterrole.rbac.authorization.k8s.io/metrics-reader created clusterrole.rbac.authorization.k8s.io/proxy-role created rolebinding.rbac.authorization.k8s.io/leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/proxy-rolebinding created configmap/manager-config created service/operator-manager-metrics-service created service/proxy-injector-svc created service/webhook-service created deployment.apps/operator-manager created deployment.apps/proxy-injector created certificate.cert-manager.io/serving-cert created issuer.cert-manager.io/selfsigned-issuer created mutatingwebhookconfiguration.admissionregistration.k8s.io/mutating-webhook-configuration created mutatingwebhookconfiguration.admissionregistration.k8s.io/proxy-injector-webhook-cfg created validatingwebhookconfiguration.admissionregistration.k8s.io/validating-webhook-configuration created 注意：要保证 flomesh 命名空间中所有的 pod 都正常运行：\n$ kubectl get pod -n flomesh NAME READY STATUS RESTARTS AGE proxy-injector-6d5c774bc-rspmc 1/1 Running 0 21s operator-manager-c95cd449-xxc77 0/1 Running 0 38s 安装 Ingress 控制器：ingress-pipy $ kubectl apply -f artifacts/ingress-pipy.yaml namespace/ingress-pipy created customresourcedefinition.apiextensions.k8s.io/ingressglobalhooks.flomesh.io created customresourcedefinition.apiextensions.k8s.io/ingressrules.flomesh.io created serviceaccount/ingress-pipy created role.rbac.authorization.k8s.io/ingress-pipy-leader-election-role created clusterrole.rbac.authorization.k8s.io/ingress-pipy-role created rolebinding.rbac.authorization.k8s.io/ingress-pipy-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/ingress-pipy-rolebinding created configmap/ingress-config created service/ingress-pipy-cfg created service/ingress-pipy-controller created service/ingress-pipy-defaultbackend created service/webhook-service created deployment.apps/ingress-pipy-cfg created deployment.apps/ingress-pipy-controller created deployment.apps/ingress-pipy-manager created certificate.cert-manager.io/serving-cert created issuer.cert-manager.io/selfsigned-issuer created mutatingwebhookconfiguration.admissionregistration.k8s.io/mutating-webhook-configuration configured validatingwebhookconfiguration.admissionregistration.k8s.io/validating-webhook-configuration configured 检查 ingress-pipy 命名空间下 pod 的状态：\n$ kubectl get pod -n ingress-pipy NAME READY STATUS RESTARTS AGE svclb-ingress-pipy-controller-qwbk9 1/1 Running 0 90s ingress-pipy-cfg-6c54d5b9b6-6s7lz 1/1 Running 0 90s ingress-pipy-manager-7988dfbf4f-lxr4b 1/1 Running 0 90s ingress-pipy-controller-9d4698887-zrpfd 1/1 Running 0 90s 至此，你已经成功安装 Flomesh 的所有组件，包括 operator 和 ingress 控制器。\n运行 Demo 创建命名空间 Demo 运行在另一个独立的命名空间 flomesh-dubbo 中，执行命令 kubectl apply -f dubbo-mesh/templates/namespace.yaml 来创建该命名空间。如果你 describe 该命名空间你会发现其使用了 flomesh.io/inject=true 标签。\n这个标签告知 operator 的 admission webHook 拦截标注的命名空间下 pod 的创建。\n$ kubectl describe ns flomesh-dubbo Name: flomesh-dubbo Labels: app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=dubbo-mesh app.kubernetes.io/version=1.19.0 flomesh.io/inject=true helm.sh/chart=dubbo-mesh-0.1.0 kubernetes.io/metadata.name=flomesh-dubbo Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource. 创建 ProxyProfile 资源 $ kubectl apply -f artifacts/proxy-profile.yaml proxyprofile.flomesh.io/poc-pf-dubbo created proxyprofile.flomesh.io/poc-pf-http created 创建 mock 服务 $ kubectl apply -f dubbo-mesh/templates/configmap-mock.yaml $ kubectl apply -f dubbo-mesh/templates/configmap-proxychains.yaml $ kubectl apply -f dubbo-mesh/templates/deployment-mock.yaml $ kubectl apply -f dubbo-mesh/templates/service-mock.yaml 部署服务 $ kubectl apply -f artifacts/deployment.yaml 测试 准备 访问 demo 服务都要通过 ingress 控制器。因此需要先获取 LB 的 ip 地址。\n//Obtain the controller IP //Here, we append port. ingressAddr=`kubectl get svc ingress-pipy-controller -n ingress-pipy -o jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;`:80 这里我们使用了是 k3d 创建的 k3s，命令中加入了 -p 80:80@loadbalancer 选项。我们可以使用 127.0.0.1:80 来访问 ingress 控制器。这里执行命令 ingressAddr=127.0.0.1:80。\nIngress 规则中，我们为每个规则指定了 host，因此每个请求中需要通过 HTTP 请求头 Host 提供对应的 host。\n或者在 /etc/hosts 添加记录：\n$ kubectl get ing ingress-canary-router -n flomesh-dubbo -o jsonpath=\u0026#34;{range .spec.rules[*]}{.host}{\u0026#39;\\n\u0026#39;}\u0026#34; dubbo.demo.flomesh.cn //添加记录到 /etc/hosts 127.0.0.1 dubbo.demo.flomesh.cn 灰度 v1、v2 服务只能访问对应版本的服务。\n$ curl --location --request POST \u0026#39;http://127.0.0.1:80/hello\u0026#39; \\ --header \u0026#39;Host: dubbo.demo.flomesh.cn\u0026#39; \\ --header \u0026#39;x-canary-version: v1\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Flomesh\u0026#34;}\u0026#39; V1-[hello-service] : Hello, Flomesh, Today is (2021-08-17), Time is (04:06:56.823) $ curl --location --request POST \u0026#39;http://127.0.0.1:80/hello\u0026#39; \\ --header \u0026#39;Host: dubbo.demo.flomesh.cn\u0026#39; \\ --header \u0026#39;x-canary-version: v2\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;Flomesh\u0026#34;}\u0026#39; V2-[hello-service] : Hello, Flomesh, Today is (Tue, 2021-Aug-17), Time is (04:06:37 +0000) ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/enhance-dubbo-service-governance-with-flomesh/","tags":["Pipy","Service Mesh","Kubernetes","Java"],"title":"使用 Flomesh 进行 Dubbo 服务治理"},{"categories":["云原生"],"contents":"写在最前 这篇是关于如何使用 Flomesh 服务网格来强化 Spring Cloud 的服务治理能力，降低 Spring Cloud 微服务架构落地服务网格的门槛，实现“自主可控”。\n文档在 github 上持续更新，欢迎大家一起讨论：https://github.com/flomesh-io/flomesh-bookinfo-demo。\n 架构 环境搭建 搭建 Kubernetes 环境，可以选择 kubeadm 进行集群搭建。也可以选择 minikube、k3s、Kind 等，本文使用 k3s。\n使用 k3d 安装 k3s。k3d 将在 Docker 容器中运行 k3s，因此需要保证已经安装了 Docker。\n$ k3d cluster create spring-demo -p \u0026#34;81:80@loadbalancer\u0026#34; --k3s-server-arg \u0026#39;--no-deploy=traefik\u0026#39; 安装 Flomesh 从仓库 https://github.com/flomesh-io/flomesh-bookinfo-demo.git 克隆代码。进入到 flomesh-bookinfo-demo/kubernetes目录。\n所有 Flomesh 组件以及用于 demo 的 yamls 文件都位于这个目录中。\n安装 Cert Manager $ kubectl apply -f artifacts/cert-manager-v1.3.1.yaml customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created namespace/cert-manager created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created 注意: 要保证 cert-manager 命名空间中所有的 pod 都正常运行：\n$ kubectl get pod -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-webhook-56fdcbb848-q7fn5 1/1 Running 0 98s cert-manager-59f6c76f4b-z5lgf 1/1 Running 0 98s cert-manager-cainjector-59f76f7fff-flrr7 1/1 Running 0 98s 安装 Pipy Operator $ kubectl apply -f artifacts/pipy-operator.yaml 执行完命令后会看到类似的结果：\nnamespace/flomesh created customresourcedefinition.apiextensions.k8s.io/proxies.flomesh.io created customresourcedefinition.apiextensions.k8s.io/proxyprofiles.flomesh.io created serviceaccount/operator-manager created role.rbac.authorization.k8s.io/leader-election-role created clusterrole.rbac.authorization.k8s.io/manager-role created clusterrole.rbac.authorization.k8s.io/metrics-reader created clusterrole.rbac.authorization.k8s.io/proxy-role created rolebinding.rbac.authorization.k8s.io/leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/proxy-rolebinding created configmap/manager-config created service/operator-manager-metrics-service created service/proxy-injector-svc created service/webhook-service created deployment.apps/operator-manager created deployment.apps/proxy-injector created certificate.cert-manager.io/serving-cert created issuer.cert-manager.io/selfsigned-issuer created mutatingwebhookconfiguration.admissionregistration.k8s.io/mutating-webhook-configuration created mutatingwebhookconfiguration.admissionregistration.k8s.io/proxy-injector-webhook-cfg created validatingwebhookconfiguration.admissionregistration.k8s.io/validating-webhook-configuration created 注意：要保证 flomesh 命名空间中所有的 pod 都正常运行：\n$ kubectl get pod -n flomesh NAME READY STATUS RESTARTS AGE proxy-injector-5bccc96595-spl6h 1/1 Running 0 39s operator-manager-c78bf8d5f-wqgb4 1/1 Running 0 39s 安装 Ingress 控制器：ingress-pipy $ kubectl apply -f ingress/ingress-pipy.yaml namespace/ingress-pipy created customresourcedefinition.apiextensions.k8s.io/ingressparameters.flomesh.io created serviceaccount/ingress-pipy created role.rbac.authorization.k8s.io/ingress-pipy-leader-election-role created clusterrole.rbac.authorization.k8s.io/ingress-pipy-role created rolebinding.rbac.authorization.k8s.io/ingress-pipy-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/ingress-pipy-rolebinding created configmap/ingress-config created service/ingress-pipy-cfg created service/ingress-pipy-controller created service/ingress-pipy-defaultbackend created service/webhook-service created deployment.apps/ingress-pipy-cfg created deployment.apps/ingress-pipy-controller created deployment.apps/ingress-pipy-manager created certificate.cert-manager.io/serving-cert created issuer.cert-manager.io/selfsigned-issuer created mutatingwebhookconfiguration.admissionregistration.k8s.io/mutating-webhook-configuration configured validatingwebhookconfiguration.admissionregistration.k8s.io/validating-webhook-configuration configured 检查 ingress-pipy 命名空间下 pod 的状态：\n$ kubectl get pod -n ingress-pipy NAME READY STATUS RESTARTS AGE svclb-ingress-pipy-controller-8pk8k 1/1 Running 0 71s ingress-pipy-cfg-6bc649cfc7-8njk7 1/1 Running 0 71s ingress-pipy-controller-76cd866d78-m7gfp 1/1 Running 0 71s ingress-pipy-manager-5f568ff988-tw5w6 0/1 Running 0 70s 至此，你已经成功安装 Flomesh 的所有组件，包括 operator 和 ingress 控制器。\n中间件 Demo 需要用到中间件完成日志和统计数据的存储，这里为了方便使用 pipy 进行 mock：直接在控制台中打印数据。\n另外，服务治理相关的配置有 mock 的 pipy config 服务提供。\nlog \u0026amp; metrics $ cat \u0026gt; middleware.js \u0026lt;\u0026lt;EOF pipy() .listen(8123) .link(\u0026#39;mock\u0026#39;) .listen(9001) .link(\u0026#39;mock\u0026#39;) .pipeline(\u0026#39;mock\u0026#39;) .decodeHttpRequest() .replaceMessage( req =\u0026gt; ( console.log(req.body.toString()), new Message(\u0026#39;OK\u0026#39;) ) ) .encodeHttpResponse() EOF $ docker run --rm --name middleware --entrypoint \u0026#34;pipy\u0026#34; -v ${PWD}:/script -p 8123:8123 -p 9001:9001 flomesh/pipy-pjs:0.4.0-118 /script/middleware.js pipy config $ cat \u0026gt; mock-config.json \u0026lt;\u0026lt;EOF { \u0026#34;ingress\u0026#34;: {}, \u0026#34;inbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1, \u0026#34;circuitBreak\u0026#34;: false, \u0026#34;blacklist\u0026#34;: [] }, \u0026#34;outbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1 } } EOF $ cat \u0026gt; mock.js \u0026lt;\u0026lt;EOF pipy({ _CONFIG_FILENAME: \u0026#39;mock-config.json\u0026#39;, _serveFile: (req, filename, type) =\u0026gt; ( new Message( { bodiless: req.head.method === \u0026#39;HEAD\u0026#39;, headers: { \u0026#39;etag\u0026#39;: os.stat(filename)?.mtime | 0, \u0026#39;content-type\u0026#39;: type, }, }, req.head.method === \u0026#39;HEAD\u0026#39; ? null : os.readFile(filename), ) ), _router: new algo.URLRouter({ \u0026#39;/config\u0026#39;: req =\u0026gt; _serveFile(req, _CONFIG_FILENAME, \u0026#39;application/json\u0026#39;), \u0026#39;/*\u0026#39;: () =\u0026gt; new Message({ status: 404 }, \u0026#39;Not found\u0026#39;), }), }) // Config .listen(9000) .decodeHttpRequest() .replaceMessage( req =\u0026gt; ( _router.find(req.head.path)(req) ) ) .encodeHttpResponse() EOF $ docker run --rm --name mock --entrypoint \u0026#34;pipy\u0026#34; -v ${PWD}:/script -p 9000:9000 flomesh/pipy-pjs:0.4.0-118 /script/mock.js 运行 Demo Demo 运行在另一个独立的命名空间 flomesh-spring 中，执行命令 kubectl apply -f base/namespace.yaml 来创建该命名空间。如果你 describe 该命名空间你会发现其使用了 flomesh.io/inject=true 标签。\n这个标签告知 operator 的 admission webHook 拦截标注的命名空间下 pod 的创建。\n$ kubectl describe ns flomesh-spring Name: flomesh-spring Labels: app.kubernetes.io/name=spring-mesh app.kubernetes.io/version=1.19.0 flomesh.io/inject=true kubernetes.io/metadata.name=flomesh-spring Annotations: \u0026lt;none\u0026gt; Status: Active No resource quota. No LimitRange resource. 我们首先看下 Flomesh 提供的 CRD ProxyProfile。这个 demo 中，其定义了 sidecar 容器片段以及所使用的的脚本。检查 sidecar/proxy-profile.yaml 获取更多信息。执行下面的命令，创建 CRD 资源。\n$ kubectl apply -f sidecar/proxy-profile.yaml 检查是否创建成功：\n$ kubectl get pf -o wide NAME NAMESPACE DISABLED SELECTOR CONFIG AGE proxy-profile-002-bookinfo flomesh-spring false {\u0026#34;matchLabels\u0026#34;:{\u0026#34;sys\u0026#34;:\u0026#34;bookinfo-samples\u0026#34;}} {\u0026#34;flomesh-spring\u0026#34;:\u0026#34;proxy-profile-002-bookinfo-fsmcm-b67a9e39-0418\u0026#34;} 27s As the services has startup dependencies, you need to deploy it one by one following the strict order. Before starting, check the Endpoints section of base/clickhouse.yaml.\n提供中间件的访问 endpoid，将 base/clickhouse.yaml、base/metrics.yaml 和 base/config.yaml 中的 ip 地址改为本机的 ip 地址（不是 127.0.0.1）。\n修改之后，执行如下命令：\n$ kubectl apply -f base/clickhouse.yaml $ kubectl apply -f base/metrics.yaml $ kubectl apply -f base/config.yaml $ kubectl get endpoints samples-clickhouse samples-metrics samples-config NAME ENDPOINTS AGE samples-clickhouse 192.168.1.101:8123 3m samples-metrics 192.168.1.101:9001 3s samples-config 192.168.1.101:9000 3m 部署注册中心 $ kubectl apply -f base/discovery-server.yaml 检查注册中心 pod 的状态，确保 3 个容器都运行正常。\n$ kubectl get pod NAME READY STATUS RESTARTS AGE samples-discovery-server-v1-85798c47d4-dr72k 3/3 Running 0 96s 部署配置中心 $ kubectl apply -f base/config-service.yaml 部署 API 网关以及 bookinfo 相关的服务 $ kubectl apply -f base/bookinfo-v1.yaml $ kubectl apply -f base/bookinfo-v2.yaml $ kubectl apply -f base/productpage-v1.yaml $ kubectl apply -f base/productpage-v2.yaml 检查 pod 状态，可以看到所有 pod 都注入了容器。\n$ kubectl get pods samples-discovery-server-v1-85798c47d4-p6zpb 3/3 Running 0 19h samples-config-service-v1-84888bfb5b-8bcw9 1/1 Running 0 19h samples-api-gateway-v1-75bb6456d6-nt2nl 3/3 Running 0 6h43m samples-bookinfo-ratings-v1-6d557dd894-cbrv7 3/3 Running 0 6h43m samples-bookinfo-details-v1-756bb89448-dxk66 3/3 Running 0 6h43m samples-bookinfo-reviews-v1-7778cdb45b-pbknp 3/3 Running 0 6h43m samples-api-gateway-v2-7ddb5d7fd9-8jgms 3/3 Running 0 6h37m samples-bookinfo-ratings-v2-845d95fb7-txcxs 3/3 Running 0 6h37m samples-bookinfo-reviews-v2-79b4c67b77-ddkm2 3/3 Running 0 6h37m samples-bookinfo-details-v2-7dfb4d7c-jfq4j 3/3 Running 0 6h37m samples-bookinfo-productpage-v1-854675b56-8n2xd 1/1 Running 0 7m1s samples-bookinfo-productpage-v2-669bd8d9c7-8wxsf 1/1 Running 0 6m57s 添加 Ingress 规则 执行如下命令添加 Ingress 规则。\n$ kubectl apply -f ingress/ingress.yaml 测试前的准备 访问 demo 服务都要通过 ingress 控制器。因此需要先获取 LB 的 ip 地址。\n//Obtain the controller IP //Here, we append port. ingressAddr=`kubectl get svc ingress-pipy-controller -n ingress-pipy -o jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;`:81 这里我们使用了是 k3d 创建的 k3s，命令中加入了 -p 81:80@loadbalancer 选项。我们可以使用 127.0.0.1:81 来访问 ingress 控制器。这里执行命令 ingressAddr=127.0.0.1:81。\nIngress 规则中，我们为每个规则指定了 host，因此每个请求中需要通过 HTTP 请求头 Host 提供对应的 host。\n或者在 /etc/hosts 添加记录：\n$ kubectl get ing ingress-pipy-bookinfo -n flomesh-spring -o jsonpath=\u0026#34;{range .spec.rules[*]}{.host}{\u0026#39;\\n\u0026#39;}\u0026#34; api-v1.flomesh.cn api-v2.flomesh.cn fe-v1.flomesh.cn fe-v2.flomesh.cn //添加记录到 /etc/hosts 127.0.0.1 api-v1.flomesh.cn api-v2.flomesh.cn fe-v1.flomesh.cn fe-v2.flomesh.cn 验证 $ curl http://127.0.0.1:81/actuator/health -H \u0026#39;Host: api-v1.flomesh.cn\u0026#39; {\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;liveness\u0026#34;,\u0026#34;readiness\u0026#34;]} //OR $ curl http://api-v1.flomesh.cn:81/actuator/health {\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;,\u0026#34;groups\u0026#34;:[\u0026#34;liveness\u0026#34;,\u0026#34;readiness\u0026#34;]} 测试 灰度 在 v1 版本的服务中，我们为 book 添加 rating 和 review。\n# rate a book $ curl -X POST http://$ingressAddr/bookinfo-ratings/ratings \\ \t-H \u0026#34;Content-Type: application/json\u0026#34; \\ \t-H \u0026#34;Host: api-v1.flomesh.cn\u0026#34; \\ \t-d \u0026#39;{\u0026#34;reviewerId\u0026#34;:\u0026#34;9bc908be-0717-4eab-bb51-ea14f669ef20\u0026#34;,\u0026#34;productId\u0026#34;:\u0026#34;2099a055-1e21-46ef-825e-9e0de93554ea\u0026#34;,\u0026#34;rating\u0026#34;:3}\u0026#39; $ curl http://$ingressAddr/bookinfo-ratings/ratings/2099a055-1e21-46ef-825e-9e0de93554ea -H \u0026#34;Host: api-v1.flomesh.cn\u0026#34; # review a book $ curl -X POST http://$ingressAddr/bookinfo-reviews/reviews \\ \t-H \u0026#34;Content-Type: application/json\u0026#34; \\ \t-H \u0026#34;Host: api-v1.flomesh.cn\u0026#34; \\ \t-d \u0026#39;{\u0026#34;reviewerId\u0026#34;:\u0026#34;9bc908be-0717-4eab-bb51-ea14f669ef20\u0026#34;,\u0026#34;productId\u0026#34;:\u0026#34;2099a055-1e21-46ef-825e-9e0de93554ea\u0026#34;,\u0026#34;review\u0026#34;:\u0026#34;This was OK.\u0026#34;,\u0026#34;rating\u0026#34;:3}\u0026#39; $ curl http://$ingressAddr/bookinfo-reviews/reviews/2099a055-1e21-46ef-825e-9e0de93554ea -H \u0026#34;Host: api-v1.flomesh.cn\u0026#34; 执行上面的命令之后，我们可以在浏览器中访问前端服务（http://fe-v1.flomesh.cn:81/productpage?u=normal、 http://fe-v2.flomesh.cn:81/productpage?u=normal），只有 v1 版本的前端中才能看到刚才添加的记录。\n熔断 这里熔断我们通过修改 mock-config.json 中的 inbound.circuitBreak 为 true，来将服务强制开启熔断：\n{ \u0026#34;ingress\u0026#34;: {}, \u0026#34;inbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1, \u0026#34;circuitBreak\u0026#34;: true, //here \u0026#34;blacklist\u0026#34;: [] }, \u0026#34;outbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1 } } $ curl http://$ingressAddr/actuator/health -H \u0026#39;Host: api-v1.flomesh.cn\u0026#39; HTTP/1.1 503 Service Unavailable Connection: keep-alive Content-Length: 27 Service Circuit Break Open 限流 修改 pipy config 的配置，将 inbound.rateLimit 设置为 1。\n{ \u0026#34;ingress\u0026#34;: {}, \u0026#34;inbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: 1, //here \u0026#34;dataLimit\u0026#34;: -1, \u0026#34;circuitBreak\u0026#34;: false, \u0026#34;blacklist\u0026#34;: [] }, \u0026#34;outbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1 } } 我们使用 wrk 模拟发送请求，20 个连接、20 个请求、持续 30s：\n$ wrk -t20 -c20 -d30s --latency http://$ingressAddr/actuator/health -H \u0026#39;Host: api-v1.flomesh.cn\u0026#39; Running 30s test @ http://127.0.0.1:81/actuator/health 20 threads and 20 connections Thread Stats Avg Stdev Max +/- Stdev Latency 951.51ms 206.23ms 1.04s 93.55% Req/Sec 0.61 1.71 10.00 93.55% Latency Distribution 50% 1.00s 75% 1.01s 90% 1.02s 99% 1.03s 620 requests in 30.10s, 141.07KB read Requests/sec: 20.60 Transfer/sec: 4.69KB 从结果来看 20.60 req/s，即每个连接 1 req/s。\n黑白名单 将 pipy config 的 mock-config.json 做如下修改：ip 地址使用的是 ingress controller 的 pod ip。\n$ kgpo -n ingress-pipy ingress-pipy-controller-76cd866d78-4cqqn -o jsonpath=\u0026#39;{.status.podIP}\u0026#39; 10.42.0.78 { \u0026#34;ingress\u0026#34;: {}, \u0026#34;inbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1, \u0026#34;circuitBreak\u0026#34;: false, \u0026#34;blacklist\u0026#34;: [\u0026#34;10.42.0.78\u0026#34;] //here }, \u0026#34;outbound\u0026#34;: { \u0026#34;rateLimit\u0026#34;: -1, \u0026#34;dataLimit\u0026#34;: -1 } } 还是访问网关的接口\ncurl http://$ingressAddr/actuator/health -H \u0026#39;Host: api-v1.flomesh.cn\u0026#39; HTTP/1.1 503 Service Unavailable content-type: text/plain Connection: keep-alive Content-Length: 20 Service Unavailable ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/enhance-springcloud-service-governance-with-flomesh/","tags":["Service Mesh","Spring","Java"],"title":"使用 Flomesh 强化 Spring Cloud 服务治理"},{"categories":["翻译"],"contents":"本文由本人翻译自 Bilgin Ibryam 的 A Framework for Open Source Evaluation，首发在云原生社区博客。\n 如今，真假开源无处不在。最近开源项目转为闭源的案例越来越多，同时也有不少闭源项目（按照 OSI 定义）像开源一样构建社区的例子。这怎么可能，开源项目不应该始终如此吗？\n开源不是非黑即白，它具有开放性、透明、协作性和信任性的多个维度。有些开源是 Github 上的任何项目，有些必须通过 OSI 定义，有些是必须遵守不成文但普遍接受的开源规范。这里通过看一些商业和技术方面，再讨论社区管理习惯，来同大家分享一下我对评估开源项目的看法。\n免责声明  这些是我的个人观点，与我的雇主或我所属的软件基金会和项目无关。 这不是法律或专业意见（我不是律师，也不是专门从事 OSS 评估的），而是外行的意见。 更新：我收到了多位开源律师的反馈并更新了文章！ 这篇博文由订阅和分享按钮赞助，点击这些按钮表示支持。  知识产权 关于“开源”项目的第一个问题是关于知识产权的所有权。好消息是，即使不了解这些法律含义，你可以应用一个简单的 Litmus 测试。该项目是否属于你信任的信誉良好的开源基金会？例如，FSF 拥有其托管项目的版权，更多情况下拥有基金会（如 ASF、LF) 通过贡献者许可协议，聚合对其项目的贡献许可权。在任何一种情况下，你都可以相信他们将充当良好的去中心化管家，并且不会在一夜之间改变项目的未来方向。如果一个项目不属于信誉良好的软件基金会，而是由一家公司提供支持，那么问题是你是否信任该公司作为供应链合作伙伴。如果这些问题的答案是肯定的，请转到下一部分。如果答案是否定的，那么你最好调查一下版权所有者是谁，以及他们对你的长期前景和潜在风险是什么。今天的单一供应商开源项目，明天可能会变成闭源。\n许可 商标出现在许可之前的原因是软件的权利人（通常是作者）通过许可授予最终用户使用一个或多个软件副本的许可。自由软件许可证是一种说明，它授予源代码或其二进制形式的使用者修改和重新分发该软件的权利。如果没有许可，这些行为将受到版权法的禁止。这里的重点是权利人可以改变主意并更改许可。权利持有人可以决定在多个许可证下分发软件或随时将许可证更改为非开源许可证。该软件也可能在公共领域，在这种情况下，它不受版权法的限制。公共领域并不等同于开源许可证，这是一种不太流行的方法，我们可以在这里忽略。\n同样，如果不是律师，这是一个外行对许可的 Litmus 测试：该项目是否根据 OSI 批准的许可清单获得的许可？如果答案是肯定的，那么你可以依靠这些基金会的尽职调查来审查、分类许可并指出任何限制。如果答案是否定的，请让你公司的律师来查看和解释许可上的每个字以及可能的许可兼容性影响。\n治理 在余下的检查中，我们正在从更多的商业和法律方面转向涉及开源项目领域的技术和社区。\n假设不担心商标持有方（未来的合作伙伴）、许可（使用开源软件的条款），下一个问题是治理。治理是项目决定谁来做什么、他们应该如何做以及何时做的规则或习惯。它定义了与不同项目角色相关的职责、特权和权限，以及人们如何分配到角色和从角色中删除。此处的示例是小型日常活动，例如谁有权批准拉取请求、投票给候选发布、就项目架构达成共识、定义项目路线图以及选举项目治理委员会。\n如果你正在评估对你的组织具有战略意义的项目，你想知道谁负责。不仅如此，你甚至可能希望你的开发人员对项目的方向有发言权。\n还有一个简单的 Litmus 测试：对于开源基金会的项目，对于谁可以对重要决策进行投票，以及如何成为决策委员会的一部分，都有明确的规则。在某些基金会（例如 ASF）中，它基于社区成员的个人功绩，而在某些基金会（例如CNCF ）中，它从成为付费成员组织的员工开始。在基于区块链的开源项目中，它是基于令牌（Token）的投票持有人。其他基金会有不同的规则，但都力求在多个参与者之间实现中立和权力下放。如果一个项目由一家公司或一个人管理，你相信他们会为项目和社区的利益做出最佳决策。其中一些项目可能已经写下了他们遵循的治理规则，而有些可能根本没有。由你来确定治理动态及其对你的项目参与的重要性。除了具有治理透明度和公开决策之外，另一个方面是治理机构的信任度和声誉。当你查看项目的治理委员会时，是否有一位或一组具有经过验证的技术和社交技能的领导者，让你相信他们可以将项目提升到一个新的水平？或者你是否看到一个在政治斗争中不断争论的团体？这些是开源项目是否会成功并长期发展的一些指标，还是可以预期的头痛和停滞。\n基础设施 拥有开源许可可能在技术上有资格作为开源项目，但这并不能说明项目是否以开源方式构建。有许多在 OSI 批准的许可下发布的软件示例，但它们是在封闭的基础设施之后开发的。通过基础设施，我的意思是用户快速提问的聊天频道。进行更深入的开发人员讨论的论坛和邮件列表。审查拉取请求的源代码管理系统，以及运行测试和每晚创建二进制文件的构建服务器。\n对于关注开源项目的商务人士和律师来说，这些可能并不重要，但对于将要使用开源项目的技术人员来说，这些是一些假设的好处。这里要做的检查是探索软件是否是使用开放式基础设施以开源方式开发的，而不是闭门造车。以下是几个示例问题：\n 用户可以在项目聊天中提出问题并在没有中间人的情况下从另一个用户那里得到答案吗？ 开发人员能否与项目提交者联系并获得深入的技术问题的答案？ 你能否运行最新版本并确认已知的错误已修复？ 架构师可以参加每周一次的社区电话会议并确定项目的未来方向吗？（原文 Can an architect the weekly community call and figure out the future direction of the project? ）  对于封闭的基础架构，你必须创建支持工单并付费才能获得类似问题的答案。通过开放的基础设施和开放的参与，那些知道如何以开源方式工作的人可以获得答案。\n社区和采用 开源软件的主要好处之一是它允许好创意的发展和传播。你可能拥有最先进的技术、最宽松的许可和开放式开发，但如果该软件没有不断壮大的社区和不断提高的采用率，那就是一个值得调查的迹象。不同的项目会有不同的采用率。有些可能会迅速成长为主流或被其他同类型项目所取代。一些项目可能有一个小但持续的增长率和一个持续数十年的生态社区。社区规模和采用率是开源项目的最终寿命指标。以下是你可以提出的一些示例问题：\n 项目中有多少活跃的开发人员（提交者），平均提交率是多少？ 上个月有多少用户订阅了用户论坛以及提出了多少问题？ 软件的最新稳定版本已被下载多少次？ 还有哪些项目和服务依赖并使用这个项目？ 有多少商业组织支持这个项目？ 是否有商业组织围绕它提供产品、支持和服务？ 这个项目有多少 StackOverflow 问题？ 有多少书籍、会议演讲和职位描述提到了这个项目？  执行这些问题会给你一个指示，即该项目是在增长并成为其领域的事实上的标准，还是停滞不前并可能被下一个大项目所取代。\n通常，开源与快节奏的开发和创新有关。同时，开源也是一种创建广泛采用和创建非官方标准的机制。许多开源项目已经变成了标准，例如用于容器编排的 Kubernetes、用于流处理的 Apache Kafka、用于 Web 服务器的 Apache httpd 等。软件中最昂贵的事情之一是找到具有合适技能的人。使用采用率高的开源项目将使你有更好的机会找到技术娴熟的人，并让他们能够更长时间地重复使用他们的技能。\n总结 根据开源项目的关键程度，有不同的风险和评估标准。对于战略性的、难以替代的项目，这将是你的 IT 基础设施的基础，你需要是已经成为其领域事实上的开源标准的完善项目。在这里确定谁拥有该项目的商标以及谁将成为你的长期合作伙伴非常重要。通常，这些合作伙伴是项目所属软件基金会的成员组织或持有项目 IP 的单个公司。对于后者，你可能需要考虑长期风险，例如核心开发人员分叉项目的机会、提供项目即服务的超大规模者、公司收购等。\n对于交付速度最重要的非战略性、战术性、短期项目，你可以让你的开发人员根据开放性、社区协作和热度（对于某些前端技术很重要）来推动选择和挑选项目。在这里，定期的安全修复、开发人员支持和许可兼容性检查等中短期风险可能就足够了。\n在任何一种情况下，都没有适合所有情况的单一评估标准。你必须在长期商业风险、技术稳定性与最新热度、创新和开发人员满意度之间取得平衡。这里的框架将为你概括需要探索的领域和需要考虑的一些风险。祝你好运！\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/a-framework-for-open-source-evaluation/","tags":["OpenSource"],"title":"开源评估框架"},{"categories":["生活"],"contents":" 趁着换工作的间隙，来一场“蓄谋已久”、“说走就走”的旅行。\n 新疆，是一个美丽而遥远的地方。前段时间看了李娟的《冬牧场》，更是提起了我对新疆对游牧哈萨克族的兴趣。以至后来梦中见到一望无际的戈壁滩，让自己下定决心做出改变。\n背景 这次北疆之行准备仓促，从路线方案到寻找同伴，都是出发前一周搞定的。\n同伴找的是云原生社区的 宋净超 Jimmy，同样也是老乡（两人在外漂多年，沟通全是普通话。。。），他从北京出发。由于他的工作性质特别\u0026ndash;远程工作，所以当初临时找同伴的时候想到了他，而且还也在有驾照在手。也由于他需要远程工作，这次我们选择了房车自驾，正好本人也想体验一下房车。\n路书发在了这里，全程 2800km+。\n建议 先说总结，假如要你跟我走同样的路线而且喜欢驾驶，不建议房车（三围太大）。理由：\n 山路多，各种的弯路，而且路的一侧就是悬崖（我好像还拍过一段 25 分钟的视频） 昭苏到琼库什台一段的山路，很窄！ 车太重，爬坡非常慢，坡太多！ 车高重心高，戈壁上高速公路横风有时比较强，车子晃动厉害（尤其是吐鲁番至乌鲁木齐的风电区域）；即使没有横风，在超大货车的时候也会有同样的感觉  房车的居住体验还是不错的（人多不一定），随停随住。\n我的理想方案（下次考虑这么来）：\n非房车 这次的路线，商业化迹象很明显，尤其是独库公路那条线。没有房车，吃住也不需要担心。\n 吃：首选牧民家，体验最好；再就是各种餐馆，只要是本地人开的，都不会差；自己做？（这次带了灶具，一次没做过） 住：酒店 \u0026gt; 牧民蒙古包（商业性的） \u0026gt; 自带帐篷（应急+体验）  不开房车，伊昭、独库公路和其他的山路都会轻松，体验更好。假如是 SUV，底盘高也方便深入草原。注意，为了保护草原牧民的操场车辆不能随便进。可以多问下牧民，付费吃住的话应该没问题。\n房车 房车适合宿在草原。假如，真想体验房车，我的建议就是：乌鲁木齐飞伊宁租车，直奔琼库什台村（这段山路实在避不开）。看过琼库什台的原生态草原，其他的草原也就没了兴趣。在那里安静住几天，骑骑马。\n行程 本来想写写的，有点多无从下手，看路书凑合下吧。先放上列表，有时间我再完善。\n Day01 广州 - 乌鲁木齐 Day02 乌鲁木齐 - 赛里木湖 Day03 赛里木湖 - 昭苏草原 Day04 昭苏草原 - 琼库什台村 Day05 琼库什台村 - 那拉提景区 Day06 那拉提 - 独库公路南段 - 库车市 Day07 库车市 - 和硕县 Day08 和硕县 - 吐鲁番市 - 乌鲁木齐  视频  Jimmy 剪辑的视频 独库公路（有点枯燥，想看的单独找我要吧）  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/2021-xinjiang-trip/","tags":["旅行","自驾"],"title":"游记 - 2021 北疆之行"},{"categories":["翻译"],"contents":"有别于前些天的文章 - 常用的几款工具让 Kubernetes 集群上的工作更容易 偏重于工具类来提升工作效率，今天这篇文章更加适合用来做选型时的参考。\n文档翻译自 Kubernetes Essential Tools: 2021，篇幅较长，做了部分增删。\n 介绍 在本文中，我将尝试总结我最喜欢的 Kubernetes 工具，并特别强调最新的和鲜为人知但我认为会非常流行的工具。\n这只是我根据我的经验得出的个人清单，但为了避免偏见，我还将尝试提及每种工具的替代方案，以便你可以根据自己的需要进行比较和决定。我将尽可能缩短这篇文章并提供链接，以便你可以自行探索更多内容。我的目标是回答这个问题：“我如何在 Kubernetes 中做 X？” 通过描述不同软件开发任务的工具。\nK3D K3D 是我最喜欢的在笔记本电脑上运行 Kubernetes (K8s) 集群的方式。它非常轻巧且速度非常快。它是使用 Docker 围绕 K3S 的包装器。所以，你只需要 Docker 来运行它并且资源使用率非常低。唯一的问题是它不完全符合 K8s 标准，但这不应该是本地开发的问题。对于测试环境，你可以使用其他解决方案。K3D 比 Kind 快，但 Kind 完全兼容。\n备选  K3S 物联网或者边缘计算 Kind 完全兼容 Kubernetes 的备选 MicroK8s MiniKube  Krew Krew 是管理的必备工具 Kubectl 插件，这是一个必须有任何 K8S 用户。我不会详细介绍超过 145 个可用插件，但至少安装 kubens 和 kubectx。\nLens Lens 是适用于 SRE、Ops 和开发人员的 K8s IDE。它适用于任何 Kubernetes 发行版：本地或云端。它快速、易于使用并提供实时可观察性。使用 Lens 可以非常轻松地管理多个集群。如果你是集群操作员，这是必须的。\n备选  对于那些喜欢轻量级终端替代品的人来说，K9s 是一个很好的选择。K9s 会持续观察 Kubernetes 的变化，并提供后续命令来与你观察到的资源进行交互。  Helm Helm 不需要介绍，它是 Kubernetes 最著名的包管理器。你应该在 K8s 中使用包管理器，就像在编程语言中使用它一样。Helm 允许你将应用程序打包到 Charts 中，将复杂的应用程序抽象为易于定义、安装和更新的可重用简单组件。\n它还提供了强大的模板引擎。Helm 很成熟，有很多预定义的 charts，很好的支持，而且很容易使用。\n备选  Kustomize 是 helm 的一个更新和伟大的替代品，它不使用模板引擎，而是一个覆盖引擎，在其中你有基本的定义和覆盖在它们之上。  ArgoCD 我相信 GitOps 是过去十年中最好的想法之一。在软件开发中，我们应该使用单一的事实来源来跟踪构建软件所需的所有移动部分，而 Git 是做到这一点的完美工具。我们的想法是拥有一个 Git 存储库，其中包含应用程序代码以及表示所需生产环境状态的基础设施 (IaC) 的声明性描述；以及使所需环境与存储库中描述的状态相匹配的自动化过程。\n GitOps: versioned CI/CD on top of declarative infrastructure. Stop scripting and start shipping.\n— Kelsey Hightower\n 尽管使用 Terraform 或类似工具，你可以实现基础设施即代码（IaC），但这还不足以将你在 Git 中的所需状态与生产同步。我们需要一种方法来持续监控环境并确保没有配置漂移。使用 Terraform，你将不得不编写脚本来运行terraform apply并检查状态是否与 Terraform 状态匹配，但这既乏味又难以维护。\nKubernetes 从头开始​​构建控制循环的思想，这意味着 Kubernetes 一直在监视集群的状态以确保它与所需的状态匹配，例如，运行的副本数量与所需的数量相匹配复制品。GitOps 的想法是将其扩展到应用程序，因此你可以将你的服务定义为代码，例如，通过定义 Helm Charts，并使用利用 K8s 功能的工具来监控你的应用程序的状态并相应地调整集群。也就是说，如果更新你的代码存储库或Helm Chart，生产集群也会更新。这是真正的持续部署。核心原则是应用程序部署和生命周期管理应该自动化、可审计且易于理解。\n对我来说，这个想法是革命性的，如果做得好，将使组织能够更多地关注功能，而不是编写自动化脚本。这个概念可以扩展到软件开发的其他领域，例如，你可以将文档存储在代码中以跟踪更改的历史并确保文档是最新的；或使用ADR跟踪架构决策。\n在我看来，在最好的 GitOps 工具 Kubernetes 是 ArgoCD。你可以在此处阅读更多信息。ArgoCD 是 Argo 生态系统的一部分，其中包括一些其他很棒的工具，其中一些我们将在稍后讨论。\n使用 ArgoCD，你可以在代码存储库中拥有每个环境，你可以在其中定义该环境的所有配置。Argo CD 在指定的目标环境中自动部署所需的应用程序状态。\nArgoCD 被实现为一个 kubernetes 控制器，它持续监控正在运行的应用程序并将当前的实时状态与所需的目标状态（如 Git 存储库中指定的）进行比较。ArgoCD 报告并可视化差异，并且可以自动或手动将实时状态同步回所需的目标状态。\n备选  Flux 刚刚发布了一个具有许多改进的新版本。它提供了非常相似的功能。  Argo 工作流（Workflows）和 Argo 事件（Events） 在 Kubernetes 中，你可能还需要运行批处理作业或复杂的工作流。这可能是你的数据管道、异步流程甚至 CI/CD 的一部分。最重要的是，你甚至可能需要运行对某些事件做出反应的驱动微服务，例如文件上传或消息发送到队列。对于所有这些，我们有 Argo Workflows 和 Argo Events。\n尽管它们是独立的项目，但它们往往会被部署在一起。\nArgo Workflows 是一个类似于 Apache Airflow 的编排引擎，但它是 Kubernetes 原生的。它使用自定义 CRD 来定义复杂的工作流程，使用 YAML 的步骤或 DAG，这在 K8s 中感觉更自然。它有一个漂亮的 UI、重试机制、基于 cron 的作业、输入和输出跟踪等等。你可以使用它来编排数据管道、批处理作业等等。\n有时，你可能希望将管道与异步服务（如 Kafka 等流引擎、队列、webhooks 或深度存储服务）集成。例如，你可能想要对上传到 S3 的文件等事件做出反应。为此，你将使用 Argo 事件（Event）。\n这两个工具组合为你的所有管道需求提供了一个简单而强大的解决方案，包括 CI/CD 管道，它允许你在 Kubernetes 中本地运行 CI/CD 管道。\n备选  对于 ML 管道，你可以使用 Kubeflow。 对于 CI/CD 管道，你可以使用 Tekton。  Kaniko 我们刚刚看到了如何使用 Argo Workflows 运行 Kubernetes 原生 CI/CD 管道。一个常见的任务是构建 Docker 镜像，这在 Kubernetes 中通常是乏味的，因为构建过程实际上是在容器本身上运行的，你需要使用变通方法来使用主机的 Docker 引擎。\n底线是你不应该使用 Docker 来构建你的镜像：改用 Kanico。Kaniko 不依赖于 Docker 守护进程，而是完全在用户空间中执行 Dockerfile 中的每个命令。这使得在无法轻松或安全地运行 Docker 守护程序的环境中构建容器镜像成为可能，例如标准的 Kubernetes 集群。这消除了在 K8s 集群中构建镜像的所有问题。\nIstio Istio 是市场上最著名的服务网格，它是开源的并且非常受欢迎。我不会详细介绍什么是服务网格，因为它是一个巨大的话题，但是如果你正在构建微服务，并且可能应该这样做，那么你将需要一个服务网格来管理通信、可观察性、错误处理、安全性。与其用重复的逻辑污染每个微服务的代码（译者：SDK 侵入），不如利用服务网格为你做这件事。\n简而言之，服务网格是一个专用的基础设施层，你可以将其添加到你的应用程序中。它允许你透明地添加可观察性、流量管理和安全性等功能，而无需将它们添加到你自己的代码中。\n如果 Istio 用于运行微服务，尽管你可以在任何地方运行 Istio 并使用微服务，但 Kubernetes 已被一次又一次地证明是运行它们的最佳平台。Istio 还可以将你的 K8s 集群扩展到其他服务，例如 VM，允许你拥有在迁移到 Kubernetes 时非常有用的混合环境。\n备选  Linkerd 是一种更轻巧且可能更快的服务网格。Linkerd 从头开始​​为安全性而构建，包括默认 mTLS、使用 Rust 构建的数据平面、内存安全语言和定期安全审计等功能 Consul 是为任何运行时和云提供商构建的服务网格，因此它非常适合跨 K8s 和云提供商的混合部署。如果不是所有的工作负载都在 Kubernetes 上运行，这是一个不错的选择。  Argo Rollouts 我们已经提到，你可以使用 Kubernetes 使用 Argo Workflows 或使用 Kanico 构建图像的类似工具来运行 CI/CD 管道。下一个合乎逻辑的步骤是继续并进行持续部署。由于涉及高风险，这在真实场景中是极具挑战性的，这就是为什么大多数公司只做持续交付，这意味着他们已经实现了自动化，但他们仍然需要手动批准和验证，这个手动步骤是这是因为团队不能完全信任他们的自动化。\n那么，你如何建立这种信任以摆脱所有脚本并完全自动化从源代码到生产的所有内容？答案是：可观察性。你需要将资源更多地集中在指标上，并收集准确表示应用程序状态所需的所有数据。目标是使用一组指标来建立这种信任。如果你在 Prometheus 中拥有所有数据，那么你可以自动部署，因为你可以根据这些指标自动逐步推出应用程序。\n简而言之，你需要比 K8s 开箱即用的滚动更新更高级的部署技术。我们需要使用金丝雀部署进行渐进式交付。目标是逐步将流量路由到应用程序的新版本，等待收集指标，分析它们并将它们与预定义的规则进行匹配。如果一切正常，我们增加流量；如果有任何问题，我们会回滚部署。 要在 Kubernetes 中执行此操作，你可以使用提供 Canary 发布等的 Argo Rollouts 。\n Argo Rollouts 是一个 Kubernetes 控制器和一组 CRD，可提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和向 Kubernetes 的渐进式交付功能。\n 尽管像 Istio 这样的服务网格提供 Canary 发布，但 Argo Rollouts 使这个过程变得更加容易并且以开发人员为中心，因为它是专门为此目的而构建的。除此之外，Argo Rollouts 可以与任何服务网格集成。\nArgo Rollouts 功能：\n 蓝绿更新策略 金丝雀更新策略 细粒度、加权的流量转移 自动回滚和促销或人工判断 可定制的指标查询和业务 KPI 分析 入口控制器集成：NGINX、ALB 服务网格集成：Istio、Linkerd、SMI 指标提供者集成：Prometheus、Wavefront、Kayenta、Web、Kubernetes Jobs  备选  Istio 作为 Canary 版本的服务网格。Istio 不仅仅是一个渐进式交付工具，它还是一个完整的服务网格。Istio 不会自动部署，Argo Rollouts 可以与 Istio 集成来实现这一点。 Flagger 与 Argo Rollouts 非常相似，并且与 Flux 很好地集成在一起，因此如果你使用 Flux，请考虑使用 Flagger。 Spinnaker 是 Kubernetes 的第一个持续交付工具，它具有许多功能，但使用和设置起来有点复杂。  Crossplane Crossplane 是我最喜欢的 K8s 工具，我对这个项目感到非常兴奋，因为它给 Kubernetes 带来了一个关键的缺失部分：像管理 K8s 资源一样管理第三方服务。这意味着，你可以使用 YAML 中定义的 K8s 资源来配置云提供商数据库，例如 AWS RDS 或 GCP Cloud SQL，就像你在 K8s 中配置数据库一样。\n使用 Crossplane，无需使用不同的工具和方法分离基础设施和代码。你可以使用 K8s 资源定义一切。这样，你就无需学习 Terraform 等新工具并将它们分开保存。\n Crossplane 是一个开源 Kubernetes 附加组件，它使平台团队能够组装来自多个供应商的基础设施，并公开更高级别的自助 API 供应用程序团队使用，而无需编写任何代码。\n Crossplane 扩展了你的 Kubernetes 集群，为你提供适用于任何基础架构或托管云服务的 CRD。此外，它允许你完全实现持续部署，因为与 Terraform 等其他工具相反，Crossplane 使用现有的 K8s 功能（例如控制循环）来持续观察你的集群并自动检测任何对其起作用的配置漂移。例如，如果你定义了一个托管数据库实例并且有人手动更改它，Crossplane 将自动检测问题并将其设置回以前的值。这将实施基础设施即代码和 GitOps 原则。Crossplane 与 ArgoCD 配合使用效果很好，它可以查看源代码并确保你的代码存储库是唯一的真实来源，并且代码中的任何更改都会传播到集群以及外部云服务。如果没有 Crossplane，你只能在 K8s 服务中实现 GitOps，而不能在不使用单独进程的情况下在云服务中实现，现在你可以做到这一点，这太棒了。\n备选  Terraform 是最著名的 IaC 工具，但它不是 K8s 原生的，需要新技能并且不会自动监视配置漂移。 Pulumi 是一种 Terraform 替代品，它使用开发人员可以测试和理解的编程语言工作。  Knative 如果你在云中开发应用程序，你可能已经使用了一些无服务器技术，例如 AWS Lambda ，它是一种称为 FaaS 的事件驱动范例。\n我过去已经讨论过 Serverless，因此请查看我之前的文章以了解更多信息。Serverless 的问题在于它与云提供商紧密耦合，因为提供商可以为事件驱动的应用程序创建一个很好的生态系统。\n对于 Kubernetes，如果你希望将函数作为代码运行并使用事件驱动架构，那么你最好的选择是 Knative。Knative 旨在在 Kubernetes 上运行函数，在 Pod 之上创建一个抽象。\n特点：\n 针对常见应用程序用例的具有更高级别抽象的重点 API。 在几秒钟内建立一个可扩展、安全、无状态的服务。 松散耦合的功能让你可以使用所需的部分。 可插拔组件让你可以使用自己的日志记录和监控、网络和服务网格。 Knative 是可移植的：在 Kubernetes 运行的任何地方运行它，不用担心供应商锁定。 惯用的开发者体验，支持 GitOps、DockerOps、ManualOps 等常用模式。 Knative 可以与常见的工具和框架一起使用，例如 Django、Ruby on Rails、Spring 等等。  备选  Argo Events 为 Kubernetes 提供了一个事件驱动的工作流引擎，可以与 AWS Lambda 等云引擎集成。它不是 FaaS，而是为 Kubernetes 提供了一个事件驱动的架构。 OpenFaas  Kyverno Kubernetes 提供了极大的灵活性，以赋予敏捷的自治团队权力，但能力越大，责任越大。必须有一组最佳实践和规则，以确保以一致且有凝聚力的方式来部署和管理符合公司政策和安全要求的工作负载。\n有几种工具可以实现这一点，但没有一个是 Kubernetes 原生的…… 直到现在。Kyverno 是为 Kubernetes 设计的策略引擎，策略作为 Kubernetes 资源进行管理，并且不需要新的语言来编写策略。Kyverno 策略可以验证、改变和生成 Kubernetes 资源。\n你可以应用有关最佳实践、网络或安全性的任何类型的策略。例如，你可以强制所有服务都有标签或所有容器都以非 root 身份运行。你可以在此处查看一些政策示例。策略可以应用于整个集群或给定的命名空间。你还可以选择是只想审核策略还是强制执行它们以阻止用户部署资源。\n备选  Open Policy Agent 是著名的云原生基于策略的控制引擎。它使用自己的声明性语言，并且可以在许多环境中运行，而不仅仅是在 Kubernetes 上。它比 Kyverno 更难管理，但更强大。  Kubevela Kubernetes 的一个问题是开发人员需要非常了解和理解平台和集群配置。许多人会争辩说 K8s 的抽象级别太低，这会给只想专注于编写和交付应用程序的开发人员带来很多摩擦。\n在开放式应用程序模型（OAM）的设立是为了克服这个问题。这个想法是围绕应用程序创建更高级别的抽象，它独立于底层运行时。你可以在此处阅读规范。\n 专注于应用程序而不是容器或协调器，开放应用程序模型 [OAM] 带来了模块化、可扩展和可移植的设计，用于使用更高级别但一致的 API 对应用程序部署进行建模。\n Kubevela 是 OAM 模型的一个实现。KubeVela 与运行时无关，可本地扩展，但最重要的是，以应用程序为中心 。在 Kubevela 中，应用程序是作为 Kubernetes 资源实现的一等公民。**集群运营商（Platform Team）和开发者（Application Team）**是有区别的。集群操作员通过定义组件（组成应用程序的可部署/可配置实体，如 Helm Chart）和特征来管理集群和不同的环境。开发人员通过组装组件和特征来定义应用程序。\nKubeVela 是一个云原生计算基金会沙箱项目，虽然它仍处于起步阶段，但它可以在不久的将来改变我们使用 Kubernetes 的方式，让开发人员无需成为 Kubernetes 专家即可专注于应用程序。但是，我确实对 OAM 在现实世界中的适用性有一些担忧，因为系统应用程序、ML 或大数据过程等一些服务在很大程度上依赖于低级细节，这些细节可能很难融入 OAM 模型中。\n备选  Shipa 遵循类似的方法，使平台和开发团队能够协同工作，轻松将应用程序部署到 Kubernetes。  Snyk 任何开发过程中一个非常重要的方面是安全性，这一直是 Kubernetes 的一个问题，因为想要迁移到 Kubernetes 的公司无法轻松实现其当前的安全原则。\nSnyk 试图通过提供一个可以轻松与 Kubernetes 集成的安全框架来缓解这种情况。它可以检测容器映像、你的代码、开源项目等中的漏洞。\n备选  Falco 是 Kubernetes 的运行时安全线程检测工具。  Velero 如果你在 Kubernetes 中运行工作负载并使用卷来存储数据，则需要创建和管理备份。Velero 提供简单的备份/恢复过程、灾难恢复机制和数据迁移。\n与其他直接访问 Kubernetes etcd 数据库执行备份和恢复的工具不同，Velero 使用 Kubernetes API 来捕获集群资源的状态并在必要时恢复它们。此外，Velero 使你能够在配置的同时备份和恢复你的应用程序持久数据。\nSchema Hero 软件开发中的另一个常见过程是在使用关系数据库时管理模式演变。\nSchemaHero 是一种开源数据库架构迁移工具，可将架构定义转换为可应用于任何环境的迁移脚本。它使用 Kubernetes 声明性来管理数据库模式迁移。你只需指定所需的状态，然后 SchemaHero 管理其余的。\n备选  LiquidBase 是最著名的替代品。它更难使用，且不是 Kubernetes 原生的，但它具有更多功能。  Bitnami Sealed Secrets 我们已经介绍了许多 GitOps 工具，例如 ArgoCD。我们的目标是将所有内容保留在 Git 中，并使用 Kubernetes 声明性来保持环境同步。我们刚刚看到我们如何（并且应该）在 Git 中保留真实来源，并让自动化流程处理配置更改。\n在 Git 中通常很难保留的一件事是诸如数据库密码或 API 密钥之类的秘密，这是因为你永远不应该在代码存储库中存储秘密。一种常见的解决方案是使用外部保管库（例如 AWS Secret Manager 或 HashiCorp Vault）来存储机密，但这会产生很多摩擦，因为你需要有一个单独的流程来处理机密。理想情况下，我们希望有一种方法可以像任何其他资源一样安全地在 Git 中存储机密。\nSealed Secrets 旨在克服这个问题，允许你使用强加密将敏感数据存储在 Git 中。Bitnami Sealed Secrets 本地集成在 Kubernetes 中，允许你仅通过在 Kubernetes 中运行的 Kubernetes 控制器而不是其他任何人来解密密钥。控制器将解密数据并创建安全存储的原生 K8s 机密。这使我们能够将所有内容作为代码存储在我们的 repo 中，从而允许我们安全地执行持续部署，而无需任何外部依赖。\nSealed Secrets 由两部分组成：\n 集群端控制器 客户端实用程序：kubeseal  该 kubeseal 实用程序使用非对称加密来加密只有控制器才能解密的机密。这些加密的秘密被编码在一个 SealedSecret K8s 资源中，你可以将其存储在 Git 中。\n备选  AWS Secret Manager HashiCorp Vault）  Capsule 许多公司使用多租户来管理不同的客户。这在软件开发中很常见，但在 Kubernetes 中很难实现。命名空间是将集群的逻辑分区创建为隔离切片的好方法，但这不足以安全地隔离客户，我们需要强制执行网络策略、配额等。你可以为每个名称空间创建网络策略和规则，但这是一个难以扩展的乏味过程。此外，租户将不能使用多个命名空间，这是一个很大的限制。\n创建分层命名空间是为了克服其中一些问题。这个想法是为每个租户拥有一个父命名空间，为租户提供公共网络策略和配额，并允许创建子命名空间。这是一个很大的改进，但它在安全和治理方面没有对租户的本地支持。此外，它还没有达到生产状态，但 1.0 版预计将在未来几个月内发布。\n当前解决此问题的常用方法是为每个客户创建一个集群，这是安全的并提供租户所需的一切，但这很难管理且非常昂贵。\nCapsule 是一种为单个集群中的多个租户提供原生 Kubernetes 支持的工具。使用 Capsule，你可以为所有租户拥有一个集群。Capsule 将为租户提供 “几乎” 原生体验（有一些小限制），他们将能够创建多个命名空间并使用集群，因为它们完全可用，隐藏了集群实际上是共享的事实。\n在单个集群中，Capsule Controller 在称为 Tenant 的轻量级 Kubernetes 抽象中聚合多个命名空间，这是一组 Kubernetes 命名空间。在每个租户内，用户可以自由创建他们的命名空间并共享所有分配的资源，而策略引擎则使不同的租户彼此隔离。\n租户级别定义的网络和安全策略、资源配额、限制范围、RBAC 和其他策略由租户中的所有命名空间自动继承，类似于分层命名空间。然后用户可以自由地自治操作他们的租户，而无需集群管理员的干预。 Capsule 是 GitOps 就绪的，因为它是声明性的，并且所有配置都可以存储在 Git 中。\nvCluster vCluster 在多租户方面更进了一步，它在 Kubernetes 集群内提供了虚拟集群。每个集群都在一个常规命名空间上运行，并且是完全隔离的。虚拟集群有自己的 API 服务器和独立的数据存储，所以你在 vCluster 中创建的每个 Kubernetes 对象都只存在于 vcluster 内部。此外，您可以将 kube 上下文与虚拟集群一起使用，以像使用常规集群一样使用它们。\n只要您可以在单个命名空间内创建部署，您就可以创建虚拟集群并成为该虚拟集群的管理员，租户可以创建命名空间、安装 CRD、配置权限等等。\nvCluster 使用 k3s 作为其 API 服务器，使虚拟集群超轻量级且经济高效；由于 k3s 集群 100% 合规，虚拟集群也 100% 合规。vClusters 是超轻量级的（1 个 pod），消耗很少的资源并且可以在任何 Kubernetes 集群上运行，而无需对底层集群进行特权访问。与 Capsule 相比，它确实使用了更多的资源，但它提供了更多的灵活性，因为多租户只是用例之一。\n其他工具  kube-burner 用于压力测试。它提供指标和警报。 混沌工程的 Litmus。 kubewatch 用于监控，但主要关注基于 Kubernetes 事件（如资源创建或删除）的推送通知。它可以与 Slack 等许多工具集成。 BotKube 是一个消息机器人，用于监控和调试 Kubernetes 集群。与 kubewatch 类似，但更新并具有更多功能。 Mizu 是一个 API 流量查看器和调试器。 kube-fledged 是一个 Kubernetes 插件，用于直接在 Kubernetes 集群的工作节点上创建和管理容器镜像的缓存。因此，应用程序 pod 几乎立即启动，因为不需要从注册表中提取图像。  结论 在本文中，我们回顾了我最喜欢的 Kubernetes 工具。我专注于可以合并到任何 Kubernetes 发行版中的开源项目。我没有涵盖诸如 OpenShift 或 Cloud Providers Add-Ons 之类的商业解决方案，因为我想让它保持通用性，但我鼓励您探索如果您在云上运行 Kubernetes 或使用商业工具，您的云提供商可以为您提供什么。\n我的目标是向您展示您可以在 Kubernetes 中完成您在本地所做的一切。我还更多地关注鲜为人知的工具，我认为这些工具可能具有很大的潜力，例如 Crossplane、Argo Rollouts或 Kubevela。我更感兴趣的工具是 vCluster、Crossplane 和 ArgoCD/Workflows。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/translation-kuberletes-essential-tools-2021/","tags":["Kubernetes","OPA","Service Mesh","Istio","Tool","DevOps"],"title":"Kubernetes 必备工具：2021"},{"categories":["笔记"],"contents":"还不知道 Pipy 是什么的同学可以看下 GitHub 。\n Pipy 是一个轻量级、高性能、高稳定、可编程的网络代理。Pipy 核心框架使用 C++ 开发，网络 IO 采用 ASIO 库。 Pipy 的可执行文件仅有 5M 左右，运行期的内存占用 10M 左右，因此 Pipy 非常适合做 Sidecar proxy。\nPipy 内置了自研的 pjs 作为脚本扩展，使得Pipy 可以用 JS 脚本根据特定需求快速定制逻辑与功能。\nPipy 采用了模块化、链式的处理架构，用顺序执行的模块来对网络数据块进行处理。这种简单的架构使得 Pipy 底层简单可靠，同时具备了动态编排流量的能力，兼顾了简单和灵活。通过使用 REUSE_PORT 的机制（主流 Linux 和 BSD 版本都支持该功能），Pipy 可以以多进程模式运行，使得 Pipy 不仅适用于 Sidecar 模式，也适用于大规模的流量处理场景。 在实践中，Pipy 独立部署的时候用作“软负载”，可以在低延迟的情况下，实现媲美硬件的负载均衡吞吐能力，同时具有灵活的扩展性。\n 在玩过几次 Pipy 并探究其工作原理后，又有了更多的想法。\n 初探可编程网关 Pipy 可编程网关 Pipy 第二弹：编程实现 Metrics 及源码解读 可编程网关 Pipy 第三弹：事件模型设计  在使用OPA的时候，一直觉得Rego不是那么顺手，使用pipy js来写规则的想法油然而生。今天就一起试试这个思路。果然，不试不知道，一试发现太多的惊喜～Pipy不止于“代理”，更有很多可以适用的场景：\n 极小的单一可执行文件（single binary）使得 pipy 可能是最好的 “云原生 sidecar” sidecar 不仅仅是代理，还可以做控制器，做运算单元 proxy 的串路结构适合各种管控类的操作，比如访问控制 Pipy js 的扩展能力和快速编程能力，很适合做 “规则引擎”，或者用最近流行的说法 “云原生的规则引擎”。对比 OPA 我认为它完全够格做一个 “羽量级规则执行引擎”  现在我更倾向于定义 pipy 是一个 “云原生的流量编程框架”，代理只是其底层的核心能力，叠加了 pipy js 以后，上层可以做的事情很多，“流量滋养万物”。\n在 使用 Open Policy Agent 实现可信镜像仓库检查 之后，就在想 Pipy 是否一样可以做到，将内核替换成 Pipy + 规则。所以今天大部分内容和上面这篇是相似的。\n来，一起看看这个“不务正业”的 Pipy 如何实现 Kubernetes 的准入控制器 来做镜像的检查。\n环境 继续使用 minikube\nminikube start 创建部署 Pipy 的命名空间 kubectl create namespace pipy kubens pipy kubectl label ns pipy pipy/webhook=ignore #后面解释 规则 在 OPA 中，通过 kube-mgmt 容器监控 configmap 的改动，将 Policy 推送到同 pod 的 opa 容器中。\n对于 Pipy 为了渐变，直接使用挂载的方式将保存了规则的 configmap 挂载到 Pipy 的容器中。\n实际的使用中，Pipy 支持轮训的方式检查控制平面中规则的变更，并实时加载；也可以实现与 OPA 的 kube-mgmt 同样的逻辑。\n实现了上一讲功能的 pipy 规则如下：\ncat \u0026gt; pipy-rule.js \u0026lt;\u0026lt;EOF pipy({ _repoPrefix: \u0026#39;192.168.64.1\u0026#39;, //192.168.64.1:5000 是笔者本地容器运行的一个私有仓库。 _tagSuffix: \u0026#39;:latest\u0026#39;, }) .listen(6443, { tls: { cert: os.readFile(\u0026#39;/certs/tls.crt\u0026#39;).toString(), key: os.readFile(\u0026#39;/certs/tls.key\u0026#39;).toString(), }, }) .decodeHttpRequest() .replaceMessage( msg =\u0026gt; ( ((req, result, invalids, reason) =\u0026gt; ( req = JSON.decode(msg.body), invalids = req.request.object.spec.containers.find(container =\u0026gt; ( (!container.image.startsWith(_repoPrefix) ? ( reason = `${container.image} repo not start with ${_repoPrefix}`, console.log(reason), true ) : (false)) || (container.image.endsWith(_tagSuffix) ? ( reason = `${container.image} tag end with ${_tagSuffix}`, console.log(reason), true ) : (false) ))), invalids != undefined ? ( result = { \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;allowed\u0026#34;: false, \u0026#34;uid\u0026#34;: req.request.uid, \u0026#34;status\u0026#34;: { \u0026#34;reason\u0026#34;: reason, }, }, } ) : ( result = { \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: { \u0026#34;allowed\u0026#34;: true, \u0026#34;uid\u0026#34;: req.request.uid }, } ), console.log(JSON.encode(result)), new Message({ \u0026#39;status\u0026#39; : 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }, JSON.encode(result)) ))() ) ) .encodeHttpResponse() EOF 将规则保存在 configmap 中：\nkubectl create configmap pipy-rule --from-file=pipy-rule.js 在 Kubernetes 上部署 Pipy Kubernetes 与准入控制器（Admission Controller）的通信需要使用 TLS。配置 TLS，使用 openssl 创建证书颁发机构（certificate authority CA）和 OPA 的证书/秘钥对。\nopenssl genrsa -out ca.key 2048 openssl req -x509 -new -nodes -key ca.key -days 100000 -out ca.crt -subj \u0026#34;/CN=admission_ca\u0026#34; 为 OPA 创建 TLS 秘钥和证书：\ncat \u0026gt;server.conf \u0026lt;\u0026lt;EOF [req] req_extensions = v3_req distinguished_name = req_distinguished_name prompt = no [req_distinguished_name] CN = pipy.pipy.svc [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = pipy.pipy.svc EOF  注意 CN 和 alt_names 必须与后面创建 Pipy service 的匹配。\n openssl genrsa -out server.key 2048 openssl req -new -key server.key -out server.csr -config server.conf openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 100000 -extensions v3_req -extfile server.conf 为 OPA 创建保存 TLS 凭证的 Secret：\nkubectl create secret tls pipy-server --cert=server.crt --key=server.key 将 Pipy 部署为准入控制器（admission controller）。为了方便调试，我们使用启动 Pipy 的时候打开了控制台。\nkind: Service apiVersion: v1 metadata: name: pipy namespace: pipy spec: selector: app: pipy ports: - name: https protocol: TCP port: 443 targetPort: 6443 - name: gui # 方便调试 protocol: TCP port: 6060 targetPort: 6060 - name: http protocol: TCP port: 6080 targetPort: 6080 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: pipy namespace: pipy name: pipy spec: replicas: 1 selector: matchLabels: app: pipy template: metadata: labels: app: pipy name: pipy spec: containers: - name: pipy image: pipy:latest imagePullPolicy: IfNotPresent args: - \u0026#34;pipy\u0026#34; - \u0026#34;/opt/data/pipy-rule.js\u0026#34; - \u0026#34;--gui-port=6060\u0026#34; # 方便调试 # - \u0026#34;--log-level=debug\u0026#34; ports: - name: gui containerPort: 6060 protocol: TCP - name: http containerPort: 6080 protocol: TCP  - name: https containerPort: 6443 protocol: TCP volumeMounts: - readOnly: true mountPath: /certs name: pipy-server - readOnly: false mountPath: /opt/data name: pipy-rule volumes: - name: pipy-server secret: secretName: pipy-server - name: pipy-rule configMap: name: pipy-rule 暴露控制台的访问：\nkubectl expose deploy pipy --name pipy-node --type NodePort kubectl get svc pipy-port minikube service --url pipy-node -n pipy # 找到控制台端口 接下来，生成将用于将 Pipy 注册为准入控制器的 manifest。\ncat \u0026gt; webhook-configuration.yaml \u0026lt;\u0026lt;EOF kind: ValidatingWebhookConfiguration apiVersion: admissionregistration.k8s.io/v1beta1 metadata: name: pipy-validating-webhook webhooks: - name: validating-webhook.pipy.flomesh-io.cn namespaceSelector: matchExpressions: - key: pipy/webhook operator: NotIn values: - ignore rules: - operations: [\u0026#34;CREATE\u0026#34;, \u0026#34;UPDATE\u0026#34;] apiGroups: [\u0026#34;*\u0026#34;] apiVersions: [\u0026#34;*\u0026#34;] resources: [\u0026#34;pods\u0026#34;] clientConfig: caBundle: $(cat ca.crt | base64 | tr -d \u0026#39;\\n\u0026#39;) service: namespace: pipy name: pipy EOF 生成的配置文件包含 CA 证书的 base64 编码，以便可以在 Kubernetes API 服务器和 OPA 之间建立 TLS 连接。\nkubectl apply -f webhook-configuration.yaml 测试 pod-bad-repo.yaml:\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: web-server name: web-server namespace: default spec: containers: - image: nginx:1.21.1 name: web-server resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} kubectl apply -f pod-bad-repo.yaml Error from server (nginx:1.21.1 repo not start with 192.168.64.1): error when creating \u0026#34;pod-bad-repo.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.pipy.flomesh-io.cn\u0026#34; denied the request: nginx:1.21.1 repo not start with 192.168.64.1 pod-bad-tag.yaml\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: web-server name: web-server namespace: default spec: containers: - image: 192.168.64.1:5000/nginx:latest name: web-server resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} kubectl apply -f pod-bad-tag.yaml Error from server (192.168.64.1:5000/nginx:latest tag end with :latest): error when creating \u0026#34;pod-bad-tag.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.pipy.flomesh-io.cn\u0026#34; denied the request: 192.168.64.1:5000/nginx:latest tag end with :latest pod-ok.yaml\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: web-server name: web-server namespace: default spec: containers: - image: 192.168.64.1:5000/nginx:1.21.1 name: web-server resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} kubectl apply -f pod-ok.yaml pod/web-server created 总结 OPA 哪哪都好，唯一缺点就是其引进的 Rego 语言抬高了使用的门槛。而 Pipy 的规则是通过 JavaScrip 来编写的，前端的同学一样可以完成规则的编写。完全替代可能夸张了一些，但确实在部分场景下可以替代 OPA。\n玩到这里，你会发现有了规则，加上功能强大的过滤器（现在我喜欢叫他们 Hook 了），Pipy 的可玩性非常强。\n比如OPA: Kubernetes 准入控制策略 Top 5，比如\u0026hellip;。大胆的想象吧。\n想写一个系列，就叫“如何把 Pipy 玩坏”？\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/pipy-implement-kubernetes-admission-control/","tags":["Kubernetes","OPA","Pipy","云原生","Container"],"title":"Rego 不好用？用 Pipy 实现 OPA"},{"categories":["云原生"],"contents":"如何使用 Open Policy Agent 实现准入策略控制，可以参考这里\n本文翻译自 Open Policy Agent: The Top 5 Kubernetes Admission Control Policies\n Kubernetes 开发人员和平台工程师通常承受着非常大的压力，以保持应用程序部署的快速进行，并且总是为了速度和进度而做出妥协。平台团队越来越有责任确保这些妥协（例如管理 Ingress）不会导致客户数据暴露在整个互联网上等后果。\n幸运的是，Kubernetes 提供了设置策略的能力，通过检查并防止部署错误将其投入生产，从而避免这些后果。为了确保团队的应用程序不会比信心更重要，以下是现在应该在集群中运行的前五个 Kubernetes 准入控制策略。\n1. 可信镜像仓库 此策略很简单，但功能强大：仅允许从受信任的镜像仓库中拉取的容器映像，并且可以选择仅拉取与允许的仓库镜像地址列表匹配的那些镜像。\n当然，从互联网（或可信镜像仓库库以外的任何地方）拉取未知镜像会带来风险——例如恶意软件。但是还有其他很好的理由来维护单一的可信来源，例如在企业中实现可支持性。通过确保镜像仅来自受信任的镜像仓库，可以密切控制镜像库存，降低软件熵和蔓延的风险，并提高集群的整体安全性。\n相关策略：\n 禁止所有带有“latest” tag 的镜像 仅允许签名镜像或匹配特定哈希/SHA 的镜像  策略示例：\npackage kubernetes.validating.images deny[msg] { some i input.request.kind.kind == \u0026quot;Pod\u0026quot; image := input.request.object.spec.containers[i].image not startswith(image, \u0026quot;hooli.com/\u0026quot;) msg := sprintf(\u0026quot;Image '%v' comes from untrusted registry\u0026quot;, [image]) } 2. 标签安全 此策略要求所有 Kubernetes 资源都包含指定的标签并使用适当的格式。由于标签决定了 Kubernetes 对象和策略的分组，包括工作负载可以运行的位置——前端、后端、数据层——以及哪些资源可以发送流量，标签错误会导致生产中无法解释的部署和可支持性问题。此外，如果没有对标签应用方式的访问控制，集群就缺乏基本的安全性。最后，手动输入标签的危险在于错误会蔓延，特别是因为标签在 Kubernetes 中既灵活又强大。应用此策略并确保标签配置正确且一致。\n相关政策：\n 确保每个工作负载都需要特定的注解（annotations） 指定污点和容忍度以限制可以部署映像的位置  策略示例：\npackage kubernetes.validating.existence deny[msg] { not input.request.object.metadata.labels.costcenter msg := \u0026quot;Every resource must have a costcenter label\u0026quot; } deny[msg] { value := input.request.object.metadata.labels.costcenter not startswith(value, \u0026quot;cccode-\u0026quot;) msg := sprintf(\u0026quot;Costcenter code must start with `cccode-`; found `%v`\u0026quot;, [value]) } \u2028## 3. 禁止（或指定）特权模式\n此策略确保默认情况下容器不能在特权模式下运行 - 除非在允许的情况下排除特定情况（通常很少见）。\n通常，希望避免在特权模式下运行容器，因为它提供对主机资源和内核功能的访问——包括禁用主机级保护的能力。虽然容器在某种程度上是隔离的，但它们最终共享相同的内核。这意味着如果特权容器遭到入侵，它可能会成为入侵整个系统的起点。尽管如此，在特权模式下运行还是有正当理由的——只要确保这些时间是例外，而不是规则。\n相关政策：\n 禁止不安全的能力（capabilities） 禁止容器以 root 身份运行（以非 root 身份运行） 设置 userID  策略示例：\npackage kubernetes.validating.privileged deny[msg] { some c input_container[c] c.securityContext.privileged msg := sprintf(\u0026quot;Container '%v' should not run in privileged mode.\u0026quot;, [c.name]) } input_container[container] { container := input.request.object.spec.containers[_] } input_container[container] { container := input.request.object.spec.initContainers[_] } 定义和控制入口 Ingress 策略允许根据需要公开特定服务（允许 Ingress），或者根据需要不公开任何服务。在 Kubernetes 中，很容易意外启动与公共互联网通信的服务（Kubernetes 故障故事中有很多这样的例子）。同时，过于宽松的 Ingress 会导致启动不必要的外部 LoadBalancer，这也可能会变得非常昂贵（如每月预算支出）非常快！此外，当两个服务尝试共享同一个 Ingress 时，它可能会破坏应用程序。\n下面的策略示例防止不同命名空间中的 Ingress 对象共享相同的主机名。这个常见问题意味着新工作负载会从现有工作负载“窃取”互联网流量，这会产生一系列负面后果——从服务中断到数据暴露等等。\n相关政策：\n 需要 TLS 禁止/允许特定端口  策略示例：\npackage kubernetes.validating.ingress deny[msg] { is_ingress input_host := input.request.object.spec.rules[_].host some other_ns, other_name other_host := data.kubernetes.ingresses[other_ns][other_name].spec.rules[_].host [input_ns, input_name] != [other_ns, other_name] input_host == other_host msg := sprintf(\u0026quot;Ingress host conflicts with ingress %v/%v\u0026quot;, [other_ns, other_name]) } input_ns = input.request.object.metadata.namespace input_name = input.request.object.metadata.name is_ingress { input.request.kind.kind == \u0026quot;Ingress\u0026quot; input.request.kind.group == \u0026quot;extensions\u0026quot; input.request.kind.version == \u0026quot;v1beta1\u0026quot; } 5. 定义和控制出口 每个应用程序都需要防护栏来控制出口流量的流动方式，此策略允许你指定集群内和集群外的通信。与 Ingress 一样，默认情况下很容易意外地“允许 Egress”到全世界的每个 IP。有时这甚至不是意外——完全的放开通常是确保可以访问新部署的应用程序的最后努力，即使它过于宽松或引入风险。在集群内级别，还有可能无意中将数据发送到不应该拥有的服务。如果服务受到损害，这两种情况都存在数据泄露和盗窃的风险。另一方面：过于严格，使用 Egress 有时会导致配置错误，从而破坏应用程序。实现两全其美意味着使用此策略选择和指定允许 Egress 发生的时间和服务。\n相关政策。\n 请参阅上面的入口策略  策略示例：\npackage kubernetes.validating.egress allow_list := { \u0026quot;10.10.0.0/16\u0026quot;, \u0026quot;192.168.100.1/32\u0026quot; } deny[reason] { network_policy_allows_all_egress reason := \u0026quot;Network policy allows access to any IP address.\u0026quot; } deny[reason] { count(allow_list) \u0026gt; 0 input.request.kind.kind == \u0026quot;NetworkPolicy\u0026quot; input.request.object.spec.policyTypes[_] == \u0026quot;Egress\u0026quot; ipBlock := input.request.object.spec.egress[_].to[_].ipBlock not any({t | t := net.cidr_contains(allow_list[_], ipBlock.cidr)}) reason := \u0026quot;Network policy allows egress traffic outside of allowed IP ranges.\u0026quot; } network_policy_allows_all_egress { input.request.kind.kind == \u0026quot;NetworkPolicy\u0026quot; input.request.object.spec.policyTypes[_] == \u0026quot;Egress\u0026quot; egress := input.request.object.spec.egress[_] not egress.to } 有了这些政策，你就可以专注于构建一个世界级的平台。当然，如果你想为 Kubernetes 添加更多基本策略，请查看 openpolicyagent.org。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/open-policy-agent-top-5-kubernetes-admission-control/","tags":["OPA","Kubernetes","云原生"],"title":"Open Policy Agent: Top 5 Kubernetes 准入控制策略"},{"categories":["翻译","云原生"],"contents":"笔者：Kubernetes 抽象了资源和工作负载的操作模式，统一了工具集，实现人机接口的标准化。正如类 Docker 工具提供了应用运行时的操作模式；Spring Framework 提供了 Java 应用的开发模式。\n Kubernetes 是关于跨云的技能、工具和实践的可移植性。不是工作负载的可移植性。 \u0026ndash; Bilgin Lbryam @bibryam\n 本文翻译自 Kubernetes magic is in enterprise standardization, not app portability\n  Kubernetes 不会神奇地使你的应用程序具有可移植性，但它可能会给你带来更好的东西。\n 云为企业提供了看似无限的选择。然而，根据 Canonical-sponsored 的一项调查，这并不是大多数企业采用 Kubernetes 等云友好技术的原因。相反，Kubernetes 的主要目标是标准化——外观和操作与其他人一样。\n可移植性不是目标 我之前已经讨论过这个问题，参考了 Gartner 关于 Kubernetes 和可移植性的指南。许多人认为 Kubernetes（和容器）可以让他们在云之间轻松移植，但事实证明并不是这样的。正如 Gartner 分析师 Marco Meinardi 所写，当被问及公司是否应该采用“Kubernetes 使他们的应用程序可移植\u0026hellip;\u0026hellip;答案是：不。” 再说一次？\n 调查显示，[在云提供商之间移动应用程序] 的可能性实际上非常低。一旦部署在供应商中，应用程序往往会留在那里。这是因为数据湖难以移植且成本高昂，因此最终成为迁移的重心。\n 因此 Kubernetes 通常不会被公司接受，以增强应用程序的可移植性；相反，谈论人员可移植性或换言之，技能可移植性更接近事实。Weaveworks 首席执行官亚历克西斯·理查森（Alexis Richardson）将这个主题打回家：\n 重点是“技能可移植性”，因为使用标准操作模型和工具链。大型组织希望开发人员使用标准的工作方式，因为这可以降低培训成本，并消除员工在不同项目之间转移的障碍。如果你的“平台”（或多个平台）基于相同的核心云原生工具集，那么它也可以更轻松、更便宜地应用策略。\n 这让我们回到规范调查。\nSamesies 当被问及确定与采用 Kubernetes 等云原生技术相关的技术目标时，调查受访者将可移植性排在最后，将更直接的问题排在第一位：\n 改进维护、监控和自动化 - 64.6%。 基础设施现代化 - 46.4%。 更快的上线时间 - 26.5%。 删除供应商依赖项 - 12.8%。 全球覆盖率 - 12.5%。 围绕流量高峰的敏捷性 - 9.2%。 确保便携性 - 8.9%  我喜欢 Google Cloud 的开发者倡导者 Kelsey Hightower 在调查报告中评论这些结果的方式：\n 很多人认为组织转向 Kubernetes 是因为规模，或者因为他们想成为超大规模者，或者与 Twitter 拥有相同的流量水平。对于大多数组织而言，情况并非一定如此。很多人都喜欢 K8s 中内置了许多决策，例如日志记录、监控和负载平衡。\n  人们往往会忘记事情有多么复杂，只是为了构建一个没有所有自动化的应用程序。如果你在公有云上，你可以使用一些本机集成和工具。但是，如果你在本地，那不是给定的——你必须自己将解决方案粘合在一起。使用 Kubernetes，你几乎将 25 种不同的工具合二为一。\n  这就是人们所说的“现代基础设施”的意思——他们并不是在谈论做一些以前从未做过的事情。他们谈论的是过去 10 年或 15 年一直在生产的东西。Kubernetes 是当今所有“现代模式”的检查站。\n 换句话说，人们真正想要从 Kubernetes 获得的是一种标准的基础设施思考方式。回到 Richardson 之前的观点，虽然 Kubernetes 和云原生技术使公司能够以更高的速度运营，但最大的好处可能是使技能在组织之间可以互换——这为雇主和员工都创造了巨大的绩效收益。这是企业不断增加对 Kubernetes 投资的另一个原因。\n声明：我为 AWS 工作，但此处表达的观点是我的。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/translation-kubernetes-magic-is-in-enterprise-standardization-not-app-portability/","tags":["Kubernetes","云原生"],"title":"Kubernetes 的魔力在于企业标准化，而不是应用程序的可移植性"},{"categories":["云原生"],"contents":"从互联网（或可信镜像仓库库以外的任何地方）拉取未知镜像会带来风险——例如恶意软件。但是还有其他很好的理由来维护单一的可信来源，例如在企业中实现可支持性。通过确保镜像仅来自受信任的镜像仓库，可以密切控制镜像库存，降低软件熵和蔓延的风险，并提高集群的整体安全性。除此以外，有时还会需要检查镜像的 tag，比如禁止使用 latest 镜像。\n这今天我们尝试用“策略即代码”的实现 OPA 来实现功能。\n还没开始之前可能有人会问：明明可以实现个 Admission Webhook 就行，为什么还要加上 OPA？\n确实可以，但是这样策略和逻辑都会耦合在一起，当策略需要调整的时候需要修改代码重新发布。而 OPA 就是用来做解耦的，其更像是一个策略的执行引擎。\n什么是 OPA Open Policy Agent（以下简称 OPA，发音 “oh-pa”）一个开源的通用策略引擎，可以统一整个堆栈的策略执行。OPA 提供了一种高级声明性语言（Rego），可让你将策略指定为代码和简单的 API，以从你的软件中卸载策略决策。你可以使用 OPA 在微服务、Kubernetes、CI/CD 管道、API 网关等中实施策略。\nRego 是一种高级的声明性语言，是专门为 OPA 建立的。更多 OPA 的介绍可以看 Open Policy Agent 官网，不想看英文直接看这里。\n现在进入正题。\n启动集群 启动 minikube\nminikube start 创建用于部署 OPA 的命名空间 创建并切换到命名空间 opa （命名空间的切换使用 kubens，更多工具介绍见这里）\nkubectl create namespace opa kubens opa 在 Kubernetes 上部署 OPA Kubernetes 和 OPA 间的通信必须使用 TLS 进行保护。配置 TLS，使用 openssl 创建证书颁发机构（certificate authority CA）和 OPA 的证书/秘钥对。\nopenssl genrsa -out ca.key 2048 openssl req -x509 -new -nodes -key ca.key -days 100000 -out ca.crt -subj \u0026#34;/CN=admission_ca\u0026#34; 为 OPA 创建 TLS 秘钥和证书：\ncat \u0026gt;server.conf \u0026lt;\u0026lt;EOF [req] req_extensions = v3_req distinguished_name = req_distinguished_name prompt = no [req_distinguished_name] CN = opa.opa.svc [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, serverAuth subjectAltName = @alt_names [alt_names] DNS.1 = opa.opa.svc EOF  注意 CN 和 alt_names 必须与后面创建 OPA service 的匹配。\n openssl genrsa -out server.key 2048 openssl req -new -key server.key -out server.csr -config server.conf openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 100000 -extensions v3_req -extfile server.conf 为 OPA 创建保存 TLS 凭证的 Secret：\nkubectl create secret tls opa-server --cert=server.crt --key=server.key 将 OPA 部署为准入控制器（admission controller）。\nadmission-controller.yaml：\n# 授权 OPA/kube-mgmt 对资源的只读权限 # kube-mgmt 会同步资源信息给 OPA，以便在策略中使用 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: opa-viewer roleRef: kind: ClusterRole name: view apiGroup: rbac.authorization.k8s.io subjects: - kind: Group name: system:serviceaccounts:opa apiGroup: rbac.authorization.k8s.io --- # 为 OPA/kube-mgmt 定义角色来在 configmaps 中更新策略状态 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: opa name: configmap-modifier rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;] verbs: [\u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- # 为 OPA/kube-mgmt 授予角色 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: opa name: opa-configmap-modifier roleRef: kind: Role name: configmap-modifier apiGroup: rbac.authorization.k8s.io subjects: - kind: Group name: system:serviceaccounts:opa apiGroup: rbac.authorization.k8s.io --- kind: Service apiVersion: v1 metadata: name: opa namespace: opa spec: selector: app: opa ports: - name: https protocol: TCP port: 443 targetPort: 8443 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: opa namespace: opa name: opa spec: replicas: 1 selector: matchLabels: app: opa template: metadata: labels: app: opa name: opa spec: containers: # WARNING: OPA is NOT running with an authorization policy configured. This # means that clients can read and write policies in OPA. If you are # deploying OPA in an insecure environment, be sure to configure # authentication and authorization on the daemon. See the Security page for # details: https://www.openpolicyagent.org/docs/security.html. - name: opa image: openpolicyagent/opa:0.30.1-rootless args: - \u0026#34;run\u0026#34; - \u0026#34;--server\u0026#34; - \u0026#34;--tls-cert-file=/certs/tls.crt\u0026#34; - \u0026#34;--tls-private-key-file=/certs/tls.key\u0026#34; - \u0026#34;--addr=0.0.0.0:8443\u0026#34; - \u0026#34;--addr=http://127.0.0.1:8181\u0026#34; - \u0026#34;--log-format=json-pretty\u0026#34; - \u0026#34;--set=decision_logs.console=true\u0026#34; volumeMounts: - readOnly: true mountPath: /certs name: opa-server readinessProbe: httpGet: path: /health?plugins\u0026amp;bundle scheme: HTTPS port: 8443 initialDelaySeconds: 3 periodSeconds: 5 livenessProbe: httpGet: path: /health scheme: HTTPS port: 8443 initialDelaySeconds: 3 periodSeconds: 5 - name: kube-mgmt image: openpolicyagent/kube-mgmt:0.11 args: - \u0026#34;--replicate=v1/pods\u0026#34; volumes: - name: opa-server secret: secretName: opa-server --- kind: ConfigMap apiVersion: v1 metadata: name: opa-default-system-main namespace: opa data: main: |package system import data.kubernetes.validating.images main = { \u0026#34;apiVersion\u0026#34;: \u0026#34;admission.k8s.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AdmissionReview\u0026#34;, \u0026#34;response\u0026#34;: response, } default uid = \u0026#34;\u0026#34; uid = input.request.uid response = { \u0026#34;allowed\u0026#34;: false, \u0026#34;uid\u0026#34;: uid, \u0026#34;status\u0026#34;: { \u0026#34;reason\u0026#34;: reason, }, } { reason = concat(\u0026#34;, \u0026#34;, images.deny) reason != \u0026#34;\u0026#34; } else = {\u0026#34;allowed\u0026#34;: true, \u0026#34;uid\u0026#34;: uid} kubectl apply -f admission-controller.yaml 接下来，生成将用于将 OPA 注册为准入控制器的 manifest。\ncat \u0026gt; webhook-configuration.yaml \u0026lt;\u0026lt;EOF kind: ValidatingWebhookConfiguration apiVersion: admissionregistration.k8s.io/v1beta1 metadata: name: opa-validating-webhook webhooks: - name: validating-webhook.openpolicyagent.org rules: - operations: [\u0026quot;CREATE\u0026quot;, \u0026quot;UPDATE\u0026quot;] apiGroups: [\u0026quot;*\u0026quot;] apiVersions: [\u0026quot;*\u0026quot;] resources: [\u0026quot;pods\u0026quot;] clientConfig: caBundle: $(cat ca.crt | base64 | tr -d '\\n') service: namespace: opa name: opa EOF 生成的配置文件包含 CA 证书的 base64 编码，以便可以在 Kubernetes API 服务器和 OPA 之间建立 TLS 连接。\nkubectl apply -f webhook-configuration.yaml 查看 OPA 日志：\nkubectl logs -l app=opa -c opa -f 定义策略并通过 Kubernetes 将其加载到 OPA 这里我们定义了对容器镜像的检查：\n 是否来自受信任的仓库 是否使用了 latest tag 的镜像  image-policy.rego\npackage kubernetes.validating.images deny[msg] { some i input.request.kind.kind == \u0026#34;Pod\u0026#34; image := input.request.object.spec.containers[i].image endswith(image, \u0026#34;:latest\u0026#34;) msg := sprintf(\u0026#34;Image \u0026#39;%v\u0026#39; used latest image\u0026#34;, [image]) } { some i input.request.kind.kind == \u0026#34;Pod\u0026#34; image := input.request.object.spec.containers[i].image not startswith(image, \u0026#34;192.168.64.1:5000\u0026#34;) msg := sprintf(\u0026#34;Image \u0026#39;%v\u0026#39; comes from untrusted registry\u0026#34;, [image]) } kubectl create configmap image-policy --from-file=image-policy.rego 检查 configmap 的 annotation openpolicyagent.org/policy-status 值是否 为 '{\u0026quot;status\u0026quot;:\u0026quot;ok\u0026quot;}'。否则，就要根据报错信息处理问题。\n注：192.168.64.1:5000 是笔者本地容器运行的一个私有仓库。\nversion: \u0026#39;3.6\u0026#39; services: registry: image: registry:2.7.1 container_name: registry restart: always environment: REGISTRY_HTTP_ADDR: 0.0.0.0:5000 REGISTRY_STORAGE: filesystem REGISTRY_STORAGE_DELETE_ENABLED: \u0026#39;true\u0026#39; REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /var/lib/registry ports: - \u0026#39;5000:5000\u0026#39; volumes: - \u0026#39;/Users/addo/Downloads/tmp/registry:/var/lib/registry\u0026#39; 测试 pod-bad-repo.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: web-server name: web-server spec: containers: - image: nginx:1.21.1 name: web-server resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} kubectl apply -f pod-bad-repo.yaml Error from server (Image \u0026#39;nginx:1.21.1\u0026#39; comes from untrusted registry): error when creating \u0026#34;pod-bad-repo.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.openpolicyagent.org\u0026#34; denied the request: Image \u0026#39;nginx:1.21.1\u0026#39; comes from untrusted registry pod-bad-tag.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: web-server name: web-server spec: containers: - image: 192.168.64.1:5000/nginx:latest name: web-server resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} kubectl apply -f pod-bad-tag.yaml Error from server (Image \u0026#39;192.168.64.1:5000/nginx:latest\u0026#39; used latest image): error when creating \u0026#34;pod-bad-tag.yaml\u0026#34;: admission webhook \u0026#34;validating-webhook.openpolicyagent.org\u0026#34; denied the request: Image \u0026#39;192.168.64.1:5000/nginx:latest\u0026#39; used latest image pod-ok.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: web-server name: web-server spec: containers: - image: 192.168.64.1:5000/nginx:1.21.1 name: web-server resources: {} dnsPolicy: ClusterFirst restartPolicy: Always status: {} kubectl apply -f pod-ok.yaml pod/web-server created 总结 策略即代码，以代码的实现表达策略；在通过策略与执行引擎的解耦分离，让策略更加的灵活。\n后面我们再探索 OPA 的更多场景。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/image-trusted-repository-with-open-policy-agent/","tags":["OPA","Kubernetes","Docker"],"title":"使用 Open Policy Agent 实现可信镜像仓库检查"},{"categories":["笔记"],"contents":"Kubernetes 使用有好几年了，但在今年 5 月才完成 CKA 的考试。虽说用了几年，还是提前刷了部分题熟悉下。\n绝大部分题都是有在 minikube 的环境上操作过，只有部分比如升级集群受限于环境问题没有实地操作。\n写在最前  保存常用文档进书签，如果有 Alfred 启用浏览器书签 workflow。效果见下图 kubectl 自动补全 echo \u0026quot;source \u0026lt;(kubectl completion bash)\u0026quot; \u0026gt;\u0026gt; ~/.bashrc; source ~/.bashrc 每道题开始前要切换 context 和 namespace，直接复制题目里的命令即可 必要的 alias 善用 --dry-run=client -o yaml 避免手动敲太多 善用 kubectl explain [resource[.field]] 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍） 看懂题目最重要，输出正确的结果更重要（重要的事讲三遍）  书签地址：K8s-CKA-CAKD-Bookmarks.html\n安全：RBAC  在默认命名空间中创建一个名为 dev-sa 的服务帐户，dev-sa 可以在 dev 命名空间中创建以下组件： Deployment、StatefulSet、DaemonSet\n 知识点  role sa rolebinding auth can-i  参考文档： https://kubernetes.io/docs/reference/access-authn-authz/rbac/#command-line-utilities\n解题思路 $ kubectl create sa dev-sa $ kubectl create role dev-role --verb=create --resource=deployment,statefulset,daemonset #检查 $ kubectl describe role dev-role Name: dev-role Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- daemonsets.apps [] [] [create] deployments.apps [] [] [create] statefulsets.apps [] [] [create] $ kubectl create rolebinding dev --serviceaccount default:dev-sa --role dev-role #检查 $ kubectl auth can-i create deployment --as system:serviceaccount:default:dev-sa yes $ kubectl auth can-i create statefulset --as system:serviceaccount:default:dev-sa yes $ kubectl auth can-i create daemonset --as system:serviceaccount:default:dev-sa yes $ kubectl auth can-i create pod --as system:serviceaccount:default:dev-sa no 多容器 Pod  创建一个pod名称日志，容器名称 log-pro 使用image busybox，在 /log/data/output.log 输出重要信息。然后另一个容器名称 log-cus 使用 image busybox，在 /log/data/output.log 加载 output.log 并打印它。 请注意，此日志文件只能在 pod 内共享。\n 知识点  pod volume: emptyDir  参考文档： https://kubernetes.io/docs/concepts/storage/volumes/#emptydir\n解题思路 kubectl run log --image busybox --dry-run=client -o yaml \u0026gt; log.yaml 修改 log.yaml\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: log name: log spec: containers: - command: - sh - -c - echo important information \u0026gt; /log/data/output.log; sleep 1d image: busybox name: log-pro resources: {} imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /log/data name: log - command: - sh - -c - tail -f /log/data/output.log image: busybox name: log-cus imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /log/data name: log dnsPolicy: ClusterFirst restartPolicy: Always volumes: - name: log emptyDir: {} status: {} 执行创建 kubectl apply -f log.yaml\n检查\n$ kubectl logs log -c log-cus important information 安全：网络策略 NetworkPolicy  只有命名空间 mysql 的 pod 只能被另一个命名空间 internal 的 pod 通过 8080 端口进行访问\n 知识点  NetworkPolicy Ingress  参考文档： https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource\n解题思路 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cka-network namespace: target #目的命名空间 spec: podSelector: {} policyTypes: - Ingress #策略影响入栈流量 ingress: - from: #允许流量的来源 - namespaceSelector: matchLabels: ns: source #源命名空间的 label ports: - protocol: TCP port: 8080 #允许访问的端口 节点状态及污点  统计这个集群中没有污染的就绪节点，并输出到文件 /root/cka/readyNode.txt。\n 知识点  Node Taint（污点）  参考文档：https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n解题思路 # Ready 状态的数量 $ kubectl get node | grep -w Ready | wc -l # 查看含有 Taint 的数量，需要排除掉这些 $ kubectl describe node | grep Taints | grep -i NoSchedule | wc -l 资源  将占用CPU资源最多的pod名称输出到文件 /root/cka/name.txt\n 知识点  kubectl top 命令 metrics  解题思路 如果是 minikube 环境，报错 error: Metrics API not available，可以执行 minikube addons enable metrics-server 命令开启 metrics server。\n通过 kubectl top 命令找到 cpu 最高的 pod，将其名字写入 /root/cka/name.txt。\n$ kubectl top pod # 或者 $ kubectl top pod | sort -k 2 -n 网络：DNS  有 pod 名称 pod-nginx，创建服务名称 service-nginx，使用 nodePort 暴露pod。 然后创建一个 pod 使用 image busybox 来 nslookup pod pod-nginx 和 service service-nginx。\n 知识点  service with nodePort kubectl expose kubectl run  参考文档：https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/\n解题思路 使用 kubectl expose 创建 service。\n# 创建 service kubectl expose pod pod-nginx --name service-nginx --type NodePort --target-port 80 # 创建 pod kubectl run busybox --image busybox:latest --command sleep 1h 获取 pod 的 ip 地址，pod 的 dns lookup 需要用用到 ip。\n$ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES busybox 1/1 Running 0 2m17s 172.17.0.5 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod-nginx 1/1 Running 0 59m 172.17.0.4 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 执行 nslookup\n$ kubectl exec busybox -it -- nslookup 172.17.0.4 4.0.17.172.in-addr.arpa\tname = 172-17-0-4.service-nginx.default.svc.cluster.local. $ kubectl exec busybox -it -- nslookup service-nginx Server:\t10.96.0.10 Address:\t10.96.0.10#53 Name:\tservice-nginx.default.svc.cluster.local Address: 10.110.253.70 工作负载：扩容  将命名空间 dev 中的 Deployment scale-deploy 缩放到三个 pod 并记录下来。\n 参考文档：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment\n知识点  deployment scale up kubectl scale  解题思路 kubectl scale 的使用，需要参数 --record 进行记录（将操作命令记录到 deployment 的 kubernetes.io/change-cause annotation 中）。\n$ kubectl scale deployment scale-deploy --replicas 3 --record 集群备份及恢复  备份 etcd 并将其保存在主节点上的 /root/cka/etcd-backup.db。\n  最后恢复备份。\n 知识点  etcd 的备份及恢复  参考文档：https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster\n解题思路 Kubernetes 的所有数据都记录在 etcd 中，对 etcd 进行备份就是对集群进行备份。\n连接 etcd 需要证书，证书可以从 apiserver 获取，因为 apiserver 需要连接 etcd。新版本的 apiserver 都是以 static pod 的方式运行，证书是通过 volume 挂载到 pod 中的。\n比如 minikube 环境，证书是从 node 节点的 /var/lib/minikube/certs 挂载进去的。\n要先 ssh 到 master 节点上。命令的执行非常快，如果长时间没结束，那就说名有问题了。\n#备份 $ ETCDCTL_API=3 etcdctl snapshot save /root/cka/etcd-backup.db \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/var/lib/minikube/certs/etcd/ca.crt \\ --cert=/var/lib/minikube/certs/apiserver-etcd-client.crt \\ --key=/var/lib/minikube/certs/apiserver-etcd-client.key 由于只说了 restore，所以就执行 restore 的命令，默认会恢复到当前目录的 default.etcd 下。\n#恢复 $ ETCDCTL_API=3 etcdctl snapshot restore /root/cka/etcd-backup.db \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/var/lib/minikube/certs/etcd/ca.crt \\ --cert=/var/lib/minikube/certs/apiserver-etcd-client.crt \\ --key=/var/lib/minikube/certs/apiserver-etcd-client.key 集群节点升级  将master节点版本从 1.20.0 升级到 1.21.0，确保 master 节点上的 pod 重新调度到其他节点，升级完成后，使 master 节点可用。\n 知识点  drain cordon  参考文档：https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/#upgrading-control-plane-nodes\n解题思路 受限于环境，没有实地操作。\n# 将节点设置为不可调度 $ kubectl cordon master # 驱逐 master 节点上的 pod $ kubectl drain master --ignore-daemonsets # 进行升级 $ apt-mark unhold kubelet kubectl \u0026amp;\u0026amp; \\ apt-get update \u0026amp;\u0026amp; apt-get install -y kubelet=1.21.0-00 kubectl=1.21.0-00 \u0026amp;\u0026amp; \\ apt-mark hold kubelet kubectl # 重新启动kubelet $ systemctl daemon-reload $ systemctl restart kubelet # 将节点设置为可调度 $ kubectl uncordon master 集群：节点故障排查  现在 node01 还没有准备好，请找出根本原因并使其准备好，然后创建一个确保它在 node01 上运行的 pod。\n 知识点  节点故障排查  解题思路 这种问题大概率问题出在 kubelet 上\nssh node01 systemctl status kubelet systemctl restart kubelet # 再检查node状态 插播一个故障，本地安装 2 个节点的 minikube 集群时，第二个节点持续 NotReady。使用 systemctl status kubelet 看到 unable to update cni config: no networks found in /etc/cni/net.mk。\n检查该目录确实没有文件，从 master 节点复制到该节点后重启 kubelet 解决。\n存储：持久化卷  集群中有一个持久卷名称 dev-pv，创建一个持久卷声明名称 dev-pvc，确保这个持久卷声明会绑定持久卷，然后创建一个 pod 名称 test-pvc，将这个 pvc 挂载到 path /tmp/data，使用 nginx 镜像。\n 知识点  PersistentVolume PersistentVolumeClaim Mount Volume  参考文档：https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume\n解题思路 创建 pvc 前先获取 pv的信息\n$ kubectl get pv dev-pv -o yaml 创建 pv\n$ cat \u0026gt; pvc.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: PersistentVolumeClaim metadata: name: dev-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi EOF $ kubectl apply -f pvc.yaml 创建 pod 的 manifest，记得使用 kubectl run --dry-run=client -o yaml\nkubectl run test-pvc --image nginx --dry-run=client -o yaml \u0026gt; test-pvc.yaml 修改之后得到最终的 pod yaml\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: test-pvc name: test-pvc spec: volumes: - name: data persistentVolumeClaim: claimName: dev-pvc containers: - image: nginx name: test-pvc resources: {} volumeMounts: - name: data mountPath: /tmp/data dnsPolicy: ClusterFirst restartPolicy: Always 理论上只要 pod 能运行，就说明成功。也可以进一步确认挂载是否成功，在 pod 的 /tmp/data 中 touch 个文件，然后到节点的目录中查看是有该文件。\n工作负载：多容器的 Deployment  创建一个名为 deploy-important 的 Deployment，标签为 id=very-important（pod 也应该有这个标签）和命名空间 dev 中的 3 个副本。 它应该包含两个容器，第一个名为 container1 并带有镜像，第二个名为 container2 的图像为 kubernetes/pause。\n  在一个工作节点上应该只运行该部署的一个 Pod。 我们有两个工作节点：cluster1-worker1 和 cluster1-worker2。 因为 Deployment 有三个副本，所以结果应该是在两个节点上都有一个 Pod 正在运行。 不会调度第三个 Pod，除非添加新的工作节点。\n 知识点  deployment pod label replicas multi container pod pod anti affinity  官方文档参考：https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#never-co-located-in-the-same-node\n解题思路 先创建模板\n$ kubectl create deployment deploy-important --image nginx --replicas 3 --dry-run=client -o yaml \u0026gt; deploy-important.yaml 修改后的 yaml\napiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: deploy-important id: very-important name: deploy-important spec: replicas: 3 selector: matchLabels: app: deploy-important id: very-important strategy: {} template: metadata: creationTimestamp: null labels: app: deploy-important id: very-important spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: id operator: In values: - very-important topologyKey: kubernetes.io/hostname containers: - image: nginx name: container1 resources: {} - image: kubernetes/pause name: container2 status: {} minikube 上测试只能调度一个 pod，符合预期\n$ kgpo NAME READY STATUS RESTARTS AGE deploy-important-659d54fc47-6cp8r 0/2 Pending 0 3h10m deploy-important-659d54fc47-92z4d 2/2 Running 0 3h10m deploy-important-659d54fc47-c6llc 0/2 Pending 0 3h10m 存储：Secret的使用  在 secret 命名空间下，使用镜像  busybox:1.31.1 创建一个名为 secret-pod 的 pod，并保证 pod 运行一段时间\n  有个名为 sercret1.yaml 的 Secret 文件，在 secret 命名空间下创建 Secret，并以只读的方式挂在到 Pod 的 /tmp/secret1 目录 创建一个新的 Secret secret2 包含 user=user1 和 pass=1234，分别以缓解变量 APP_USER 和 APP_PASS 输入到 Pod 中\n 知识点  secret toleration taints  参考文档： https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables\n解题思路 创建 namespace\n$ kubectl create ns secret 创建 pod 模板\n$ kubectl run secret-pod --image busybox:1.31.1 --dry-run=client -o yaml --command -- sleep 1d \u0026gt; secret-pod.yaml 修改 secret1.yaml，使用 secret namespace\napiVersion: v1 data: halt: IyEvYmluL2Jhc2g= kind: Secret metadata: creationTimestamp: \u0026#34;2021-05-15T07:48:02Z\u0026#34; name: secret1 namespace: secret type: Opaque 创建 secret2\n$ kubectl create secret generic secret2 --from-literal user=user1 --from-literal pass=1234 修改模板\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: secret-pod name: secret-pod spec: containers: - command: - sleep - 1d image: busybox:1.31.1 name: secret-pod env: - name: APP_USER valueFrom: secretKeyRef: name: secret2 key: user - name: APP_PASS valueFrom: secretKeyRef: name: secret2 key: pass resources: {} volumeMounts: - mountPath: /tmp/secret1 name: sec dnsPolicy: ClusterFirst restartPolicy: Always volumes: - name: sec secret: secretName: secret1 status: {} 检查结果：\n$ kubectl exec secret-pod -- cat /tmp/secret1/halt #!/bin/bash $ kubectl exec secret-pod -- env | grep \u0026#39;APP_\u0026#39; APP_USER=user1 APP_PASS=1234 工作负载：静态 Pod  在 cluster3-master1 上的 default 命名空间中创建一个名为 my-static-pod 的静态 Pod。 使用镜像 nginx:1.16-alpine 并分配 10m CPU 和 20Mi 内存的资源。\n  Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if its reachable through the cluster3-master1 internal IP address. You can connect to the internal node IPs from your main terminal. 然后创建一个名为 static-pod-service 的 NodePort Service，该服务在端口 80 上公开该静态 Pod，并检查它是否具有端点以及是否可以通过 cluster3-master1 内部 IP 地址访问它。\n 知识点  static pod resource nodeport service endpoints  参考文档：\n https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/ https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory  \u0008解题思路 创建pod模板\n$ kubectl run my-static-pod --image nginx:1.16-alpine --dry-run=client -o yaml \u0026gt; static-pod.yaml 修改模板，增加资源配置\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: my-static-pod name: my-static-pod spec: containers: - image: nginx:1.16-alpine name: my-static-pod resources: requests: cpu: \u0026#34;10m\u0026#34; memory: \u0026#34;20Mi\u0026#34; dnsPolicy: ClusterFirst restartPolicy: Always status: {} ssh 到主机，找到 kubelet 配置文件的位置 ps -ef | grep kubelet\n查看配置文件（minikube：/var/lib/kubelet/config.yaml）中 staticPodPath 配置的就是静态 pod 的 manifest 的位置（minikube：/etc/kubernetes/manifests）\n将 static-pod.yaml 放到正确的文件夹中，然后重启 kubelet\n$ systemctl restart kubelet 检查pod是否正确运行\n创建 node port\n$ kubectl expose pod my-static-pod --name static-pod-service --type NodePort --port 80 检查是否创建成功\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE static-pod-service NodePort 10.97.248.99 \u0026lt;none\u0026gt; 80:31938/TCP 68s 获取 node 的 ip\n$ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cka Ready master 10h v1.18.8 192.168.64.3 \u0026lt;none\u0026gt; Buildroot 2020.02.10 4.19.171 docker://20.10.4 在 minikube 的环境下可直接通过 minikube ip 获取\n测试\n$ http 192.168.64.3:31938 --headers HTTP/1.1 200 OK Accept-Ranges: bytes Connection: keep-alive Content-Length: 612 Content-Type: text/html Date: Sat, 15 May 2021 08:35:11 GMT ETag: \u0026#34;5d52db33-264\u0026#34; Last-Modified: Tue, 13 Aug 2019 15:45:55 GMT Server: nginx/1.16.1 调度：污点和容忍度  在命名空间 default 中创建图像 httpd:2.4.41-alpine 的单个 Pod。Pod 应命名为 pod1，容器名为 pod1-container。在不给任何节点添加新标签的前提下，将该 pod 调度到主节点上。\n 知识点  Taint Label Tolerance  参考文档：https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n解题思路 #找出master节点（一般考试只有一个节点） $ kubectl get node #找到 master 节点的 taints，需要在 pod 的 .spec.tolerations 排除掉 $ kubectl describe node xxxx | grep -w Taints #找到 master 节点的 labels $ kubectl describe node xxxx | grep -w Labels -A10 创建pod模板\n$ kubectl run pod1 --image httpd:2.4.41-alpine --dry-run=client -o yaml \u0026gt; pod1.yaml 修改模板： 这里假设主节点的 Taint 为 node-role.kubernetes.io/master=:NoSchedule\n# minikube 集群名为 cka，主节点同名 $ kubectl describe node cka | grep -i taint Taints: node-role.kubernetes.io/master:NoSchedule 最终的 pod 如下\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: pod1 name: pod1 spec: containers: - image: httpd:2.4.41-alpine name: pod1 resources: {} dnsPolicy: ClusterFirst tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule nodeSelector: node-role.kubernetes.io/master: \u0026#34;\u0026#34; restartPolicy: Always status: {} 最后检查下是否调度到主节点上：\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod1 1/1 Running 0 102s 10.244.0.3 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubectl 命令和排序  所有命名空间中都有各种 Pod。 将命令写入 /opt/course/5/find_pods.sh，其中列出所有按 AGE 排序的 Pod（metadata.creationTimestamp）。\n  将第二个命令写入 /opt/course/5/find_pods_uid.sh，其中列出按字段 metadata.uid 排序的所有 Pod。对这两个命令都使用 kubectl 排序。\n 知识点  kubectl 命令的使用，主要是 --all-namespaces （缩写 -A） 和 --sort-by  解题思路 $ cat \u0026gt; /opt/course/5/find_pods.sh \u0026lt;\u0026lt;EOF kubectl get pod -A --sort-by \u0026#39;.metadata.creationTimestamp\u0026#39; EOF $ cat \u0026gt; /opt/course/5/find_pods_uid.sh \u0026lt;\u0026lt;EOF kubectl get pod -A --sort-by \u0026#39;.metadata.uid\u0026#39; EOF 存储：持久化卷和挂载  创建一个名为 safari-pv 的新 PersistentVolume。它应该具有 2Gi 的容量、accessMode ReadWriteOnce、hostPath /Volumes/Data 并且没有定义 storageClassName。\n  接下来在命名空间 project-tiger 中创建一个名为 safari-pvc 的新 PersistentVolumeClaim。 它应该请求 2Gi 存储，accessMode ReadWriteOnce 并且不应定义 storageClassName。 PVC 应该正确绑定到 PV。\n  最后在命名空间 project-tiger 中创建一个新的 Deployment safari，它将该卷挂载到 /tmp/safari-data。该 Deployment 的 Pod 应该是镜像 httpd:2.4.41-alpine。\n 知识点  pv pvc pod 使用 pvc deployment mount PVC volume  参考文档：\n https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes  解题思路 创建 pv\napiVersion: v1 kind: PersistentVolume metadata: name: safari-pv labels: type: local spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/Volumes/Data\u0026#34; 创建 pvc\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: safari-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 2Gi 检查是否绑定成功\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE safari-pvc Bound pvc-d4c15825-2de3-470f-8ed0-9519cacaad21 2Gi RWO standard 24s 创建 deployment 模板\n$ kubectl create deployment safari --image httpd:2.4.41-alpine --dry-run=client -o yaml 最终的yaml\napiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: safari name: safari spec: replicas: 1 selector: matchLabels: app: safari strategy: {} template: metadata: creationTimestamp: null labels: app: safari spec: containers: - image: httpd:2.4.41-alpine name: httpd resources: {} volumeMounts: - mountPath: /tmp/safari-data name: data volumes: - name: data persistentVolumeClaim: claimName: safari-pvc status: {} kubectl 命令和 context  可以通过 kubectl 上下文从主终端访问多个集群。将所有这些上下文名称写入 /opt/course/1/contexts。\n  接下来在 /opt/course/1/context_default_kubectl.sh 中写一个显示当前上下文的命令，该命令应该使用kubectl。\n  最后在 /opt/course/1/context_default_no_kubectl.sh 中写入第二个执行相同操作的命令，但不使用 kubectl。\n 知识点 kubectl config 相关命令的使用\n解题思路 kubectl config get-contexts -o name \u0026gt; /opt/course/1/contexts cat \u0026gt; /opt/course/1/context_default_kubectl.sh \u0026lt;\u0026lt;EOF kubectl config current-context EOF chmod +x /opt/course/1/context_default_kubectl.sh cat ~/.kube/config | grep current-context | awk \u0026#39;{print $2}\u0026#39; 工作负载：缩容  命名空间 project-c13 中有两个名为 o3db-* 的 Pod。 C13 管理层要求将 Pod 缩减为一个副本以节省资源。 记录动作。\n 知识点  scale deploy statefulset  参考文档： https://kubernetes.io/zh/docs/tasks/run-application/scale-stateful-set/ https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment\n解题思路 kubectl scale \u0026lt;resource\u0026gt; xxx --replicas=1 scale 命令需要确认资源类型：deployment/statefulset\n应用就绪和探活  在命名空间 default 中执行以下操作。为 nginx:1.16.1-alpine 创建一个名为 ready-if-service-ready 的 Pod。配置一个 LivenessProbe，它只是运行 true。还要配置一个 ReadinessProbe 来检查 url http://service-am-i-ready:80 是否可达，可以使用 wget -T2 -O- http://service-am-i-ready:80。 启动 Pod 并确认它因为 ReadinessProbe 而没有准备好。\n  创建第二个名为 am-i-ready 的 Pod 镜像 nginx:1.16.1-alpine，标签 id:cross-server-ready。已经存在的服务 service-am-i-ready 现在应该有第二个 Pod 作为端点。\n 知识点  probe pod  参考文档： https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n解题思路 kubectl run ready-if-service-ready --image nginx:1.16.1-alpine --dry-run=client -o yaml \u0026gt; ready-if-service-ready.yaml kubectl run am-i-ready --image nginx:1.16.1-alpine --labels id=cross-server-ready --dry-run=client -o yaml \u0026gt; am-i-ready.yaml 添加 probes\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: ready-if-service-ready name: ready-if-service-ready spec: containers: - image: nginx:1.16.1-alpine name: ready-if-service-ready resources: {} livenessProbe: exec: command: - echo - hi readinessProbe: exec: command: - wget - -T2 - -O- - http://service-am-i-ready:80 dnsPolicy: ClusterFirst restartPolicy: Always status: {} 集群：控制平面  使用 ssh cluster1-master1 ssh 进入主节点。检查 master 组件 kubelet、kube-apiserver、kube-scheduler、kube-controller-manager 和 etcd 如何在 master 节点上启动/安装。还要找出 DNS 应用的名称以及它是如何在主节点上启动/安装的。\n  将结果写入文件 /opt/course/8/master-components.txt。该文件的结构应如下所示：\n  # /opt/course/8/master-components.txt kubelet: [TYPE] kube-apiserver: [TYPE] kube-scheduler: [TYPE] kube-controller-manager: [TYPE] etcd: [TYPE] dns: [TYPE] [NAME]   Choices of [TYPE] are: not-installed, process, static-pod, pod\n 知识点 Kubernetes components 的安装方式\n解题思路 当前比较的组件都是以static pod的形式运行的，而 static pod 都是由 Kubelet 管理的，所以从 kubelet 处入手。\n以 minikube 为例：\n$ ps -ef | grep -w kubelet root 140597 1 2 May15 ? 00:36:00 /var/lib/minikube/binaries/v1.18.8/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=cka --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.3 $ systemctl is-active kubelet active #kubelet: process 根据前面进程中的信息，查看 /var/lib/kubelet/config.yaml中的内容。可以得到：\netcd: static-pod kube-apiserver: static-pod kube-controller-manager: static-pod\n$ cat /var/lib/kubelet/config.yaml | grep -i staticpod staticPodPath: /etc/kubernetes/manifests ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml\tkube-controller-manager.yaml kube-scheduler.yaml 最后上下dns，查看下pod，得知 dns: pod\n$ kubectl get pod -A | grep dns kube-system coredns-66bff467f8-6k2br 1/1 Running 0 32h 最后将上面的结果写入到 /opt/course/8/master-components.txt，不能前功尽弃。\n集群：Pod 调度  使用 ssh cluster2-master1 ssh 进入主节点。暂时停止 kube-scheduler，这意味着可以在之后再次启动它。\n  为镜像 httpd:2.4-alpine 创建一个名为 manual-schedule 的 Pod，确认它已启动但未在任何节点上调度。\n  现在您是调度程序并拥有所有权力，在节点 cluster2-master1 上手动调度该 Pod。 确保它正在运行。\n  再次启动 kube-scheduler 并通过在镜像 httpd:2.4-alpine 创建第二个名为 manual-schedule2 的 Pod 并检查它是否在 cluster2-worker1 上运行来确认其运行正常。\n 知识点  kubernetes 组件的运行方式 创建 pod pod 调度  解题思路 kube-scheduler 是以 static pod 的方式运行，因此我们需要 ssh 到节点上，将 scheduler 的 yaml 移出（记住不要删掉，还要还原回去），重启 kubelet\n$ ps -ef | grep -w kubelet root 140597 1 2 May15 ? 00:36:00 /var/lib/minikube/binaries/v1.18.8/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=cka --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.3 $ cat /var/lib/kubelet/config.yaml | grep -i staticpod staticPodPath: /etc/kubernetes/manifests $ ls /etc/kubernetes/manifests etcd.yaml kube-apiserver.yaml\tkube-controller-manager.yaml kube-scheduler.yaml $ mv /etc/kubernetes/manifests/kube-scheduler.yaml /etc/kubernetes $ systemctl restart kubelet 检查下 schedule pod 没有运行，然后尝试创建 pod，并查看 pod 处于 pending 状态，即没有 kube-scheduler 为其调度。\n$ kubectl run manual-schedule --image httpd:2.4-alpine $ kubectl get pod | grep manual-schedule manual-schedule 0/1 Pending 0 16s 手动调度，即为 pod 指定一个 nodeName，我的 minikube 只有一个 node 名为 cka，修改pod：\n$ kubectl get pod manual-schedule -o yaml \u0026gt; manual-schedule.yaml 添加 nodeName 之后\napiVersion: v1 kind: Pod metadata: creationTimestamp: \u0026#34;2021-05-16T07:27:16Z\u0026#34; labels: run: manual-schedule name: manual-schedule namespace: dev resourceVersion: \u0026#34;84805\u0026#34; selfLink: /api/v1/namespaces/dev/pods/manual-schedule uid: c4b592f6-1e07-4911-a7fe-867d813c7a55 spec: containers: - image: httpd:2.4-alpine imagePullPolicy: IfNotPresent name: manual-schedule resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-v7f28 readOnly: true dnsPolicy: ClusterFirst nodeName: cka #node name here enableServiceLinks: true priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - name: default-token-v7f28 secret: defaultMode: 420 secretName: default-token-v7f28 status: phase: Pending qosClass: BestEffort 强制更新 pod（运行时只能修改部分内容）：\n$ kubectl replace -f manual-schedule.yaml --force pod \u0026#34;manual-schedule\u0026#34; deleted pod/manual-schedule replaced 再次检查\n$ kubectl get pod | grep manual-schedule manual-schedule 1/1 Running 0 15s 恢复 kube-scheduler 的运行：\n$ mv /etc/kubernetes/kube-scheduler.yaml /etc/kubernetes/manifests $ systemctl restart kubelet 检查是否运行\n$ kubectl get pod -A | grep kube-scheduler kube-system kube-scheduler-cka 1/1 Running 0 66s 创建第二个pod，并检查是否在运行（running）状态\n$ kubectl run manual-schedule2 --image httpd:2.4-alpine pod/manual-schedule2 created kubectl get pod manual-schedule2 NAME READY STATUS RESTARTS AGE manual-schedule2 1/1 Running 0 6s 集群：备份及恢复  对在 cluster3-master1 上运行的 etcd 进行备份，并将其保存在主节点上的 /tmp/etcd-backup.db。\n  然后在集群中创建一个你喜欢的 Pod。\n  最后恢复备份，确认集群仍在工作并且创建的 Pod 不再与我们在一起。\n 知识点  etc 的作用：存储集群的状态信息，包括 pod 信息 etc 的备份和恢复  参考文档： https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster\n解题思路 etcd的命令执行，记得设置API的版本 ETCDCTL_API=3\n操作 etcd 需要 endpoints、cacert、cert、key。Kubernetes 的所有组件与 etcd 的数据交互都是通过 api-server 完成的，我只需要找到 api-server 的运行命令就行，两种方式：到 master 主机查看 api-server 的进程；或者去 api-server 的 pod 查看 .spec.containers[].command\n#ssh to master $ ps -ef | grep kube-apiserver $ kubectl get pod -n kube-system kube-apiserver-cka -o jsonpath=\u0026#39;{.spec.containers[].command}\u0026#39; etcd 备份，命令直接从 Kubernetes 官方文档复制再修改\n#ssh to master $ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \\  --cacert=/var/lib/minikube/certs/etcd/ca.crt --cert=/var/lib/minikube/certs/apiserver-etcd-client.crt --key=/var/lib/minikube/certs/apiserver-etcd-client.key \\  snapshot save /tmp/etcd-backup.db Snapshot saved at /tmp/etcd-backup.db 创建 pod\n$ kubectl run sleep1d --image busybox --command -- sleep 1d #检查 pod 运行情况 $ kubectl get pod NAME READY STATUS RESTARTS AGE sleep1d 1/1 Running 0 10s 恢复 etcd 的备份，复制前面的命令并修改，恢复备份到 /var/lib/etcd-backup\n$ ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/var/lib/minikube/certs/etcd/ca.crt --cert=/var/lib/minikube/certs/apiserver-etcd-client.crt --key=/var/lib/minikube/certs/apiserver-etcd-client.key snapshot restore /tmp/etcd-backup.db --data-dir /var/lib/etcd-backup 2021-05-16 08:09:17.797061 I | mvcc: restore compact to 85347 2021-05-16 08:09:17.803208 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32 修改 etcd 的配置， /etc/kubernetes/manifests/etcd.yaml\nvolumes: - hostPath: path: /var/lib/minikube/certs/etcd type: DirectoryOrCreate name: etcd-certs - hostPath: path: /var/lib/etcd-backup  #原来是/var/lib/minikube/etcd type: DirectoryOrCreate name: etcd-data status: {} 保存后重启kubelet\n$ systemctl restart kubelet 检查pod是否存在：\n$ kubectl get pod sleep1d Error from server (NotFound): pods \u0026#34;sleep1d\u0026#34; not found 安全：网络策略  发生了一起安全事件，入侵者能够从一个被黑的后端 Pod 访问整个集群。\n  为了防止这种情况，在命名空间 project-snake 中创建一个名为 np-backend 的 NetworkPolicy。它应该只允许 backend-* Pods：\n  连接到端口 1111 上的 db1-* Pod 连接到端口 2222 上的 db2-* Pod 在策略中使用 Pod 的应用程序标签。\n  实施后，例如，端口 3333 上从 backend-* Pod 到 vault-* Pod 的连接应该不再有效。\n 知识点  NetworkPolicy  参考文档： https://kubernetes.io/docs/concepts/services-networking/network-policies\n解题思路 为 backend-* pod 设置 egress 的 NetworkPolicy，只允许其访问 db1-* 的 1111 端口和 db2-* 的 2222 端口，策略中使用 app label 来进行匹配。\n从 Kubernetes 官网文档中复制一段yaml配置进行修改。\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: np-backend namespace: project-snake spec: podSelector: matchLabels: app: backend policyTypes: - Egress egress: - to: - podSelector: matchLabels: app: db1 ports: - protocol: TCP port: 1111 - to: - podSelector: matchLabels: app: db2  ports: - protocol: TCP port: 2222 假设 backend pod 的 app label 为 backend，db1 的 为 db1，db2 的为 db2。\n创建环境：\n$ kubectl run backend --image nginx --labels app=backend $ kubectl run db1 --image nginx --labels app=db1 $ kubectl run db2 --image nginx --labels app=db2 $ kubectl run vault --image nginx --labels app=vault $ kubectl get pod -L app NAME READY STATUS RESTARTS AGE APP backend 1/1 Running 0 13s backend db1 1/1 Running 0 66s db1 db2 1/1 Running 0 71s db2 vault 1/1 Running 0 79s vault 由于我们用的 nginx 镜像，将前面的 NetworkPolicy 端口修改一下：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: np-backend spec: podSelector: matchLabels: app: backend policyTypes: - Egress egress: - to: - podSelector: matchLabels: app: db1 ports: - protocol: TCP port: 80 - to: - podSelector: matchLabels: app: db2  ports: - protocol: TCP port: 80 检查一下：\n$ kubectl get networkpolicy NAME POD-SELECTOR AGE np-backend app=backend 31s 测试下网络：\n#获取pod ip $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES backend 1/1 Running 0 3m15s 172.17.0.7 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; db1 1/1 Running 0 4m8s 172.17.0.6 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; db2 1/1 Running 0 4m13s 172.17.0.3 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; vault 1/1 Running 0 4m21s 172.17.0.4 cka \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 集群：kubelet 启动方式  节点 cluster2-worker1 已使用 kubeadm 和 TLS 引导添加到集群中。 找到 cluster2-worker1 的 “Issuer” 和 “Extended Key Usage” 值： kubelet 客户端证书，用于向外连接到 kube-apiserver 的证书。 kubelet 服务器证书，用于来自 kube-apiserver 的传入连接。 将信息写入文件 /opt/course/23/certificate-info.txt。\n 知识点  kubelet 的功能：连接 api-server；接受来自 api-server 的响应。两种情况都需要 TLS  解题思路 kubelet 连接 apiserver 的方式在配置文件中，先找出配置文件的保存位置。\n# ssh 到节点上，查看 kubelet 的启动命令 $ ps -ef | grep kubelet root 3935 1 1 12:54 ? 00:00:23 /var/lib/minikube/binaries/v1.20.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cni-conf-dir=/etc/cni/net.mk --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=cka-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --network-plugin=cni --node-ip=192.168.64.9 docker 13653 13616 0 13:22 pts/0 00:00:00 grep kubelet #kubelet 连接 api server 的信息， client cert 的配置所在 /var/lib/kubelet/pki/kubelet-client-current.pem cat /var/lib/kubelet/config.yaml #kubelet 的启动信息， servert cert 的配置所在 /var/lib/minikube/certs/ca.crt cat /etc/kubernetes/kubelet.conf $ openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep -i issuer Issuer: CN = minikubeCA $ openssl x509 -noout -text -in /var/lib/kubelet/pki/kubelet-client-current.pem | grep -i -A1 extended X509v3 Extended Key Usage: TLS Web Client Authentication $ openssl x509 -noout -text -in /var/lib/minikube/certs/ca.crt | grep -i issuer Issuer: CN = minikubeCA $ openssl x509 -noout -text -in /var/lib/minikube/certs/ca.crt | grep -i -A1 extended X509v3 Extended Key Usage: TLS Web Client Authentication, TLS Web Server Authentication 最后记得将信息写入到 /opt/course/23/certificate-info.txt\n集群：证书  检查 kube-apiserver 服务器证书在 cluster2-master1 上的有效期。使用 openssl 或 cfssl 执行此操作。将到期日期写入 /opt/course/22/expiration。\n  同时运行正确的 kubeadm 命令以列出到期日期并确认两种方法显示相同的日期。\n  将更新 apiserver 服务器证书的正确 kubeadm 命令写入 /opt/course/22/kubeadm-renew-certs.sh。\n 知识点  api-server openssl kubeadm  参考文档：https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#check-certificate-expiration\n解题思路 通过 kube-apiserver pod 的启动命令，或者 ssh 到 master 来查看命令参数，tls-cert-file=/var/lib/minikube/certs/apiserver.crt\n$ openssl x509 -noout -text -in /var/lib/minikube/certs/apiserver.crt | grep -i valid -A2 Validity Not Before: May 13 22:33:43 2021 GMT Not After : May 14 22:33:43 2022 GMT 将 May 14 22:33:43 2022 GMT 写入 /opt/course/22/expiration\n通过 kubeadm 来检查\n$ kubeadm certs check-expiration | grep -i apiserver #macos 无法安装 kubeadm #minikube 无法使用 kubeadm 检查 将 kubeadm certs renew apiserver 写入 /opt/course/22/kubeadm-renew-certs.sh\n集群：升级节点  你的同事说节点 cluster3-worker2 运行的是较旧的 Kubernetes 版本，甚至不属于集群的一部分。将 kubectl 和 kubeadm 更新为在 cluster3-master1 上运行的确切版本。然后将此节点添加到集群中，您可以为此使用kubeadm。\n 知识点  kubeadm 升级集群  参考文档：\n解题思路 检查node\n$ kubectl get nodes 检查当前组件版本\n$ ssh cluster3-worker2 $ kubeadm version $ kubectl version --short Client Version: vx.xx.x Server Version: vx.xx.x $ kubelet --version #使用命令并升级各个组件，并重启 kubelet #如果启动失败，一般是需要token连接到api-server，需要ssh到master上运行 kubeadm create token --print-join-command #再ssh到 node上，执行打印的命令，重启kubelet并检查装填 #最后检查node是否成功加入集群 Docker 命令  在命名空间 project-tiger 中创建一个名为 Tigers-reunite 的 Pod 镜像 httpd:2.4.41-alpine，标签为 pod=container 和 container=pod。找出 Pod 被安排在哪个节点上。ssh 进入该节点并找到属于该 Pod 的 docker 容器。\n  将容器的 docker ID 和这些正在运行的进程/命令写入 /opt/course/17/pod-container.txt。\n  最后，使用 docker 命令将主 Docker 容器（来自 yaml 中指定的那个）的日志写入 /opt/course/17/pod-container.log。\n 知识点  docker 命令：ps、logs、inspect  解题思路 创建 pod\n$ kubectl run tigers-reunite --image httpd:2.4.41-alpine --labels pod=container,container=pod 检查 pod 的信息\n$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS tigers-reunite 1/1 Running 0 34s container=pod,pod=container 获取pod所在的节点\n$ kubectl get pod tigers-reunite -o jsonpath=\u0026#39;{.spec.nodeName}\u0026#39; cka ssh到节点上\ndocker ps | grep tigers-reunite e6ff69b437bc 54b0995a6305 \u0026#34;httpd-foreground\u0026#34; About a minute ago Up About a minute k8s_tigers-reunite_tigers-reunite_dev_53391212-911d-4275-a19d-e8f8b0f85a98_0 06d3ca65eb08 k8s.gcr.io/pause:3.2 \u0026#34;/pause\u0026#34; About a minute ago Up About a minute k8s_POD_tigers-reunite_dev_53391212-911d-4275-a19d-e8f8b0f85a98_0 #使用docker inspect 或者 进入容器直接查看进程 $ docker inspect e6ff69b437bc | grep -i \u0026#39;cmd\\|entrypoint\u0026#39; -A1 \u0026#34;Cmd\u0026#34;: [ \u0026#34;httpd-foreground\u0026#34; -- \u0026#34;Entrypoint\u0026#34;: null, \u0026#34;OnBuild\u0026#34;: null, $ docker inspect 06d3ca65eb08 | grep -i \u0026#39;cmd\\|entrypoint\u0026#39; -A1 \u0026#34;Cmd\u0026#34;: null, \u0026#34;Image\u0026#34;: \u0026#34;k8s.gcr.io/pause:3.2\u0026#34;, -- \u0026#34;Entrypoint\u0026#34;: [ \u0026#34;/pause\u0026#34; 结果写入文件\ne6ff69b437bc httpd-foreground 06d3ca65eb08 pause 写日志到文件\ndocker logs e6ff69b437bc \u0026gt; /opt/course/17/pod-container.log ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/notes-for-cka-preparation/","tags":["Kubernetes","云原生"],"title":"Kubernetes CKA 证书备考笔记"},{"categories":["源码解析","笔记"],"contents":"自从参加了 Flomesh 的 workshop，了解了可编程网关 Pipy。对这个“小东西”充满了好奇，前后写了两篇文章，看了部分源码解开了其部分面纱。但始终未见其全貌，没有触及其核心设计。\n不是有句话，“好奇害死猫”。其实应该还有后半句，“满足了就没事”（见维基百科）。\n所有就有了今天的这一篇，对前两篇感兴趣的可以跳转翻看。\n 初探可编程网关 Pipy 可编程网关 Pipy 第二弹：编程实现 Metrics 及源码解读  言归正传。\n事件模型 上篇写了 Pipy 基于事件的信息流转，其实还未深入触及其核心的事件模型。既然是事件模型，先看事件。\nsrc/event.hpp:41 中定义了 Pipy 的四种事件：\n Data MessageStart MessageEnd SessionEnd  翻看源码可知（必须吐槽文档太少）这几种事件其实是有顺序的：MessageStart -\u0026gt; Data -\u0026gt; MessageEnd -\u0026gt; SessionEnd。\n这种面向事件模型，必然有生产者和消费者。又是翻看源码可知，生产者和消费者都是 pipy::Filter。我们在上篇文章中讲过：每个 Pipeline 都有一个过滤器链，类似单向链表的数据结构。\n那是不是按照上面说的，事件是从一个 Filter 流向下一个 Filter？也对，也不对。\n矛盾？ 先看 Filter 如何向下传递事件，src/session.cpp:55 处，Filter 持有 output 变量，类似为 Event::Receiver（参数为 Event 的 std::function 的别名，作为外行的笔者并不懂 c++，但不妨碍了解程序设计）。通过 Receiver 调用下一个 Filter 的 #process 方法。\n这里的 Receiver 就可以理解为事件发送的窗口，而 #process(Context *ctx, Event *inp) 就是事件的接收窗口。\n这就是前面为什么说 “事件是从一个 Filter 流向下一个 Filter” 是正确的。\n为什么不对？首先，一个 Filter 会产生多个事件，比如 decodeHttpRequest 可能会产生 MessageStart、Data 和 MessageEnd 事件，并且每产生一个事件都会通过Receiver 向下传递，不会等 #process 流程结束才传递事件；再就是下一个 Filter 可能并不会对某个事件感兴趣（下一个 Filter 的 #process 方法不做任何处理就返回了）。\n可能看下图会更容易理解（图中 no output 表明事件不会向下传递）：\n最简单的示例 在 test/001-echo/pipy.js 提供了的示例：\npipy() .listen(6080) .decodeHttpRequest() .encodeHttpResponse() 发起请求\n$ curl -X POST localhost:6080 -d \u0026#39;{}\u0026#39; {} HTTP 消息体\n#request POST / HTTP/1.1 Content-Type: application/javascript User-Agent: PostmanRuntime/7.28.1 Accept: */* Postman-Token: fc84b575-7fea-487b-a55d-f6085bc62cf7 Host: localhost:6080 Accept-Encoding: gzip, deflate, br Connection: keep-alive Content-Length: 2  {} #response HTTP/1.1 200 OK postman-token: fc84b575-7fea-487b-a55d-f6085bc62cf7 accept-encoding: gzip, deflate, br host: localhost:6080 accept: */* user-agent: PostmanRuntime/7.28.1 content-type: application/javascript Connection: keep-alive Content-Length: 2 {} 这里我们以过滤器 decodeHttpRequest 为例，官方的说明是 Deframes an HTTP request message。前面提到它会产生 3 个事件，都是在 deframe 的过程中发出的。\nSession 调用第一个 Filter 时，传入的事件类型是 event::Data。decodeHttpRequest 关注该事件，并按照 HTTP 协议开始解析。\n在上图可以看到解析的不同阶段，会发出不同的事件。调用 Receiver 传输事件，调用 encodeHttpResponse 的 #process() 方法。\n这里又会好奇，假如上面的示例中去掉两个过滤器中的任何一个，或者都去掉，能不能正常工作？\n答案是都不能！响应状态码都是 502 Bad Gateway（curl/httpie）。\n分析 这里需要结合本文的第一张图 event-handling-flow。\n去掉两个过滤器 假如两个都去掉了，HTTP Request 请求消息会被直接回传给客户端，协议错误。\n去掉 decodeHttpRequest 前面提到 Session 传给第一个 Filter 的事件是 event::Data。而 encodeHttpResponse 针对该事件只会将其保存到 buffer 中。\n然后整个链路在此结束，没有回传任何数据。客户端会等待响应，超时退出（curl）。\n去掉 encodeHttpResponse 先说结果，与前面一样超时退出。\n为什么会这样，明明 decodeHttpRequest 产生了 3 个时间，Session 里的 Receiver 也有收到，也确实写回了请求 body 里的 {}。\nencodeHttpResponse 过滤器有写回响应头，缺少了这些信息，响应就不并不是合法的 HTTP 协议，只是普通的 TCP 协议。\n总结 Pipy 基于事件模型的设计，提供了强大的灵活性。允许我们在“规则”中使用不同过滤器针对不同的事件，对请求和响应的信息进行处理。\n“规则” 就是业务逻辑的核心，而 Pipy 就是这逻辑的执行引擎。\n最后，“好奇心是成长的驱动力，永远保持好奇心。”\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/pipy-event-handling-design/","tags":["Pipy"],"title":"可编程网关 Pipy 第三弹：事件模型设计"},{"categories":["云原生"],"contents":"之前写过一篇 介绍了工具加速云原生 Java 开发。\n其实日常工作中在集群上的操作也非常多，今天就来介绍我所使用的工具。\nkubectl-alias 使用频率最高的工具，我自己稍微修改了一下，加入了 StatefulSet 的支持。\n这个是我的 https://github.com/addozhang/kubectl-aliases，基于 https://github.com/ahmetb/kubectl-aliases。\n比如输出某个 pod 的 json，kgpoojson xxx 等同于 kubectl get pod xxx -o json。\n结合 jq 使用效果更好 😂。\n语法解读  k=kubectl  sys=--namespace kube-system   commands:  g=get d=describe rm=delete a:apply -f ak:apply -k k:kustomize ex: exec -i -t lo: logs -f   resources:  po=pod, dep=deployment, ing=ingress, svc=service, cm=configmap, sec=secret,ns=namespace, no=node   flags:  output format: oyaml, ojson, owide all: --all or --all-namespaces depending on the command sl: --show-labels w=-w/--watch   value flags (should be at the end):  n=-n/--namespace f=-f/--filename l=-l/--selector    kubectx + kubens 安装看这里\nkubectx 用于在不同的集群间进行快速切换。假如用 kubectl，你需要：\n# context 列表 kubectl config current-context # 设置 context kubectl config use-context coffee kubens 就是在不同 namespace 间快速切换的工具。用 kubectl 的话，需要：\n# namespace 列表 kbuectl get ns # kubectl config set-context --current --namespace=kube-system k9s 没错，只比 k8s 多了个 1 😂。\nk9s 提供了终端 UI 与 Kubernetes 集群进行编辑交互。本人常用的比如：\n F 配置端口转发 l 输出 pod 日志 e 修改资源对象 s pod 终端交互模式 y yaml 方式输出资源对象 d describe 资源对象 ctrl+d 删除 pod  启动方式\n# 指定 namespace 运行 k9s -n mycoolns # 指定 context 运行 k9s --context coolCtx # 只读模式运行 k9s --readonly 键入问号“?” 就可以打开快捷操作指引。\nstern stern 可以用来 tail 集群上的多个 pod 和 pod 中多个容器的日志。不同的 pod 和容器以不同的颜色区分，方便 debug。\n比如使用命令 stern -l tier=control-plane -n kube-system 可以输出 kube-system 命名空间下控制平面（label 为 tier=control-plane） pod 的日志。\n命令行选项\nTail multiple pods and containers from Kubernetes Usage: stern pod-query [flags] Flags: -A, --all-namespaces If present, tail across all namespaces. A specific namespace is ignored even if specified with --namespace. --color string Color output. Can be \u0026#39;always\u0026#39;, \u0026#39;never\u0026#39;, or \u0026#39;auto\u0026#39; (default \u0026#34;auto\u0026#34;) --completion string Outputs stern command-line completion code for the specified shell. Can be \u0026#39;bash\u0026#39; or \u0026#39;zsh\u0026#39; -c, --container string Container name when multiple containers in pod (default \u0026#34;.*\u0026#34;) --container-state string If present, tail containers with status in running, waiting or terminated. Default to running. (default \u0026#34;running\u0026#34;) --context string Kubernetes context to use. Default to current context configured in kubeconfig. -e, --exclude strings Regex of log lines to exclude -E, --exclude-container string Exclude a Container name --field-selector string Selector (field query) to filter on. If present, default to \u0026#34;.*\u0026#34; for the pod-query. -h, --help help for stern -i, --include strings Regex of log lines to include --init-containers Include or exclude init containers (default true) --kubeconfig string Path to kubeconfig file to use -n, --namespace strings Kubernetes namespace to use. Default to namespace configured in Kubernetes context. To specify multiple namespaces, repeat this or set comma-separated value. -o, --output string Specify predefined template. Currently support: [default, raw, json] (default \u0026#34;default\u0026#34;) -l, --selector string Selector (label query) to filter on. If present, default to \u0026#34;.*\u0026#34; for the pod-query. -s, --since duration Return logs newer than a relative duration like 5s, 2m, or 3h. Defaults to 48h. --tail int The number of lines from the end of the logs to show. Defaults to -1, showing all logs. (default -1) --template string Template to use for log lines, leave empty to use --output flag -t, --timestamps Print timestamps --timezone string Set timestamps to specific timezone (default \u0026#34;Local\u0026#34;) -v, --version Print the version and exit Lens Lens 是用来控制 Kubernetes 的 IDE，开源且免费。\n消除了集群操作的复杂性、提供了实时的可观察性、方便故障排查、支持多系统的桌面客户端、兼容多种集群。\nInfra App Infra App 跟 Lens 差不多，UI 较 Lens 好些，但是功能就弱很多，类似 Lens 的只读模式。\n免费版比收费版的区别只在于支持的集群数量，免费版只支持一个集群。\nkubefwd kubefwd，这个一直有安装但是使用次数寥寥，因为应用之间的访问没有走 service，不过偶尔做些实验的时候会用的上。\n kubefwd 是一个用于端口转发Kubernetes中指定namespace下的全部或者部分pod的命令行工具。 kubefwd 使用本地的环回IP地址转发需要访问的service，并且使用与service相同的端口。 kubefwd 会临时将service的域条目添加到 /etc/hosts 文件中。\n  启动kubefwd后，在本地就能像在Kubernetes集群中一样使用service名字与端口访问对应的应用程序。\n 总结 善用工具可以提升效率，但并不是不可或缺的。\n如果你有其他的工具，欢迎留言提出。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/tools-accelerate-work-on-kubernetes-cluster/","tags":["Kubernetes","Tool"],"title":"常用的几款工具让 Kubernetes 集群上的工作更容易"},{"categories":["云原生"],"contents":"本文详细介绍了 Jenkins 如何通过 tekton-client-plugin 实现与 Kubernetes 上的 Tekton Pipeline 交互，包括 Kubernetes 上安装 Jenkins、Tekton Pipelines 等。\n关于如何使用 Tekton Pipeline 实现 CICD 可以看这篇文章 云原生 CICD: Tekton Pipeline 实战\n本文用于构建的项目以及所有 manifest yaml 都在可以在这里下载。\nTL;DR 惯例，先上总结。tekton-client-plugin 虽然还是处于初期阶段，但是 其价值非常明显，尤其是对先用使用 Jenkins 作为 CICD 实现的用户来说。从 Jenkins 迁移到云原生的 Tekton 时，可以省掉用户界面的开发成本，而且尽可能少的改变用户习惯 ，依靠版本管理可以控制迁移的节奏。\ntekton-client-plugin 在今年 5 月 7 日发布的 1.0.0 版本，目前为 1.0.02。目前还处于初期阶段，我个人感觉目前仅仅算是打通 Jenkins 与 Tekton 交互这条路，扩展性还不够好。\n比如目前仅仅支持如下几个参数注入到 PipelineRun 中，难以支撑复杂的流程控制，支持的 Pipeline 参数 hardcode 在代码中。\n  BUILD_ID - the build id/number of the Jenkins job JOB_NAME - the name of the jenkins job that triggered this pipeline PULL_BASE_REF - name of the base branch PULL_PULL_SHA - the commit sha of the pull request or branch REPO_NAME - name of the repository REPO_OWNER - owner of the repository REPO_URL - the URL of the repository   希望后面会支持自定义参数，比如将更多的项目元数据信息注册到 Pipeline 中。\n值得一提的是，tekton-client-plugin 提供了对 Job DSL 的支持，本文后面没有用这种方式，而是用的 FreeStyle Project。\npipeline { agent any stages { stage('Stage') { steps { checkout scm tektonCreateRaw(inputType: 'FILE', input: '.tekton/pipeline.yaml') } } } } 前置条件 环境  Kubernetes：推荐 minikube Jenkins：建议在 Kubernetes 上安装 Tekton 用于构建的项目  工具  kubectl tektoncd-cli kubectx、kubens helm  Kubernetes 上安装 Jenkins（Helm） Jenkins 这里使用 Helm 安装到 Kubernetes 上。\n初始化命名空间、持久化卷、ServiceAccount 等。\nkubectl create namespace jenkins kubens jenkins # 持久化存储，笔者将容量修改为 5G http https://raw.githubusercontent.com/jenkins-infra/jenkins.io/master/content/doc/tutorials/kubernetes/installing-jenkins-on-kubernetes/jenkins-volume.yaml --body \u0026gt; jenkins-volume.yaml # 创建 PV kubectl apply -f jenkins-volume.yaml # 创建 service account kubectl apply -f https://raw.githubusercontent.com/jenkins-infra/jenkins.io/master/content/doc/tutorials/kubernetes/installing-jenkins-on-kubernetes/jenkins-sa.yaml 准备 helm 环境并添加 Jenkins ChartRepo # homebrew 安装 helm brew install helm # 添加 jenkins chart repo helm repo add jenkinsci https://charts.jenkins.io helm repo update 配置 Jenkins Chart  下载官方的 values yaml进行修改：http https://raw.githubusercontent.com/jenkinsci/helm-charts/main/charts/jenkins/values.yaml \u0026gt; jenkins-values.yaml 修改 serviceType 为 NodePort，并增加 nodePort: 32000。用于从 minikube 外访问 Jenkins 修改 storageClass 为 jenkins-pv。前面创建 PV 的时候使用了 jenkins-pv 作为 Dynamic Volume Provisioning 的 storageClass 修改 serviceAccount 部分，将 create 设置为 false（上面已经创建了 serviceAccount），同时将 name 指定为前面的 sa 名字 jenkins installPlugins 下增加 tekton-client:1.0.2 修改 adminPassword 为 admin。指定初始密码（不指定也可以通过安装输出的说明获取初始密码） 修改 persistence 的 size 为 5Gi （我的 minikube 的虚拟机只有 20Gi 大小）  修改后的文件在这里 jenkins-values.yaml。\n执行安装 chart=jenkinsci/jenkins helm install jenkins -n jenkins -f jenkins-values.yaml $chart 输出结果：\nNAME: jenkins LAST DEPLOYED: Sun Jun 20 22:05:53 2021 NAMESPACE: jenkins STATUS: deployed REVISION: 1 获取 Jenkins 的访问地址 echo $(minikube ip):$(kubectl get svc jenkins -o jsonpath=\u0026quot;{.spec.ports[0].nodePort}\u0026quot;)，然后使用前面设置的账号登录。\nTekton 安装 kubectl create ns tekton-pipelines kubens tekton-pipelines kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 安装 CLI brew install tektoncd-cli\nRBAC Tekton Pipeline 安装完成后，需要给前面创建的 ServiceAccount jenkins 增加 tekon 资源的操作权限。\n//tekton-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: tekton-role namespace: tekton-pipelines rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods - pods/log verbs: - get - list - watch - apiGroups: - tekton.dev resources: - tasks - taskruns - pipelines - pipelineruns verbs: - create - delete - deletecollection - get - list - patch - update - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: tekton-role-binding namespace: tekton-pipelines roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: tekton-role subjects: - kind: ServiceAccount name: jenkins namespace: jenkins Jenkins 与 Tekton 交互 前面大篇幅的都只是准备工作，Jenkins 安装时我们已经添加了 tekton-client-plugin 插件。\n添加一个名为 tekton-client-sample 的 FreeStyle project。\nSCM 这里填入用于构建的项目仓库地址以及分支。\nBuild 模块中选择 Tekton: Create Resource (RAW)\n这里选择 FILE 类型，因为我已经将 PipelineRun 的 yaml 放进了代码仓库中了。\n执行一次构建\n检查下应用\n$ kubectl get pod | grep tekton-test tekton-test-75975dcc88-xkzb6 1/1 Running 0 3m13s tekton-test-c26lw-deploy-to-k8s-28trp-pod-w8tgc 0/1 Completed 0 3m18s tekton-test-c26lw-fetch-from-git-pv66g-pod-nm5qh 0/1 Completed 0 6m30s tekton-test-c26lw-source-to-image-k8mpg-pod-qh7g4 0/2 Completed 0 6m15s $ curl $(minikube ip):$(kubectl get svc tekton-test -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34;)/hi hello world 参考  Jenkins on Kubernetes Tekton Client Plugin Tutorial Easily reuse Tekton and Jenkins X from Jenkins  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/jenkins-interact-with-tekton-pipelines-via-plugin/","tags":["Jenkins","Tekton","Kubernetes","DevOps"],"title":"Jenkins 如何与 Kubernetes 集群的 Tekton Pipeline 交互？"},{"categories":["教程","云原生"],"contents":"更新历史：\n v1：2020.1.21 基于 Tekton Pipline v0.9.0 v2（当前）：2021.6.22 基于 Tekton Pipeline v0.25.0   Tekton 是 Google 开源的 Kubernetes 原生CI/CD 系统, 功能强大扩展性强. 前身是 Knavite 里的 build-pipeline 项目, 后期孵化成独立的项目. 并成为 CDF 下的四个项目之一, 其他三个分别是 Jenkins, Jenkins X, Spinnaker.\n为什么说 Tekton 是 Kubernetes 原生的, 以内其基于 Kubernetes 的 CRD 定义了 Pipeline 流水线.\nCRD 及说明:\n Task: 构建任务, 可以定义一些列的 steps. 每个 step 由一个 container 执行. TaskRun: task 实际的执行, 并提供执行所需的参数. 这个对象创建后, 就会有 pod 被创建. Pipeline: 定义一个或者多个 task 的执行, 以及 PipelineResource 和各种定义参数的集合 PipelineRun: 类似 task 和 taskrun 的关系: 一个定义一个执行. PipelineRun 则是 pipeline 的实际执行. 创建后也会创建 pod 来执行各个 task. PipelineResource: 流水线的输入资源, 比如 github/gitlab 的源码, 某种存储服务的文件, 或者镜像等. 执行时, 也会作为 pod 的其中一个 container 来运行(比如拉取代码). PipelineResource 目前处于 Alaha，至于原因可以看Why Aren\u0026rsquo;t PipelineResources in Beta? Condition: 在 pipeline 的 task 执行时通过添加 condition 来对条件进行评估, 进而判断是否执行 task. 目前是WIP的状态, 待#1137的完成.  组件:\n tekton-pipelines-controller: 监控 CRD 对象(TaskRun, PipelineRun)的创建, 为该次执行创建 pod. tekton-pipelines-webhook: 对 apiserver 提供 http 接口做 CRD 对象的校验.  前置条件 文中使用的一些工具，基本都可以通过 homebrew 安装：\n jq ：操作 json 的命令行工具 httpie：HTTP 客户端命令行工具 minikube 环境  文中的 Java 项目以及 tekton 的相关 yaml 都已经提交到了 tekton-test.\n安装 参考上一篇文章, 文章中有个简单的\u0026quot;hello world\u0026quot;.\n实践 到了这里相信已经安装好了 Tekton. 我们使用Spring Initializer生成的项目为例, 演示如何使用 Tekton 实现 CICD.\n开始之前简单整理下这个项目的 CICD 流程:\n 拉取代码 maven 打包 构建镜像并推送 部署  注: 所有的操作都是在 tekton-pipelines namespace 下操作\n0x00 添加 Dockerfile 和部署用的 yaml 用于构建镜像的Dockerfile\nFROM openjdk:8-jdk-alpine RUN mkdir /app WORKDIR /app COPY target/*.jar /app/app.jar ENTRYPOINT [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;java -Xmx128m -Xms64m -jar app.jar\u0026quot;] 用于部署 K8s Deployment 的 deployment.yml，同时通过创建 NodePort 类型的 Service 用于访问应用。\napiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; metadata: labels: app: \u0026#34;tekton-test\u0026#34; name: \u0026#34;tekton-test\u0026#34; spec: replicas: 1 selector: matchLabels: app: \u0026#34;tekton-test\u0026#34; template: metadata: labels: app: \u0026#34;tekton-test\u0026#34; spec: containers: - image: \u0026#34;addozhang/tekton-test:latest\u0026#34; imagePullPolicy: \u0026#34;Always\u0026#34; livenessProbe: failureThreshold: 3 httpGet: path: \u0026#34;/actuator/info\u0026#34; port: 8080 scheme: \u0026#34;HTTP\u0026#34; initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 name: \u0026#34;tekton-test\u0026#34; ports: - containerPort: 8080 name: \u0026#34;http\u0026#34; protocol: \u0026#34;TCP\u0026#34; readinessProbe: failureThreshold: 3 httpGet: path: \u0026#34;/actuator/info\u0026#34; port: 8080 scheme: \u0026#34;HTTP\u0026#34; initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 10 --- apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: app: tekton-test name: tekton-test spec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: tekton-test type: NodePort  0x01 RBAC 创建 ServiceAccount 用于 Pipeline 的运行。\n注：这里为了方便，授予了 ClusterRole admin。\n# serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tekton-build --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: pipeline-admin-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: admin # user cluster role admin subjects: - kind: ServiceAccount name: tekton-build namespace: tekton-pipelines 0x02 拉取代码 代码作为构建的输入, 需要提供一个 Pipeline CRD 对象来表示输入是从 git 仓库来获取代码。\n访问 Tekton Hub 可以找到现成的 git-clone task。\n使用 kubectl 安装：\nkubectl apply -f https://raw.githubusercontent.com/tektoncd/catalog/main/task/git-clone/0.4/git-clone.yaml 或者使用 tekton-cli 安装：\ntkn hub install task git-clone 0x03 maven 打包 Task source-to-image.yaml的 step maven：\nspec: workspaces: - name: source steps: - name: maven image: maven:3.5.0-jdk-8-alpine workingDir: $(workspaces.source.path) command: - mvn args: - clean - install - -DskipTests volumeMounts: - name: m2 mountPath: /root/.m2 volumes: - name: m2 hostPath: path: /home/docker/.m2  说明:\n有了代码下一步就是执行 maven 的编译打包, 在maven:3.5.0-jdk-8-alpine镜像中执行mvn的相关命令.\n这里挂在了一个本地的volume, 避免每次构建重复下载依赖包, 同时里面还有 settings.xml\n注意: 对于 minikube, hostPath 请使用/data/.m2, 否则minikube重启后无法持久化\n0x04 构建镜像并推送 Task source-to-image.yaml 的 step build-and-push：\nspec: params: - name: pathToDockerFile description: The path to the dockerfile to build (relative to the context) default: Dockerfile - name: imageUrl description: Url of image repository - name: imageTag description: Tag to apply to the built image default: latest workspaces: - name: source - name: dockerconfig mountPath: /kaniko/.docker # config.json 的挂载目录 steps: - name: build-and-push image: gcr.io/kaniko-project/executor:v1.6.0-debug command: - /kaniko/executor args: - --dockerfile=$(params.pathToDockerFile) - --destination=$(params.imageUrl):$(params.imageTag) - --context=$(workspaces.source.path) 说明:\n镜像的构建, 我们采用了 kaniko。\n镜像仓库我们选择了Docker Hub, 推送的时候需要使用 credentials。\nkaniko 需要将 docker config 的文件存在于 /kanika/.docker 目录下。这里的思路是将 docker 的 config.json，以 secret 的方式持久化，在通过先添加 docker-registry类型的 secret，然后通过 workspace 的方式输入到 kaniko 运行环境中。\nconfig.json 里面保存的 json 结构化的数据，为了方便通过 dry run 创建\nkubectl create secret docker-registry dockerhub --docker-server=https://index.docker.io/v1/ --docker-username=[USERNAME] --docker-password=[PASSWORD] --dry-run=client -o json | jq -r \u0026#39;.data.\u0026#34;.dockerconfigjson\u0026#34;\u0026#39; | base64 -d \u0026gt; /tmp/config.json \u0026amp;\u0026amp; kubectl create secret generic docker-config --from-file=/tmp/config.json \u0026amp;\u0026amp; rm -f /tmp/config.json 执行:\nkubectl apply -f tasks/source-to-image.yaml 0x05 部署 deploy-to-k8s.yaml:\napiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: deploy-to-k8s spec: inputs: resources: - name: git-source type: git params: - name: pathToYamlFile description: The path to the yaml file to deploy within the git source default: deployment.yaml steps: - name: run-kubectl image: lachlanevenson/k8s-kubectl command: [\u0026#34;kubectl\u0026#34;] args: - \u0026#34;apply\u0026#34; - \u0026#34;-f\u0026#34; - \u0026#34;/workspace/git-source/$(inputs.params.pathToYamlFile)\u0026#34; 说明:\n pathToYamlFile: 指定部署应用的 yaml。  执行:\nkubectl apply -f tasks/deploy-to-k8s.yaml 0x06 组装流水线 build-pipeline.yaml\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: build-pipeline spec: params: - name: git-url - name: git-revision - name: pathToContext description: The path to the build context, used by Kaniko - within the workspace default: . - name: imageUrl description: Url of image repository - name: imageTag description: Tag to apply to the built image workspaces: - name: git-source - name: docker-config tasks: - name: fetch-from-git taskRef: name: git-clone params: - name: url value: \u0026#34;$(params.git-url)\u0026#34; - name: revision value: \u0026#34;$(params.git-revision)\u0026#34; workspaces: - name: output workspace: git-source - name: source-to-image taskRef: name: source-to-image params: - name: imageUrl value: \u0026#34;$(params.imageUrl)\u0026#34; - name: imageTag value: \u0026#34;$(params.imageTag)\u0026#34; workspaces: - name: source workspace: git-source - name: dockerconfig workspace: docker-config runAfter: - fetch-from-git - name: deploy-to-k8s taskRef: name: deploy-to-k8s params: - name: pathToYamlFile value: deployment.yaml workspaces: - name: source workspace: git-source runAfter: - source-to-image 执行:\nkubectl apply -f tasks/deploy-to-k8s.yaml 0x07 执行流水线 apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: generic-pr- name: generic-pipeline-run spec: pipelineRef: name: build-pipeline params: - name: git-revision value: main - name: git-url value: https://github.com/addozhang/tekton-test.git  - name: imageUrl value: addozhang/tekton-test - name: imageTag value: latest workspaces: - name: git-source volumeClaimTemplate: spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi - name: docker-config secret: secretName: docker-config serviceAccountName: tekton-build 执行:\nkubectl apply -f run/run.yaml 0x08 结果 执行流水线后, 可以看到分别创建了下面的几个 pod:\n generic-pipeline-run-deploy-to-k8s-xxx generic-pipeline-run-fetch-from-git-xxx generic-pipeline-run-source-to-image-xxx  以及我们的应用 tekton-test-xxx，发起请求测试：\n$ http $(minikube ip):$(kubectl get svc tekton-test -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34;)/hi --body hello world 总结 目前 Tekton 进入 beta 阶段, 最新的版本是 0.25.0。基于 CRD 的实现让 Tekton 在实际使用中可以灵活的设计自己的 CICD 流程.\n生态也越来越完善，比如 Tekton Hub 提供了大量的可重用最佳实现的 Task 和 Pipeline。\n下一篇，我们尝试下如何在 Jenkins 中与 Tekton Pipeline 进行交互。\n更多文章:\n Tekton 的工作原理 Tekton Dashboard 安装 Tekton Trigger 介绍 Tekton Trigger 实战  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/tekton-pipeline-practice/","tags":["DevOps","Tekton","Kubernetes","云原生"],"title":"云原生 CICD: Tekton Pipeline 实战"},{"categories":["源码解析"],"contents":"本文主要介绍 kubelet 功能、核心组件，以及启动流程的源码分析，总结了 kubelet 的工作原理。\nkubelet 简介 从官方的架构图中很容易就能找到 kubelet\n执行 kubelet -h 看到 kubelet 的功能介绍：\n kubelet 是每个 Node 节点上都运行的主要“节点代理”。使用如下的一个向 apiserver 注册 Node 节点：主机的 hostname；覆盖 host 的参数；或者云提供商指定的逻辑。 kubelet 基于 PodSpec 工作。PodSpec 是用 YAML 或者 JSON 对象来描述 Pod。Kubelet 接受通过各种机制（主要是 apiserver）提供的一组 PodSpec，并确保里面描述的容器良好运行。  除了由 apiserver 提供 PodSpec，还可以通过以下方式提供：\n 文件 HTTP 端点 HTTP 服务器  kubelet 功能归纳一下就是上报 Node 节点信息，和管理（创建、销毁）Pod。 功能看似简单，实际不然。每一个点拿出来都需要很大的篇幅来讲，比如 Node 节点的计算资源，除了传统的 CPU、内存、硬盘，还提供扩展来支持类似 GPU 等资源；Pod 不仅仅有容器，还有相关的网络、安全策略等。\nkubelet 架构 重要组件 kubelet 的架构由 N 多的组件组成，下面简单介绍下比较重要的几个：\nPLEG 即 Pod Lifecycle Event Generator，字面意思 Pod 生命周期事件（ContainerStarted、ContainerDied、ContainerRemoved、ContainerChanged）生成器。\n其维护着 Pod 缓存；定期通过 ContainerRuntime 获取 Pod 的信息，与缓存中的信息比较，生成如上的事件；将事件写入其维护的通道（channel）中。\nPodWorkers 处理事件中 Pod 的同步。核心方法 managePodLoop() 间接调用 kubelet.syncPod() 完成 Pod 的同步：\n 如果 Pod 正在被创建，记录其延迟 生成 Pod 的 API Status，即 v1.PodStatus：从运行时的 status 转换成 api status 记录 Pod 从 pending 到 running 的耗时 在 StatusManager 中更新 pod 的状态 杀掉不应该运行的 Pod 如果网络插件未就绪，只启动使用了主机网络（host network）的 Pod 如果 static pod 不存在，为其创建镜像（Mirror）Pod 为 Pod 创建文件系统目录：Pod 目录、卷目录、插件目录 使用 VolumeManager 为 Pod 挂载卷 获取 image pull secrets 调用容器运行时（container runtime）的 #SyncPod() 方法  PodManager 存储 Pod 的期望状态，kubelet 服务的不同渠道的 Pod\nStatsProvider 提供节点和容器的统计信息，有 cAdvisor 和 CRI 两种实现。\nContainerRuntime 顾名思义，容器运行时。与遵循 CRI 规范的高级容器运行时进行交互。\nDeps.PodConfig PodConfig 是一个配置多路复用器，它将许多 Pod 配置源合并成一个单一的一致结构，然后按顺序向监听器传递增量变更通知。\n配置源有：文件、apiserver、HTTP\n#syncLoop 接收来自 PodConfig 的 Pod 变更通知、定时任务、PLEG 的事件，以及 ProbeManager 的事件，将 Pod 同步到期望状态。\nPodAdmitHandlers Pod admission 过程中调用的一系列处理器，比如 eviction handler（节点内存有压力时，不会驱逐 QoS 设置为 BestEffort 的 Pod）、shutdown admit handler（当节点关闭时，不处理 pod 的同步操作）等。\nOOMWatcher 从系统日志中获取容器的 OOM 日志，将其封装成事件并记录。\nVolumeManger VolumeManager 运行一组异步循环，根据在此节点上调度的 pod 确定需要附加/挂载/卸载/分离哪些卷并执行操作。\nCertificateManager 处理证书轮换。\nProbeManager 实际上包含了三种 Probe，提供 probe 结果缓存和通道。\n LivenessManager ReadinessManager StartupManager  EvictionManager 监控 Node 节点的资源占用情况，根据驱逐规则驱逐 Pod 释放资源，缓解节点的压力。\nPluginManager PluginManager 运行一组异步循环，根据此节点确定哪些插件需要注册/取消注册并执行。如 CSI 驱动和设备管理器插件（Device Plugin）。\nCSI Container Storage Interface，由存储厂商实现的存储驱动。\n设备管理器插件（Device Plugin） Kubernetes 提供了一个 设备插件框架，你可以用它来将系统硬件资源发布到 Kubelet。\n供应商可以实现设备插件，由你手动部署或作为 DaemonSet 来部署，而不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、 InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。\nkubelet 的启动流程 要分析 kubelet 的启动流程，可以从 kubelet 运行方式着手。找一个 Node 节点，很容易就能找到 kubelet 的进程。由于其是以 systemd 的方式启动，也可以通过 systemctl 查看其状态。\nkubelet 启动命令 kubelet 的启动命令（minikube 环境）\n$ ps -aux | grep \u0026#39;/kubelet\u0026#39; | grep -v grep root 4917 2.6 0.3 1857652 106152 ? Ssl 01:34 13:05 /var/lib/minikube/binaries/v1.21.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=1.21.0 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.5 或者\n$ systemctl status kubelet.service ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/usr/lib/systemd/system/kubelet.service; disabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Sun 2021-06-13 01:34:42 UTC; 11h ago Docs: http://kubernetes.io/docs/ Main PID: 4917 (kubelet) Tasks: 15 (limit: 38314) Memory: 39.4M CGroup: /system.slice/kubelet.service └─4917 /var/lib/minikube/binaries/v1.21.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=1.21.0 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64 源码分析 从 git@github.com:kubernetes/kubernetes.git 仓库获取代码，使用最新的 release-1.21 分支。\n cmd/kubelet/kubelet.go:35 的 main 方法为程序入口。  调用 NewKubeletCommand 方法，创建 command 执行 command  cmd/kubelet/app/server.go:434 的 Run 方法。  调用 RunKubelet 方法。  调用 createAndInitKubelet 方法，创建并初始化 kubelet  pkg/kubelet/kubelet.go 的 NewMainKubelet 方法，创建 kubelet的 各种组件。共十几个组件，见 kubelet 的构架。 调用 BirtyCry 方法：放出 Starting 事件 调用 StartGarbageCollection 方法，开启 ContainerGC 和 ImageGC   调用 startKubelet 方法（大量使用 goroutine 和通道）  goroutine：kubelet.Run()  初始化模块  metrics 相关 创建文件系统目录目录 创建容器日志目录 启动 ImageGCManager 启动 ServerCertificateManager 启动 OOMWatcher 启动 ResourceAnalyzer   goroutine：VolumeManager.Run() 开始处理 Pod Volume 的卸载和挂载 goroutine：状态更新 fastStatusUpdateOnce() （更新 Pod CIDR -\u0026gt; 更新 ContainerRuntime 状态 -\u0026gt; 更新 Node 节点状态） goroutine： NodeLeaseController.Run() 更新节点租约 goroutine：podKiller.PerformPodKillingWork 杀掉未被正确处理的 pod StatusManager.Start() 开始向 apiserver 更新 Pod 状态 RuntimeClassManager.Start() PLEG.Start()：持续从 ContainerRuntime 获取 Pod/容器的状态，并与 kubelet 本地 cache 中的比较，生成对应的 Event syncLoop() 重点，持续监控并处理来自文件、apiserver、http 的变更。包括 Pod 的增加、更新、优雅删除、非优雅删除、调和。       启动 server，暴露 /healthz 端点 通知 systemd kuberlet 服务已经启动        kubelet 的工作原理  来静态文件、apiserver 以及 HTTP 请求的 Pod 配置变更，被发送到 kubelet.syncLoop PLEG 会定期通过容器运行时获取节点上 Pod 的状态，与其缓存中的 Pod 信息进行比较，封装成事件，进入 PLEG 的通道 定期检查工作队列中的 Pod ProbeManager 的通道中的 Pod 以上 1~4，都会进入 syncLoopIteration，并从对应的通道中获取到对应 Pod，将 Pod 的信息保存到 PodManager；然后分发给 PodWorker，完成一些列的同步工作。  总结 kubelet 启动流量就讲到这里，虽然复杂，还是有迹可循。只要了解了 kubelet 在 Kubernetes 中的定位及角色，就很容易理解其工作流量。\n后面会再深入分析 Pod 创建及启动流程。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/kubelet-source-code-analysis/","tags":["Kubernetes","云原生"],"title":"源码解析：一文读懂 Kubelet"},{"categories":["源码分析","笔记"],"contents":"由于要给团队做一下关于 Flomesh 的分享，准备下材料。\n“分享是最好的学习方法。”\n上一回初探可编程网关 Pipy，领略了 Pipy 的“风骚”。从 Pipy 的 GUI 交互深入了解了 Pipy 的配置加载流程。\n今天看一下 Pipy 如何实现 Metrics 的功能，顺便看下数据如何在多个 Pipeline 中进行流转。\n前置 首先，需要对 Pipy 有一定的了解，如果不了解看一下上一篇文章。\n其次构建好 Pipy 环境，关于构建还是去看上一篇文章。\nMetrics 功能实现 至于 Pipy 实现 Metrics 的方式，源码中就有，位于 test/006-metrics/pipy.js。\n 代理监听 6080 端口，后端服务在 8080 端口，Metrics 在 9090 端口 共有 5 个 Pipeline：3 个 listen 类型，2 个 Pipeline 类型 7 种过滤器：fork、connect、decodeHttpRequest、onMessageStart、decodeHttpResponse、encodeHttpRespnse、replaceMessage  贴一下源码：\npipy({ _metrics: { count: 0, }, _statuses: {}, _latencies: [ 1,2,5,7,10,15,20,25,30,40,50,60,70,80,90,100, 200,300,400,500,1000,2000,5000,10000,30000,60000, Number.POSITIVE_INFINITY ], _buckets: [], _timestamp: 0, }) .listen(6080) .fork(\u0026#39;in\u0026#39;) .connect(\u0026#39;127.0.0.1:8080\u0026#39;) .fork(\u0026#39;out\u0026#39;) // Extract request info .pipeline(\u0026#39;in\u0026#39;) .decodeHttpRequest() .onMessageStart( () =\u0026gt; ( _timestamp = Date.now(), _metrics.count++ ) ) // Extract response info .pipeline(\u0026#39;out\u0026#39;) .decodeHttpResponse() .onMessageStart( e =\u0026gt; ( ((status, latency, i) =\u0026gt; ( status = e.head.status, latency = Date.now() - _timestamp, i = _latencies.findIndex(t =\u0026gt; latency \u0026lt;= t), _buckets[i]++, _statuses[status] = (_statuses[status]|0) + 1 ))() ) ) // Expose as Prometheus metrics .listen(9090) .decodeHttpRequest() .replaceMessage( () =\u0026gt; ( (sum =\u0026gt; new Message( [ `count ${_metrics.count}`, ...Object.entries(_statuses).map( ([k, v]) =\u0026gt; `status{code=\u0026#34;${k}\u0026#34;} ${v}` ), ..._buckets.map((n, i) =\u0026gt; `bucket{le=\u0026#34;${_latencies[i]}\u0026#34;} ${sum += n}`) ] .join(\u0026#39;\\n\u0026#39;) ))(0) ) ) .encodeHttpResponse() // Mock service on port 8080 .listen(8080) .decodeHttpRequest() .replaceMessage( new Message(\u0026#39;Hello!\\n\u0026#39;) ) .encodeHttpResponse() 测试 使用 ab 做请求模拟 ab -n 2000 -c 10 http://localhost:6080/，然后检查下记录的指标。\n$ http :9090 --body count 2000 status{code=\u0026#34;200\u0026#34;} 2000 bucket{le=\u0026#34;1\u0026#34;} 1762 bucket{le=\u0026#34;2\u0026#34;} 1989 bucket{le=\u0026#34;5\u0026#34;} 1994 bucket{le=\u0026#34;7\u0026#34;} 1999 bucket{le=\u0026#34;10\u0026#34;} 2000 分析 TL;DR：本次示例的核心是 fork，从字面意思就很容易理解：新开一个处理分支（Pipeline），与主线并行执行。\n在 src/inbound.cpp:104 109 处，Pipy 接收一个新的连接。 创建 Context 和 Session，并在 L178 处注册事件的处理器，然后在 L187 处开始接收数据。 在 #receive 方法中，定义了数据接收处理器：将读到的数据写入 buffer 中。这个 buffer 存储的是 Event类型数据。（所以说 Pipy 是基于数据流事件，将一些封装成了事件）\n接着调用 Session#input。\n实际上调用的是 ReusableSession#input，调用 m_filters 的 #process 方法。m_filters 实际上是 Filter 类型。\n为什么只有一个 Filter？重点来了，看下 ReusableSession 的构造过程就能明白了（这里用了个反向迭代器）。output 是当前 Filter 处理完要执行的，实现类似链式的执行。\n再回头看上面的示例，可以想象 fork 就是 Session 的 m_filters。\nsrc/filters/fork.cpp:85，在 fork 过滤器中，在 1 处从 module 中获取到目标 Pipeline，并在 3 和 4 处 创建了新的 Session 并保存原 Session 的数据。\n然后在 5 处将原 Event 输入到新的 Session 中，触发目标 Pipeline 的 Filter 链。值得注意的是，这里是基于事件的处理，并不是阻塞的。这就意味着，fork 的目标 pipline，与 fork 所在的 pipeline 是并行执行的。 在示例中，就是 Pipeline ‘in’ 与 主 Pipeline 的 connect 是并行执行的。\n最终在 6 处，继续使用原 Session 的 Filter 链。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/programming-archive-metrics-with-pipy/","tags":["Pipy"],"title":"可编程网关 Pipy 第二弹：编程实现 Metrics 及源码解读"},{"categories":["翻译"],"contents":"本文翻译自 learnk8s 的 Architecting Kubernetes clusters — choosing the best autoscaling strategy，有增删部分内容。\nTL;DR: 在默认设置下，扩展 Kubernetes 集群中的 pod 和节点可能需要几分钟时间。了解如何调整集群节点的大小、配置水平和集群自动缩放器以及过度配置集群以加快扩展速度。\n自动扩展器 在 Kubernetes 中，常说的“自用扩展”有：\n HPA：Pod 水平缩放器 VPA：Pod 垂直缩放器 CA：集群自动缩放器  不同类型的自动缩放器，使用的场景不一样。\nHPA HPA 定期检查内存和 CPU 等指标，自动调整 Deployment 中的副本数，比如流量变化：\nVPA 有些时候无法通过增加 Pod 数来扩容，比如数据库。这时候可以通过 VPA 增加 Pod 的大小，比如调整 Pod 的 CPU 和内存：\nCA 当集群资源不足时，CA 会自动配置新的计算资源并添加到集群中：\n自动缩放 Pod 出错时 比如一个应用需要 1.5 GB 内存和 0.25 个 vCPU。一个 8GB 和 2 个 vCPU 的节点，可以容纳 4 个这样的 Pod，完美！\n做如下配置：\n HPA：每增加 10 个并发，增加一个副本。即 40 个并发的时候，自动扩展到 4 个副本。（这里使用自定义指标，比如来自 Ingress Controller 的 QPS） CA：在资源不足的时候，增加计算节点。  当并发达到 30 的时候，系统是下面这样。完美！HPA 工作正常，CA 没工作。\n当增加到 40 个并发的时候，系统是下面的情况：\n HPA 增加了一个 Pod Pod 挂起 CA 增加了一个节点  为什么 Pod 没有部署成功？\n节点上的操作系统进程和 kubelet 也会消耗一部分资源，8G 和 2 vCPU 并不是全都可以提供给 Pod 用的。并且还有一个驱逐阈值：在节点系统剩余资源达到阈值时，会驱逐 Pod，避免 OOM 的发生。\n当然上面的这些都是可配置的。\n那为什么在创建该 Pod 之前，CA 没有增加新的节点呢？\nCA 如何工作？ CA 在触发自动缩放时，不会查看可用的内存或 CPU。\nCA 是面向事件工作的，并每 10 秒检查一次是否存在不可调度（Pending）的 Pod。\n当调度器无法找到可以容纳 Pod 的节点时，这个 Pod 是不可调度的。\n此时，CA 开始创建新节点：CA 扫描集群并检查是否有不可调度的 Pod。\n当集群有多种节点池，CA 会通过选择下面的一种策略：\n random：默认的扩展器，随机选择一种节点池 most-pods：能够调度最多 Pod 的节点池 least-waste：选择扩展后，资源空闲最少的节点池 price：选择成本最低的节点池 priority：选择用户分配的具有最高优先级的节点池  确定类型后，CA 会调用相关 API 来创建资源。（云厂商会实现 API，比如 AWS 添加 EC2；Azure 添加 Virtual Machine；阿里云增加 ECS；GCP 增加 Compute Engine）\n计算资源就绪后，就会进行节点的初始化。\n注意，这里需要一定的耗时，通常比较慢。\n探索 Pod 自动缩放的前置时间 四个因素：\n HPA 的响应耗时 CA 的响应耗时 节点的初始化耗时 Pod 的创建时间  默认情况下，kubelet 每 10 秒抓取一次 Pod 的 CPU 和内存占用情况。\n每分钟，Metrics Server 会将聚合的指标开放给 Kubernetes API 的其他组件使用。\nCA 每 10 秒排查不可调度的 Pod。\n 少于 100 个节点，且每个节点最多 30 个 Pod，时间不超过 30s。平均延迟大约 5s。 100 到 1000个节点，不超过 60s。平均延迟大约 15s。  节点的配置时间，取决于云服务商。通常在 3~5 分钟。\n容器运行时创建 Pod：启动容器的几毫秒和下载镜像的几秒钟。如果不做镜像缓存，几秒到 1 分钟不等，取决于层的大小和梳理。\n对于小规模的集群，最坏的情况是 6 分 30 秒。对于 100 个以上节点规模的集群，可能高达 7 分钟。\nHPA delay: 1m30s + CA delay: 0m30s + Cloud provider: 4m + Container runtime: 0m30s + ========================= Total 6m30s 突发情况，比如流量激增，你是否愿意等这 7 分钟？\n这 7 分钟，如何优化压缩？\n HPA 的刷新时间，默认 15 秒，通过 --horizontal-pod-autoscaler-sync-period 标志控制。 Metrics Server 的指标抓取时间，默认 60 秒，通过 metric-resolution 控制。 CA 的扫描间隔，默认 10 秒，通过 scan-interval 控制。 节点上缓存镜像，比如 kube-fledged 等工具。  即使调小了上述设置，依然会受云服务商的时间限制。\n那么，如何解决？\n两种尝试：\n 尽量避免被动创建新节点 主动创建新节点  为 Kubernetes 选择最佳规格的节点 这会对扩展策略产生巨大影响。\n这样的场景\n应用程序需要 1GB 内存和 0.1 vCPU；有一个 4GB 内存和 1 个 vCPU 的节点。\n排除操作系统、kubelet 和阈值保留空间后，有 2.5GB 内存和 0.7 个 vCPU 可用。\n最多只能容纳 2 个 Pod，扩展副本时最长耗时 7 分钟（HPA、CA、云服务商的资源配置耗时）\n假如节点的规格是 64GB 内存和 16 个 vCPU，可用的资源为 58.32GB 和 15.8 个 vCPU。\n这个节点可以托管 58 个 Pod。只有扩容第 59 个副本时，才需要创建新的节点。\n这样触发 CA 的机会更少。\n选择大规格的节点，还有另外一个好处：资源的利用率会更高。\n节点上可以容纳的 Pod 数量，决定了效率的峰值。\n物极必反！更大的实例，并不是一个好的选择：\n 爆炸半径（Blast radius）：节点故障时，少节点的集群和多节点的集群，前者影响更大。 自动缩放的成本效益低：增加一个大容量的节点，其利用率会比较低（调度过去的 Pod 数少）  即使选择了正确规格的节点，配置新的计算单元时，延迟仍然存在。怎么解决？\n能否提前创建节点？\n为集群过度配置节点 即为集群增加备用节点，可以：\n 创建一个节点，并留空 （比如 SchedulingDisabled） 一旦空节点中有了一个 Pod，马上创建新的空节点  这种会产生额外的成本，但是效率会提升。\nCA 并不支持此功能 \u0026ndash; 总是保留一个空节点。\n但是，可以伪造。创建一个只占用资源，不使用资源的 Pod 占用整个 Node 节点。\n一旦有了真正的 Pod，驱逐占位的 Pod。 待后台完成新的节点配置后，将“占位” Pod 再次占用整个节点。\n这个“占位”的 Pod 可以通过永久休眠来实现空间的保留。\napiVersion: apps/v1 kind: Deployment metadata: name: overprovisioning spec: replicas: 1 selector: matchLabels: run: overprovisioning template: metadata: labels: run: overprovisioning spec: containers: - name: pause image: k8s.gcr.io/pause resources: requests: cpu: \u0026#39;1739m\u0026#39; memory: \u0026#39;5.9G\u0026#39; 使用优先级和抢占，来实现创建真正的 Pod 后驱逐“占位”的 Pod。\n使用 PodPriorityClass 在配置 Pod 优先级：\napiVersion: scheduling.k8s.io/v1beta1 kind: PriorityClass metadata: name: overprovisioning value: -1 #默认的是 0，这个表示比默认的低 globalDefault: false description: \u0026#39;Priority class used by overprovisioning.\u0026#39; 为“占位” Pod 配置优先级：\napiVersion: apps/v1 kind: Deployment metadata: name: overprovisioning spec: replicas: 1 selector: matchLabels: run: overprovisioning template: metadata: labels: run: overprovisioning spec: priorityClassName: overprovisioning #HERE containers: - name: reserve-resources image: k8s.gcr.io/pause resources: requests: cpu: \u0026#39;1739m\u0026#39; memory: \u0026#39;5.9G\u0026#39; 已经做完过度配置，应用程序是否需要优化？\n为 Pod 选择正确的内存和 CPU 请求 Kubernetes 是根据 Pod 的内存和 CPU 请求，为其分配节点。\n如果 Pod 的资源请求配置不正确，可能会过晚（或过早）触发自动缩放器。\n这样一个场景：\n 应用程序平均负载下消耗 512MB 内存和 0.25 个 vCPU。 高峰时，消耗 4GB 内存 和 1 个 vCPU。（即资源限制，Limit）  有三种请求的配置选择：\n 远低于平均使用量 匹配平均使用量 尽量接近限制  第一种的问题在于超卖严重，过度使用节点。kubelet 负载高，稳定性差。\n第三种，会造成资源的利用率低，浪费资源。这种通常被称为 QoS：Quality of Service class 中的 Guaranteed 级别，Pod 不会被终止和驱逐。 如何在稳定性和资源使用率间做权衡？\n这就是 QoS：Quality of Service class 中的 Burstable 级别，即 Pod 偶尔会使用更多的内存和 CPU。\n 如果节点中有可用资源，应用程序会在返回基线（baseline）前使用这些资源。 如果资源不足，Pod 将竞争资源（CPU），kubelet 也有可能尝试驱逐 Pod（内存）。  在 Guaranteed 和 Burstable 之前如何做选择？取决于：\n 想尽量减少 Pod 的重新调度和驱逐，应该是用 Guaranteed。 如果想充分利用资源时，使用 Burstable。比如弹性较大的服务，Web 或者 REST 服务。  如何做出正确的配置？\n应该分析应用程序，并测算空闲、负载和峰值时的内存和 CPU 消耗。\n甚至可以通过部署 VPA 来自动调整。\n如何进行集群缩容？ 每 10 秒，当请求（request）利用率低于 50%时，CA 才会决定删除节点。\nCA 会汇总同一个节点上的所有 Pod 的 CPU 和内存请求。小于节点容量的一半，就会考虑对当前节点进行缩减。\n 需要注意的是，CA 不考虑实际的 CPU 和内存使用或者限制（limit），只看请求（request）。\n 移除节点之前，CA 会：\n 检查 Pod 确保可以调度到其他节点上。 检查节点，避免节点被过早的销毁，比如两个节点的请求都低于 50%。  检查都通过之后，才会删除节点。\n为什么不根据内存或 CPU 进行自动缩放？ 基于内存和 CPU 的自动缩放器，不关心 pod。\n比如配置缩放器在节点的 CPU 达到总量的 80%，就自动增加节点。\n当你创建 3 个副本的 Deployment，3 个节点的 CPU 达到了 85%。这时会创建一个节点，但你并不需要第 4 个副本，新的节点就空闲了。\n不建议使用这种类型的自动缩放器。\n总结 定义和实施成功的扩展策略，需要掌握以下几点：\n 节点的可分配资源。 微调 Metrics Server、HPA 和 CA 的刷新间隔。 设计集群和节点的规格。 缓存容器镜像到节点。 应用程序的基准测试和分析。  配合适当的监控工具，可以反复测试扩展策略并调整集群的缩放速度和成本。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/auto-scaling-best-practice-in-kubernetes/","tags":["Kubernetes","云原生"],"title":"Kubernetes 的自动伸缩你用对了吗？"},{"categories":["云原生"],"contents":"有幸参加了 Flomesh 组织的workshop，了解了他们的 Pipy 网络代理，以及围绕 Pipy 构建起来的生态。Pipy 在生态中，不止是代理的角色，还是 Flomesh 服务网格​中的数据平面。\n整理一下，做个记录，顺便瞄一下 Pipy 的部分源码。\n介绍 下面是摘自 Github 上关于 Pipy 的介绍：\n Pipy 是一个轻量级、高性能、高稳定、可编程的网络代理。Pipy 核心框架使用 C++ 开发，网络 IO 采用 ASIO 库。 Pipy 的可执行文件仅有 5M 左右，运行期的内存占用 10M 左右，因此 Pipy 非常适合做 Sidecar proxy。\n  Pipy 内置了自研的 pjs 作为脚本扩展，使得Pipy 可以用 JS 脚本根据特定需求快速定制逻辑与功能。\n  Pipy 采用了模块化、链式的处理架构，用顺序执行的模块来对网络数据块进行处理。这种简单的架构使得 Pipy 底层简单可靠，同时具备了动态编排流量的能力，兼顾了简单和灵活。通过使用 REUSE_PORT 的机制（主流 Linux 和 BSD 版本都支持该功能），Pipy 可以以多进程模式运行，使得 Pipy 不仅适用于 Sidecar 模式，也适用于大规模的流量处理场景。 在实践中，Pipy 独立部署的时候用作“软负载”，可以在低延迟的情况下，实现媲美硬件的负载均衡吞吐能力，同时具有灵活的扩展性。\n Pipy 的核心是消息流处理器：\nPipy 流量处理的流程：\n核心概念  流（Stream） 管道（Pipeline） 模块（Module） 会话（Session） 上下文（Context）  以下是个人浅见：\nPipy 使用 pjs 引擎将 JavaScript格式的配置，解析成其抽象的 Configuration 对象。每个 Configuration 中包含了多个 Pipeline，每个 Configuration 中又会用到多个 Filter。这些都属于 Pipy 的静态配置部分。（后面会提到 Pipeline 的三种不同类型）\n而属于运行时的就是流、会话和上下文了，在 Pipy 中，数据流是由对象（Pipy 的抽象）组成的。而这些对象抵达 Pipy，被抽象成不同的事件。而事件触发不同的过滤器的执行。\n我个人更喜欢将其核心理解为：对数据流的事件处理引擎。\n理解归理解，实践出真知。“大胆假设，小心求证！”\n本地编译 从编译 Pipy 开始。\n环境准备 #安装 nodejs $ nvm install lts/erbium #安装 cmake $ brew install cmake 编译 Pipy 从 https://github.com/flomesh-io/pipy.git 克隆代码。\nPipy 的编译包括了两个部分，GUI 和 Pipy 本体。\nGUI 是 Pipy 提供的一个用于开发模式下进行配置的界面，首先编译Pipy GUI。\n# pipy root folder $ cd gui $ npm install $ npm run build 接着编译 Pipy 的本体\n# pipy root folder $ mkdir build $ cd build $ cmake -DCMAKE_BUILD_TYPE=Release -DPIPY_GUI=ON .. $ make 完成后检查根目录下的 bin 目录，可以看到 pipy 的可执行文件，大小只有 11M。\n$ bin/pipy --help Usage: pipy [options] \u0026lt;script filename\u0026gt; Options: -h, -help, --help Show help information -v, -version, --version Show version information --list-filters List all filters --help-filters Show detailed usage information for all filters --log-level=\u0026lt;debug|info|warn|error\u0026gt; Set the level of log output --verify Verify configuration only --reuse-port Enable kernel load balancing for all listening ports --gui-port=\u0026lt;port\u0026gt; Enable web GUI on the specified port Demo：Hello Pipy 开发模式下可以让 Pipy 携带 GUI 启动，通过 GUI 进行配置。\n#指定 gui 的端口为 6060，从 test 目录中加载配置 $ bin/pipy --gui-port=6060 test/ 2021-05-30 22:48:41 [info] [gui] Starting GUI service... 2021-05-30 22:48:41 [info] [listener] Listening on 0.0.0.0:6060 浏览器中打开 配置界面 展开 002-hello 子目录点选 pipy 并点击运行按钮：\n$ curl -i localhost:6080 HTTP/1.1 200 OK Connection: keep-alive Content-Length: 7 Hello! Pipy 过滤器 通过 pipe 的命令可以输出其支持的过滤器列表，一共 31 个。通过将一系列过滤器进行组装，可以实现复杂的流处理。\n比如 007-logging 的配置实现了日志的功能：记录请求和响应的数据，并批量发送到 ElasticSearch。这里就用到了 fork、connect、onSessionStart、encodeHttpRequest、decodeHttpRequest、onMessageStart、onMessage、decodeHttpResponse、replaceMessage、link、mux、task 等十多种过滤器。\n$ bin/pipy --list-filters connect (target[, options]) Sends data to a remote endpoint and receives data from it demux (target) Sends messages to a different pipline with each one in its own session and context decodeDubbo () Deframes a Dubbo message decodeHttpRequest () Deframes an HTTP request message decodeHttpResponse () Deframes an HTTP response message dummy () Eats up all events dump ([tag]) Outputs events to the standard output encodeDubbo ([head]) Frames a Dubbo message encodeHttpRequest ([head]) Frames an HTTP request message encodeHttpResponse ([head]) Frames an HTTP response message exec (command) Spawns a child process and connects to its input/output fork (target[, sessionData]) Sends copies of events to other pipeline sessions link (target[, when[, target2[, when2, ...]]]) Sends events to a different pipeline mux (target[, selector]) Sends messages from different sessions to a shared pipeline session onSessionStart (callback) Handles the initial event in a session onData (callback) Handles a Data event onMessageStart (callback) Handles a MessageStart event onMessageEnd (callback) Handles a MessageEnd event onSessionEnd (callback) Handles a SessionEnd event onMessageBody (callback) Handles a complete message body onMessage (callback) Handles a complete message including the head and the body print () Outputs raw data to the standard output replaceSessionStart (callback) Replaces the initial event in a session replaceData ([replacement]) Replaces a Data event replaceMessageStart ([replacement]) Replaces a MessageStart event replaceMessageEnd ([replacement]) Replaces a MessageEnd event replaceSessionEnd ([replacement]) Replaces a SessionEnd event replaceMessageBody ([replacement]) Replaces an entire message body replaceMessage ([replacement]) Replaces a complete message including the head and the body tap (quota[, account]) Throttles message rate or data rate use (module, pipeline[, argv...]) Sends events to a pipeline in a different module wait (condition) Buffers up events until a condition is fulfilled 原理 “Talk is cheap, show me the code.”\n配置加载 个人比较喜欢看源码来理解实现，即使是 C++。从浏览器请求入手发现运行时向/api/program 发送了 POST 请求，请求的内容是配置文件的地址。\n检查源码后，找到逻辑的实现在 src/gui.cpp:189：\n 创建新的 worker 加载配置，将 JavaScrip 代码解析成 Configuration 对象 启动 worker，执行Configuration::apply() 卸载旧的 worker  从 src/api/configuration.cpp:267 处看：pipeline、listen 和 task 配置实际在 Pipy 的配置中都是被抽象为 Pipeline 对象，只是在类型上有差异分别为：NAMED、LISTEN 和 TASK。比如 listen 中可以通过 fork 过滤器将事件的副本发送到指定的 pipeline 中。\n基于数据流事件的处理 src/inbound.cpp:171\n结语 Pipy 虽小（只有 11M），但以其可编程的特性提供了灵活的配置能力，潜力无限。\nPipy 像处理 HTTP 一样处理任意的七层协议。内部版本支持Dubbo、Redis、Socks 等，目前正在迁移到开源版本。\n期待即将开源的 Portal，以及服务网格 Flomesh。持续关注，后面考虑再写几篇。\n“未来可期！”\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/glance-at-programmable-gateway-pipy/","tags":["Pipy","云原生"],"title":"初探可编程网关 Pipy"},{"categories":["翻译"],"contents":"Quarkus 的文章之前写过三篇了，讲过了 Quarkus 的小而快。\n Hello, Quarkus 应\u0026quot;云\u0026quot;而生的 Java 框架 Quarkus：构建本机可执行文件 谁说 Java 不能用来跑 Serverless？  一直在酝酿写一篇 Quarkus 生态相关的，因为最近一直在忙 Meetup 的事情而搁浅。正好看到了这篇文章，就拿来翻译一下，补全云原生中的“微服务”这一块。\n本文译自《Implementing Microservicilities with Quarkus and MicroProfile》 。\n 为什么要使用微服务特性？ 在微服务架构中，一个应用程序是由几个相互连接的服务组成的，这些服务一起工作来实现所需的业务功能。\n因此，典型的企业微服务架构如下所示：\n刚开始，使用微服务架构实现应用程序看起来很容易。\n但是，因为有了单体架构没有一些新的挑战，因此做起来并不容器\n举几个例子，比如容错、服务发现、扩展性、日志记录和跟踪。\n为了解决这些挑战，每个微服务都应实现我们在 Red Hat 所说的“微服务特性”。\n 该术语是指除业务逻辑以外，服务还必须实现来解决的跨领域关注点清单，如下图所示：  可以用任何语言（Java、Go、JavaScript）或任何框架（Spring Boot、Quarkus）实现业务逻辑，但是围绕业务逻辑，应实现以下关注点：\nAPI：可通过一组定义的 API 操作来访问该服务。例如，对于 RESTful Web API，HTTP 用作协议。此外，可以使用诸如 Swagger 之类的工具来记录 API 。 服务发现（Discovery）：服务需要发现其他服务。\n调用服务（Invocation）：发现服务后，需要使用一组参数对其进行调用，并选择性地返回响应。\n弹性（Elasticity）：微服务架构的重要特征之一是每个服务都是弹性的，这意味着可以根据系统的关键程度或当前的工作量等参数独立地进行缩放。（译者注：这里的弹性只是资源的弹性）\n弹性（Resiliency）：在微服务架构中，我们在开发时应牢记失败，尤其是在与其他服务进行通信时。在单体应用中，整个应用程序处于启动或关闭状态。但是，当此应用程序分解为微服务体系结构时，该应用程序由多个服务组成，并且所有这些服务都通过网络互连，这意味着该应用程序的某些部分可能正在运行，而其他部分可能会失败。遏制故障对避免通过其他服务传播错误很重要。弹性（或应用程序弹性）是应用程序/服务对问题做出反应并仍然提供最佳结果的能力。（译者注：这里的弹性与容错相关，对失败处理的弹性）\n管道（Pipeline）：服务应独立部署，而无需进行任何形式的编排。因此，每个服务应具有自己的部署管道。\n身份验证（Authentication）：关于微服务体系结构中的安全性的关键方面之一是如何对内部服务之间的调用进行身份验证/授权。Web 令牌（通常是令牌）是在内部服务中安全地表示声明的首选方式。\n日志记录（Logging）：在单体应用程序中，日志记录很简单，因为该应用程序的所有组件都在同一节点上运行。然后现在组件以服务的形式分布在多个节点上，因此，要拥有完整的日志记录视图，需要一个统一的日志记录系统/数据收集器。\n监控（Monitoring）：衡量系统的性能、了解应用程序的整体运行状况，以及在出现问题时发出警报是保持基于微服务的应用程序正确运行的关键方面。监控是控制应用程序的关键方面。\n跟踪（Tracing）：跟踪用于可视化程序的流程和数据进度。作为开发人员/运维人员，当我们需要检查用户在整个应用程序中的行程时，这特别有用。\nKubernetes正在成为部署微服务的实际工具。这是一个用于自动化、编排、扩展和管理容器的开源系统。\n使用 Kubernetes 时，十个微服务特性中只有三个被涵盖。\n**服务发现 **是通过 Kubernetes 服务的概念实现的。它提供了一种使用稳定的虚拟 IP 和 DNS 名称将 Kubernetes Pod 分组（作为一个整体）的方法。发现服务只是使用 Kubernetes 的服务名作为 hostname 进行请求。\n使用 Kubernetes 可以很容易地调用服务，因为平台本身提供了调用任何服务所需的网络。\n从一开始，Kubernetes 就一直在考虑弹性（或可伸缩性），例如运行时kubectl scale deployment myservice --replicas=5 command，myservice deployment 可伸缩至五个副本或实例。Kubernetes 平台负责寻找合适的节点，部署服务并始终保持所需数量的副本并正常运行。\n但是其余的微服务特性又如何呢？Kubernetes 仅涵盖其中的三个，那么我们如何实现剩下的呢？\n根据所使用的语言或框架，可以遵循的策略很多。但是在本文中，我们将了解如何使用 Quarkus 实现其中的一些策略。\n什么是 Quarkus？ Quarkus 是针对 Java 虚拟机（JVM）和本机编译的全栈 Kubernetes 本地 Java 框架，专门针对容器优化 Java，使其成为无服务器（Serverless）、云和 Kubernetes 环境的高效平台。\nInstead of reinventing the wheel, Quarkus uses well-known enterprise-grade frameworks backed by standards/specifications and makes them compilable to a binary using GraalVM. Quarkus不用重新发明轮子，而是使用以标准/规范为后盾的知名企业级框架，并使用 GraalVM 将其编译为二进制文件。\n什么是 MicroProfile？ Quarkus 与 MicroProfile 规范集成，从而将企业 Java 生态系统迁移到微服务体系结构中。\n在下图中，我们看到了构成 MicroProfile 规范的所有 API。某些 API（例如 CDI、JSON-P 和 JAX-RS）基于 Jakarta EE（以前的 Java EE）规范。其余的由 Java 社区开发。\nLet’s implement API, invocation, resilience, authentication, logging, monitoring, and tracing microservicilities using Quarkus. 让我们使用Quarkus实现API、调用、弹性、身份验证、日志记录、监视和跟踪微服务特性。\n如何使用 Quarkus 实现微服务特性 入门 开始使用 Quarkus 的最快方法是通过在开始页面中选择所需的依赖。对于当前示例，选择如下依赖关系以满足微服务需求：\nAPI：RESTEasy JAX-RS、RESTEasy JSON-B、OpenAPI 调用：REST Client JSON-B 弹性：Fault Tolerance 认证：JWT 记录：GELF 监控：Micrometer metrics 跟踪：OpenTracing\n我们可以手动选择各自的依赖关系，或浏览以下链接 Quarkus 微服务特性生成器，所有这些都会被选中。然后按“生成应用程序”按钮以下载包含支架应用程序的zip文件。\n服务 对于当前示例，仅使用两个服务生成了一个非常简单的应用程序。一个名为“评级服务 rating service”的服务返回给定书籍的评级，而另一个名为“书籍服务 book service”的服务则返回一本书的信息及其评级。服务之间的所有调用都必须经过身份验证。\n在下图中，我们看到了整个系统的概述：\n评级服务已经开发并作为 Linux 容器提供。通过运行以下命令，在端口 9090 上启动服务：\ndocker run --rm -ti -p 9090:8080 quay.io/lordofthejars/rating-service:1.0.0 要验证服务，请向 http://localhost:9090/rate/1 发出请求\ncurl localhost:8080/rate/1 -vv \u0026gt; GET /rate/1 HTTP/1.1 \u0026gt; Host: localhost:8080 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 401 Unauthorized \u0026lt; www-authenticate: Bearer {token} \u0026lt; Content-Length: 0 返回的状态码是 401 Unauthorized 因为没有在请求中携带令牌（JWT）提供授权信息。只有带有 group Echoer 有效令牌才能访问评级服务。\nAPI Quarkus 使用众所周知的 JAX-RS 规范来定义 RESTful Web API。在幕后，Quarkus 使用 RESTEasy 实现直接与 Vert.X 框架一起使用，而无需使用 Servlet 技术。\n让我们为实现最常见操作的图书服务定义一个 API：\nimport javax.ws.rs.Consumes; import javax.ws.rs.DELETE; import javax.ws.rs.GET; import javax.ws.rs.POST; import javax.ws.rs.Path; import javax.ws.rs.PathParam; import javax.ws.rs.Produces; import javax.ws.rs.QueryParam; import javax.ws.rs.core.MediaType; import javax.ws.rs.core.Response; import javax.ws.rs.core.UriBuilder; @Path(\u0026#34;/book\u0026#34;) public class BookResource { @GET @Path(\u0026#34;/{bookId}\u0026#34;) @Produces(MediaType.APPLICATION_JSON) public Book book(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId) { // logic  } @POST @Consumes(MediaType.APPLICATION_JSON) public Response getBook(Book book) { // logic  return Response.created( UriBuilder.fromResource(BookResource.class) .path(Long.toString(book.bookId)) .build()) .build(); } @DELETE @Path(\u0026#34;/{bookId}\u0026#34;) public Response delete(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId) { // logic  return Response.noContent().build(); } @GET @Produces(MediaType.APPLICATION_JSON) @Path(\u0026#34;search\u0026#34;) public Response searchBook(@QueryParam(\u0026#34;description\u0026#34;) String description) { // logic  return Response.ok(books).build(); } } 首先要注意的是，定义了四个不同的端点：\n GET /book/{bookId} 使用 GET HTTP 方法返回带有其评级的图书信息。return 元素会自动解编为 JSON。 POST /book 使用 POST HTTP 方法插入一本书作为正文内容。正文内容会自动从 JSON 编组到 Java 对象。 DELETE /book/{bookId} 使用 DELETE HTTP 方法通过书的 ID 删除书。 GET /book/search?description={description} 按书名搜索书籍。  注意的第二件事是返回类型，有时是 Java 对象，有时是 Java 实例 javax.ws.rs.core.Response。使用 Java 对象时，会将其从 Java 对象序列化为 @Produces 注解中设置的媒体类型。在此特定服务中，输出为 JSON 文档。通过该 Response 对象，我们可以对返回给调用方的内容进行细粒度的控制。可以设置 HTTP 状态代码、标头或返回给调用方的内容。取决于使用场景，是偏爱一种方法而不是另一种方法。\n调用 在定义了用于访问图书服务的 API 之后，是时候开发一段代码来调用评级服务以检索图书的评级了。\nQuarkus 使用 MicroProfile Rest Client 规范来访问外部（HTTP）服务。它提供了一种类型安全的方法，以通过某些 JAX-RS 2.0 API 通过 HTTP 调用 RESTful 服务，以实现一致性和更易于重用。\n要创建的第一个元素是一个使用 JAX-RS 批注表示远程服务的接口。\nimport javax.ws.rs.GET; import javax.ws.rs.Path; import javax.ws.rs.PathParam; import javax.ws.rs.Produces; import javax.ws.rs.core.MediaType; import org.eclipse.microprofile.rest.client.inject.RegisterRestClient; @Path(\u0026#34;/rate\u0026#34;) @RegisterRestClient public interface RatingService { @GET @Path(\u0026#34;/{bookId}\u0026#34;) @Produces(MediaType.APPLICATION_JSON) Rate getRate(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId); } When the getRate() method is called, a remote HTTP call is invoked at /rate/{bookId} replacing the bookId with the value set in the method parameter. It is important to annotate the interface with the @RegisterRestClient annotation. Then the RatingService interface needs to be injected into BookResource to execute the remote calls. 当 getRate() 方法被调用时，远程 HTTP 请求在调用 /rate/{bookId} 替换 bookId 用在该方法中的参数值集合。用 @RegisterRestClient 注解对接口进行注解很重要。\n然后 RatingService 需要将接口注入 BookResource 以执行远程调用。\nimport org.eclipse.microprofile.rest.client.inject.RestClient; @RestClient RatingService ratingService; @GET @Path(\u0026#34;/{bookId}\u0026#34;) @Produces(MediaType.APPLICATION_JSON) public Book book(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId) { final Rate rate = ratingService.getRate(bookId); Book book = findBook(bookId); return book; } The @RestClient annotation injects a proxied instance of the interface, providing the implementation of the client. The last thing is to configure the service location (the hostname part). In Quarkus, the configuration properties are set in src/main/resources/application.properties file. To configure the location of the service, we need to use the fully qualified name of the Rest Client interface with URL as key, and the location as a value: 该 @RestClient 注解注入界面的代理实例，提供客户端的实现。\n最后一件事是配置服务位置（hostname 部分）。在 Quarkus 中，配置属性在 src/main/resources/application.properties 文件中设置。要配置服务的位置，我们需要使用 Rest Client 接口的标准名称，其中 URL 作为键，而 location 作为值：\norg.acme.RatingService/mp-rest/url=http://localhost:9090 在正确访问评估服务而没有 401 Unauthorized 问题之前，必须解决相互认证问题。\n身份验证 基于令牌的身份验证机制允许系统基于安全令牌对身份进行身份验证、授权和验证。Quarkus 与 MicroProfile JWT RBAC 安全规范集成在一起，以使用 JWT 令牌保护服务。\n要使用 MicroProfile JWT RBAC 安全性保护端点，我们只需要使用批注对方法进行 @RolesAllowed 注解。\n@GET @Path(\u0026#34;/{bookId}\u0026#34;) @RolesAllowed(\u0026#34;Echoer\u0026#34;) @Produces(MediaType.APPLICATION_JSON) public Book book(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId) 然后，我们配置令牌的发行方和公钥的位置，以验证令牌在 application.properties 文件中的签名：\nmp.jwt.verify.publickey.location=https://raw.githubusercontent.com/redhat-developer-demos/quarkus-tutorial/master/jwt-token/quarkus.jwt.pub mp.jwt.verify.issuer=https://quarkus.io/using-jwt-rbac 此扩展名自动验证：令牌有效；发行方是正确的；令牌尚未修改；签名有效；没有过期。\n这两种图书服务和评级服务现在是由同一 JWT 发行方和密钥保护，因此服务之间的通信要求验证提供在令牌的有效承载用户 Authentication 头部。\n评级服务启动和运行，让我们开始用下面的命令图书服务：\n./mvnw compile quarkus:dev Finally, we can make a request to get book information providing a valid JSON Web Token as a bearer token. The generation of the token is out of the scope of this article, and a token has been already generated: 最后，我们可以请求获取提供有效 JSON Web 令牌作为承载令牌的图书信息。\n令牌的生成不在本文的讨论范围之内，并且已经生成了令牌：\ncurl -H \u0026quot;Authorization: Bearer eyJraWQiOiJcL3ByaXZhdGVLZXkucGVtIiwidHlwIjoiSldUIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJqZG9lLXVzaW5nLWp3dC1yYmFjIiwiYXVkIjoidXNpbmctand0LXJiYWMiLCJ1cG4iOiJqZG9lQHF1YXJrdXMuaW8iLCJiaXJ0aGRhdGUiOiIyMDAxLTA3LTEzIiwiYXV0aF90aW1lIjoxNTcwMDk0MTcxLCJpc3MiOiJodHRwczpcL1wvcXVhcmt1cy5pb1wvdXNpbmctand0LXJiYWMiLCJyb2xlTWFwcGluZ3MiOnsiZ3JvdXAyIjoiR3JvdXAyTWFwcGVkUm9sZSIsImdyb3VwMSI6Ikdyb3VwMU1hcHBlZFJvbGUifSwiZ3JvdXBzIjpbIkVjaG9lciIsIlRlc3RlciIsIlN1YnNjcmliZXIiLCJncm91cDIiXSwicHJlZmVycmVkX3VzZXJuYW1lIjoiamRvZSIsImV4cCI6MjIwMDgxNDE3MSwiaWF0IjoxNTcwMDk0MTcxLCJqdGkiOiJhLTEyMyJ9.Hzr41h3_uewy-g2B-sonOiBObtcpkgzqmF4bT3cO58v45AIOiegl7HIx7QgEZHRO4PdUtR34x9W23VJY7NJ545ucpCuKnEV1uRlspJyQevfI-mSRg1bHlMmdDt661-V3KmQES8WX2B2uqirykO5fCeCp3womboilzCq4VtxbmM2qgf6ag8rUNnTCLuCgEoulGwTn0F5lCrom-7dJOTryW1KI0qUWHMMwl4TX5cLmqJLgBzJapzc5_yEfgQZ9qXzvsT8zeOWSKKPLm7LFVt2YihkXa80lWcjewwt61rfQkpmqSzAHL0QIs7CsM9GfnoYc0j9po83-P3GJiBMMFmn-vg\u0026quot; localhost:8080/book/1 -v 响应又是 forbidden 错误：\n\u0026lt; HTTP/1.1 401 Unauthorized \u0026lt; Content-Length: 0 你可能想知道为什么在提供有效令牌后仍然出现此错误。如果我们检查图书服务的控制台，就会发现抛出了以下异常：\norg.jboss.resteasy.client.exception.ResteasyWebApplicationException: Unknown error, status code 401 at org.jboss.resteasy.client.exception.WebApplicationExceptionWrapper.wrap(WebApplicationExceptionWrapper.java:107) at org.jboss.resteasy.microprofile.client.DefaultResponseExceptionMapper.toThrowable(DefaultResponseExceptionMapper.java:21) 发生此异常的原因是，我们已获得身份验证并有权访问图书服务，但承载令牌尚未传播到评级服务。\n为了自动将 Authorization 标头从传入请求传播到其余客户端请求，需要进行两次修改。\n第一个修改是修改 Rest Client 界面，并使用对其进行注解 org.eclipse.microprofile.rest.client.inject.RegisterClientHeaders。\n@Path(\u0026#34;/rate\u0026#34;) @RegisterRestClient @RegisterClientHeaders public interface RatingService {} 第二个修改是配置在请求之间传播哪些标头。这是在 application.properties 文件中设置的：\norg.eclipse.microprofile.rest.client.propagateHeaders=Authorization 执行与之前相同的 curl 命令，我们将获得正确的输出：\n\u0026lt; HTTP/1.1 200 OK \u0026lt; Content-Length: 39 \u0026lt; Content-Type: application/json \u0026lt; * Connection #0 to host localhost left intact {\u0026#34;bookId\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Book 2\u0026#34;,\u0026#34;rating\u0026#34;:1}* Closing connection 0 弹性 在微服务架构中，具有容错能力很重要，这样可以避免故障从一个服务传播到该服务的所有直接和间接调用方。Quarkus 将 MicroProfile Fault Tolerance 规范与以下用于处理故障的注释集成在一起：\n● @Timeout：定义抛出异常之前执行的最长时间。 ● @Retry：如果调用失败，请再次重试执行。 ● @Bulkhead：限制并发执行，以使该区域中的故障不会使整个系统过载。 ● @CircuitBreaker：执行反复失败时，将自动进行快速故障切换。 ● @Fallback：执行失败时，提供备用解决方案/默认值。\n让我们添加三次重试，其中重试之间的延迟计时器为一秒，以防访问评级服务时发生错误。\n@Retry(maxRetries = 3, delay = 1000) Rate getRate(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId); 现在，停止评级服务并执行请求。引发以下异常：\norg.jboss.resteasy.spi.UnhandledException: javax.ws.rs.ProcessingException: RESTEASY004655: Unable to invoke request: org.apache.http.conn.HttpHostConnectException: Connect to localhost:9090 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused 显然，这里存在错误，但是请注意，由于执行了三次重试（延迟一秒），因此引发异常之前，经过了三秒钟。\n在这种情况下，评级服务已关闭，因此无法进行恢复，但是在一个实际示例中，评级服务可能仅在短时间内就恢复了，或者部署了该服务的多个副本，因此可以简单地重试操作可能足以恢复并提供有效的响应。\n但是，当引发异常时重试次数不够时，我们可以将错误传播给调用方，也可以为调用提供替代值。这种选择可以是对另一个系统的调用（即分布式缓存）或静态值。\n对于此用例，当与评级服务的连接失败时，将返回评级值 0。\n要实现回退逻辑，首先要做的是实现将 org.eclipse.microprofile.faulttolerance.FallbackHandler 返回类型设置为与回退策略方法提供的替代类型相同的接口。对于这种情况，将 Rate 返回默认对象。\nimport org.eclipse.microprofile.faulttolerance.ExecutionContext; import org.eclipse.microprofile.faulttolerance.FallbackHandler; public class RatingServiceFallback implements FallbackHandler\u0026lt;Rate\u0026gt; { @Override public Rate handle(ExecutionContext context) { Rate rate = new Rate(); rate.rate = 0; return rate; } } 最后要做的是用注解对 getRating() 方法进行 @org.eclipse.microprofile.faulttolerance.Fallback 注解，以配置无法恢复时要执行的回退类。\n@Retry(maxRetries = 3, delay = 1000) @Fallback(RatingServiceFallback.class) Rate getRate(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId); 如果重复与以前相同的请求，则不会引发任何异常，但是有效值的输出将 rating 字段设置为 0。\n* Connection #0 to host localhost left intact {\u0026quot;bookId\u0026quot;:2,\u0026quot;name\u0026quot;:\u0026quot;Book 2\u0026quot;,\u0026quot;rating\u0026quot;:0}* Closing connection 0 规范提供的任何其他策略都可以使用相同的方法。例如，对于断路器模式：\n@CircuitBreaker(requestVolumeThreshold = 4, failureRatio=0.75, delay = 1000) 如果在四个连续调用的滚动窗口中发生了三个（4 x 0.75）故障，则电路将断开 1000 ms，然后恢复到半断开状态。如果在半开时调用成功，则将其再次关闭。否则，它将保持打开状态\n日志记录 在微服务架构中，建议将所有服务的日志收集在一个统一的日志中，以更有效地使用和理解。\n一种解决方案是使用 Fluentd，它是 Kubernetes 中用于统一日志记录层的开源数据收集器。Quarkus 使用 Graylog 扩展日志格式（GELF）与 Fluentd 集成。\n集成真的很简单。首先，与其他任何 Quarkus 应用程序一样使用日志逻辑：\nimport org.jboss.logging.Logger; private static final Logger LOG = Logger.getLogger(BookResource.class); @GET @Path(\u0026#34;/{bookId}\u0026#34;) @RolesAllowed(\u0026#34;Echoer\u0026#34;) @Produces(MediaType.APPLICATION_JSON) public Book book(@PathParam(\u0026#34;bookId\u0026#34;) Long bookId) { LOG.info(\u0026#34;Get Book\u0026#34;); 接下来，启用 GELF 格式并设置 Fluentd 服务器位置：\nquarkus.log.handler.gelf.enabled=true quarkus.log.handler.gelf.host=localhost quarkus.log.handler.gelf.port=12201 最后，我们可以向记录的端点发出请求：\ncurl -H \u0026#34;Authorization: Bearer ...\u0026#34; localhost:8080/book/1 {\u0026#34;bookId\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Book 1\u0026#34;,\u0026#34;rating\u0026#34;:3} 输出方面没有任何变化，但是日志行已传输到 Fluentd。如果使用 Kibana 可视化数据，我们将看到存储的日志行：\n监控 监控是另一个需要在我们的微服务架构中实现的 “微服务特性”。Quarkus 与 Micrometer 集成在一起以进行应用程序监控。Micrometer 提供了最流行的监控系统的单个入口点，使你无需供应商锁定即可检测基于 JVM 的应用程序代码。\n对于此示例，监控输出采用 Prometheus 格式，但 Micrometer（和 Quarkus）还支持其他格式，例如 Azure Monitor、Stackdriver、SignalFx、StatsD 和 DataDog。\n你可以注册以下 Maven 依赖项以提供 Prometheus 输出：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.quarkus\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;quarkus-micrometer-registry-prometheus\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 默认情况下，Micrometer 扩展注册了一些与系统，JVM 或 HTTP 相关的度量。收集的指标的一个子集在 /q/metrics 端点处可用，如下所示：\ncurl localhost:8080/q/metrics jvm_threads_states_threads{state=\u0026#34;runnable\u0026#34;,} 22.0 jvm_threads_states_threads{state=\u0026#34;blocked\u0026#34;,} 0.0 jvm_threads_states_threads{state=\u0026#34;waiting\u0026#34;,} 10.0 http_server_bytes_read_count 1.0 http_server_bytes_read_sum 0.0 但是，也可以使用 Micrometer API 来实现特定于应用程序的指标。 让我们实现一个自定义指标，该指标用于衡量评价最高的图书。\n使用 io.micrometer.core.instrument.MeterRegistry 该类可以完成指标（在这种情况下为量规）的注册。\nprivate final MeterRegistry registry; private final LongAccumulator highestRating = new LongAccumulator(Long::max, 0); public BookResource(MeterRegistry registry) { this.registry = registry; registry.gauge(\u0026#34;book.rating.max\u0026#34;, this, BookResource::highestRatingBook); } 请求一下，并验证量规是否正确更新。\ncurl -H \u0026#34;Authorization: Bearer ...\u0026#34; localhost:8080/book/1 {\u0026#34;bookId\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Book 1\u0026#34;,\u0026#34;rating\u0026#34;:3} curl localhost:8080/q/metrics # HELP book_rating_max # TYPE book_rating_max gauge book_rating_max 3.0 我们还可以设置一个计时器来记录从评级服务获取评级信息所花费的时间。\nSupplier\u0026lt;Rate\u0026gt; rateSupplier = () -\u0026gt; { return ratingService.getRate(bookId); }; final Rate rate = registry.timer(\u0026#34;book.rating.test\u0026#34;).wrap(rateSupplier).get(); 请求一下，并验证收集评价​​所花费的时间。\n# HELP book_rating_test_seconds # TYPE book_rating_test_seconds summary book_rating_test_seconds_count 4.0 book_rating_test_seconds_sum 1.05489108 # HELP book_rating_test_seconds_max # TYPE book_rating_test_seconds_max gauge book_rating_test_seconds_max 1.018622001 Micrometer 使用 MeterFilter 实例来自定义 MeterRegistry 实例发出的度量。Micrometer 扩展将检测 MeterFilter CDI bean，并在初始化 MeterRegistry 实例时使用它们。\n例如，我们可以定义一个通用标签来设置运行应用程序的环境（产品、测试、预发布等）。\n@Singleton public class MicrometerCustomConfiguration { @Produces @Singleton public MeterFilter configureAllRegistries() { return MeterFilter.commonTags(Arrays.asList( Tag.of(\u0026#34;env\u0026#34;, \u0026#34;prod\u0026#34;))); } } 发送新请求并验证指标是否已标记。\nhttp_client_requests_seconds_max{clientName=\u0026quot;localhost\u0026quot;,env=\u0026quot;prod\u0026quot;,method=\u0026quot;GET\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/rate/2\u0026quot;,} 0.0 请注意 env 包含值为 prod 的标签。\n跟踪 Quarkus 应用程序利用 OpenTracing 规范为交互式 Web 应用程序提供分布式跟踪。\n让我们配置 OpenTracing 以连接到 Jaeger 服务器，将 book-service 设置为服务名称以标识跟踪：\nquarkus.jaeger.enabled=true quarkus.jaeger.endpoint=http://localhost:14268/api/traces quarkus.jaeger.service-name=book-service quarkus.jaeger.sampler-type=const quarkus.jaeger.sampler-param=1 现在发一个请求：\ncurl -H \u0026#34;Authorization: Bearer ...\u0026#34; localhost:8080/book/1 {\u0026#34;bookId\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Book 1\u0026#34;,\u0026#34;rating\u0026#34;:3} 访问Jaeger UI以验证是否跟踪了该调用：\n总结 与开发整体应用程序相比，开发和实现微服务体系结构更具挑战性。我们认为，微服务可以驱动你根据应用程序基础结构正确地开发服务。\n此处介绍的大多数微服务（API 和管道除外）是新的，或者在整体应用中实现方式有所不同。原因是现在应用程序被分解成几部分，所有部分都在网络中互连。\n如果你May 26, 2021打算开发微服务并将其部署到 Kubernetes，那么 Quarkus 是一个很好的解决方案，因为它可以与 Kubernetes 顺利集成。实施大多数微服务很简单，只需要几行代码。\n本文演示的源代码可以在 github 上找到。\n关于作者 Alex Soto 是 Red Hat 开发人员经验总监。他对 Java 世界，软件自动化充满热情，并且他相信开源软件模型。Soto 是 Manning 的合著者 | 测试 Java 微服务和 O\u0026rsquo;Reilly Quarkus Cookbook 和几个开源项目的贡献者。自 2017 年以来一直是 Java 冠军，他还是 Salle URL University 的国际演讲者和老师。你可以在 Twitter （Alex Soto）上关注他，以随时了解 Kubernetes 和 Java 世界中正在发生的事情。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/microservicilities-quarkus/","tags":["Quarkus","Java"],"title":"使用 Quarkus 和 MicroProfile 实现微服务特性"},{"categories":["云原生"],"contents":"在配置系统监控的时候，是不是即使绞尽脑汁监控的也还是不够全面，或者不知如何获取想要的指标。\nAwesome Prometheus alerts 维护了一套开箱即用的 Prometheus 告警规则集合，有 300 多个告警规则。同时，还是说明如何获取对应的指标。这些规则，对每个 Prometheus 都是通用的。\n涉及如主机、硬件、容器等基础资源，到数据库、消息代理、运行时、反向代理、负责均衡器，运行时、服务编排，甚至是网络层面和 Prometheus 自身和集群。\nPrometheus 的安装和配置不做赘述，配置可以看这里。下面简单看下几个常用规则\n主机和硬件资源 主机和硬件资源的告警依赖 node-exporter 输出的指标。例如：\n内存不足 可用内存低于阈值 10% 就会触发告警。\n- alert: HostOutOfMemory expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 \u0026lt; 10 for: 2m labels: severity: warning annotations: summary: Host out of memory (instance {{ $labels.instance }}) description: \u0026#34;Node memory is filling up (\u0026lt; 10% left)\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; 主机异常的网络吞吐 最近两分钟入站的流量超过 100m。\n rate 语法见这里。\n - alert: HostUnusualNetworkThroughputIn expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 \u0026gt; 100 for: 5m labels: severity: warning annotations: summary: Host unusual network throughput in (instance {{ $labels.instance }}) description: \u0026#34;Host network interfaces are probably receiving too much data (\u0026gt; 100 MB/s)\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; Mysql Mysql 的告警依赖 prometheus/mysqld_exporter 输出的指标。\n连接数过多 Mysql 实例的连接数最近一分钟的连接数超过最大值的 80% 触发告警\n- alert: MysqlTooManyConnections(\u0026gt;80%) expr: avg by (instance) (rate(mysql_global_status_threads_connected[1m])) / avg by (instance) (mysql_global_variables_max_connections) * 100 \u0026gt; 80 for: 2m labels: severity: warning annotations: summary: MySQL too many connections (\u0026gt; 80%) (instance {{ $labels.instance }}) description: \u0026#34;More than 80% of MySQL connections are in use on {{ $labels.instance }}\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; 慢查询 最近一分钟慢查询数量大于 0 时触发。\n- alert: MysqlSlowQueries expr: increase(mysql_global_status_slow_queries[1m]) \u0026gt; 0 for: 2m labels: severity: warning annotations: summary: MySQL slow queries (instance {{ $labels.instance }}) description: \u0026#34;MySQL server mysql has some new slow query.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; 运行时 JVM JVM 的运行时告警，居然只有可怜巴巴的一个。堆空间占用超过 80% 触发告警。\n依赖 java-client 输出的指标。\n- alert: JvmMemoryFillingUp expr: (sum by (instance)(jvm_memory_used_bytes{area=\u0026#34;heap\u0026#34;}) / sum by (instance)(jvm_memory_max_bytes{area=\u0026#34;heap\u0026#34;})) * 100 \u0026gt; 80 for: 2m labels: severity: warning annotations: summary: JVM memory filling up (instance {{ $labels.instance }}) description: \u0026#34;JVM memory is filling up (\u0026gt; 80%)\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; Kubernetes Kubernetes 相关的告警规则有 33 个，比较丰富。\n摘个比较常见的：容器OOM告警。\n- alert: KubernetesContainerOomKiller expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m \u0026gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\u0026#34;OOMKilled\u0026#34;}[10m]) == 1 for: 0m labels: severity: warning annotations: summary: Kubernetes container oom killer (instance {{ $labels.instance }}) description: \u0026#34;Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; SSL 证书过期 通过  输出的指标，可以监控证书过期：未来 7 天 有证书过期便会触发告警。\n- alert: SslCertificateExpiry(\u0026lt;7Days) expr: ssl_verified_cert_not_after{chain_no=\u0026#34;0\u0026#34;} - time() \u0026lt; 86400 * 7 for: 0m labels: severity: warning annotations: summary: SSL certificate expiry (\u0026lt; 7 days) (instance {{ $labels.instance }}) description: \u0026#34;{{ $labels.instance }} Certificate is expiring in 7 days\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\u0026#34; 今天列出来的也仅仅是冰山一角，而且用户也可以贡献出更多的规则。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/introduction-awesome-prometheus-alerts/","tags":["Prometheus"],"title":"开箱即用的 Prometheus 告警规则集"},{"categories":["源码解析"],"contents":"去年写过一篇博客：控制 Pod 内容器的启动顺序，分析了 TektonCD 的容器启动控制的原理。\n为什么要做容器启动顺序控制？我们都知道 Pod 中除了 init-container 之外，是允许添加多个容器的。类似 TektonCD 中 task 和 step 的概念就分别与 pod 和 container 对应，而 step 是按照顺序执行的。此外还有服务网格的场景，sidecar 容器需要在服务容器启动之前完成配置的加载，也需要对容器的启动顺序加以控制。否则，服务容器先启动，而 sidecar 还无法提供网络上的支持。\n现实 期望 到了这里肯定有同学会问，spec.containers[] 是一个数组，数组是有顺序的。Kubernetes 也确实是按照顺序来创建和启动容器，但是 容器启动成功，并不表示容器可以对外提供服务。\n在 Kubernetes 1.18 非正式版中曾在 Lifecycle 层面提供了对 sidecar 类型容器的 支持，但是最终该功能并没有落地。\n那到底该怎么做？\nTL;DR 笔者准备了一个简单的 go 项目，用于模拟 sidecar 的启动及配置加载。\n克隆代码后可以通过 make build 构建出镜像，假如你是用的 minikube 进行的实验，可以通过命令 make load-2-minikube 将镜像加载到 minikube 节点中。\n使用 Deployment 的方式进行部署，直接用 Pod 也可以。\napiVersion: apps/v1 kind: Deployment metadata: creationTimestamp: null labels: app: sample name: sample spec: replicas: 1 selector: matchLabels: app: sample strategy: {} template: metadata: creationTimestamp: null labels: app: sample spec: containers: - image: addozhang/k8s-container-sequence-sidecar:latest name: sidecar imagePullPolicy: IfNotPresent lifecycle: postStart: exec: command: - /entrypoint - wait - image: busybox:latest name: app imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;] args: [\u0026#34;date; echo \u0026#39;app container started\u0026#39;; tail -f /dev/null\u0026#34;] 下面的截图中，演示了在 sample 命名空间中，pod 内两个容器的执行顺序。\nKubernetes 源码 在 kubelet 的源码 pkg/kubelet/kuberuntime/kuberuntime_manager.go 中，#SyncPod 方法用于创建 Pod，步骤比较繁琐，直接看第 7 步：创建普通容器。\n// SyncPod syncs the running pod into the desired pod by executing following steps: // // 1. Compute sandbox and container changes. // 2. Kill pod sandbox if necessary. // 3. Kill any containers that should not be running. // 4. Create sandbox if necessary. // 5. Create ephemeral containers. // 6. Create init containers. // 7. Create normal containers. func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { ... // Step 7: start containers in podContainerChanges.ContainersToStart. \tfor _, idx := range podContainerChanges.ContainersToStart { start(\u0026#34;container\u0026#34;, containerStartSpec(\u0026amp;pod.Spec.Containers[idx])) } return } 在 #start 方法中调用了 #startContainer 方法，该方法会启动容器，并返回容器启动的结果。注意，这里的结果还 包含了容器的 Lifecycle hooks 调用。\n也就是说，假如容器的 PostStart hook 没有正确的返回，kubelet 便不会去创建下一个容器。\n// startContainer starts a container and returns a message indicates why it is failed on error. // It starts the container through the following steps: // * pull the image // * create the container // * start the container // * run the post start lifecycle hooks (if applicable) func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, podIPs []string) (string, error) { ... // Step 4: execute the post start hook. \tif container.Lifecycle != nil \u0026amp;\u0026amp; container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { m.recordContainerEvent(pod, container, kubeContainerID.ID, v1.EventTypeWarning, events.FailedPostStartHook, msg) if err := m.killContainer(pod, kubeContainerID, container.Name, \u0026#34;FailedPostStartHook\u0026#34;, reasonFailedPostStartHook, nil); err != nil { klog.ErrorS(fmt.Errorf(\u0026#34;%s: %v\u0026#34;, ErrPostStartHook, handlerErr), \u0026#34;Failed to kill container\u0026#34;, \u0026#34;pod\u0026#34;, klog.KObj(pod), \u0026#34;podUID\u0026#34;, pod.UID, \u0026#34;containerName\u0026#34;, container.Name, \u0026#34;containerID\u0026#34;, kubeContainerID.String()) } return msg, fmt.Errorf(\u0026#34;%s: %v\u0026#34;, ErrPostStartHook, handlerErr) } } return \u0026#34;\u0026#34;, nil } 实现方案 cmd/entrypoint/wait.go#L26 （这里参考了 Istio 的 pilot-agent 实现 ）\n在 PostStart 中持续的去检查 /ready 断点，可以 hold 住当前容器的创建流程。保证 /ready 返回 200 后，kubelet 才会去创建下一个容器。\n这样就达到了前面截图中演示的效果。\nfor time.Now().Before(timeoutAt) { err = checkIfReady(client, url) if err == nil { log.Println(\u0026#34;sidecar is ready\u0026#34;) return nil } log.Println(\u0026#34;sidecar is not ready\u0026#34;) time.Sleep(time.Duration(periodMillis) * time.Millisecond) } return fmt.Errorf(\u0026#34;sidecar is not ready in %d second(s)\u0026#34;, timeoutSeconds) 参考  Sidecar container lifecycle changes in Kubernetes 1.18 Delaying application start until sidecar is ready  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/k8s-1.18-container-start-sequence-control/","tags":["Kubernetes","Istio"],"title":"Kubernetes 上如何控制容器的启动顺序？"},{"categories":["翻译"],"contents":"本文由 Addo Zhang 翻译自 A Reference Architecture for Fine-Grained Access Management on the Cloud\n什么是访问管理？ 访问管理是识别用户或一组用户是否应该能够访问给定资源（例如主机、服务或数据库）的过程。例如，对于开发人员来说是否可以使用 SSH 登录生产应用程序服务器，如果可以，那么可以登录多长时间？如果 SRE 在非支持时间尝试访问数据库，他们这样做？如果数据工程师已转移到其他团队，他们是否应该继续访问 ETL 管道的 S3 存储桶？\n现在如何进行访问管理？ 在云上各种基础设施和数据服务激增之前，访问管理是 DevOps 和 Security 团队要解决的相对简单的问题。VPN 和堡垒主机是（现在仍然是）在网络级别封锁所有关键资源的首选机制。用户必须先通过 VPN 服务器进行身份验证，或者登录到堡垒主机，然后才能访问专用网络上的所有资源。\n当资源是静态的并且它们的数量相对较小时，此方法效果很好。但是，随着越来越多的资源动态地涌入专用网络的各处，VPN / 堡垒主机解决方案变得站不住脚。\n具体来说，在三个方面，VPN 和堡垒主机不足以作为一种有效的访问管理机制。\n 它们作用于网络层面：用户通过 VPN 进行身份验证并获得对专用网络的访问权限后，他们实际上就可以访问其上运行的所有服务。无法根据用户的身份在基础架构或数据服务的粒度上管理访问。 凭据是攻击的媒介：VPN 和堡垒主机都要求用户记住并存储凭据。过期和轮换凭证作为安全策略非常困难，尤其是在涉及大量用户的情况下，凭证因此成为潜在的攻击媒介。 不能管理第三方 SaaS 工具：SaaS 工具（如 Looker、Tableau 和 Periscope Data）需要直接访问数据端点。因此，使用这些工具访问数据的任何人都无法通过使用了相同的机制和凭据的基础设施进行身份验证。  云上访问管理的新架构 在本文中，我们将定义新的参考架构，为那些正在寻求简化访问管理云资源（从 SSH 主机、数据库、数据仓库到消息管道和云存储终结点）解决方案的云原生企业。\n它解决了 VPN 和堡垒主机无法克服的以下特定挑战：\n 在细粒度的服务级别上进行访问鉴权 消除共享凭据和个人帐户管理 通过第三方 SaaS 工具控制访问  此外，它为具有敏感数据的组织带来以下商业利益：\n 通过跨所有服务的会话记录和活动监视来满足 FedRamp 和 SOC2 等合规性标准的可审核性 基于访问者的身份，通过细粒度的授权策略来限制或清除敏感数据，从而实现隐私和数据治理  该架构建立在以下三个核心原则的基础上，这些原则的实现使 DevOps 和 Security 团队可以在对所有环境进行全面控制的同时，通过简单而一致的体验来提高用户的工作效率。\n 为访问资源的用户建立不可否认的身份 使用短期的短暂令牌和证书代替静态凭证和密钥 在一处集中所有资源类型的细粒度访问策略  下图显示了参考架构及其组件。\n上图中的 VPN / 堡垒主机已替换为接入网关（Access Gateway）。接入网关实际上是微服务的集合，负责验证单个用户、基于特定属性授权他们的请求，并最终授予他们访问专用网络中的基础结构和数据服务的权限。\n接下来，让我们看一下各个组件，以了解之前概括的核心原理是如何实现的。\n访问控制器 支持此体系结构的关键见解是将用户身份验证委派给单个服务（访问控制器），而不是将责任分配给用户可能需要访问的服务。这种联合在 SaaS 应用程序世界中很常见。由单一服务负责身份验证，可以简化应用程序所有者的用户配置和接触配置，并加快应用程序开发。\n对于实际的身份验证序列，访问控制器本身通常会与身份提供商集成，例如 Auth0 或 Okta，因此，可以跨提供者和协议提供有用的抽象。最终，身份提供商以签名的 SAML 声明\\JWT 令牌或临时证书的形式保证用户的身份不可否认。这样就无需依赖受信任的子网作为用户身份的代理。与 VPN 允许用户访问网络上的所有服务不同，它还允许将访问策略配置到服务的粒度。\n将身份验证委派给身份提供者的另一个好处是，可以使用零信任原则对用户进行身份验证。 具体来说，可以创建身份提供者策略以强制执行以下操作：\n 禁止从信誉不佳的地理位置和 IP 地址访问 禁止从已知漏洞的设备（未修补的 OS、较旧的浏览器等）进行访问 成功进行 SAML 交换后立即触发 MFA  身份验证序列如何工作：  用户首先通过访问控制器进行身份验证，访问控制器又将身份验证委派给身份提供者。 成功登录到身份提供者后，访问控制器将生成一个短暂的临时证书，进行签名并将其返回给用户。或者，它可以代替证书生成令牌。只要证书或令牌有效，就可以将其用于连接到 接入网关管理的任何授权基础设施或数据服务。到期后，必须获取新的证书或令牌。 用户将在步骤（2）中获得的证书传递给他们选择的工具，然后连接到接入网关。根据用户请求访问的服务，基础设施网关或数据网关将首先允许访问控制器验证用户的证书，然后再允许他们访问该服务。因此，访问控制器充当用户与其访问的服务之间的 CA，因此为每个用户提供了不可否认的身份。  策略引擎 当访问控制器强制对用户进行身份验证时，策略引擎会对用户的请求强制进行细粒度的授权。它以易于使用的 YAML 语法接受授权规则（查看最后的示例），并根据用户请求和响应对它们进行评估。\n开放策略代理（OPA）是一个开源的 CNCF 项目，是策略引擎的一个很好的例子。它可以自己作为微服务运行，也可以用作其他微服务进程空间中的库。OPA 中的策略以称为 Rego 的语言编写。另外，也可以在 Rego 之上轻松构建一个简单的 YAML 界面，以简化政策规范。\n具有独立于基础结构和数据服务的安全模型的独立策略引擎的优点如下：\n 可以以与服务和位置无关的方式指定安全策略  例如在所有 SSH 服务器上禁止特权命令 例如强制执行 MFA 检查所有服务（基础设施和数据）   策略可以保存在一个地方并进行版本控制  策略可以作为代码签入 GitHub 存储库 每项变更在提交之前都要经过协作审核流程 存在版本历史记录，可以轻松地还原策略更改    基础设施网关和数据网关都依赖于策略引擎，以分别评估用户的基础设施和数据活动。\n基础设施网关 基础设施网关管理和监控对基础设施服务的访问，例如 SSH 服务器和 Kubernetes 集群。它与策略引擎连接，以确定细化的授权规则，并在用户会话期间对所有基础设施活动强制执行这些规则。 为了实现负载平衡，网关可以包含一组工作节点，可以在 AWS 上部署为自动扩展组，也可以在 Kubernetes 集群上作为副本集运行。\nHashicorp 边界 是基础设施网关的示例。这是一个开源项目，使开发人员、DevOps 和 SRE 可以使用细粒度的授权来安全地访问基础设施服务（SSH 服务器、Kubernetes 群集），而无需直接访问网络，同时又禁止使用 VPN 或堡垒主机。\n基础设施网关支持 SSH 服务器和 Kubernetes 客户端使用的各种连接协议，并提供以下关键功能：\n会话记录 这涉及复制用户在会话期间执行的每个命令。捕获的命令通常会附加其他信息，例如用户的身份、他们所属的各种身份提供者组、当天的时间、命令的持续时间以及响应的特征（是否成功、是否有错误、是否已读取或写入数据等）。\n活动监控 监控使会话记录的概念更进一步。除了捕获所有命令和响应，基础设施网关还将安全策略应用于用户的活动。在发生违规的情况下，它可以选择触发警报、阻止有问题的命令及其响应或完全终止用户的会话。\n数据网关 数据网关管理和监控对数据服务的访问，例如 MySQL、PostgreSQL 和 MongoDB 等托管数据库、AWS RDS 等 DBaaS 端点、Snowflake 和 Bigquery 等数据仓库、AWS S3 等云存储以及 Kafka 和 Kinesis。它与策略引擎连接，以确定细化的授权规则，并在用户会话期间对所有数据活动强制执行这些规则。\n与基础设施网关类似，数据网关可以包含一组工作节点，可以在 AWS 上部署为自动扩展组，也可以在 Kubernetes 集群上作为副本集运行。\n由于与基础设施服务相比，数据服务的种类更多，因此数据网关通常将支持大量的连接协议和语法。\n此类数据网关的示例是 Cyral，这是一种轻量级的拦截服务，以边车（sidecar）的方式部署来监控和管理对现代数据终端节点的访问，如 AWS RDS、Snowflake、Bigquery，、AWS S3、Apache Kafka 等。其功能包括：\n会话记录 这类似于记录基础设施活动，并且涉及用户在会话期间执行的每个命令的副本，并使用丰富的审计信息进行注释。\n活动监控 同样，这类似于监视基础设施活动。例如，以下策略阻止数据分析人员读取敏感的客户 PII。\n隐私权执行 与基础设施服务不同，数据服务授予用户对通常位于数据库、数据仓库、云存储和消息管道中的与客户、合作伙伴和竞争对手有关的敏感数据的读写访问权限。 出于隐私原因，对数据网关的一个非常普遍的要求是能够清理（也称为令牌化或屏蔽）PII，例如电子邮件、姓名、社会保险号、信用卡号和地址。\n那么这种体系结构如何简化访问管理？ 让我们看一些常见的访问管理方案，以了解与使用 VPN 和堡垒主机相比，接入网关架构如何提供细粒度的控制。\n特权活动监控（PAM） 这是一个简单的策略，可以在一个地方监视所有基础设施和数据服务中的特权活动：\n 仅允许属于 Admins 和 SRE 组的个人在 SSH 服务器、Kubernetes 集群和数据库上运行特权命令。 虽然可以运行特权命令，但是有一些例外形式的限制。具体来说，以下命令是不允许的：  “sudo” 和 “yum” 命令可能无法在任何 SSH 服务器上运行 “kubectl delete” 和 “kubectl taint” 命令可能无法在任何 Kubernetes 集群上运行 “drop table” 和 “create user” 命令可能无法在任何数据库上运行    零特权（ZSP）执行 The next policy shows an example of enforcing zero standing privileges \u0026ndash; a paradigm where no one has access to an infrastructure or data service by default. Access may be obtained only upon satisfying one or more qualifying criteria:\n Only individuals belonging to the Support group are allowed access An individual must be on-call to gain access. On call status may be determined by checking their schedule in an incident response service such as PagerDuty A multi-factor authentication (MFA) check is triggered upon successful authentication They must use TLS to connect to the infrastructure or data service Lastly, if a data service is being accessed, full table scans (e.g. SQL requests lacking a WHERE or a LIMIT clause that end up reading an entire dataset) are disallowed.  下一个策略显示了一个实施零特权的示例 \u0026ndash; 一种默认情况下没有人可以访问基础设施或数据服务的范例。只有满足一个或多个合格标准，才能获得访问权限：\n 只允许属于支持组的个人访问 个人必须 on-call 才能获得访问权限。可以通过检查事件响应服务（例如 PagerDuty）中的时间表来确定通话状态 成功通过身份验证后会触发多因子身份验证（MFA）检查 他们必须使用 TLS 连接到基础设施或数据服务 最后，如果正在访问数据服务，则不允许进行全表扫描（例如，缺少 WHERE 或 LIMIT 子句的 SQL 请求最终将读取整个数据集）。  隐私和数据保护 The last policy shows an example of data governance involving data scrubbing:\n If anyone from Marketing is accessing PII (social security number (SSN), credit card number (CCN), age), scrub the data before returning If anyone is accessing PII using the Looker or Tableau services, also scrub the data Scrubbing rules are defined by the specific type of the PII  For SSNs, scrub the first 5 digits For CCNs, scrub the last 4 digits For ages, scrub the last digit i.e., the requestor will know the age brackets but never the actual ages    最后一条策略显示了涉及数据清理的数据治理示例：\n 如果市场营销人员正在访问 PII（社会保险号（SSN）、信用卡号（CCN）、年龄），先清洗数据然后再返回 如果有人正在使用 Looker 或 Tableau 服务访问 PII，同时清洗数据 清理规则由 PII 的特定类型定义  对于 SSN，清洗前 5 位数字 对于 CCN，清洗最后 4 位数字 对于年龄，请清洗最后一位数字，即请求者将知道年龄段，但从不知道实际年龄    概括 我们看到，对于高度动态的云环境，VPN 和堡垒主机不足以作为高效云环境中的有效访问管理机制。一种新的访问管理体系结构，其重点是不可否认的用户身份，短暂的证书或令牌以及集中的细粒度授权引擎，可有效解决 VPN 和堡垒主机无法解决的难题。除了为访问关键基础设施和数据服务的用户提供全面的安全性之外，该体系结构还可以帮助组织实现其审核、合规性、隐私和保护目标。\n我们还讨论了该架构的参考实现，其中使用了以开发人员为中心的著名开源解决方案，例如 Hashicorp Boundary 和 OPA 以及 Cyral（一种用于现代数据服务的快速且无状态的辅助工具）。 他们一起可以在云上提供细粒度且易于使用的访问管理解决方案。\n关于作者 Manav Mital 是 Cyral 的联合创始人兼首席执行官，Cyral 是首个为数据云提供可见性、访问控制和保护的云原生安全服务。Cyral 成立于 2018 年，与各种组织合作 - 从云原生初创企业到财富 500 强企业，因为它们采用 DevOps 文化和云技术来管理和分析数据。 Manav 拥有 UCLA 的计算机科学硕士学位和坎普尔的印度理工学院的计算机科学学士学位。\n关于译者 Addo Zhang 云原生从业人员，爱好各种代码。更多翻译：\n 分布式系统在 Kubernetes 上的进化 2021 年及未来的云原生预测 应用架构：为什么要随着市场演进  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/translation-access-management-reference-architecture/","tags":["OPA","云原生"],"title":"云上细粒度访问管理的参考架构"},{"categories":["笔记","云原生"],"contents":"想到这个标题的时候，我第一时间想到的就是星爷的《唐伯虎点秋香》的这一幕。\n当讨论起世界上最好的开发语言是什么的时候，Java 的粉丝们总会遇到这种场景：\n 吹：“Java 语法简单，容易上手！” 黑：“Java 启动慢，性能差，耗资源！” 吹：“Java 有世界上最多的程序员！” 黑：“Java 启动慢，性能差，耗资源！” 吹：“Java 生态好！” 黑：“Java 启动慢，性能差，耗资源！” 吹：“滚！”\n 今天我们继续说说 Quarkus，应“云”而生的 Java 框架。今天算是第三篇了，没看过的同学可以回顾一下：\n Hello, Quarkus 应\u0026quot;云\u0026quot;而生的 Java 框架 Quarkus：构建本机可执行文件  上一篇的结尾预告：试试 Quarkus 在 ArgoCD 中的应用，看下 Serverless 上的使用体验。不过不想用 ArgoCD 了，因为这 workflow 这种场景实在体现不出 Quarkus 到底有多快。但又想做 Serverless，那就想到了 Knative Serving 了。\n其实，还有一个原因是比较懒，上次的镜像还可以直接拿来用。\nTL;DR 废话不多说，先上结论。Quarkus 与 Spring 首个请求的响应耗时：2.5s vs 5.7s。\n注：为了忽略拉取镜像的时间差异，提前 pull 镜像。\n验证 环境准备  Kubernetes 1.18+ via minikube Istio 1.9.2 Knative 0.22.0 Knative CLI （brew 安装） watch （brew 安装）  环境的安装准备参考官方的文档。\n镜像 资源镜像就使用上一篇文章构建的，但需要做下调整：\ndocker tag quarkus/quarkus-getting-started:distroless dev.local/quarkus/quarkus-getting-started:distroless docker tag spring/spring-getting-started:latest dev.local/spring/spring-getting-started:latest 注：knative 会忽略 dev.local 镜像的预加载，不会在创建 knative service 的时候拉取。\n然后使用 minikube image load 加载到 minikube环境中：\nminikube image load dev.local/quarkus/quarkus-getting-started:distroless minikube image load dev.local/spring/spring-getting-started:latest knative 配置（可选） 修改 istio-system namespace 下的 configmap config-domain，增加新的 domain：nip.io\n注：这个操作纯属个人喜好，不喜欢那个 example.com，可跳过。\n获取 Istio Ingress 地址 使用命令获取 Ingress 的访问方式，这里 http2/80 后的 http://192.168.64.2:31608 就是我们需要的，记下这个 ip 和端口。\nminikube service list |------------------|----------------------------|-------------------|---------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |------------------|----------------------------|-------------------|---------------------------| | default | kubernetes | No node port | | istio-system | istio-egressgateway | No node port | | istio-system | istio-ingressgateway | status-port/15021 | http://192.168.64.2:32431 | | | | http2/80 | http://192.168.64.2:31608 | | | | https/443 | http://192.168.64.2:31795 | | | | tcp/31400 | http://192.168.64.2:31369 | | | | tls/15443 | http://192.168.64.2:30293 | | istio-system | istiod | No node port | | istio-system | knative-local-gateway | No node port | | knative-eventing | broker-filter | No node port | | knative-eventing | broker-ingress | No node port | | knative-eventing | eventing-webhook | No node port | | knative-eventing | imc-dispatcher | No node port | | knative-serving | activator-service | No node port | | knative-serving | autoscaler | No node port | | knative-serving | autoscaler-bucket-00-of-01 | No node port | | knative-serving | autoscaler-hpa | No node port | | knative-serving | controller | No node port | | knative-serving | istio-webhook | No node port | | knative-serving | webhook | No node port | | kube-system | kube-dns | No node port | |------------------|----------------------------|-------------------|---------------------------| 创建 Knative service #quarkus apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello-quarkus namespace: default spec: template: spec: containers: - image: dev.local/quarkus/quarkus-getting-started:distroless imagePullPolicy: Never --- #spring apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello-spring namespace: default spec: template: spec: containers: - image: dev.local/spring/spring-getting-started:latest imagePullPolicy: Never 通过 cli kn 命令查看下 service 的信息：\nkn service describe hello-quarkus -n default Name: hello-quarkus Namespace: default Age: 21s URL: http://hello-quarkus.default.nip.io Revisions: 100% @latest (hello-quarkus-00001) [1] (21s) Image: dev.local/quarkus/quarkus-getting-started:distroless Conditions: OK TYPE AGE REASON ++ Ready 9s ++ ConfigurationsReady 10s ++ RoutesReady 9s kn service describe hello-spring -n default Name: hello-spring Namespace: default Age: 44s URL: http://hello-spring.default.nip.io Revisions: 100% @latest (hello-spring-00001) [1] (44s) Image: dev.local/spring/spring-getting-started:latest Conditions: OK TYPE AGE REASON ++ Ready 31s ++ ConfigurationsReady 32s ++ RoutesReady 31s 从描述信息中可以拿到服务的访问地址，分别是 http://hello-quarkus.default.nip.io 和 http://hello-spring.default.nip.io。\n接下来就需要在本地主机的 hosts 中加入解析：\n192.168.64.2 hello-quarkus.default.nip.io 192.168.64.2 hello-spring.default.nip.io 测试 上面操作完之后，就可以使用下面的地址访问服务了。\n http://hello-quarkus.default.nip.io:31608/hello/greeting/quarkus http://hello-spring.default.nip.io:31608/hello/greeting/spring\n 在测试的过程中，可以通过 watch -n 1 'kubectl get po -n default | grep hello' 命令来查看 pod 的创建和销毁。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/quarkus-enable-java-running-in-serverless/","tags":["Java","Serverless","Quarkus","Knative"],"title":"Quarkus：谁说 Java 不能用来跑 Serverless？"},{"categories":["笔记"],"contents":"为什么 说起服务网格，这幅图大家肯定不会陌生。这就是服务网格的网络，也是网格架构的终极形态。\n那在迁移到网格架构之前，我们的系统是什么样的？\n我们的系统在演进的过程中，不可避免的会遇到各种 0 到 1 过程中的中间态。比如下面这种，可以比较直观的看出 Istio 或者网格是部分覆盖的。这个过程中，我们需要平滑、可控的推进，才能在保障系统可用性的前提下进行架构的演进。\n怎么做 Sidecar 的注入分两种：手动和自动。\n手动 手动就是利用 Istio 的 cli 工具 istioctl kube-inject 对资源 yaml 进行修改：\n$ istioctl kube-inject -f samples/sleep/sleep.yaml | kubectl apply -f - serviceaccount/sleep created service/sleep created deployment.apps/sleep created 手动的方式比较适合开发阶段使用。\n自动 sidecar 的自动注入则是通过 mutating webhook admission controller 实现的。其原理简单说就是拦截 pod的创建请求来对 pod 的资源定义进行修改。\n我们对截取了 istio-sidecar-injector MutatingWebhookConfiguration 的部分内容。\nwebhooks: ... matchPolicy: Exact #1 name: sidecar-injector.istio.io namespaceSelector: #2 matchLabels: istio-injection: enabled objectSelector: #3 matchExpressions: - key: sidecar.istio.io/inject operator: NotIn values: - \u0026#34;false\u0026#34; rules: #4 - apiGroups: - \u0026#34;\u0026#34; apiVersions: - v1 operations: - CREATE resources: - pods scope: \u0026#39;*\u0026#39; ...  这里的 matchPolicy: Exact 针对的是 #4 中的 apiGroups与apiVersions 的组合，即精确匹配 v1/pods 的 CREATE 请求 顾名思义，匹配符合条件的 namespace 同2，匹配符合条件的 object  注： #2、#3 支持 Kubernetes 的标签选择语法\n按照前面的说明，这个 hook 会拦截打了 istio-injection: enabled label 的 namespace 下，没有打 sidecar.istio.io/inject: false 标签的 v1/pod 的创建。通过 https://istiod.istio-system:443/inject 端点对 pod 的定义进行定制（添加 init-container、sidecar 容器等）。\n有人可能会说这样还不够精准，因为可能某个 namespace 下只有部分对象才会注入 sidecar。\n这就需要借助 istiod 的逻辑了。\n只针对特定 pod 注入sidecar 或忽略注入 在 configmap istio-sidecar-injector 中有两个字段 alwaysInjectSelector 和 neverInjectSelector。从名字来看这两个分别提供了白名单、黑名单的功能。\nalwaysInjectSelector: [] neverInjectSelector:[] 我们只需如下调整（需要重启 istiod），然后为需要注入 sidecar 的资源打上相应的标签即可。\nalwaysInjectSelector: - matchExpressions: - key: sidecar.istio.io/inject operator: In values: - \u0026#34;true\u0026#34; - \u0026#34;enabled\u0026#34; - \u0026#34;yes\u0026#34; ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/how-to-control-istio-sidecar-injection/","tags":["Istio","Kubernetes","Service Mesh"],"title":"服务网格平稳落地：Istio 中精准控制 Sidecar 的注入"},{"categories":["教程","笔记"],"contents":" 电影《功夫》中，火云邪神有句话：“天下武功无坚不摧，唯快不破。”\n 在 上一篇文章 中，我们写了第一个 Quarkus 应用，并尝试着构建了 legacy-jar 和 fast-jar。\n今天来看一下 Quarkus 构建出来的本机可执行文件到底比 Spring 应用能快多少，生态的成熟度不在这里讨论。\nTLDR 先上结论， 与只有一个 Controller 的Spring Web 应用做下对比。\n应用启动时间：0.012s vs 2.294s 镜像大小：49MB vs 237 MB Spring 应用镜像使用 openjdk:11.0-jre-slim 作为 base 镜像，大小为 220MB。\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE spring/spring-getting-started latest 5f47030c5c3f 6 minutes ago 237MB quarkus/quarkus-getting-started distroless2 fe973c5ac172 24 minutes ago 49MB quarkus/quarkus-getting-started distroless 6fe27dd44e86 31 minutes ago 51MB quarkus/quarkus-getting-started ubi 8f86f5915715 58 minutes ago 132MB Java 应用容器化的困境 云原生世界中，应用容器化是个显著的特点。Java 应用容器化时面临了如下问题：\n 应用启动慢：其实这是 Java 应用的问题。Java 应用占用内存多；JVM 虚拟机启动时需要做环境的初始化、预加载大量的类、初始化线程等等。启动耗时视应用情况需要几秒，甚至可达分钟级。较长的启动耗时，也抑制了水平伸缩性。即使在 Serverless 这种响应耗时要求不高的场景，也会被嫌弃。 镜像过大：其实使用了镜像的分层设计，常见的一个 SpringCloud 应用的 über-jar 包可能都有 7、80MB。 空间占用：虽然用了镜像分层，但积少成多，也会增加存储成本。  Quarkus 与本机映像（native image） Quarkus 的开发遵从了容器优先的原则：\n 支持 Graal/SubstrateVM 构建时处理元数据 减少反射的使用 本机映像预启动   本机映像是将 Java 代码提前编译为可执行文件（称为本机映像）的技术。该可执行文件包括应用程序类、其依赖项中的类、运行时库类以及 JDK 中的静态链接本机代码。它不是在 Java VM 上运行，而是包括必要的组件，例如内存管理，线程调度等，这些组件来自另一个运行时系统 “Substrate VM”。“Substrate VM” 是运行时组件（例如反优化器，垃圾收集器，线程调度等）的名称。与 JVM 相比，生成的程序具有更快的启动时间和更低的运行时内存开销。\n 如何构建本机映像 环境配置参考上一篇文章，可以直接从这里下载源码。\n配置 GraalVM 之前我们使用了sdkman 进行 GraalVM 安装。设置 GRAALVM_HOME 环境变量：\nexport GRAALVM_HOME=`sdk home java 21.0.0.2.r11-grl` 使用 gu 安装 native-image：\n${GRAALVM_HOME}/bin/gu install native-image 构建本机可执行文件 在源码的 pom.xml 中，我们可以看到如下的 profile：\n\u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;native\u0026lt;/id\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;quarkus.package.type\u0026gt;native\u0026lt;/quarkus.package.type\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; 我们使用这个 profile 进行本机可执行文件的构建，整个构建耗时 几分钟 。\n./mvnw package -Pnative 部分构建日志：\n[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ quarkus-getting-started --- [INFO] [INFO] --- quarkus-maven-plugin:1.13.0.Final:build (default) @ quarkus-getting-started --- [INFO] [org.jboss.threads] JBoss Threads version 3.2.0.Final [INFO] [io.quarkus.deployment.pkg.steps.JarResultBuildStep] Building native image source jar: /Users/addo/Workspaces/private_w/quarkus-getting-started/target/quarkus-getting-started-1.0.0-SNAPSHOT-native-image-source-jar/quarkus-getting-started-1.0.0-SNAPSHOT-runner.jar [INFO] [io.quarkus.deployment.pkg.steps.NativeImageBuildStep] Building native image from /Users/addo/Workspaces/private_w/quarkus-getting-started/target/quarkus-getting-started-1.0.0-SNAPSHOT-native-image-source-jar/quarkus-getting-started-1.0.0-SNAPSHOT-runner.jar [INFO] [io.quarkus.deployment.pkg.steps.NativeImageBuildContainerRunner] Using docker to run the native image builder [INFO] [io.quarkus.deployment.pkg.steps.NativeImageBuildContainerRunner] Checking image status quay.io/quarkus/ubi-quarkus-native-image:21.0.0-java11 21.0.0-java11: Pulling from quarkus/ubi-quarkus-native-image Digest: sha256:becf08de869e707beaa5e57444b533ef93ebef15aad90c92ac660ddf7cea2b11 Status: Image is up to date for quay.io/quarkus/ubi-quarkus-native-image:21.0.0-java11 quay.io/quarkus/ubi-quarkus-native-image:21.0.0-java11 [INFO] [io.quarkus.deployment.pkg.steps.NativeImageBuildStep] Running Quarkus native-image plugin on GraalVM Version 21.0.0 (Java Version 11.0.10+8-jvmci-21.0-b06) [INFO] [io.quarkus.deployment.pkg.steps.NativeImageBuildRunner] docker run --env LANG=C --rm -v /Users/addo/Workspaces/private_w/quarkus-getting-started/target/quarkus-getting-started-1.0.0-SNAPSHOT-native-image-source-jar:/project:z quay.io/quarkus/ubi-quarkus-native-image:21.0.0-java11 -J-Dsun.nio.ch.maxUpdateArraySize=100 -J-Djava.util.logging.manager=org.jboss.logmanager.LogManager -J-Dvertx.logger-delegate-factory-class-name=io.quarkus.vertx.core.runtime.VertxLogDelegateFactory -J-Dvertx.disableDnsResolver=true -J-Dio.netty.leakDetection.level=DISABLED -J-Dio.netty.allocator.maxOrder=1 -J-Duser.language=en -J-Duser.country=CN -J-Dfile.encoding=UTF-8 --initialize-at-build-time= -H:InitialCollectionPolicy=com.oracle.svm.core.genscavenge.CollectionPolicy\\$BySpaceAndTime -H:+JNI -H:+AllowFoldMethods -jar quarkus-getting-started-1.0.0-SNAPSHOT-runner.jar -H:FallbackThreshold=0 -H:+ReportExceptionStackTraces -J-Xmx5g -H:-AddAllCharsets -H:EnableURLProtocols=http --no-server -H:-UseServiceLoaderFeature -H:+StackTrace quarkus-getting-started-1.0.0-SNAPSHOT-runner [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] classlist: 5,859.24 ms, 0.96 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (cap): 633.34 ms, 0.94 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] setup: 2,468.19 ms, 0.94 GB 00:06:00,437 INFO [org.jbo.threads] JBoss Threads version 3.2.0.Final [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (clinit): 516.65 ms, 2.23 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (typeflow): 12,642.02 ms, 2.23 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (objects): 11,340.37 ms, 2.23 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (features): 525.87 ms, 2.23 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] analysis: 26,032.67 ms, 2.23 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] universe: 1,394.06 ms, 2.16 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (parse): 2,690.38 ms, 2.16 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (inline): 4,336.77 ms, 2.73 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] (compile): 17,580.03 ms, 2.71 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] compile: 26,152.06 ms, 2.71 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] image: 3,288.43 ms, 2.70 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] write: 1,904.64 ms, 2.70 GB [quarkus-getting-started-1.0.0-SNAPSHOT-runner:25] [total]: 67,414.16 ms, 2.70 GB [WARNING] [io.quarkus.deployment.pkg.steps.NativeImageBuildStep] objcopy executable not found in PATH. Debug symbols will not be separated from executable. [WARNING] [io.quarkus.deployment.pkg.steps.NativeImageBuildStep] That will result in a larger native image with debug symbols embedded in it. [INFO] [io.quarkus.deployment.QuarkusAugmentor] Quarkus augmentation completed in 74739ms [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 01:21 min [INFO] Finished at: 2021-04-17T08:06:47+08:00 [INFO] ------------------------------------------------------------------------  假如构建时出现类似 Caused by: java.lang.RuntimeException: Image generation failed. Exit code was 137 which indicates an out of memory error. Consider increasing the Xmx value for native image generation by setting the \u0026quot;quarkus.native.native-image-xmx\u0026quot; property 这种报错。需要调整下 Docker 的设置，比如笔者使用的 macOS，打开 Docker Desktop \u0026gt; Preference \u0026gt; Resource \u0026gt; Advanced，将内存从默认的 2GB 调大，比如 8GB。\n从构建日志可以看出，构建的过程是在 quay.io/quarkus/ubi-quarkus-native-image 的容器中完成的。虽然异常提示调整 \u0026ldquo;quarkus.native.native-image-xmx\u0026rdquo; ，其实是容器内存太小导致的。\n 构建成功后，可以在 target 中找到 quarkus-getting-started-1.0.0-SNAPSHOT-runner。这是一个可执行文件，大小为 28MB。\n尝试执行该文件，收到 zsh: exec format error: ./target/quarkus-getting-started-1.0.0-SNAPSHOT-runner 错误。因为这是一个 Linux 可执行文件，因此我们需要在容器中运行。\n构建本机镜像 在源文件的 src/main/docker 目录中，我们可以找到 Dockerfile.native：\nFROMregistry.access.redhat.com/ubi8/ubi-minimal:8.3WORKDIR/work/RUN chown 1001 /work \\  \u0026amp;\u0026amp; chmod \u0026#34;g+rwX\u0026#34; /work \\  \u0026amp;\u0026amp; chown 1001:root /workCOPY --chown=1001:root target/*-runner /work/applicationEXPOSE8080USER1001CMD [\u0026#34;./application\u0026#34;, \u0026#34;-Dquarkus.http.host=0.0.0.0\u0026#34;]运行镜像 本地运行一下，可以看出启动只需要 0.013s。\ndocker run --rm -p 8080:8080 quarkus/quarkus-getting-started:latest __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,\u0026lt; / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2021-04-17 00:22:27,146 INFO [io.quarkus] (main) quarkus-getting-started 1.0.0-SNAPSHOT native (powered by Quarkus 1.13.0.Final) started in 0.013s. Listening on: http://0.0.0.0:8080 2021-04-17 00:22:27,147 INFO [io.quarkus] (main) Profile prod activated. 2021-04-17 00:22:27,147 INFO [io.quarkus] (main) Installed features: [cdi, resteasy] 测试一下端点：\nhttp :8080/hello/greeting/quarkus HTTP/1.1 200 OK Content-Length: 14 Content-Type: text/plain;charset=UTF-8 Hello, quarkus 看下镜像的信息，大小为 132MB，其中 base 镜像 ubi-minimal 就占了 103 MB。感觉还是有点大，是否继续精简一下？\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE quarkus/quarkus-getting-started latest 8f86f5915715 4 minutes ago 132MB registry.access.redhat.com/ubi8/ubi-minimal 8.3 604ddd554fec 2 weeks ago 103MB 镜像瘦身 在 src/main/docker 中还有个名为 Dockerfile.native-distroless 的Dockerfile，里面使用了 quay.io/quarkus/quarkus-distroless-image:1.0 作为 base 镜像\n使用这个Dockerfile进行构建，得到的镜像就小很多，只有 51MB：\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE quarkus/quarkus-getting-started distroless 6fe27dd44e86 33 seconds ago 51MB quarkus/quarkus-getting-started ubi 8f86f5915715 27 minutes ago 132MB quay.io/quarkus/quarkus-distroless-image 1.0 062663862a83 6 days ago 21.3MB registry.access.redhat.com/ubi8/ubi-minimal 8.3 604ddd554fec 2 weeks ago 103MB 运行成功：\ndocker run --rm -p 8080:8080 quarkus/quarkus-getting-started:distroless __ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,\u0026lt; / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2021-04-17 00:51:26,070 INFO [io.quarkus] (main) quarkus-getting-started 1.0.0-SNAPSHOT native (powered by Quarkus 1.13.0.Final) started in 0.013s. Listening on: http://0.0.0.0:8080 2021-04-17 00:51:26,071 INFO [io.quarkus] (main) Profile prod activated. 2021-04-17 00:51:26,071 INFO [io.quarkus] (main) Installed features: [cdi, resteasy] 极致瘦身，参考了这里，我们创建 Dockerfile.native-distroless2。\n最终镜像的大小为 49MB，与官方提供的 distroless base 镜像只小了 2MB。\ndocker images REPOSITORY TAG IMAGE ID CREATED SIZE quarkus/quarkus-getting-started distroless2 fe973c5ac172 3 seconds ago 49MB 前面对比，用来构建 Spring 应用的 base 镜像 openjdk:11.0-jre-slim 已经有 220MB，这还没算上应用的大小。即使是 openjdk:17-alpine3.13 也有 182 MB。\nNEXT 下一回，我们试试 Quarkus 在 ArgoCD 中的应用，看下 Serverless 上的使用体验如何。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/quarkus-build-native-executable-file/","tags":["云原生","Java","Quarkus","Graalvm","Container"],"title":"应“云”而生的 Java 框架：构建本机可执行文件"},{"categories":["笔记"],"contents":"Wikipedia上有关 Quarkus 的信息还很少，只有一句简单的介绍：\n Quarkus 是专为 OpenJDK HotSpot 和 GraalVM 定制的全栈 Kubernetes 原生 Java 应用程序框架。与如 Spring 之类的其他框架相比，它提供了较小的内存占用并缩短了启动时间。它允许结合命令式和非阻塞响应式编程。\n 从 Quarkus 的官网，可以看到其有几个特性：\n 容器优先 统一了命令式和响应式编程 开发者友好 最佳品种的库及标准  更多 Quarkus 可以参考官网的介绍及文档。今天主要就是跑一下 Quarkus 的 Hello world。\n放一张官网的图：\n环境准备  基于 Java 11 的 GraalVM Maven 3.6.2+  笔者使用的是 macos 10.15.4，GraalVM 和 Maven 建议通过 sdkman 进行安装。\n$ sdk install java 21.0.0.2.r11-grl #如果已使用其他 java 版本，可以使用命令 sdk use java 21.0.0.2.r11-grl 进行切换 $ sdk install maven 3.6.3 验证安装 $ java -version openjdk version \u0026#34;11.0.10\u0026#34; 2021-01-19 OpenJDK Runtime Environment GraalVM CE 21.0.0.2 (build 11.0.10+8-jvmci-21.0-b06) OpenJDK 64-Bit Server VM GraalVM CE 21.0.0.2 (build 11.0.10+8-jvmci-21.0-b06, mixed mode, sharing) $ mvn -version Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f) Maven home: /Users/addo/.sdkman/candidates/maven/current Java version: 11.0.10, vendor: GraalVM Community, runtime: /Users/addo/.sdkman/candidates/java/21.0.0.2.r11-grl Default locale: en_CN, platform encoding: UTF-8 OS name: \u0026#34;mac os x\u0026#34;, version: \u0026#34;10.15.4\u0026#34;, arch: \u0026#34;x86_64\u0026#34;, family: \u0026#34;mac\u0026#34; 快速开始 创建项目 创建 quarkus 项目最快的方式是通过 quarkus-maven-plugin 来创建，使用如下的命令快速可以创建\n$ mvn io.quarkus:quarkus-maven-plugin:1.13.0.Final:create \\  -DprojectGroupId=com.atbug.quickstart \\  -DprojectArtifactId=quarkus-getting-started \\  -DclassName=\u0026#34;com.atbug.quickstart.GreetingResource\u0026#34; \\  -Dpath=\u0026#34;/hello\u0026#34; 在 ./quarkus-getting-started 中提供了：\n maven 的项目结构 com.atbug.quickstart.GreetingResource 暴露了 /hello 端点，通过 JAX-RS 注解实现 相关的单元测试 应用启动后可以通过 http://localhost:8080 打开的启动页面 src/main/docker 下提供了 native 和 jvm 风格的 Dockerfile 应用配置文件  运行应用 执行 ./mvnw compile quarkus:dev 命令可启动应用\n__ ____ __ _____ ___ __ ____ ______ --/ __ \\/ / / / _ | / _ \\/ //_/ / / / __/ -/ /_/ / /_/ / __ |/ , _/ ,\u0026lt; / /_/ /\\ \\ --\\___\\_\\____/_/ |_/_/|_/_/|_|\\____/___/ 2021-04-05 19:48:36,419 INFO [io.quarkus] (Quarkus Main Thread) quarkus-getting-started 1.0.0-SNAPSHOT on JVM (powered by Quarkus 1.13.0.Final) started in 2.135s. Listening on: http://localhost:8080 2021-04-05 19:48:36,448 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2021-04-05 19:48:36,448 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, resteasy] 访问 /hello 断点\n$ http :8080/hello HTTP/1.1 200 OK Content-Length: 5 Content-Type: text/plain;charset=UTF-8 Hello  笔者通过 httpie 进行访问，可以通过 brew install httpie 进行安装，推荐使用。\n 增加新的断点 @GET @Produces(MediaType.TEXT_PLAIN) @Path(\u0026#34;/greeting/{name}\u0026#34;) public String greeting(@PathParam String name) { return \u0026#34;Hello, \u0026#34; + name; } 注：PathParam 来自 org.jboss.resteasy.annotations.jaxrs.PathParam\n测试：\n$ http :8080/hello/greeting/Quarkus HTTP/1.1 200 OK Content-Length: 14 Content-Type: text/plain;charset=UTF-8 Hello, Quarkus 为新的端点增加单元测试\n@Test public void testGreetingEndpoint() { final String uuid = UUID.randomUUID().toString(); given() .pathParam(\u0026#34;name\u0026#34;, uuid) .when().get(\u0026#34;/hello/greeting/{name}\u0026#34;) .then() .statusCode(200) .body(is(\u0026#34;Hello, \u0026#34; + uuid)); } 通过 ./mvnw test 运行单元测试\n 注意这里使用 intellij 运行单元测试的话，会报错。需要修改 Java Compiler 的配置，添加额外的命令行参数 -parameters\n 打包 与通常的 maven 项目打包方式一样，执行 ./mvnw package，在 target 目录中：\n quarkus-getting-started-1.0.0-SNAPSHOT.jar 仅包含了项目编译的类和资源文件，是不可执行的 jar quarkus-app 目录中包含了可执行的 jar 文件 quarkus-run.jar ，但是，其并不是一个 über-jar，项目的依赖库都位于 lib目录中。  可以通过执行 java -jar target/quarkus-app/quarkus-run.jar 在启动应用。\n 这意味着假如你想在容器中运行，需要部署整个 quarkus-app 目录\n 使用 fast-jar qurakus 的打包方式有两种：legacy-jar 和 fast-jar。可以在 application.properties 文件中进行指定，未显式指定默认为 legacy-jar。\nquarkus.package.type=fast-jar  如果要在容器中运行，同样需要部署整个 quarkus-app 目录 fast-jar 类型的包比 legacy-jar 的包启动会快一点点，同时占用的内存也更低。因为 fast-jar 的包含了依赖包中的类和资源文件的索引，避免在类和资源文件加载时对 classpath 下的包的查找。\n 下一篇，试试构建一个原生的可执行文件。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/hello-quarkus/","tags":["云原生","Java","Graalvm"],"title":"应“云”而生的 Java 框架：Hello, Quarkus"},{"categories":["翻译"],"contents":"本文译自 The Evolution of Distributed Systems on Kubernetes\n在 3 月份的 QCon 上，我做了一个关于 Kubernetes 的分布式系统进化的演讲。首先，我想先问一个问题，微服务之后是什么？我相信大家都有各自的答案，我也有我的答案。你会在最后发现我的想法是什么。为了达到这个目的，我建议大家看看分布式系统的需求是什么？以及这些需求在过去是如何发展的，从单体应用开始到 Kubernetes，再到最近的 Dapr、Istio、Knative 等项目，它们是如何改变我们做分布式系统的方式。我们将尝试对未来做一些预测。\n现代分布式应用 为了给这个话题提供更多的背景信息，我认为的分布式系统是由数百个组件组成的系统。这些组件可以是有状态的、无状态的或者无服务器的。此外，这些组件可以用不同的语言创建，运行在混合环境上，并开发开源技术、开放标准和互操作性。我相信你可以使用闭源软件来构建这样的系统，也可以在 AWS 和其他地方构建。具体到这次演讲，我将关注 Kubernetes 生态系统，以及你如何在 Kubernetes 平台上构建这样一个系统。\n我们从分布式系统的需求讲起。我认为是我们要创建一个应用或者服务，并写一些业务逻辑。那从运行时的平台到构建分布式系统，我们还需要什么呢？在底层，最开始是我们要一些生命周期的能力。当你用任一语言开发你的应用时，我们希望有能力把这个应用可靠地打包和部署、回滚、健康检查。并且能够把应用部署到不同的节点上，并实现资源隔离、扩展、配置管理，以及所有这些。这些都是你创建分布式应用所需要的第一点。\n第二点是围绕网络。我们有了应用之后，我们希望它能够可靠地连接到其他服务，无论该服务是在集群内部还是在外部。我们希望其具有服务发现、负载均衡的能力。为了不同的发布策略或是其他的一些原因的我们希望有流量转移的能力。然后我们还希望其具有与其他系统进行弹性通信的能力，无论是通过重试、超时还是断路器。要有适当的安全保障，并且要有足够的监控、追踪、可观察性等等。\n我们有了网络之后，接下来就是我们希望有能力与不同的 API 和端点交互，即资源绑定\u0026ndash;与其他协议和不同的数据格式交互。甚至能够从一种数据格式转换成另一种数据格式。我还会在这里加入诸如滤光的功能，也就是说，当我们订阅一个主题时，我们也许只对某些事件感兴趣。\n你认为最后一类是什么？是状态。当我在说状态和有状态的抽象时，我并不是在谈论实际的状态管理，比如数据库或者文件系统的功能。我要说的更多是有关幕后依赖状态的开发人员抽象。可能，你需要具有工作流管理的能力。也许你想管理运行时间长的进程或者做临时调度或者某些定时任务来定期运行服务。也许你还想进行分布式缓存，具有幂等性或者支持回滚。所有这些都是开发人员级的原语，但在幕后，它们依赖于具有某种状态。你想随意使用这些抽象俩创建完善的分布式系统。\n我们将使用这个分布式系统原语的框架来评估它们在 Kubernetes 和其他项目上的变化情况。\n单体架构 \u0026ndash; 传统中间件功能 假设我们从单体架构以及如何获得这些能力开始。在那种情况下，首先是当我说单体的时候，在分布式应用的情况下我想到的是 ESB。ESB 是相当强大的，当我们检查我们的需求列表时，我们会说 ESB 对所有有状态的抽象有很好的支持。\n使用 ESB，你可以进行长时间运行的流程的编排、分布式事务、回滚和幂等。此外，ESB 还提供了出色的资源绑定能力，并且有数百个连接器，支持转换、编排，甚至有联网功能。最后，ESB 甚至可以做服务发现和负载均衡。\n它具有围绕网络连接的弹性的所有功能，因此它可以进行重试。可能 ESB 本质上不是很分布式，所以它不需要非常高级的网络和发布能力。ESB 欠缺的主要是生命周期管理。因为它是单一运行时，所以第一件事就是你只能使用一种语言。通常是创建实际运行时的语言，Java、.NET、或者其他的语言。然后，因为是单一运行时，我们不能轻松地进行声明式的部署或者自动防止。部署是相当大且非常重的，所以它通常涉及到人机交互。这种单体架构的另一个难点是扩展：“我们无法扩展单个组件。”\n最后却并非最不重要的一点是，围绕隔离，无论是资源隔离还是故障隔离。使用单体架构无法完成所有这些工作。从我们的需求框架来看，ESB 的单体架构不符合条件。\n云原生架构 \u0026ndash; 微服务和 Kubernetes 接下来，我建议我们研究一下云原生架构以及这些需求是如何变化的。如果我们从一个非常高的层面来看，这些架构是如何发生变化的，云原生可能始于微服务运动。微服务使我们可以按业务领域进行拆分单体应用。事实证明，容器和 Kubernetes 实际上是管理这些微服务的优秀平台。让我们来看一下 Kubernetes 对于微服务特别有吸引力的一些具体特性和功能。\n从一开始，进行健康状况探测的能力就是 Kubernetes 受欢迎的原因。在实践中，这意味着当你将容器部署到 Pod 中时，Kubernetes 会检查进程的运行状况。通常情况下，该过程模型还不够好。你可能仍然有一个已启动并正在运行的进程，但是它并不健康。这就是为什么还可以使用就绪度和存活度检查的原因。Kubernetes 会做一个就绪度检查，以确定你的应用在启动期间何时准备接受流量。它将进行活跃度检查，以检查服务的运行状况。在 Kubernetes 之前，这并不是很流行，但今天几乎所有语言、所有框架、所有运行时都有健康检查功能，你可以在其中快速启动端点。\nKubernetes 引入的下一个特性是围绕应用程序的托管生命周期\u0026ndash;我的意思是，你不再控制何时启动、何时关闭服务。你相信平台可以做到这一点。Kubernetes 可以启动你的应用；它可以将其关闭，然后在不同的节点上移动它。为此，你必须正确执行平台在应用启动和关闭期间告诉你的事件。\nKubernetes 刘兴的另一件特性是围绕着声明式部署。这意味着你不再需要启动服务；检查日志是否已经启动。你不必手动升级实例\u0026ndash;支持声明式部署的 Kubernetes 可以为你做到这一点。根据你选择的策略，它可以停止旧实例并启动新实例。此外，如果出现问题，可以进行回滚。\n另外就是声明你的资源需求。创建服务时，将其容器化。最好告诉平台该服务将需要多少 CPU 和内存。Kubernetes 利用这些信息为你的工作负载找到最佳节点。在使用 Kubernetes 之前，我们必须根据我们的标准将实例手动放置到一个节点上。现在，我们可以根据自己的偏好来指导 Kubernetes，它将为我们做出最佳的决策。\n如今，在 Kubernetes 上，你可以进行多语言配置管理。无需在应用程序运行时进行配置查找就可以进行任何操作。Kubernetes 会确保配置最终在工作负载所在的同一节点上。这些配置被映射为卷或环境变量，以供你的应用程序使用。\n事实证明，我刚才谈到的那些特定功能也是相关的。比如说，如果要进行自动放置，则必须告诉 Kubernetes 服务的资源需求。然后，你必须告诉它要使用的部署策略。为了让策略正确运行，你的应用程序必须执行来自环境的事件。它必须执行健康检查。一旦采用了所有这些最佳实践并使用所有这些功能，你的应用就会成为出色的云原生公民，并且可以在 Kubernetes 上实现自动化了（这是在 Kubernetes 上运行工作负载的基本模式）。最后，还有围绕着构建 Pod 中的容器、配置管理和行为，还有其他模式。\n我要简要介绍的下一个主题是工作负载。从生命周期的角度来看，我们希望能够运行不同的工作负载。我们也可以在 Kubernetes 上做到这一点。运行十二要素应用程序和无状态微服务非常简单。Kubernetes 可以做到这一点。这不是你将要承担的唯一工作量。可能你还有有状态的工作负载，你可以使用有状态集在 Kubernetes 上完成此工作。\n你可能还有的另一个工作负载是单例。也许你希望某个应用程序的实例是整个集群中应用程序的唯一一个实例\u0026ndash;你希望它成为可靠的单例。如果失败，则重新启动。因此，你可以根据需求以及是否希望单例至少具有一种或最多一种语义来在有状态集和副本集之间进行选择。你可能还有的另一个工作负载是围绕作业和定时作业\u0026ndash;有了 Kubernetes，你也可以实现这些。\n如果我们将所有这些 Kubernetes 功能映射到我们的需求，则 Kubernetes 可以满足生命周期需求。我通常创建的需求列表主要是由 Kubernetes 今天提供给我们的。这些是任何平台上的预期功能，而 Kubernetes 可以为你的部署做的是配置管理、资源隔离和故障隔离。此外，除了无服务器本身之外，它还支持其他工作负载。\n然后，如果这就是 Kubernetes 给开发者提供的全部功能，那么我们该如何扩展 Kubernetes 呢？以及如何使它具有更多功能？因此，我想描述当今使用的两种常用方法。\n进程外扩展机制 首先是 Pod 的概念，Pod 是用于在节点上部署容器的抽象。此外，Pod 给我们提供了两组保证：\n 第一组是部署保证 \u0026ndash; Pod 中的所有容器始终位于同一个节点上。这意味着它们可以通过 localhost 相互通信，也可以使用文件系统或通过其他 IPC 机制进行异步通信。 Pod 给我们的另一组保证是围绕生命周期的。Pod 中的所有容器并非都相等。  根据使用的是 init 容器还是应用程序容器，你会获得不同的保证。例如，init 容器在开始时运行；当 Pod 启动时，它按顺序一个接一个地运行。他们仅在之前的容器已成功完成时运行。它们有助于实现由容器驱动的类似工作流的逻辑。\n另一方面，应用程序容器是并行运行的。它们在整个 Pod 的生命周期中运行，这也是 sidecar 模式的基础。sidecar 可以运行多个容器，这些容器可以协作并共同为用户提供价值。这也是当今我们看到的扩展 Kubernetes 附加功能的主要机制之一。\n为了解释以下功能，我必须简要地告诉你 Kubernetes 内部的工作方式。它是基于调谐循环的。调谐循环的思想是将期望状态驱动到实际状态。在 Kubernetes 中，很多功能都是靠这个来实现的。例如，当你说我要两个 Pod 实例，这系统的期望状态。有一个控制循环不断地运行，并检查你的 Pod 是否有两个实例。如果不存在两个实例，它将计算差值。它将确保存在两个实例。\n这方面的例子有很多。一些是副本集或有状态集。资源定义映射到控制器是什么，并且每个资源定义都有一个控制器。该控制器确保现实世界与所需控制器相匹配，你甚至可以编写自己的自定义控制器。\n当在 Pod 中运行应用程序时，你将无法在运行时加载任何配置文件更改。然而，你可以编写一个自定义控制器，检测 config map 的变化，重新启动 Pod 和应用程序\u0026ndash;从而获取配置更改。\n事实证明，即使 Kubernetes 拥有丰富的资源集合，但它们并不能满足你的所有不同需求。Kubernetes 引入了自定义资源定义的概念。这意味着你可以对需求进行建模并定义适用于 Kubernetes 的 API。它与其他 Kubernetes 原生资源共存。你可以用能理解模型的任何语言编写自己的控制器。你可以设计一个用 Java 实现的 ConfigWatcher，描述我们前面所解释的内容。这就是 operator 模式，即与自定义资源定义一起使用的控制器。如今，我们看到很多 operator 假如，这就是第二种扩展 Kubernetes 附加功能的方式。\n接下来，我想简单介绍一下基于 Kubernetes 构建的一些平台，这些平台大量使用 sidecar 和 operator 来给开发者提供额外的功能。\n什么是服务网格？ 让我们从服务网格开始，什么是服务网格？\n我们有两个服务，服务 A 要调用服务 B，并且可以用任何语言。把这个当做是我们的应用工作负载。服务网格使用 sidecar 控制器，并在我们的服务旁边注入一个代理。你最终会在 Pod 中得到两个容器。代理是一个透明的代理，你的应用对这个代理完全无感知\u0026ndash;它拦截所有传入和传出的流量。此外，代理还充当数据防火墙。\n这些服务代理的集合代表了你的数据平面，并且很小且无状态。为了获得所有状态和配置，它们依赖于控制平面。控制平面是保持所有配置，收集指标，做出决定并与数据平面进行交互的有状态部分。此外，它们是不同控制平面和数据平面的正确选择。事实证明，我们还需要一个组件-一个 API 网关，以将数据获取到我们的集群中。一些服务网格具有自己的 API 网关，而某些使用第三方。如果你研究下所有这些组件，它们将提供我们所需的功能。\nAPI 网关主要专注于抽象我们服务的实现。它隐藏细节并提供边界功能。服务网格则相反。在某种程度上，它增强了服务内的可见性和可靠性。可以说，API 网关和服务网格共同提供了所有网络需求。要在 Kubernetes 上获得网络功能，仅使用服务是不够的：“你需要一些服务网格。”\n什么是 Knative？ 我要讨论的下一个主题是 Knative，这是 Google 几年前启动的一个项目。它是 Kubernetes 之上的一层，可为您提供无服务器功能，并具有两个主要模块：\n Knative 服务 - 围绕着请求-应答交互，以及 Knative Eventing - 更多的是用于事件驱动的交互。  只是让你感受一下，Knative Serving 是什么？通过 Knative Serving，你可以定义服务，但这不同于 Kubernetes 服务。这是 Knative 服务。使用 Knative 服务定义工作负载后，你就会得到具有无服务器的特征的部署。你不需要有启动并运行实例。它可以在请求到达时从零开始。你得到的是无服务器的能力；它可以迅速扩容，也可以缩容到零。\nKnative Eventing 为我们提供了一个完全声明式的事件管理系统。假设我们有一些要与之集成的外部系统，以及一些外部的事件生产者。在底部，我们将应用程序放在具有 HTTP 端点的容器中。借助 Knative Eventing，我们可以启动代理，该代理可以触发 Kafka 映射的代理，也可以在内存或者某些云服务中。此外，我们可以启动连接到外部系统的导入器，并将事件导入到我们的代理中。这些导入器可以基于，例如，具有数百个连接器的 Apache Camel。\n一旦我们将事件发送给代理，然后用 YAML 文件声明，我们可以让容器订阅这些事件。在我们的容器中，我们不需要任何消息客户端\u0026ndash;比如 Kafka 客户端。我们的容器将使用云事件通过 HTTP POST 获取事件。这是一个完全平台管理的消息传递基础设施。作为开发人员，你必须在容器中编写业务代码，并且不处理任何消息传递逻辑。\n从我们的需求的角度来看，Knative 可以满足其中的一些要求。从生命周期的角度来看，它为我们的工作负载提供了无服务器的功能，因此能够将其扩展到零，并从零开始激活。从网络的角度来看，如果服务网格之间存在某些重叠，则 Knative 也可以进行流量转移。从绑定的角度来看，它对使用 Knative 导入程序进行绑定提供了很好的支持。它可以使我们进行发布/订阅，或点对点交互，甚至可以进行一些排序。它可以满足几类需求。\n什么是 Dapr？ 另一个使用 sidecar 和 operator 的项目是 Dapr，它是微软几个月前才开始并且正在迅速流行起来。此外，1.0 版本 被认为是生产可用的。它是一个作为 sidecar 的分布式系统工具包\u0026ndash;Dapr 中的所有内容都是作为 sidecar 提供的，并且有一套他们所谓的构件或功能集的集合。\n这些功能是什么呢？第一组功能是围绕网络。Dapr 可以进行服务发现和服务之间的点对点集成。同样，它也可以进行服务网格的追踪、可靠通信、重试和恢复。第二套功能是围绕资源绑定：\n 它有很多云 API、不同系统的连接器，以及 也可以做消息发布/订阅和其他逻辑。  有趣的是，Dapr 还引入了状态管理的概念。除了 Knative 和服务网格提供的功能外，Dapr 在状态存储之上进行了抽象。此外，你通过存储机制支持与 Dapr 进行基于键值的交互。\n在较高的层次上，架构是你的应用程序位于顶部，可以使用任何语言。你可以使用 Dapr 提供的客户端库，但你不必这样做。你可以使用语言功能来执行称为 sidecar 的 HTTP 和 gRPC。与 服务网格的区别在于，这里的 Dapr sidecar 不是一个透明的代理。它是一个显式代理，你必须从你的应用中调用它，并通过 HTTP 或 gRPC 与之交互。根据你需要的功能，Dapr 可以与其他如云服务的系统对话。\n在 Kubernetes 上，Dapr 是作为 sidecar 部署的，并且可以在 Kubernetes 之外工作（不仅仅是 Kubernetes）。此外，它还有一个 operator \u0026ndash; 而 sidecar 和 Operator 是主要的扩展机制。其他一些组件管理证书、处理基于 actor 的建模并注入 sidecar。你的工作负载与 sidecar 交互，并尽其所能与其他服务对话，让你与不同的云提供商进行互操作。它还为你提供了额外的分布式系统功能。\n综上所述，这些项目所提供的功能，我们可以说 ESB 是分布式系统的早期化身，其中我们有集中式的控制平面和数据平面\u0026ndash;但是扩展性不好。在云原生中，集中式控制平面仍然存在，但是数据平面是分散的\u0026ndash;并且具有隔音功能和高度的可扩展性。\n我们始终需要 Kubernetes 来做良好的生命周期管理，除此之外，你可能还需要一个或多个附加组件。你可能需要 Istio 来进行高级联网。你可能会使用 Knative 来进行无服务器工作负载，或者使用 Dapr 来做集成。这些框架可与 Istio 和 Envoy 很好的配合使用。从 Dapr 和 Knative 的角度来看，你可能必须选择一个。它们共同以云原生的方式提供了我们过去在 ESB 上拥有的东西。\n未来云原生趋势\u0026ndash;生命周期趋势 在接下来的部分，我列出了一些我认为在这些领域正在发生令人振奋的发展的项目。\n我想从生命周期开始。通过 Kubernetes，我们可以为应用程序提供一个有用的生命周期，这可能不足以进行更复杂的生命周期管理。比如，如果你有一个更复杂的有状态应用，则可能会有这样的场景，其中 Kubernetes 中的部署原语不足以为应用提供支持。\n在这些场景下，你可以使用 operator 模式。你可以使用一个 operator 来进行部署和升级，还可以将 S3 作为服务备份的存储介质。此外，你可能还会发现 Kubernetes 的实际健康检查机制不够好。假设存活检查和就绪检查不够好。在这种情况下，你可以使用 operator 对你的应用进行更智能的存活和就绪检查，然后在此基础上进行恢复。\n第三个领域就是自动伸缩和调整。你可以让 operator 更好的了解你的应用，并在平台上进行自动调整。目前，编写 operator 的框架主要有两个，一个是 Kubernetes 特别兴趣小组的 Kubebuilder，另一个是红帽创建的 operator 框架的一部分\u0026ndash;operator SDK。它有以下几个方面的内容：\nOperator SDK 让你可以编写 operator \u0026ndash; operator 生命周期管理器来管理 operator 的生命周期，以及可以发布你的 operator 到 OperatorHub。如今在 OperatorHub，你会看到 100 多个 operator 用于管理数据库、消息队列和监控工具。从生命周期空间来看，operator 可能是 Kubernetes 生态系统中发展最活跃的领域。\n网络趋势 - Envoy 我选的另一个项目是 Envoy。服务网格接口规范的引入将使你更轻松地切换不同的服务网格实现。在部署上 Istio 对架构进行了一些整合。你不再需要为控制平面部署 7 个 Pod；现在，你只需要部署一次就可以了。更有趣的是在 Envoy 项目的数据平面上所正在发生的：越来越多的第 7 层协议被添加到 Envoy 中。\n服务网格增加了对更多协议的支持，比如 MongoDB、ZooKeeper、MySQL、Redis，而最新的协议是 Kafka。我看到 Kafka 社区现在正在进一步改进他们的协议，使其对服务网格更加友好。我们可以预料将会有更紧密的集成、更多的功能。最有可能的是，会有一些桥接的能力。你可以从服务中在你的应用本地做一个 HTTP 调用，而代理将在后台使用 Kafka。你可以在应用外部，在 sidecar 中针对 Kafka 协议进行转换和加密。\n另一个令人兴奋的发展是引入了 HTTP 缓存。现在 Envoy 可以进行 HTTP 缓存。你不必在你的应用中使用缓存客户端。所有这些都是在 sidecar 中透明地完成的。有了 tap 过滤器，你可以 tap 流量并获得流量的副本。最近，WebAssembly 的引入，意味着如果你要为 Envoy 编写一些自定义的过滤器，你不必用 C++ 编写，也不必编译整个 Envoy 运行时。你可以用 WebAssembly 写你的过滤器，然后在运行时进行部署。这些大多数还在进行中。它们不存在，说明数据平面和服务网格无意停止，仅支持 HTTP 和 gRPC。他们有兴趣支持更多的应用层协议，为你提供更多的功能，以实现更多的用例。最主要的是，随着 WebAssembly 的引入，你现在可以在 sidecar 中编写自定义逻辑。只要你没有在其中添加一些业务逻辑就可以了。\n绑定趋势 - Apache Camel Apache Camel 是一个用于集成的项目，它具有很多使用企业集成模式连接到不同系统的连接器。 比如 Camel version 3 就深度集成到了 Kubernetes 中，并且使用了我们到目前为止所讲的那些原语，比如 operator。\n你可以在 Camel 中用 Java、JavaScript 或 YAML 等语言编写你的集成逻辑。最新的版本引入了一个 Camel operator，它在 Kubernetes 中运行并理解你的集成。当你写好 Camel 应用，将其部署到自定义资源中，operator 就知道如何构建容器或查找依赖项。根据平台的能力，不管是只用 Kubernetes，还是带有 Knative 的 Kubernetes，它都可以决定要使用的服务以及如何实现集成。在运行时之外有相当多的智能 \u0026ndash; 包括 operator \u0026ndash; 所有这些都非常快地发生。为什么我会说这是一个绑定的趋势？主要是因为 Apache Camel 提供的连接器的功能。这里有趣的一点是它如何与 Kubernetes 深度集成。\n状态趋势 - Cloudstate 另一个我想讨论的项目是 Cloudstate 和与状态相关的趋势。Cloudstate 是 Lightbend 的一个项目，主要致力于无服务器和功能驱动的开发。最新发布的版本，正在使用 sidecar 和 operator 与 Kubernetes 进行深度集成。\n这个创意是，当你编写你的功能时，你在功能中要做的就是使用 gRPC 来获取状态并与之进行交互。整个状态管理在与其他 sidecar 群集的 sidear 中进行。它使你能够进行事件溯源、CQRS、键值查询、消息传递。\n从应用程序角度来看，你并不了解所有这些复杂性。你所做的只是调用一个本地的 sidecar，而 sidecar 会处理这些复杂的事情。它可以在后台使用两个不同的数据源。而且它拥有开发人员所需的所有有状态抽象。\n到目前为止，我们已经看到了云原生生态系统中的最新技术以及一些仍在进行中的开发。我们如何理解这一切？\n多运行时微服务已经到来 如果你看微服务在 Kubernetes 上的样子，则将需要使用某些平台功能。此外，你将需要首先使用 Kubernetes 的功能进行生命周期管理。然后，很有可能透明地，你的服务会使用某些服务网格（例如 Envoy）来获得增强的网络功能，无论是流量路由、弹性、增强的安全性，甚至出于监控的目的。除此之外，根据你的场景和使用的工作负载可能需要 Dapr 或者 Knative。所有这些都代表了进程外附加的功能。剩下的就是编写业务逻辑，不是放在最上面而是作为一个单独的运行时来编写。未来的微服务很有可能将是由多个容器组成的这种多运行时。有些是透明的，有些则是非常明确的。\n智能的 sidecar 和愚蠢的管道 如果更深入地看，那可能是什么样的，你可以使用一些高级语言编写业务逻辑。是什么并不重要，不必仅是 Java，因为你可以使用任何其他语言并在内部开发自定义逻辑。\n你的业务逻辑与外部世界的所有交互都是通过 sidecar 发生的，并与平台集成进行生命周期管理。它为外部系统执行网络抽象，为你提供高级的绑定功能和状态抽象。sidecar 是你不需要开发的东西。你可以从货架上拿到它。你用一点 YAML 或 JSON 配置它，然后就可以使用它。这意味着你可以轻松地更新 sidecar，因为它不再被嵌入到你的运行时。这使得打补丁、更新变得更加更容易。它为我们的业务逻辑启用了多语言运行时。\n微服务之后是什么？ 这让我想到了最初的问题，微服务之后是什么？\n如果我们看下架构的发展历程，应用架构在很高的层面上是从单体应用开始的。然而微服务给我们提供了如何把一个单体应用拆分成独立的业务域的指导原则。之后又出现了无服务器和功能即服务（FaaS），我们说过可以按操作将其进一步拆分，从而实现极高的可扩展性-因为我们可以分别扩展每个操作。\n我想说的是 FaaS 并不是最好的模式 \u0026ndash; 因为功能并不是实现合理的复杂服务的最佳模式，在这种情况下，当多个操作必须与同一个数据集进行交互时，你希望它们驻留在一起。可能是多运行时（我把它称为 Mecha 架构），在该架构中你将业务逻辑放在一个容器中，而所有与基础设施相关的关注点作为一个单独的容器存在。它们共同代表多运行时微服务。也许这是一个更合适的模型，因为它有更好的属性。\n你可以获得微服务的所有好处。仍然将所有域和所有限界上下文放在一处。你将所有的基础设施和分布式应用需求放在一个单独的容器中，并在运行时将它们组合在一起。大概，现在最接近这种模型的是 Dapr。他们正在遵循这种模型。如果你仅对网络方面感兴趣，那么可能使用 Envoy 也会接近这种模型。\n关于作者 Bilgin Ibryam 是红帽公司的产品经理和前架构师、提交人，并且是 Apache 软件基金会的成员。他是开源布道者，经常写博客、发表演讲，是 Kubernetes Patterns 和 Camel Design Patterns 书籍的作者。Bilgin 目前的工作主要集中在分布式系统、事件驱动架构以及可重复的云原生应用开发模式和实践上。请关注他 @bibryam 了解未来类似主题的更新。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/translation-distributed-systems-kubernetes/","tags":["云原生","Kubernetes","Service Mesh","Knative","Serverless","Dapr"],"title":"分布式系统在 Kubernetes 上的进化"},{"categories":["翻译"],"contents":"本文译自 Cloud Native Predictions for 2021 and Beyond\n原文发布在 Chris Aniszczyk 的个人博客\n我希望每个人都有一个美好的假期，因为 2021 年 1 月的前几周一直非常疯狂，从叛乱到新的 COVID 菌株。在云原生国度，CNCF 最近发布了关于我们去年完成的所有工作的年度报告。我建议大家找个机会去看一下这份报告，在疫情大流行的这一年，我们收获颇丰。https://twitter.com/CloudNativeFdn/status/1343914259177222145\n作为我工作的一部分，我对云原生趋势有一个独特的观点，送给所有与我合作的会员公司和开发人员，所以我想我会分享我对 2021 年及以后云原生发展的想法。\n云原生的 IDE\n作为一个在 Eclipse 基金会内部从事开发者工具工作的人，我对最近的技术状态进展感到无比兴奋。未来，开发生命周期（代码、构建、调试）将主要发生在云端，而不是你本地的 Emacs 或 VSCode。你将每一个拉动请求最终得到一个完整的开发环境设置，预先配置并连接到他们自己的部署，以协助你的开发和调试需求。今天这种技术的一个具体例子是通过 GitHub Codespaces 和 GitPod 实现的。虽然 GitHub Codespaces 还处于测试阶段，但今天你可以通过 GitPod 来体验，以 Prometheus 为例。一分钟左右，你就拥有了一个有编辑器和预览环境的完全实时的开发环境。最疯狂的是，这个开发环境（工作空间）是 用代码描述，并且可以像其他代码工件一样，与你团队的其他开发者共享。\n最后，我期望在接下来的一年里，能看到云原生 IDE 领域出现令人难以置信的创新，特别是随着 GitHub Codespaces 进入测试版之后，并得到广泛地使用，让开发者可以体验到这个新概念，并爱上它。\n边缘的 Kubernetes\nKubernetes 是通过在大规模数据中心的使用而诞生的，但 Kubernetes 会像 Linux 一样为新的环境而进化。Linux 所发生的事情是，终端用户最终对内核进行了扩展，以支持从移动、嵌入式等各种新的部署场景。我坚信 Kubernetes 也会经历类似的进化，我们已经见证了 Telcos（和其他初创公司）通过将 VNFs 转化为 云原生网络功能（CNFs），以及 k3s、KubeEdge、k0s、LFEdge、Eclipse ioFog 等开源项目，来探索 Kubernetes 作为边缘平台。推动超大规模云服务支持电信公司和边缘的能力，再加上重用云原生软件的能力，以及建立在现有庞大的生态系统基础上的能力，将巩固 Kubernetes 在未来几年内成为边缘计算的主导平台。\n云原生 + Wasm\nWeb Assembly(Wasm) 是一项新的技术，但我预计它将成为云原生生态系统中不断增长的实用工具和工作负载，特别是随着 WASI 的成熟，以及 Kubernetes 更多地作为边缘编排工具使用，如前所述。一个场景是增强扩展机制，就像 Envoy 对过滤器和 LuaJIT 所做的那样。你可以与一个支持各种编程语言的更小的优化运行时协同，而不是直接与 Lua 打交道。Envoy 项目目前正处于 采用 Wasm 的过程中，我预计任何使用脚本语言作为流行扩展机制的环境都会出现被 Wasm 全盘取代的情况。\n在 Kubernetes 方面，有像微软的 Krustlet 这样的项目，正在探索如何在 Kubernetes 中支持基于 WASI 的运行时。这不应该太令人惊讶，因为 Kubernetes 已经在通过 CRD 和其他机制扩展，以运行不同类型的工作负载，如 VM（KubeVirt）等等。\n另外，如果你是 Wasm 的新手，我推荐 Linux 基金会的这本新的 入门课程，它对其进行了介绍，以及优选的文档。\nFinOps 的崛起（CFM）\n新冠病毒的爆发加速了向云原生的转变。至少有一半的公司在危机中加快了他们的云计划。近 60% 的受访者表示，由于 COVID-19 大流行，云计算的使用量将超过之前的计划 (2020 年云计算现状报告)。除此之外，云财务管理 (或 FinOps) 对许多公司来说是一个日益严重的问题和 关注，老实说，在过去 6 个月里，我与正在进行云原生之旅的公司进行的讨论中，大约有一半的讨论都会提到这个问题。你也可以说，云提供商没有动力让云财务管理变得更容易，因为这将使客户更容易减少支出，然而，在我看来，真正的痛苦是缺乏围绕云财务管理的开源创新和标准化（所有的云都以不同的方式进行成本管理）。在 CNCF 的背景下，试图让 FinOps 变得更容易的开源项目并不多，有 KubeCost 项目，但还相当早期。\n另外，Linux 基金会最近推出了 FinOps 基金会 来帮助这个领域的创新，他们在这个领域有一些 很棒的入门材料。我期望在未来几年，在 FinOps 领域能看到更多的开源项目和规范。\n云原生中更多的使用 Rust\nRust 仍然是一门年轻而小众的编程语言，特别是如果你以 Redmonk 的 编程语言排名 为例。然而，我的感觉是，鉴于已经有一些 使用 Rust 的 CNCF 项目，以及它出现在像 microvm Firecracker 这样有趣的基础设施项目中，你将在未来一年中看到 Rust 出现在更多的云原生项目中。虽然 CNCF 目前有超多的项目是用 Golang 编写的，但我预计随着 Rust 社区的成熟，几年后基于 Rust 的项目将与基于 Go 的项目平起平坐。\nGitOps+CD/PD 增长显著\nGitOps 是云原生技术的一种操作模式，提供了一套统一部署、管理和监控应用程序的最佳实践 (最初是由 Weaveworks 名气很大的 Alexis Richardson创造)。GitOps 最重要的方面是通过声明的方式描述所需的在 Git 中版本化的系统状态，这基本上可以使一系列复杂的系统变更被正确地应用，然后进行验证（通过 Git 和其他工具启用的漂亮的审计日志）。从实用的角度来看，GitOps 改善了开发者的体验，随着 Argo、GitLab、Flux 等项目的发展，我预计今年 GitOps 工具会更多地冲击企业。如果你看过 GitLab 的 数据，GitOps 还是一个大部分公司还没有探索出来的新兴的实践，但随着越来越多的公司大规模采用云原生软件，我认为 GitOps 自然会随之而来。如果你有兴趣了解更多关于这个领域的信息，我推荐你去看看 CNCF 中 新 成立的 GitOps 工作组。\n服务目录2.0：云原生开发者仪表盘\n服务目录的概念并不是一个新事物，对于我们一些在 ITIL 时代成长起来的老人们来说，你可能还记得 CMDBs （恐怖）等东西。然而，随着微服务和云原生开发的兴起，对服务进行编目和索引各种实时服务元数据的能力对于推动开发者自动化是至关重要的。这可以包括使用服务目录来了解所有权来处理事件管理、管理 SLO 等。\n在未来，你将看到开发人员仪表盘的趋势，它不仅是一个服务目录，而且提供了通过各种自动化功能在扩展仪表盘的能力。这方面的典范开源例子是 Lyft 的 Backstage 和 Clutch，然而，任何拥有相当现代的云原生部署的公司往往都有一个平台基础设施团队，他们已经尝试构建类似的东西。随着开源开发者仪表盘与 大型插件生态系统 的成熟，你会看到其被各地的平台工程团队加速采用。\n跨云变得更真实\nKubernetes 和云原生运动已经证明了云原生和多云方式在生产环境中是可行的，数据很清楚地表明“93% 的企业都有使用微软 Azure、亚马逊网络服务和谷歌云等多个提供商的策略” (2020 年云计算现状报告)。事实上，Kubernetes 这些年伴随着云市场的发展而更加成熟，将有望解锁程序化的跨云管理服务。这种方法的一个具体例子体现在 Crossplane 项目中，该项目提供了一个开源的跨云控制平面，利用 Kubernetes API 的可扩展性来实现跨云工作负载管理（参见 \u0026ldquo;GitLab 部署 Crossplane 控制平面，提供多云部署 \u0026ldquo;）。\n主流 eBPF\neBPF 允许你在不改变内核代码或加载模块的情况下，在 Linux 内核中运行程序，你可以把它看作是一种沙箱扩展机制。eBPF 允许 新一代软件 从改进的网络、监控和安全等各种不同的方向扩展 Linux 内核的行为。从历史上看，eBPF 的缺点是它需要一个现代的内核版本来利用它，在很长一段时间里，这对许多公司来说都不是一个现实的选择。然而，事情正在发生变化，甚至新版本的 RHEL 终于支持 eBPF，所以你会看到更多的项目利用其 [优势]（https://sysdig.com/blog/sysdig-and-falco-now-powered-by-ebpf/）。如果你看过 Sysdig 最新的 容器报告，你会发现 Falco 的采用率最近在上升，虽然 Sysdig 的报告可能有点偏颇，但它反映在生产使用上。所以请继续关注，并期待未来更多基于 eBPF 的项目。\n最后，祝大家 2021 年快乐！\n我还有一些预测和趋势要分享，尤其是围绕终端用户驱动的开源、服务网格拆解/标准化、Prometheus+OTel、保障软件供应链安全的 KYC 等等，但我会把这些留到更详细的文章中去，9 个预测足以开启新的一年！总之，感谢大家的阅读，希望在 2021 年 5 月的 KubeCon+CloudNativeCon EU 上与大家见面，报名已开始！\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/translation-cloud-native-predictions-for-2021-and-beyond/","tags":["云原生"],"title":"【译】2021 年及未来的云原生预测"},{"categories":["翻译"],"contents":"本文译自 Application architecture: why it should evolve with the market 最初由Mia Platform团队发布在Mia Platform的博客上\n如今，IT 挑战在于通过有效选择应用架构来适应市场和业务需求的发展。为了满足业务和客户的需求，IT 部门应能够对技术和方法采取行动以确保软件具有灵活性，并实现产品和服务的持续创新流程，从而做出更快的反应 。\n当然，过去的单体应用程序和刚性基础设施无法做到这一点。相反，它可以通过为演化而设计的架构来实现，该架构在需要时易于更新和重构。容器化实践的广泛应用（根据 Gartner，到2022年，大公司的就业人数将从目前的 30％ 增长到 75％），这种情况下采用云原生方法重新设计微服务应用是成功的关键。\n如何构建不断发展的应用架构 海外专家称它们为可演进的架构，以将它们与当今阻碍或无助于改变的传统架构区分开。应用架构基于微服务架构风格 ，被设计成在现代虚拟化 IT 和云环境中发挥最佳性能。\n基本思想是创建可以轻松“分解”的应用程序，其组件可以在其他上下文或组合中重用，如 Lego 系列。开发一系列微服务，每个微服务都用于执行单个业务功能（根据“单一职责原则”），可以在应用本身的开发和演进中获得相当大的灵活性。实际上，可以根据支持功能的特定生命周期独立开发、更新和测试服务。\n此外，谈到部署，微服务应用的架构具有很大的优势：可以根据需要在内部或云中通过使用可用资源来扩展单个微服务。\n为此，微服务应用获得基于容器的基础设施的支持，该基础设施通过业务编排系统（通常为 Kubernetes）进行管理，该流程可以自动化并促进公司系统之间以及从这些系统到云提供商服务的软件作业的迁移。\n随着业务发展的应用架构的优势 基于微服务的应用架构在开发和部署方面具有更大的自治权。如我们所见，微服务可以在其他应用程序中单独实现、“分解”、更新和重用。因此，通过产品或客户需求的演变，它有降低减少市场所需的每个新产品的设计/开发时间和成本。\n此外，通过使用容器化实践，可以简化在本地、云、多云或混合环境的任何环境中应用程序的部署，从而优化成本。\n在微服务架构风格的优点中，我们还发现有可能在各种服务之间的对话及其健康状况上获得更大的透明度：更好的可观察性意味着可以轻松解决复杂应用的问题。实际上，管理员可以更快地定位和解决性能和安全性问题，在运维和代码层面实施措施，从而使响应速度与变更的长期有效性保持一致。\n通过采用微服务以及新的开发和部署方法，可以创建能够随时间发展的应用架构。除了 IT 团队必须掌握的新技能外，还必须对公司的未来有一个清晰的愿景，以确保所提供的服务对业务发展有用。\n创建可演进的应用架构 我们已经看到了基于微服务的现代应用架构如何保证软件的灵活性，并允许你利用本地和按需使用的所有资源，在可以方便地获得所需性能、降低成本或保护数据的位置分配作业。\n为了使之成为可能，有必要在云和混合环境中创建和管理虚拟化的 IT 环境，并采用最合适的方法和策略。例如，在用于将开发和运维活动链接在一起的DevOps领域中，持续集成/持续交付（CI / CD）策略的方法学支持可帮助提高更新速度和应用软件的质量。\n此外，微服务可促进对遗留应用程序的集成，从而使公司更加敏捷，并利用市场上最先进的解决方案。除了需要新的技术和工作方法外，现在还需要可演进的应用架构来支持数字化转型所决定的不断变化的需求。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/translation-application-architecture-why-it-should-evolve-with-the-market/","tags":["云原生"],"title":"【译】应用架构：为什么要随着市场演进"},{"categories":["笔记"],"contents":"最近在看 openservicemesh 相关内容，这周更新了 main 分支的代码之后。发现原本 v0.5.0 时可以正常代理的 mysql 流量，在新的 commit 中无法代理了。\n开启 envoy 的 filter debug 日志后发现出现了超时。\n[2020-12-09 08:54:42.285][15][debug][filter] [source/extensions/filters/listener/original_dst/original_dst.cc:18] original_dst: New connection accepted [2020-12-09 08:54:42.285][15][debug][filter] [source/extensions/filters/listener/http_inspector/http_inspector.cc:38] http inspector: new connection accepted [2020-12-09 08:54:42.285][15][trace][filter] [source/extensions/filters/listener/http_inspector/http_inspector.cc:105] http inspector: recv: 0 [2020-12-09 08:54:57.286][15][debug][conn_handler] [source/server/connection_handler_impl.cc:273] listener filter times out after 15000 ms 贴一下 outbound listener 的配置片段:\n{ \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ListenersConfigDump\u0026#34;, \u0026#34;version_info\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;dynamic_listeners\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;outbound_listener\u0026#34;, \u0026#34;active_state\u0026#34;: { \u0026#34;version_info\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;listener\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.listener.v3.Listener\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;outbound_listener\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 15001 } }, \u0026#34;filter_chains\u0026#34;: [ //省略其他 filter chain { \u0026#34;filters\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.tcp_proxy\u0026#34;, \u0026#34;typed_config\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\u0026#34;, \u0026#34;stat_prefix\u0026#34;: \u0026#34;passthrough-outbound\u0026#34;, \u0026#34;cluster\u0026#34;: \u0026#34;passthrough-outbound\u0026#34; } }], \u0026#34;name\u0026#34;: \u0026#34;outbound-egress-filter-chain\u0026#34; }], \u0026#34;listener_filters\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.listener.original_dst\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.listener.http_inspector\u0026#34; } ], \u0026#34;traffic_direction\u0026#34;: \u0026#34;OUTBOUND\u0026#34; }, \u0026#34;last_updated\u0026#34;: \u0026#34;2020-12-08T10:07:41.191Z\u0026#34; } }] } 分析 Envoy 那边看到了有个 issue，为此增加了 timeout 的设置，默认 15s。\n listener_filters_timeout：默认 15s。等待 listener filter 完成的超时时间，如果设置为 0，则不超时。如果没有设置下面的配置为true，则会直接关闭连接。 continue_on_listener_filters_timeout：默认 false。超时时是否使用默认的 filter_chain 创建连接。  对比之前的版本，不同之处是在 filter 中多了 envoy.filters.listener.http_inspector。一个 http_inspector 为何 hold 住整个 follow。\n正好昨天配置了 Clion 用于源码阅读，看下 http_inspector 的源码：\n在http_inspector.cc的onRead方法，在做MSG_PEEK是铁定读不到数据的，见上面的日志，同时查看 stats http_inspector.read_error: 0。可见，返回了ParseState::Continue。\n Api::IoError::IoErrorCode::Again的意思是：No data available right now, try again later. 对于ParseState::Continue的解释是：Parser expects more data.\n 由于onRead返回了ParseState::Continue，onAccept方法会返回 Network::FilterStatus::StopIteration。返回之前会注册个针对Event::FileReadyType::Read | Event::FileReadyType::Closed 的 callback，用于读取到数据或者关闭文件时进行回调处理。\n Network::FilterStatus::StopIteration：Stop executing further filters.\n source/server/connection_handler_impl.cc 的 continueFilterChain方法，看注释：Blocking at the filter but no error。等待上面的 callback 被调用。\n由于前面直接从for loop 中跳出，迭代并没有执行到end，会启动一个超时的 timer。 参考：\n 配置 Clion 阅读 envoy 源码 allow fallback to default filter chain when listener filters timeout  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/envoy-listener-filter-times-out/","tags":["Envoy"],"title":"Envoy listener filter times out 问题"},{"categories":["源码解析"],"contents":"虽然不写 C++，但是看点代码还是能看懂。Envoy 的功能配置复杂，有时候处理问题还是需要看下源码的。\nVim 或者 Code 就算了，我只是阅读源码需要关联跳转就行。在 Clion 中，代码的关联跳转需要一个CMakeLists.txt 文件。\n我将生成的内容挂在了 gist 上了，不想花费时间生成的可以直接复制。\n准备环境 1. 安装依赖的工具 brew install coreutils wget cmake libtool go automake ninja clang-format 2. bazel 使用 homebrew 进行安装： brew install bazel 即可\n安装问运行编译测试下：bazel build //source/exe:envoy-static，提示版本太高。\n现在 homebrew 安装的版本是3.7.1，而 envoy 需要3.4.1。\n按照提示下载3.4.1的版本：cd \u0026quot;/usr/local/Cellar/bazel/3.7.1/libexec/bin\u0026quot; \u0026amp;\u0026amp; curl -fLO https://releases.bazel.build/3.4.1/release/bazel-3.4.1-darwin-x86_64 \u0026amp;\u0026amp; chmod +x bazel-3.4.1-darwin-x86_64，然后 进到目录里进行下替换 。\n注意：编译耗时很长，吃 cpu。\nINFO: Elapsed time: 1844.351s, Critical Path: 336.48s INFO: 4918 processes: 4918 darwin-sandbox. INFO: Build completed successfully, 6088 total actions 3. 生成CMakeLists.txt文件 这就是为什么上面需要安装编译工具了，clone git@github.com:lizan/bazel-cmakelists.git 这个仓库。\n只要bazel编译能通过，就可以使用这个工具生成文件了。\n在 envoy 源码根目录执行命令：/path_to_bazel-cmakelists/bazel-cmakelists //source/exe:envoy-static。\n执行结果：\nCMakeLists.txt generated in following directory: /Users/addo/Workspaces/github_w/envoyproxy/envoy 4. Clion 刷新 Clion 检测到 CMakeLists.txt 之后就会自动刷新 workspace，自此就可以在 Clion 中愉快的阅读源码了。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/read-envoy-source-code-in-clion/","tags":["Envoy"],"title":"使用 cLion 阅读 envoy 源码"},{"categories":["笔记"],"contents":"上篇扒了 HPA 的源码，但是没深入细节，今天往细节深入。\n开局先祭出一张图：\n为什么要有 Informer？ Kubernetes 中的持久化数据保存在 etcd中，各个组件并不会直接访问 etcd，而是通过 api-server暴露的 RESTful 接口对集群进行访问和控制。\n资源的控制器（图中右侧灰色的部分）读取数据也并不会直接从 api-server 中获取资源信息（这样会增加 api-server 的压力），而是从其“本地缓存”中读取。这个“本地缓存”只是表象的存在，加上缓存的同步逻辑就是今天要是说的Informer（灰色区域中的第一个蓝色块）所提供的功能。\n从图中可以看到 Informer 的几个组件：\n Reflector：与 api-server交互，监听资源的变更。 Delta FIFO Queue：增量的 FIFO 队列，保存 Reflector 监听到的资源变更（简单的封装）。 Indexer：Informer 的本地缓存，FIFO 队列中的数据根据不同的变更类型，在该缓存中进行操作。  Local Store：    上篇 提到了水平自动伸缩的控制器HorizontalController，其构造方法就需要提供 Informer。\n//pkg/controller/podautoscaler/horizontal.go  type HorizontalController struct { scaleNamespacer scaleclient.ScalesGetter hpaNamespacer autoscalingclient.HorizontalPodAutoscalersGetter mapper apimeta.RESTMapper replicaCalc *ReplicaCalculator eventRecorder record.EventRecorder downscaleStabilisationWindow time.Duration hpaLister autoscalinglisters.HorizontalPodAutoscalerLister hpaListerSynced cache.InformerSynced podLister corelisters.PodLister podListerSynced cache.InformerSynced queue workqueue.RateLimitingInterface recommendations map[string][]timestampedRecommendation } func NewHorizontalController( evtNamespacer v1core.EventsGetter, scaleNamespacer scaleclient.ScalesGetter, hpaNamespacer autoscalingclient.HorizontalPodAutoscalersGetter, mapper apimeta.RESTMapper, metricsClient metricsclient.MetricsClient, //从HorizontalPodAutoscalerInformer 获取hpa 实例信息 \thpaInformer autoscalinginformers.HorizontalPodAutoscalerInformer, //从PodInformer 中获取 pod 信息 \tpodInformer coreinformers.PodInformer, resyncPeriod time.Duration, downscaleStabilisationWindow time.Duration, tolerance float64, cpuInitializationPeriod, delayOfInitialReadinessStatus time.Duration, ) *HorizontalController { ...... hpaInformer.Informer().AddEventHandlerWithResyncPeriod( //添加事件处理器 \tcache.ResourceEventHandlerFuncs{ AddFunc: hpaController.enqueueHPA, UpdateFunc: hpaController.updateHPA, DeleteFunc: hpaController.deleteHPA, }, resyncPeriod, ) ...... } type HorizontalPodAutoscalerInformer interface { Informer() cache.SharedIndexInformer Lister() v1.HorizontalPodAutoscalerLister } HorizontalPodAutoscalerInformer的实例化方法中就出现了今天的正主cache.NewSharedIndexInformer()。\n//staging/src/k8s.io/client-go/informers/autoscaling/v1/horizontalpodautoscaler.go func NewFilteredHorizontalPodAutoscalerInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer { return cache.NewSharedIndexInformer( //用于 list 和 watch api-server 中的资源。比如用来创建 Reflector \t\u0026amp;cache.ListWatch{ ListFunc: func(options metav1.ListOptions) (runtime.Object, error) { if tweakListOptions != nil { tweakListOptions(\u0026amp;options) } //使用 HPA API 获取 HPA资源 \treturn client.AutoscalingV1().HorizontalPodAutoscalers(namespace).List(options) }, WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) { if tweakListOptions != nil { tweakListOptions(\u0026amp;options) } //使用 HPA API 监控 HPA资源 \treturn client.AutoscalingV1().HorizontalPodAutoscalers(namespace).Watch(options) }, }, \u0026amp;autoscalingv1.HorizontalPodAutoscaler{}, resyncPeriod, indexers, ) } 初始化 Informer //staging/src/k8s.io/client-go/tools/cache/index.go type Indexers map[string]IndexFunc type IndexFunc func(obj interface{}) ([]string, error) 实例化 Indexers cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}\n//staging/src/k8s.io/client-go/tools/cache/shared_informer.go // ListerWatcher 用于 list 和watch api-server 上的资源 //runtime.Object要监控的资源的运行时对象 //time.Duration同步的间隔时间 //Indexers 提供不同资源的索引数据的信息查询方法，如 namespace =\u0026gt; MetaNamespaceIndexFunc func NewSharedIndexInformer(lw ListerWatcher, objType runtime.Object, defaultEventHandlerResyncPeriod time.Duration, indexers Indexers) SharedIndexInformer { realClock := \u0026amp;clock.RealClock{} sharedIndexInformer := \u0026amp;sharedIndexInformer{ processor: \u0026amp;sharedProcessor{clock: realClock}, indexer: NewIndexer(DeletionHandlingMetaNamespaceKeyFunc, indexers), //初始化 Indexer \tlisterWatcher: lw, objectType: objType, resyncCheckPeriod: defaultEventHandlerResyncPeriod, defaultEventHandlerResyncPeriod: defaultEventHandlerResyncPeriod, cacheMutationDetector: NewCacheMutationDetector(fmt.Sprintf(\u0026#34;%T\u0026#34;, objType)), clock: realClock, } return sharedIndexInformer } Indexer Indexer提供了本地缓存的实现：计算 key 和对数据进行控制（通过调用ThreadSafeStore的接口）\ntype Indexer interface { Store Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexedValue string) ([]string, error) ListIndexFuncValues(indexName string) []string ByIndex(indexName, indexedValue string) ([]interface{}, error) GetIndexers() Indexers AddIndexers(newIndexers Indexers) error } Indexer 的创建\n//staging/src/k8s.io/client-go/tools/cache/store.go //keyFunc：key 的生成规则 //indexers：提供了索引资源的不同信息的访问方法，如用于查询命名空间的 namespace =\u0026gt; MetaNamespaceIndexFunc func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer { return \u0026amp;cache{ cacheStorage: NewThreadSafeStore(indexers, Indices{}), keyFunc: keyFunc, } } ThreadSafeStore ThreadSafeStore提供了对存储的并发访问接口\n注意事项：不能修改Get或List返回的任何内容，因为它不仅会破坏线程安全，还会破坏索引功能。\n//staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go func NewThreadSafeStore(indexers Indexers, indices Indices) ThreadSafeStore { return \u0026amp;threadSafeMap{ items: map[string]interface{}{}, indexers: indexers, indices: indices, } } type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} //key =\u0026gt; value \tindexers Indexers //value 的信息的访问方法 \tindices Indices //索引 } Reflector Reflector通过 ListerWatcher（API）与api-server交互，对资源进行监控。将资源实例的创建、更新、删除等时间封装后保存在Informer的FIFO 队列中。\n//staging/src/k8s.io/client-go/tools/cache/reflector.go func NewReflector(lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { return NewNamedReflector(naming.GetNameFromCallsite(internalPackages...), lw, expectedType, store, resyncPeriod) } // NewNamedReflector same as NewReflector, but with a specified name for logging func NewNamedReflector(name string, lw ListerWatcher, expectedType interface{}, store Store, resyncPeriod time.Duration) *Reflector { r := \u0026amp;Reflector{ name: name, listerWatcher: lw, store: store, //FIFO队列 \tperiod: time.Second, resyncPeriod: resyncPeriod, clock: \u0026amp;clock.RealClock{}, } r.setExpectedType(expectedType) return r } 添加同步事件监听器 通过sharedIndexInformer#AddEventHandlerWithResyncPeriod()注册事件监听器。\n以前面的 HorizontalController为例，创建 informer 的时候添加了三个处理方法：AddFunc、UpdateFunc、DeleteFunc。这三个方法的实现是将对应的元素的 key（固定格式 namespace/name）从 workequeue中进行入队、出队的操作。（资源控制器监听了该 workqueue）\n运行 controller-manager 在通过InformerFactory创建Informer完成后，都会将新建的 Informer加入到InformerFactory的一个map中。\n在controller-manager在完成所有的控制器（各种Controller，包括 CRD）后，会调用InformerFactory#Start()来启动InformerFactory的map中的所有 Informer（调用Informer#Run()方法）\nsharedIndexInformer#Run() //staging/src/k8s.io/client-go/tools/cache/shared_informer.go func (s *sharedIndexInformer) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() //创建一个增量的 FIFO队列：DeltaFIFO \tfifo := NewDeltaFIFO(MetaNamespaceKeyFunc, s.indexer) cfg := \u0026amp;Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, } //启动前的初始化，创建 Controller \tfunc() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop \tdefer close(processorStopCh) // Tell Processor to stop \twg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) //退出时的状态清理 \tdefer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don\u0026#39;t want any new listeners \t}() //实行控制逻辑 \ts.controller.Run(stopCh) } controller#Run() //staging/src/k8s.io/client-go/tools/cache/controller.go func (c *controller) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() go func() { \u0026lt;-stopCh c.config.Queue.Close() }() //创建一个 Reflector，用于从 api-server list 和 watch 资源 \tr := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.clock = c.clock //为 controller 指定 Reflector \tc.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group defer wg.Wait() //执行Reflector#Run()：会启动一个goroutine开始监控资源，将 watch 到的数据写入到queue（FIFO 队列）中 \twg.StartWithChannel(stopCh, r.Run) //持续从 queue（FIFO 队列） 获取数据并进行处理，处理的逻辑在sharedIndexInformer#HandleDeltas() \twait.Until(c.processLoop, time.Second, stopCh) } sharedIndexInformer#HandleDeltas() //staging/src/k8s.io/client-go/tools/cache/shared_informer.go func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest \tfor _, d := range obj.(Deltas) { //循环处理 FIFO 队列中取出的资源实例 \tswitch d.Type { case Sync, Added, Updated: //同步（后面详细解读）、新增、更新事件 \tisSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026amp;\u0026amp; exists { if err := s.indexer.Update(d.Object); err != nil { //如果 indexer 中已经存在，更掉用 update 方法进行更新 \treturn err } //更新成功后发送“更新”通知：包含了新、旧资源实例 \ts.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { //如果 indexer 中没有该资源实例，则放入 indexer 中 \tif err := s.indexer.Add(d.Object); err != nil { return err } //添加成功后，发送“新增”通知：包含了新加的资源实例 \ts.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: //删除事件 \tif err := s.indexer.Delete(d.Object); err != nil {//从 indexer 中删除 \treturn err } //删除成功后，发送“删除通知”：包含了删除的资源实例 \ts.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 总结 Informer 的实现不算复杂，却在 Kubernetes 中很常见，每种资源的控制也都通过 Informer 来获取api-server的资源实例的变更。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/kubernetes-source-code-how-informer-work/","tags":["Kubernetes","Go"],"title":"Kubernetes 源码解析 - Informer"},{"categories":["笔记"],"contents":"HPA - Horizontal Pod Autoscaler 的缩写，Pod 水平自动伸缩。通过对 Pod 负载的监控，来自动增加或者减少 Pod 的副本数量。\n从字面意思来看，其主要包含了两部分：\n 监控 Pod 的负载 控制 Pod 的副本数量  那具体是如何实现的呢？以下基于1.17 源码，来分析下 HPA 如何工作。\n注意：文章中的代码在源码的基础上进行了精简：删掉了注释、序列化等信息，或保留了部分核心代码，加上新的注释。\n资源 HPA 的资源是HorizontalPodAutoscaler，在v1版本中，只支持基于 CPU 指标的计算；在v2beta2版本中加入了基于内存和自定义指标的计算。\nv1 //staging/src/k8s.io/api/autoscaling/v1/types.go type HorizontalPodAutoscaler struct { metav1.TypeMeta metav1.ObjectMeta Spec HorizontalPodAutoscalerSpec Status HorizontalPodAutoscalerStatus } type HorizontalPodAutoscalerSpec struct { ScaleTargetRef CrossVersionObjectReference //监控的目标资源 \tMinReplicas *int32 //最小副本数 \tMaxReplicas int32 //最大副本数 \tTargetCPUUtilizationPercentage *int32 //触发调整的CPU 使用率 } v2 //staging/src/k8s.io/api/autoscaling/v2beta2/types.go type HorizontalPodAutoscaler struct { metav1.TypeMeta metav1.ObjectMeta Spec HorizontalPodAutoscalerSpec Status HorizontalPodAutoscalerStatus } type HorizontalPodAutoscalerSpec struct { ScaleTargetRef CrossVersionObjectReference //监控的目标资源 \tMinReplicas *int32 MaxReplicas int32 Metrics []MetricSpec //新加入的自定义指标 } type MetricSpec struct { Type MetricSourceType //指标源的类型：Object（基于某个对象）、Pods（基于pod 数）、Resource（基于资源使用计算，比如v1 版本中cpu）、External（基于外部的指标）。对应 MetricsClient 接口的四个方法 \tObject *ObjectMetricSource //对应 Object 类型的指标源 \tPods *PodsMetricSource //对应 Pod 类型的指标源 \tResource *ResourceMetricSource //对应 Resource 类型的指标源 \tExternal *ExternalMetricSource //对应 External 类型的指标源 } type ObjectMetricSource struct { DescribedObject CrossVersionObjectReference //目标对象 \tTarget MetricTarget //指定指标的目标值、平均值或者平均使用率 \tMetric MetricIdentifier //指标标识：名字、label选择器 } type PodsMetricSource struct { Metric MetricIdentifier Target MetricTarget } type ResourceMetricSource struct { Name v1.ResourceName Target MetricTarget } type ExternalMetricSource struct { Metric MetricIdentifier Target MetricTarget } type MetricTarget struct { Type MetricTargetType //类型：Utilization、Value、AverageValue \tValue *resource.Quantity AverageValue *resource.Quantity AverageUtilization *int32 } 控制器 HorizontalController HorizontalController被通过 key horizontalpodautoscaling 加入到 controller manager 中。用来控制HorizontalPodAutoscaler实例。\n///cmd/kube-controller-manager/app/controllermanager.go func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc { ... controllers[\u0026#34;horizontalpodautoscaling\u0026#34;] = startHPAController ... } 获取负载指标 既然 Pod 副本数量的计算是基于 Pod 的负载情况，那边需要途径获取负载数据，这个途径就是MetricsClient。\nMetricsClient有两种实现：REST 方式和传统（Legacy）方式，分别是restMetricsClient和HeapsterMetricsClient。一个是REST 实现以支持自定义的指标；一个是传统的 Heapster 指标（heapster 已经从 1.13 版本开始被废弃了）。\n//cmd/kube-controller-manager/app/autoscaling.go func startHPAController(ctx ControllerContext) (http.Handler, bool, error) { if !ctx.AvailableResources[schema.GroupVersionResource{Group: \u0026#34;autoscaling\u0026#34;, Version: \u0026#34;v1\u0026#34;, Resource: \u0026#34;horizontalpodautoscalers\u0026#34;}] { return nil, false, nil } if ctx.ComponentConfig.HPAController.HorizontalPodAutoscalerUseRESTClients { // use the new-style clients if support for custom metrics is enabled \treturn startHPAControllerWithRESTClient(ctx) } return startHPAControllerWithLegacyClient(ctx) } 控制器逻辑HorizontalController#Run() //pkg/controller/podautoscaler/horizontal.go func (a *HorizontalController) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() defer a.queue.ShutDown() klog.Infof(\u0026#34;Starting HPA controller\u0026#34;) defer klog.Infof(\u0026#34;Shutting down HPA controller\u0026#34;) // 等待 informer 完成HorizontalPodAutoscaler相关事件的同步 \tif !cache.WaitForNamedCacheSync(\u0026#34;HPA\u0026#34;, stopCh, a.hpaListerSynced, a.podListerSynced) { return } // start a single worker (we may wish to start more in the future) \t//执行 worker 逻辑，直到收到退出指令 \tgo wait.Until(a.worker, time.Second, stopCh) \u0026lt;-stopCh } worker的核心是从工作队列中获取一个 key（格式为：namespace/name），然后对 key 进行 reconcile（这个词是Kubernetes 的核心，翻译为“调和”、“和解”。个人更喜欢“调整”，即将实例的状态调整为期望的状态。此处，对于 hpa 的实例的每个事件，都会按照特定的逻辑调整目标实例的 Pod 的副本数量。）。\n//pkg/controller/podautoscaler/horizontal.go func (a *HorizontalController) worker() { for a.processNextWorkItem() { } klog.Infof(\u0026#34;horizontal pod autoscaler controller worker shutting down\u0026#34;) } func (a *HorizontalController) processNextWorkItem() bool { key, quit := a.queue.Get() if quit { return false } defer a.queue.Done(key) deleted, err := a.reconcileKey(key.(string)) if err != nil { utilruntime.HandleError(err) } if !deleted { a.queue.AddRateLimited(key) } return true } 对 key 进行 reconcile 的调用栈：HorizontalController#reconcileKey -\u0026gt; HorizontalController#reconcileAutoscaler -\u0026gt; HorizontalController#computeReplicasForMetrics -\u0026gt; ScaleInterface#Update\n简单来说就是先从Informer中拿到 key 对应的HorizontalPodAutoscaler资源实例；然后通过HorizontalPodAutoscaler实例中的信息，检查目标资源的Pod 负载以及当前的副本数，得到期望的 Pod 副本数；最终通过 Scale API 来调整 Pod 的副本数。最后会将调整的原因、计算的结果等信息写入HorizontalPodAutoscaler实例的 condition 中。\n计算期望的副本数 对每个指标进行计算，都会得到建议的副本数，然后最大的那个就是最终的期望副本数。\n//pkg/controller/podautoscaler/horizontal.go func (a *HorizontalController) computeReplicasForMetrics(hpa *autoscalingv2.HorizontalPodAutoscaler, scale *autoscalingv1.Scale, metricSpecs []autoscalingv2.MetricSpec) (replicas int32, metric string, statuses []autoscalingv2.MetricStatus, timestamp time.Time, err error) { ...... for i, metricSpec := range metricSpecs { replicaCountProposal, metricNameProposal, timestampProposal, condition, err := a.computeReplicasForMetric(hpa, metricSpec, specReplicas, statusReplicas, selector, \u0026amp;statuses[i]) if err != nil { if invalidMetricsCount \u0026lt;= 0 { invalidMetricCondition = condition invalidMetricError = err } invalidMetricsCount++ } if err == nil \u0026amp;\u0026amp; (replicas == 0 || replicaCountProposal \u0026gt; replicas) { timestamp = timestampProposal replicas = replicaCountProposal metric = metricNameProposal } } ...... } #computeStatusForObjectMetric（注意这个方法名少了个 \u0026ldquo;s\u0026rdquo;）使用MetricsClient得到指定指标的值。\n这个流程的细节还可以继续深挖，但到此已够我们理解 HPA​ 的实现方式了。​\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/kubernetes-source-code-how-hpa-work/","tags":["Kubernetes","Go","DevOps"],"title":"Kubernetes 源码解析 - HPA 水平自动伸缩如何工作"},{"categories":["笔记"],"contents":"Spring Cloud 中 Ribbon有在 Zuul 和 Feign 中使用，当然也可以通过在RestTemplate的 bean 定义上添加@LoadBalanced注解方式获得一个带有负载均衡更能的RestTemplate。\n不过实现的方法都大同小异：对HttpClient进行封装，加上实例的”选择“（这个选择的逻辑就是我们所说的负载均衡）。\n要学习某个框架的时候，最简单的方案就是：Running+Debugging。\n跑就是了。\n debug 不一定是为了 bug\ndebug 出真知\nDebugging = Learning\n 选用 Ali Spittel 的一条推文：\n以 Zuul 路由的线程栈为例 调整下顺序：\nRetryableRibbonLoadBalancingHttpClient#execute(RibbonApacheHttpRequest, IClientConfig) RetryableRibbonLoadBalancingHttpClient#executeWithRetry(...) RetryTemplate#execute(RetryCallback\u0026lt;T, E\u0026gt;, RecoveryCallback\u0026lt;T\u0026gt;) RetryTemplate#doExecute(RetryCallback\u0026lt;T, E\u0026gt;, RecoveryCallback\u0026lt;T\u0026gt;, RetryState) RetryTemplate#canRetry(RetryPolicy, RetryContext) InterceptorRetryPolicy#canRetry(RetryContext) AbstractLoadBalancingClient#choose(String serviceId) ZoneAwareLoadBalancer#chooseServer(Object key) //key as serviceId BaseLoadBalancer#chooseServer(Object key) PredicateBasedRule#choose(Object key) AbstractServerPredicate#chooseRoundRobinAfterFiltering(List\u0026lt;Server\u0026gt; servers, Object loadBalancerKey) AbstractServerPredicate#apply(Predicate) 分析 Zuul 收到请求经过一系列 Filter 的处理，来到 RibbonRoutingFilter；将请求封装成 RibbonCommandContext，然后使用 context 构建 RibbonCommand。最终调用RibbonCommand#execute()方法，将请求路由到下游。\nRibbonCommand持有AbstractLoadBalancerAwareClient的对象，通过该 client 在处理请求和响应。\n对于 retryable 的 client（比如此处的RetryableRibbonLoadBalancingHttpClient）， 每次处理请求的时候都会创建一个 RetryTemplate对象来处理请求；同时根据RetryPolicy来创建RetryContext对象，用来保存重试的上下文，并 检查实例是否可以进行重试 。\n注意重点就在这里：检查的时候如果重试次数为 0 且要检查的实例为空（说明是第一次请求），这时便会通过负载均衡器客户端（基本都是AbstractLoadBalancingClient的子类）从后端列表择出一个实例，保存在RetryContext中。\n负载均衡器客户端使用负载均衡器（ILoadBalancer的实现）来选择实例。每个负载均衡器都有自己的规则（IRule的实现类），通过规则来选择实例。\nIRule的实现不是很多，\n其中的ClientConfigEnabledRoundRobinRule在RoundRobinRule的基础上，增加了配置的接口（因为其实现了IClientConfigAware接口）可以对规则进行配置。\n某些ClientConfigEnabledRoundRobinRule的子类了，增加了Predicate逻辑：使用Predicate（AbstractServerPredicate的子类）的逻辑进行选择；而ClientConfigEnabledRoundRobinRule只是简单的使用RoundRobinRule进行选择。\n因此选择的逻辑都是在AbstractServerPredicate子类中，其有个特别的子类CompositePredicate，顾名思义就是将多个逻辑整合在一起（使用Predicate#and()将所有逻辑串联起来，达到\u0026amp;\u0026amp;的效果），所有的逻辑检查都通过（返回true）时，这个实例就会被选中。\n 那么现在要你写个自己负载均衡规则，应该知道从哪里入手了吧？:D\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/how-loadbalancer-works-in-ribbon/","tags":["Spring","Java"],"title":"带你了解 Ribbon 负载均衡器的实现"},{"categories":["笔记"],"contents":"这是真实发生在生产环境的 case，实例启动后正常运行，而在注册中心的状态一直保持STARTING，而本地的状态为UP。导致服务的消费方无法发现可用实例。\n这种情况的出现概率非常低，运行一年多未发现两个实例同时出现问题的情况，因此多实例运行可以避免。文末有问题的解决方案，不想花时间看分析过程可直接跳到最后。\n环境说明：\n eureka-client: 1.7.2 spring-boot: 1.5.12.RELEASE spring-cloud: Edgware.SR3\n 问题重现 借助Btrace重现, java -noverify -cp .:btrace-boot.jar -javaagent:btrace-agent.jar=script=\u0026lt;pre-compiled-btrace-script\u0026gt; \u0026lt;MainClass\u0026gt; \u0026lt;AppArguments\u0026gt;\n思路 主线程更新实例本地状态(STARTING-\u0026gt;UP)前, 等待心跳线程完成第一次心跳并尝试注册实例, 获取到当前的状态STARTING. 主线程更新状态后触发\nBtrace 脚本\nimport com.sun.btrace.annotations.BTrace; import com.sun.btrace.annotations.Kind; import com.sun.btrace.annotations.Location; import com.sun.btrace.annotations.OnMethod; import java.util.concurrent.atomic.AtomicBoolean; import static com.sun.btrace.BTraceUtils.currentThread; import static com.sun.btrace.BTraceUtils.println; /** * @author Addo.Zhang * @date 2019-07-31 */ @BTrace(unsafe = true) public class EurekaRequest { public static final AtomicBoolean heartbeatThreadRegistrationStarted = new AtomicBoolean(false); public static final AtomicBoolean replicatorThreadRegistrationCompleted = new AtomicBoolean(false); public static final AtomicBoolean statusUP = new AtomicBoolean(false); @OnMethod(clazz = \u0026#34;org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry\u0026#34;, location = @Location(value = Kind.LINE, line = 45)) public static void waitHeartbeatExecution() { println(currentThread() + \u0026#34; is waiting heartbeatThreadRegistrationStarted thread executing first\u0026#34;); while (!heartbeatThreadRegistrationStarted.get()) ; } @OnMethod(clazz = \u0026#34;org.springframework.cloud.netflix.eureka.serviceregistry.EurekaServiceRegistry\u0026#34;, location = @Location(value = Kind.LINE, line = 46)) public static void markStatusUp() { statusUP.set(true); println(\u0026#34;Heartbeat thread executed and \u0026#34; + currentThread() + \u0026#34; continues procedure to change status to [UP]\u0026#34;); } @OnMethod(clazz = \u0026#34;com.netflix.discovery.converters.EurekaJacksonCodec$InstanceInfoSerializer\u0026#34;, location = @Location(value = Kind.LINE, line = 369)) public static void continueRegistrationExecution() { doExecution(); } @OnMethod(clazz = \u0026#34;com.logancloud.forge.discovery.converters.LoganEurekaJacksonCodec$LoganInstanceInfoSerializer\u0026#34;, location = @Location(value = Kind.LINE, line = 117)) public static void continueRegistrationExecution2() { doExecution(); } private static void doExecution() { println(currentThread() + \u0026#34; started to proceed registration\u0026#34;); if (Thread.currentThread().getName().contains(\u0026#34;HeartbeatExecutor\u0026#34;)) { heartbeatThreadRegistrationStarted.set(true); while (!statusUP.get() || !replicatorThreadRegistrationCompleted.get()) { } try { Thread.sleep(500); //interval for replicator registration request completed.  } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } else if (Thread.currentThread().getName().contains(\u0026#34;InstanceInfoReplicator\u0026#34;)) { replicatorThreadRegistrationCompleted.set(true); } println(currentThread() + \u0026#34; thread registration completed\u0026#34;); } }  心跳线程HeartbeatThread发送心跳请求(PUT), 注册中心返回404. 实例信息同步线程InstanceInfoReplicator发送注册请求(POST): 状态为UP, lastDirtyTimestamp为a 心跳线程发送实例注册请求(POST): 状态为STARTING, lastDirtyTimestamp为a  服务注册 先分析服务实例的注册逻辑.\nInstanceInfo初始化 通过InstanceInfoFactory#create()方法来初始化ApplicationInfoManager.instanceInfo实例时, 实例状态被设置为STARTING\n服务实例注册 服务实例注册的真正逻辑是在DiscoveryClient#register()中完成的. 但是这个方法的调用却有两个入口, 在整个过程中可解释为主动注册和被动注册.\n一. 主动注册 EurekaAutoServiceRegistration实现了SmartLifecycle接口, 在EurekaClientAutoConfiguration#eurekaAutoServiceRegistration()被实例化.\nEurekaAutoServiceRegistration#start()方法将EurekaRegistration注册给EurekaServiceRegistry:\npublic void start() { ... if (!this.running.get() \u0026amp;\u0026amp; this.registration.getNonSecurePort() \u0026gt; 0) { //调用EurekaServiceRegistry进行注册  this.serviceRegistry.register(this.registration); //发布实例注册的事件  this.context.publishEvent( new InstanceRegisteredEvent\u0026lt;\u0026gt;(this, this.registration.getInstanceConfig())); this.running.set(true); } }  EurekaServiceRegistry#register(): 先将实例状态设置为初始状态UP(可通过eureka.instance.initial-status修改, 默认为UP). 这里会触发StatusChangeListener#notify().\npublic void register(EurekaRegistration reg) { maybeInitializeClient(reg); if (log.isInfoEnabled()) { log.info(\u0026#34;Registering application \u0026#34; + reg.getInstanceConfig().getAppname() + \u0026#34; with eureka with status \u0026#34; + reg.getInstanceConfig().getInitialStatus()); } //1 \treg.getApplicationInfoManager() .setInstanceStatus(reg.getInstanceConfig().getInitialStatus()); if (reg.getHealthCheckHandler() != null) { //2 \treg.getEurekaClient().registerHealthCheck(reg.getHealthCheckHandler()); } } DiscoveryClient内部匿名类提供了StatusChangeListener的实现, 调用InstanceInfoReplicator#onDemandUpdate()\nstatusChangeListener = new ApplicationInfoManager.StatusChangeListener() { @Override public String getId() { return \u0026#34;statusChangeListener\u0026#34;; } @Override public void notify(StatusChangeEvent statusChangeEvent) { if (InstanceStatus.DOWN == statusChangeEvent.getStatus() || InstanceStatus.DOWN == statusChangeEvent.getPreviousStatus()) { // log at warn level if DOWN was involved  logger.warn(\u0026#34;Saw local status change event {}\u0026#34;, statusChangeEvent); } else { logger.info(\u0026#34;Saw local status change event {}\u0026#34;, statusChangeEvent); } instanceInfoReplicator.onDemandUpdate(); } }; InstanceInfoReplicator是在DiscoveryClient#initScheduledTasks()中实例化的Runnable的实现, 实例化之后, 使用其内部的调度线程池调度一个线程. 而onDemandUpdate()也同样会使用调度线程池调度一个线程.\n其#run()方法会调用DiscoveryClient#refreshInstanceInfo()来更新状态. 状态的更新是通过HealthCheckHandler来实现的, 具体请看状态检查. 然后调用DiscoveryClient#register()方法进行注册.\n二. 被动注册 上面提到了DiscoveryClient#initScheduledTasks(), 这里的task除了InstanceInfoReplicator之外还有其他的线程. 其中一个是线条线程HeartbeatThread. 这个线程会每隔一段时间向注册中心发送一个PUT类型的HTTP请求: 上报实例的状态(状态(status), 以及状态修改的时间(lastDirtyTimestamp)).\n这个请求可能会有两种结果: 404和200. 前者说明注册中心中还没有这个实例的注册信息; 后者说明状态上报成功.\n假如是404, 便直接发起注册的动作, 即调用DiscoveryClient#register()方法进行注册.\n状态检查 CloudEurekaClient通过HealthCheckHandler来检查实例的健康状态, 看下HealthCheckCallbackToHandlerBridge实现: callback为空, 或者当前状态为STARTING或者OUT_OF_SERVICE时, 返回当前的状态. 我们没有设置callback, 故而总是会返回当前的状态. 比如应用启动的初始状态为STARTING\npublic InstanceInfo.InstanceStatus getStatus(InstanceInfo.InstanceStatus currentStatus) { if (null == callback || InstanceInfo.InstanceStatus.STARTING == currentStatus || InstanceInfo.InstanceStatus.OUT_OF_SERVICE == currentStatus) { // Do not go to healthcheck handler if the status is starting or OOS.  return currentStatus; } return callback.isHealthy() ? InstanceInfo.InstanceStatus.UP : InstanceInfo.InstanceStatus.DOWN; } 问题分析 现象 TCP抓包 HeartBeat请求和Fetch请求正常. status=UP\u0026amp;lastDirtyTimestamp=1545039481813\n堆信息 本地状态为UP, lastDirtyTimestamp为1545039481813, lastUpdatedTimestamp为1545039472888\n注册中心里的实例信息 状态为STARTING, lastDirtyTimestamp为1545039481813, registrationTimestamp为1545039481898, lastUpdatedTimestamp为1545039481899\n\u0026lt;instance\u0026gt; \u0026lt;instanceId\u0026gt;xp-xtower-webapp-boot-6-txcxb:xp-xtower-webapp-boot:10100\u0026lt;/instanceId\u0026gt; \u0026lt;hostName\u0026gt;10.128.41.74\u0026lt;/hostName\u0026gt; \u0026lt;app\u0026gt;XP-XTOWER-WEBAPP-BOOT\u0026lt;/app\u0026gt; \u0026lt;ipAddr\u0026gt;10.128.41.74\u0026lt;/ipAddr\u0026gt; \u0026lt;status\u0026gt;STARTING\u0026lt;/status\u0026gt; \u0026lt;overriddenstatus\u0026gt;UNKNOWN\u0026lt;/overriddenstatus\u0026gt; \u0026lt;port enabled=\u0026#34;true\u0026#34;\u0026gt;10100\u0026lt;/port\u0026gt; \u0026lt;securePort enabled=\u0026#34;false\u0026#34;\u0026gt;443\u0026lt;/securePort\u0026gt; \u0026lt;countryId\u0026gt;1\u0026lt;/countryId\u0026gt; \u0026lt;dataCenterInfo class=\u0026#34;com.netflix.appinfo.InstanceInfo$DefaultDataCenterInfo\u0026#34;\u0026gt; \u0026lt;name\u0026gt;MyOwn\u0026lt;/name\u0026gt; \u0026lt;/dataCenterInfo\u0026gt; \u0026lt;leaseInfo\u0026gt; \u0026lt;renewalIntervalInSecs\u0026gt;5\u0026lt;/renewalIntervalInSecs\u0026gt; \u0026lt;durationInSecs\u0026gt;20\u0026lt;/durationInSecs\u0026gt; \u0026lt;registrationTimestamp\u0026gt;1545039481898\u0026lt;/registrationTimestamp\u0026gt; \u0026lt;lastRenewalTimestamp\u0026gt;1545950719063\u0026lt;/lastRenewalTimestamp\u0026gt; \u0026lt;evictionTimestamp\u0026gt;0\u0026lt;/evictionTimestamp\u0026gt; \u0026lt;serviceUpTimestamp\u0026gt;0\u0026lt;/serviceUpTimestamp\u0026gt; \u0026lt;/leaseInfo\u0026gt; \u0026lt;metadata\u0026gt; \u0026lt;forge\u0026gt;1.0.0\u0026lt;/forge\u0026gt; \u0026lt;management.port\u0026gt;10100\u0026lt;/management.port\u0026gt; \u0026lt;jmx.port\u0026gt;1099\u0026lt;/jmx.port\u0026gt; \u0026lt;group\u0026gt;innovation\u0026lt;/group\u0026gt; \u0026lt;/metadata\u0026gt; \u0026lt;homePageUrl\u0026gt;http://10.128.41.74:10100/\u0026lt;/homePageUrl\u0026gt; \u0026lt;statusPageUrl\u0026gt;\u0026lt;/statusPageUrl\u0026gt; \u0026lt;healthCheckUrl\u0026gt;http://10.128.41.74:10100/health\u0026lt;/healthCheckUrl\u0026gt; \u0026lt;vipAddress\u0026gt;xp-xtower-webapp-boot\u0026lt;/vipAddress\u0026gt; \u0026lt;secureVipAddress\u0026gt;xp-xtower-webapp-boot\u0026lt;/secureVipAddress\u0026gt; \u0026lt;isCoordinatingDiscoveryServer\u0026gt;false\u0026lt;/isCoordinatingDiscoveryServer\u0026gt; \u0026lt;lastUpdatedTimestamp\u0026gt;1545039481899\u0026lt;/lastUpdatedTimestamp\u0026gt; \u0026lt;lastDirtyTimestamp\u0026gt;1545039481813\u0026lt;/lastDirtyTimestamp\u0026gt; \u0026lt;actionType\u0026gt;ADDED\u0026lt;/actionType\u0026gt; \u0026lt;/instance\u0026gt;    Scope Status lastDirtyTimestamp lastUpdatedTimestamp registrationTimestamp     Request UP 1545039481813     Local UP 1545039481813 1545039472888    Remote STARTING 1545039481813 1545039481899 1545039481898    结合起来看, 问题出在lastDirtyTimestamp未更新, 导致注册中心的状态未更新. 而lastUpdatedTimestamp的时间为1545039481899, 与lastDirtyTimestamp相差86毫秒.\n服务端InstanceResource#validateDirtyTimestamp()根据本地保存的实例的信息, 和心跳请求发送过来的请求做比较, 决定响应的状态码200, 404或者409\n推理 注册中心里实例的状态为STARTING, 可以确定实例是[被动注册](#二. 被动注册)的.\n这里有几个时间点:\n 1545039472888: InstanceInfo对象实例化的时间, 因为本地对象的#lastUpdatedTimestamp字段只有在实例化才会赋值, 此后不会被修改. 见堆信息 1545039481813: 状态从STARTING变为UP的时间, 也是实例状态的最后一次更新时间. 此后的心跳请求都会带上实例的最新状态(UP)和状态的最后一次更新时间(1545039481813), 见TCP抓包. 1545039481898: 注册中心收到实例的注册请求的时间. 见注册中心里的实例信息 1545039481899: 注册中心中的实例信息被更新的时间. 这个时间只比注册的时间晚了1毫秒. 见注册中心里的实例信息  综上可见, 被动注册时发送请求, 拿到的实例的旧的状态STARTING, 修改时间确实最新的1545039481813. 后续的心跳上报实例状态为最新的UP, 修改时间也是最新的1545039481813. 但是由于最后修改时间与注册时的最后修改时间相同, 即使状态已经变为UP, 注册中心在收到心跳请求之后也不会将状态更新为UP.\n服务端InstanceResource#renewLease() -\u0026gt; InstanceResource#validateDirtyTimestamp(): 如果请求中的lastDirtyTimestamp与当前保存的实例的相同, 则直接返回OK, 不会更新注册中心中保存的实例的状态.\nprivate Response validateDirtyTimestamp(Long lastDirtyTimestamp, boolean isReplication) { InstanceInfo appInfo = registry.getInstanceByAppAndId(app.getName(), id, false); if (appInfo != null) { if ((lastDirtyTimestamp != null) \u0026amp;\u0026amp; (!lastDirtyTimestamp.equals(appInfo.getLastDirtyTimestamp()))) { Object[] args = {id, appInfo.getLastDirtyTimestamp(), lastDirtyTimestamp, isReplication}; if (lastDirtyTimestamp \u0026gt; appInfo.getLastDirtyTimestamp()) { logger.debug( \u0026#34;Time to sync, since the last dirty timestamp differs -\u0026#34; + \u0026#34; ReplicationInstance id : {},Registry : {} Incoming: {} Replication: {}\u0026#34;, args); return Response.status(Status.NOT_FOUND).build(); } else if (appInfo.getLastDirtyTimestamp() \u0026gt; lastDirtyTimestamp) { // In the case of replication, send the current instance info in the registry for the  // replicating node to sync itself with this one.  if (isReplication) { logger.debug( \u0026#34;Time to sync, since the last dirty timestamp differs -\u0026#34; + \u0026#34; ReplicationInstance id : {},Registry : {} Incoming: {} Replication: {}\u0026#34;, args); return Response.status(Status.CONFLICT).entity(appInfo).build(); } else { return Response.ok().build(); } } } } return Response.ok().build(); } 为什么会出现这种情况? 应用启动过程中会有两个线程会触发注册的动作\n InstanceInfoReplicator线程: DiscoveryClient中的ApplicationInfoManager.StatusChangeListener监听到实例状态发生变化, 会新建一个线程将实例注册到注册中心 DiscoveryClient$HeartbeatThread线程: 这个线程在DiscoveryClient实例初始化后延迟(与心跳间隔时间相同, 默认是30s, 中台为提高实例发现效率将其改为了5s)启动运行. 第一次发送心跳请求是如果注册中心返回404(说明心跳线程提实例状态更新线程先启动), 则会先将实例注册到注册中心.  上面两个线程都通过调用AbstractJerseyEurekaHttpClient$register()方法并使用EurekaJacksonCodec$InstanceInfoSerializer将实例信息序列化. 序列化的过程中先记录实例的状态后记录实例状态的最后修改时间(lastDirtyTimestamp), 这两个操作不是一个原子操作.\n非常极端的情况下(缩小心跳间隔增加了出现的概率, 但依然极地), 两个操作之间(心跳线程先拿到实例状态STARTING)主线程修改了实例状态为UP, 同时修改了lastDirtyTimestamp, 并触发了InstanceInfoReplicator线程的注册操作, 此时心跳线程获取到的实例的最后修改时间与STARTING状态并不一致. 之后同样注册动作覆盖了实例在注册中心的状态: UP -\u0026gt; STARTING.\n后续的心跳请求带去的最新状态UP和lastDirtyTimestamp, 并不会更新在注册中心的状态.\n解决方案 在EurekaJacksonCodec$InstanceInfoSerializer#serialize()方法中, 将#autoMarshalEligible() 的调用移到jgen.writeStartObject()后面. 这样就使得lastDirtyTimestamp的获取比status早, 就能保证即使注册时的lastDirtyTimestamp小于真正的, 但是状态是与实际相符. lastDirtyTimestamp会在后续的心跳请求中更新. addInstance-info.getStatus(): UP addInstance-info.getLastDirtyTimestamp(): 1565164484429 addInstance-info.getStatus(): STARTING addInstance-info.getLastDirtyTimestamp(): 1565164484415 renew-status-in-registry: UP renew-lastDirtyTimestamp: 1565164484429 renew-appInfo.getLastDirtyTimestamp(): 1565164484429 PR 已经提交并合并完成，然而 1.7.x 的版本不知何时会发布修复版本\n参考:\n GitHub issue PR  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/troubleshooting-on-eureka-instance-keep-starting/","tags":["Spring","Troubleshooting"],"title":"Eureka 实例注册状态保持 STARTING 的问题排查"},{"categories":["笔记"],"contents":"这篇文章是基于 Tekton Pipeline 的最新版本v0.12.1版本。\n快速入门请参考：云原生 CICD: Tekton Pipeline 实战 ，实战是基于版本 v0.10.x。\nPipeline CRD 与核心资源的关系 $ k api-resources --api-group=tekton.dev NAME SHORTNAMES APIGROUP NAMESPACED KIND clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task Tekton Pipelines提供了上面的CRD，其中部分CRD与k8s core中资源相对应\n Task =\u0026gt; Pod Task.Step =\u0026gt; Container  工作原理 (图片来自)\nTekton Pipeline 是基于 Knative 的实现，pod tekton-pipelines-controller 中有两个 Knative Controller的实现：PipelineRun 和 TaskRun。\nTask的执行顺序 PipelineRun Controller 的 #reconcile()方法，监控到有PipelineRun被创建。然后从PipelineSpec的 tasks 列表，构建出一个图（graph），用于描述Pipeline中 Task 间的依赖关系。依赖关系是通过runAfter和from，进而控制Task的执行顺序。与此同时，准备PipelineRun中定义的PipelineResources。\n// Node represents a Task in a pipeline. type Node struct { // Task represent the PipelineTask in Pipeline \tTask Task // Prev represent all the Previous task Nodes for the current Task \tPrev []*Node // Next represent all the Next task Nodes for the current Task \tNext []*Node } // Graph represents the Pipeline Graph type Graph struct { //Nodes represent map of PipelineTask name to Node in Pipeline Graph \tNodes map[string]*Node } func Build(tasks Tasks) (*Graph, error) { ... } PipelineRun中定义的参数（parameters）也会注入到PipelineSpec中：\npipelineSpec = resources.ApplyParameters(pipelineSpec, pr) 接下来就是调用dag#GetSchedulable()方法，获取未完成（通过Task状态判断）的 Task 列表；\nfunc GetSchedulable(g *Graph, doneTasks ...string) (map[string]struct{}, error) { ... } 为 Task A 创建TaskRun，假如Task配置了Condition。会先为 condition创建一个TaskRun，只有在 condition 的TaskRun运行成功，才会运行 A 的TaskRun；否则就跳过。\nStep的执行顺序 这一部分篇幅较长，之前的文章 控制 Pod 内容器的启动顺序 中提到过。\n这里补充一下Kubernetes Downward API的使用，Kubernetes Downward API的引入，控制着 Task 的第一个 Step 在何时执行。\nTaskRun Controller 在 reconciling 的过程中，在相应的 Pod 状态变为Running时，会将tekton.dev/ready=READY写入到 Pod 的 annotation 中，来通知第一个Step的执行。\nPod的部分内容：\nspec: containers: - args: - -wait_file - /tekton/downward/ready - -wait_file_content - -post_file - /tekton/tools/0 - -termination_path - /tekton/termination - -entrypoint - /ko-app/git-init - -- - -url - ssh://git@gitlab.nip.io:8022/addozhang/logan-pulse.git - -revision - develop - -path - /workspace/git-source command: - /tekton/tools/entrypoint volumeMounts: - mountPath: /tekton/downward name: tekton-internal-downward volumes: - downwardAPI: defaultMode: 420 items: - fieldRef: apiVersion: v1 fieldPath: metadata.annotations[\u0026#39;tekton.dev/ready\u0026#39;] path: ready name: tekton-internal-downward 对原生的排序step container进一步处理：启动命令使用entrypoint提供，并设置执行参数：\nentrypoint.go\nfunc orderContainers(entrypointImage string, steps []corev1.Container, results []v1alpha1.TaskResult) (corev1.Container, []corev1.Container, error) { initContainer := corev1.Container{ Name: \u0026#34;place-tools\u0026#34;, Image: entrypointImage, Command: []string{\u0026#34;cp\u0026#34;, \u0026#34;/ko-app/entrypoint\u0026#34;, entrypointBinary}, VolumeMounts: []corev1.VolumeMount{toolsMount}, } if len(steps) == 0 { return corev1.Container{}, nil, errors.New(\u0026#34;No steps specified\u0026#34;) } for i, s := range steps { var argsForEntrypoint []string switch i { case 0: argsForEntrypoint = []string{ // First step waits for the Downward volume file. \t\u0026#34;-wait_file\u0026#34;, filepath.Join(downwardMountPoint, downwardMountReadyFile), \u0026#34;-wait_file_content\u0026#34;, // Wait for file contents, not just an empty file. \t// Start next step. \t\u0026#34;-post_file\u0026#34;, filepath.Join(mountPoint, fmt.Sprintf(\u0026#34;%d\u0026#34;, i)), \u0026#34;-termination_path\u0026#34;, terminationPath, } default: // All other steps wait for previous file, write next file. \targsForEntrypoint = []string{ \u0026#34;-wait_file\u0026#34;, filepath.Join(mountPoint, fmt.Sprintf(\u0026#34;%d\u0026#34;, i-1)), \u0026#34;-post_file\u0026#34;, filepath.Join(mountPoint, fmt.Sprintf(\u0026#34;%d\u0026#34;, i)), \u0026#34;-termination_path\u0026#34;, terminationPath, } } ... } 自动运行的容器 这些自动运行的容器作为 pod 的initContainer会在 step 容器运行之前运行\ncredential-initializer 用于将 ServiceAccount 的相关secrets持久化到容器的文件系统中。比如 ssh 相关秘钥、config文件以及know_hosts文件；docker registry 相关的凭证则会被写入到 docker 的配置文件中。\nworking-dir-initializer 收集Task内的各个Step的workingDir配置，初始化目录结构\nplace-scripts 假如Step使用的是script配置（与command+args相对），这个容器会将脚本代码（script字段的内容）持久化到/tekton/scripts目录中。\n注：所有的脚本会自动加上#!/bin/sh\\nset -xe\\n，所以script字段里就不必写了。\nplace-tools 将entrypoint的二进制文件，复制到/tekton/tools/entrypoint.\nTask/Step间的数据传递 针对不同的数据，有多种不同的选择。比如Workspace、Result、PipelineResource。对于由于Task的执行是通过Pod来完成的，而Pod会调度到不同的节点上。因此Task间的数据传递，需要用到持久化的卷。\n而Step作为Pod中的容器来运行，\nWorkspace 工作区，可以理解为一个挂在到容器上的卷，用于文件的传递。\npersistentVolumeClaim 引用已存在persistentVolumeClaim卷（volume）。这种工作空间，可多次使用，需要先进行创建。比如 Java 项目的 maven，编译需要本地依赖库，这样可以节省每次编译都要下载依赖包的成本。\nworkspaces: - name: m2 persistentVolumeClaim: claimName: m2-pv-claim apiVersion: v1 kind: PersistentVolume metadata: name: m2-pv labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteMany hostPath: path: \u0026#34;/data/.m2\u0026#34; --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: m2-pv-claim spec: storageClassName: manual # volumeName: m2-pv accessModes: - ReadWriteMany resources: requests: storage: 10Gi volumeClaimTemplate 为每个PipelineRun或者TaskRun创建PersistentVolumeClaim卷（volume）的模板。比如一次构建需要从 git 仓库克隆代码，而针对不同的流水线代码仓库是不同的。这里就会用到volumeClaimTemplate，为每次构建创建一个PersistentVolumeClaim卷。（从0.12.0开始）\n生命周期同PipelineRun或者TaskRun，运行之后释放。\nworkspaces: - name: git-source volumeClaimTemplate: spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi 相较于persistantVolumeClain类型的workspace，volumeClaimTemplate不需要在每次在PipelineRun完成后清理工作区；并发情况下可能会出现问题。\nemptyDir 引用emptyDir卷，跟随Task生命周期的临时目录。适合在Task的Step间共享数据，无法在多个Task间共享。\nworkspaces: - name: temp emptyDir: {} configMap 引用一个configMap卷，将configMap卷作为工作区，有如下限制：\n 挂载的卷是只读的 需要提前创建configMap configMap的大小限制为1MB（K8s的限制）  使用场景，比如使用maven编译Java项目，配置文件settings.xml可以使用configMap作为工作区\nworkspaces: - name: maven-settings configmap: name: maven-settings secret 用于引用secret卷，同configMap工作区一样，也有限制：\n 挂载的卷是只读的 需要提前创建secret secret的大小限制为1MB（K8s的限制）  Result results字段可以用来配置多个文件用来存储Tasks的执行结果，这些文件保存在/tekton/results目录中。\n在Pipeline中，可以通过tasks.[task-nanme].results.[result-name]注入到其他Task的参数中。\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: print-date annotations: description: | A simple task that prints the date spec: results: - name: current-date-unix-timestamp description: The current date in unix timestamp format - name: current-date-human-readable description: The current date in human readable format steps: - name: print-date-unix-timestamp image: bash:latest script: |#!/usr/bin/env bash date +%s | tee $(results.current-date-unix-timestamp.path) - name: print-date-humman-readable image: bash:latest script: |#!/usr/bin/env bash date | tee $(results.current-date-human-readable.path) --- apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: pass-date spec: pipelineSpec: tasks: - name: print-date taskRef: name: print-date - name: read-date runAfter: #配置执行顺序 - print-date taskSpec: params: - name: current-date-unix-timestamp type: string - name: current-date-human-readable type: string steps: - name: read image: busybox script: |echo $(params.current-date-unix-timestamp) echo $(params.current-date-human-readable) params: - name: current-date-unix-timestamp value: $(tasks.print-date.results.current-date-unix-timestamp) # 注入参数 - name: current-date-human-readable value: $(tasks.print-date.results.current-date-human-readable) # 注入参数  执行结果：\n┌──────Logs(tekton-pipelines/pass-date-read-date-rhlf2-pod-9b2sk)[all] ────────── │ │ place-scripts stream closed ││ step-read 1590242170 │ │ step-read Sat May 23 13:56:10 UTC 2020 ││ step-read + echo 1590242170 │ │ step-read + echo Sat May 23 13:56:10 UTC 2020 │ │ place-tools stream closed │ │ step-read stream closed │ │ PipelineResource PipelineResource在最后提，因为目前只是alpha版本，何时会进入beta或者弃用目前还是未知数。有兴趣的可以看下这里：Why Aren’t PipelineResources in Beta?\n简单来说，PipelineResource可以通过其他的方式实现，而其本身也存在弊端：比如实现不透明，debug有难度；功能不够强；降低了Task的重用性等。\n比如git类型的PipelineResource，可以通过workspace和git-clone Task来实现；存储类型的，也可以通过workspace来实现。\n这也就是为什么上面介绍workspace的篇幅比较大。个人也偏向于使用workspace，灵活度高；使用workspace的Task重用性强。\n参考  云原生 CICD: Tekton Pipeline 实战 控制 Pod 内容器的启动顺序 Knative Controller Why Aren’t PipelineResources in Beta?  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/how-tekton-works/","tags":["Tekton","Kubernetes","DevOps"],"title":"Tekton 的工作原理"},{"categories":["笔记"],"contents":"(Photo by Andrea Piacquadio from Pexels)\n话说工作十多年，mysql 还真没用几年。起初是外企银行，无法直接接触到 DB；后来一直从事架构方面，也多是解决问题为主。\n这次搭建海外机房，围绕时区大家做了一番讨论。不说最终的结果是什么，期间有同事认为 DB 返回的是 UTC 时间。\n这里简单做个验证，顺便看下时区的问题到底是如何处理。\n环境  openjdk version \u0026ldquo;1.8.0_242\u0026rdquo; mysql-connector-java \u0026ldquo;8.0.20\u0026rdquo; mysql \u0026ldquo;5.7\u0026rdquo; 时区 TZ=Europe/London 本地时区 GMT+8\n 创建个简单的库test及表user， 表结构如下：\nCREATE TABLE `user` ( `name` varchar(50) NOT NULL, `birth_date` timestamp NULL DEFAULT CURRENT_TIMESTAMP ) ENGINE=InnoDB DEFAULT CHARSET=latin1 插入一条测试数据：\nmysql\u0026gt; insert into `user` -\u0026gt; values (\u0026#39;Tom\u0026#39;, time(\u0026#39;2020-05-15 08:00:00\u0026#39;)); Query OK, 1 row affected (0.01 sec) mysql\u0026gt; select * from user; +------+---------------------+ | name | birth_date | +------+---------------------+ | Tom | 2020-05-14 08:00:00 | +------+---------------------+ 1 row in set (0.00 sec) 测试代码：\nConnection conn = DriverManager.getConnection(\u0026#34;jdbc:mysql://localhost:3306/test?useSSL=false\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;root\u0026#34;); Statement stmt = conn.createStatement(); stmt.execute(\u0026#34;select * from user where name = \u0026#39;Tom\u0026#39;\u0026#34;); ResultSet rs = stmt.getResultSet(); while (rs.next()) { Timestamp timestamp = rs.getTimestamp(\u0026#34;birth_date\u0026#34;); System.out.println(timestamp.toLocalDateTime().toString()); } 执行结果：\n2020-05-14T15:00 分析 程序的执行过程同时用 wireshark 抓了包。可以看到一次查询，做了这么多次的交互（包含了会话初始化）。这里可以看到 #177 的交互返回查询的结果：Tom 2020-05-14 08:00:00，与 DB 中的数据相符。可见，返回的并不是 UTC 时间。\n在 TCP 抓包结果中 #155 的查询语句：\n/* mysql-connector-java-8.0.20 (Revision: afc0a13cd3c5a0bf57eaa809ee0ee6df1fd5ac9b) */ SELECT @@session.auto_increment_increment AS auto_increment_increment, @@character_set_client AS character_set_client, @@character_set_connection AS character_set_connection, @@character_set_results AS character_set_results, @@character_set_server AS character_set_server, @@collation_server AS collation_server, @@collation_connection AS collation_connection, @@init_connect AS init_connect, @@interactive_timeout AS interactive_timeout, @@license AS license, @@lower_case_table_names AS lower_case_table_names, @@max_allowed_packet AS max_allowed_packet, @@net_write_timeout AS net_write_timeout, @@performance_schema AS performance_schema, @@query_cache_size AS query_cache_size, @@query_cache_type AS query_cache_type, @@sql_mode AS sql_mode, @@system_time_zone AS system_time_zone, @@time_zone AS time_zone, @@transaction_isolation AS transaction_isolation, @@wait_timeout AS wait_timeout; 服务端返回的 time_zone 为 BST。与本地时区的转换，由 mysql 的 connector 自动完成。\n进阶 时区自动转换 实现源码：\nResultSetImpl源码\nthis.defaultTimestampValueFactory = new SqlTimestampValueFactory(pset, null, this.session.getServerSession().getServerTimeZone()); @Override public Timestamp getTimestamp(int columnIndex) throws SQLException { checkRowPos(); checkColumnBounds(columnIndex); return this.thisRow.getValue(columnIndex - 1, this.defaultTimestampValueFactory); } 如何确认服务端时区？ 使用会话中的服务端时区进行服务端时区。会话初始化时会进行时区的确认，比如前面获取的到BST。确认时区的逻辑在NativeProtocol#configureTimezone()中：\npublic void configureTimezone() { #从mysql的响应获取 time_zone 和 system_time_zone 的设置 String configuredTimeZoneOnServer = this.serverSession.getServerVariable(\u0026#34;time_zone\u0026#34;); if (\u0026#34;SYSTEM\u0026#34;.equalsIgnoreCase(configuredTimeZoneOnServer)) { configuredTimeZoneOnServer = this.serverSession.getServerVariable(\u0026#34;system_time_zone\u0026#34;); } #从 jdbc url 参数 serverTimezone 获取时区 String canonicalTimezone = getPropertySet().getStringProperty(PropertyKey.serverTimezone).getValue(); if (configuredTimeZoneOnServer != null) { //如果 jdbc url 中未通过 serverTimezone 指定时区。则从TimeZoneMapping.properties中获取mysql 回传的时区缩写对应的标准时区，比如此处的 BST =\u0026gt; Europe/London  //会出现无法映射的情况，不如 CEST 无法映射到 =\u0026gt; Europe/Berlin，可以指定自定义的 Properties 文件进行映射  // user can override this with driver properties, so don\u0026#39;t detect if that\u0026#39;s the case  if (canonicalTimezone == null || StringUtils.isEmptyOrWhitespaceOnly(canonicalTimezone)) { try { canonicalTimezone = TimeUtil.getCanonicalTimezone(configuredTimeZoneOnServer, getExceptionInterceptor()); } catch (IllegalArgumentException iae) { throw ExceptionFactory.createException(WrongArgumentException.class, iae.getMessage(), getExceptionInterceptor()); } } } //如果 jdbc url 中通过 serverTimezone 指定了时区，则优先使用该时区  if (canonicalTimezone != null \u0026amp;\u0026amp; canonicalTimezone.length() \u0026gt; 0) { this.serverSession.setServerTimeZone(TimeZone.getTimeZone(canonicalTimezone)); //  // The Calendar class has the behavior of mapping unknown timezones to \u0026#39;GMT\u0026#39; instead of throwing an exception, so we must check for this...  //  if (!canonicalTimezone.equalsIgnoreCase(\u0026#34;GMT\u0026#34;) \u0026amp;\u0026amp; this.serverSession.getServerTimeZone().getID().equals(\u0026#34;GMT\u0026#34;)) { throw ExceptionFactory.createException(WrongArgumentException.class, Messages.getString(\u0026#34;Connection.9\u0026#34;, new Object[] { canonicalTimezone }), getExceptionInterceptor()); } } } 关于 serverTimezone 的官方说明  Override detection/mapping of time zone. Used when time zone from server doesn\u0026rsquo;t map to Java time zone\n 修改一下 jdbc url，通过serverTimezone指定时区为 GMT+8：jdbc:mysql://localhost:3306/test?serverTimezone=GMT%2B8\u0026amp;useSSL=false\n再次执行代码：\n2020-05-14T08:00 ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/pexels-marcel-eberle-10622582.jpg","permalink":"https://atbug.com/mysql-timezone-in-java/","tags":["Java"],"title":"Java 中的 Mysql 时区问题"},{"categories":null,"contents":"这样文章通过Google翻译和人工逐字修改的方式完成的，某些位置也加上自己的理解。如有错误，请指出。\n翻译这篇文章的目的其实是为了自己加深对微服务、分布式架构以及多运行时架构的理解。整篇文章从”战略“上分析了微服务”从古至今“解决的问题，以及带来的新问题；进而在“战术”层面，给出了解决这些新问题的手段。\n个人见解：架构从来都是解决问题并带来问题， 取舍之道 。\n背景知识 微服务的 12 要素：\n 基准代码：一份基准代码，多份部署 依赖：显式声明依赖关系 配置：在环境中存储配置 后端服务：把后端服务当做附加资源 构建、发布、运行：严格分离构建和运行 进程：以一个或多个无状态进程运行应用 端口绑定：通过端口绑定提供服务 并发：通过进程模型进行扩展 易处理：快速启动和优雅终止可最大化健壮性 开发环境与线上环境等价：尽可能的保持开发、预发布、线上环境相同 日志：把日志当做事件流 管理进程：后台管理任务当做一次性进程运行  原文从此处开始：\n 创建分布式系统并非易事。围绕“微服务”架构和“ 12要素应用程序”设计出现了最佳实践。这些提供了与交付生命周期，网络，状态管理以及对外部依赖项的绑定有关的准则。 但是，以可扩展和可维护的方式一致地实施这些原则是具有挑战性的。 解决这些原理的以技术为中心的传统方法包括企业服务总线（ESB）和面向消息的中间件（MOM）。虽然这些解决方案提供了良好的功能集，但主要的挑战是整体架构以及业务逻辑和平台之间的紧密技术耦合。 随着云，容器和容器协调器（Kubernetes）的流行，出现了解决这些原理的新解决方案。例如，Knative用于交付，服务网格用于网络，而Camel-K用于绑定和集成。 通过这种方法，业务逻辑（称为“微逻辑”）构成了应用程序的核心，并且可以创建提供强大的现成分布式原语的sidecar“ mecha”组件。 微观组件和机械组件的这种分离可以改善第二天的操作，例如打补丁和升级，并有助于维持业务逻辑内聚单元的长期可维护性。  创建良好的分布式应用程序并非易事：此类系统通常遵循12要素应用程序和微服务原则。它们必须是无状态的，可伸缩的，可配置的，独立发布的，容器化的，可自动化的，并且有时是事件驱动的和无服务器的。创建后，它们应该易于升级，并且长期可以承受。在当今的技术中，要在这些相互竞争的要求之间找到良好的平衡仍然是一项艰巨的努力。\n在本文中，我将探讨分布式平台如何发展以实现这种平衡，更重要的是，在分布式系统的演进中还需要发生什么事情，以简化可维护的分布式体系结构的创建。如果您想让我实时谈论这个话题，请加入我的QCon 三月的伦敦。\n分布式应用程序需求 在此讨论中，我将把现代分布式应用程序的需求分为四个类别-生命周期，网络，状态，绑定-并简要分析它们在最近几年中的发展情况。\n生命周期 Lifecycle  打包 Packaging 健康检查 Healthcheck 部署 Deployment 扩展 Scaling 配置 Configuration  让我们从基础开始。当我们编写一项功能时，编程语言将指示生态系统中的可用库，打包格式和运行时。例如，Java使用.jar格式，将所有Maven依赖项用作生态系统，并将JVM用作运行时。如今，随着发布周期的缩短，生命周期更重要的是能够自动部署，从错误中恢复以及扩展服务的能力。这组功能广泛地代表了我们的应用程序生命周期需求。\n 译者：错误恢复依赖健康检查\n 网络 Networking  服务发现 Service Discovery A/B 测试、金丝雀部署 A/B Testing，Canary rollouts 重试、超时、断路器 Retry，timeout，circuit breaker 点到点、发布/订阅 Point-to-point，pub/sub 安全、可观测性 Security observability  从某种意义上讲，今天几乎每个应用程序都是分布式应用程序，因此需要联网。但是现代分布式系统需要从更广泛的角度掌握网络。从服务发现和错误恢复开始，到启用现代软件发布技术以及各种跟踪和遥测。为了我们的目的，我们甚至将不同的消息交换模式，点对点和发布/订阅方法以及智能路由机制包括在此类别中。\n状态 State  工作流管理 Workflow mgmt 幂等性 Idempotency 临时调度 Temporal scheduling 缓存 Caching 应用状态 Application State  当我们谈论状态时，通常是关于服务状态以及为什么最好是无状态的。但是管理服务的平台本身需要状态。这对于执行可靠的服务编排和工作流，分布式单例，临时调度（cron作业），幂等性，有状态的错误恢复，缓存等是必需的。此处列出的所有功能都依赖于底层的状态。虽然实际的状态管理不在本文讨论范围之内，但关注状态的分布式原语及其抽象却很受关注。\n绑定 Binding  连接器 Connectors 协议转换 Protocol conversion 消息转换 Message transformation 消息路由 Message routeing 事务性 Transactionality  分布式系统的组件不仅必须彼此对话，而且还必须与现代或旧式外部系统集成。这就要求连接器能够转换各种协议，支持不同的消息交换模式（例如轮询，事件驱动，请求/答复）转换消息格式，甚至能够执行自定义的错误恢复过程和安全机制。\n 译者：执行自定义的错误恢复过程和安全机制 \u0026ndash; 事务\n 在不涉及一次性使用案例的情况下，以上内容代表了创建良好的分布式系统所需的通用原语的良好集合。如今，许多平台都提供了这样的功能，但是本文中我们要寻找的是过去十年中我们使用这些功能的方式如何变化，以及在下一个十年中它将如何变化。为了进行比较，让我们看一下过去的十年，看看基于Java的中间件如何满足这些需求。\n传统中间件的限制 上一代的企业服务总线（ESB）及其变体（例如面向消息的中间件，更轻量级的集成框架等）可满足上述需求，这是一种众所周知的传统解决方案。ESB 是一种中间件，可以使用面向服务的体系结构（即经典SOA）在异构环境之间实现互操作性。\nESB可以为您提供良好的功能集，但ESB的主要挑战是整体架构以及业务逻辑和平台之间紧密的技术耦合，从而导致技术和组织集中化。在将服务开发并部署到这样的系统中时，它与分布式系统框架紧密结合，从而限制了服务的发展。这通常只会在软件生命周期的后期才变得明显。\n以下是每类需求的一些问题和局限性，这些问题和局限性使得ESB在现代时代不再有用。\n生命周期 在传统的中间件中，通常只有一个受支持的语言运行时（例如Java），它规定了软件的打包方式，可用的库，必须定期对其进行打补丁等。业务服务必须使用那些使其与以相同语言编写的平台紧密结合的库。实际上，这导致协调的服务和平台升级，从而阻止了独立和常规的服务和平台发布。\n联网 尽管传统的中间件拥有围绕与其他内部和外部服务交互的高级功能集，但它具有一些主要缺点。 网络功能集中于一种主要语言及其相关技术。 对于Java来说，即JMS，JDBC，JTA等。更重要的是，网络问题和语义也深深地刻在业务服务中。 有一些具有抽象的库来解决网络问题（例如曾经很受欢迎的Hystrix项目），但是该库的抽象“泄漏”到了服务的编程模型，交换模式和错误处理语义以及库本身中。 虽然可以方便地在一个位置编写和读取与网络方面混合的整个业务逻辑，但是这将两个问题紧密地耦合到一个实现中，最终形成了一条共同的演进路径。\n译者：这里问题在于一些通用的高级功能与语言绑定。这里提到的 Hystrix 作为断路器的一个实现，使用的时候需要采用 Hystrix 的编程模型。如果切换到其他的实现，则需要学习和改造的成本。\n状态 为了进行可靠的服务编排，业务流程管理以及实施模式（例如Saga模式和其他运行缓慢的流程），平台需要在幕后保持持久状态。同样，临时动作（例如触发计时器和cron作业）建立在状态之上，并且需要在分布式环境中对数据库进行集群化和恢复。这里的主要约束是以下事实：与状态交互的库和接口没有完全抽象出来，也没有与服务运行时分离。通常，这些库必须配置有数据库详细信息，并且它们存在于服务中，从而将语义和依赖关系泄漏到应用程序域中\n绑定 使用集成中间件的主要驱动程序之一是能够使用不同的协议，数据格式和消息交换模式连接到其他各种系统。但是，这些连接器必须与应用程序一起使用，这意味着必须将依赖关系与业务逻辑一起进行更新和修补。这意味着必须在服务内来回转换数据类型和数据格式。这意味着必须根据消息交换模式来构造代码并设计流程。这些是即使抽象的端点如何影响传统中间件中服务实现的一些示例。\n 译者：使用不同 RPC 实现的服务之间的对话，比如某个服务调用会同时（空间上）调用使用 Dubbo 协议的服务和使用 RESTful 协议的服务\n 云原生趋势 传统的中间件功能强大。它具有所有必要的技术功能，但缺乏现代数字业务需求所要求的快速更改和扩展的能力。这就是微服务体系结构及其设计现代分布式应用程序的指导原则所要解决的问题。\n微服务背后的思想及其技术要求促进了容器和Kubernetes的普及和广泛使用。这开始了一种新的创新方式，这种方式将影响我们今后几年处理分布式应用程序的方式。让我们看看Kubernetes和相关技术如何影响每组需求。\n生命周期 容器和Kubernetes将我们打包、分发和部署应用程序的方式发展为与语言无关的格式。关于Kubernetes模式和Kubernetes对开发人员的影响的文章很多，在这里我将简短介绍。但是请注意，对于Kubernetes，要管理的最小原语是容器，它专注于在容器级别和流程模型上交付分布式原语。这意味着它在管理应用程序的生命周期方面，运行状况检查、恢复、部署和扩展方面做得很好，但是在容器内的分布式应用程序的其他方面却没有做得那么好，例如灵活的网络、状态管理和绑定。\n您可能会指出，Kubernetes具有状态工作负载、服务发现、cron作业和其他功能。的确如此，但是所有这些原语都是在容器级别的，并且在容器内部，开发人员仍然必须使用特定于语言的库来访问我们在本文开头列出的更详细的功能。这就是推动诸如Envoy、Linkerd、Consul、Knative、Dapr、Camel-K等项目的原因。\n网络 事实证明，Kubernetes提供的围绕服务发现的基本网络功能是一个很好的基础，但对于现代应用程序来说还不够。随着微服务数量的增加和部署速度的加快，对更高级的发布策略、管理安全性、指标、跟踪、从错误中恢复、错误模拟等等方面的需求变得越来越具有吸引力，并产生了一种新的软件类别，称为服务网格。\n这里更令人兴奋的是，趋势是将与网络相关的问题从包含业务逻辑的服务移出到单独的运行时（无论是sidecar还是节点级代理）。如今，服务网格可以执行高级路由、助力测试、处理安全性的某些部分，更甚至特定于应用程序的协议（例如，Envoy支持Kafka，MongoDB，Redis，MySQL等）。尽管作为解决方案的服务网格可能尚未得到广泛采用，但它触及了分布式系统中的真正痛点，我相信它将找到其形状和存在形式。\n除了典型的服务机制外，还有其他项目，例如Skupper，这些项目证实了将网络功能放入外部运行时代理的趋势。Skupper通过第7层虚拟网络解决了多集群间的通信难题，并提供了先进的路由和连接功能。但是，Skupper并没有被嵌入到业务服务运行时中，而是每个Kubernetes命名空间运行一个实例，该实例充当共享的补充工具。\n综上所述，容器和Kubernetes在应用程序的生命周期管理方面迈出了重要的一步。服务网格和相关技术遇到了真正的痛点，并为将更多职责从应用程序移到代理中奠定了基础。让我们看看下一步。\n状态 我们在前面列出了依赖状态的主要集成原语。管理状态非常困难，应将其委派给专门的存储软件和托管服务。这不是这里的主题，而是在语言无关的抽象中使用状态来帮助集成用例。今天，许多努力试图在语言无关的抽象后面提供有状态的原语。有状态的工作流管理是基于云的服务中的强制性功能，例如AWS Step Functions，Azure Durable Functions等示例。在基于容器的部署中，CloudState和Dapr都依赖于sidecar模型来进一步解耦分布式应用程序中的状态抽象。\n我也期待将上面列出的所有有状态功能抽象到一个单独的运行时中。这意味着工作流管理、单例、幂等、事务管理、cron作业触发器和有状态错误处理都可靠地发生在Sidecar（或主机级代理）中，而不是存在于服务中。业务逻辑不需要在应用程序中包含此类依赖关系和语义，并且可以从绑定环境中声明性地请求此类行为。例如，Sidecar可以充当cron作业触发器、幂等消费者和工作流管理器，而自定义业务逻辑可以作为回调调用或插入工作流的某些阶段、错误处理、临时调用或唯一幂等要求等。\n另一个有状态用例是缓存。无论是由服务网格层执行请求缓存，还是使用诸如Infinispan，Redis，Hazelcast等之类的数据缓存，都有一些将缓存功能推到应用程序运行时之外的示例。\n绑定 尽管我们的主题是将所有分布式需求与应用程序运行时脱钩，但这种趋势也继续伴随着绑定。连接器、协议转换、消息转换 、错误处理和安全中介都可以移出服务运行时。我们还没有到那里，但是在诸如Knative和Dapr之类的项目中朝这个方向进行了尝试。将所有这些职责移出应用程序运行时将导致更小，更注重业务逻辑的代码。这样的代码将在独立于分布式系统需求的运行时中运行，而分布式系统需求可以作为预包装功能使用。\nApache Camel-K 采用了另一种有趣的方法。该项目没有使用来伴随主应用程序的代理运行时，而是依靠智能的Kubernetes Operator来构建具有Kubernetes和Knative的附加平台功能的应用程序运行时。在这里，单一代理是负责包括应用程序所需的分布式系统原语的操作员。不同之处在于，某些分布式原语已添加到应用程序运行时中，而某些已在平台中启用（也可能包括Sidecar）。\n未来架构趋势 概括地说，我们可以得出结论，通过将功能部件转移到平台级别，分布式应用程序的产品化达到了新的领域。除了生命周期之外，现在我们还可以观察到联网，状态抽象，声明性事件和端点绑定（拆箱即用），并且EIPs在此列表中排在后面。有趣的是，产品化使用进程外模型（sidecar）进行功能扩展，而不是使用运行时库或纯平台功能（例如新的Kubernetes功能）。\n 译者：产品化可以理解为分布式原语的内聚：便于独立发布和演进。\n 现在，我们通过将所有传统的中间件功能（也称为ESB）转移到其他运行时中来，不久，我们在服务中要做的就只是编写业务逻辑。\n传统中间件平台和云原生平台概述\n与传统的ESB时代相比，此体系结构将业务逻辑与平台更好地分离了，但是还没有完全分离。许多分布式原语，例如经典的企业集成模式（EIP）：拆分器、聚合器、过滤器、基于内容的路由器；流处理模式：映射、过滤、折叠、联接、合并、滑动窗口；仍然必须包含在业务逻辑运行时中，许多其他依赖于多个不同且重叠的平台附加组件。\n如果我们将在不同领域进行创新的各种云原生项目进行堆叠，那么最终将得到如下图所示：\n多运行时微服务\n这里的图仅用于说明目的，它有目的地选择代表性的项目并将其映射到分布式原语的类别。实际上，您不会同时使用所有这些项目，因为其中一些项目是重叠的且不兼容的工作负载模型。如何解释这个图？\n Kubernetes和容器在多语言应用程序的生命周期管理中取得了巨大飞跃，并为未来的创新奠定了基础。 服务网格技术通过高级网络功能在Kubernetes上进行了改进，并开始涉足应用程序方面。 尽管Knative通过快速扩展主要专注于无服务器工作负载，但它也满足了服务编排和事件驱动的绑定需求。 Dapr以Kubernetes、Knative和Service Mesh的思想为基础，并深入应用程序运行时以解决有状态的工作负载、绑定和集成需求，充当现代的分布式中间件。  该图可帮助您直观地看到，很可能在将来，我们最终将使用多个运行时来实现分布式系统。多个运行时，不是因为有多个微服务，而是因为每个微服务都将由多个运行时组成，最有可能是两个运行时-自定义业务逻辑运行时和分布式原语运行时。\n引入多运行时微服务 这是开始形成的多运行时微服务体系结构的简要说明。\n您还记得科学家们制作的电影中的Avatar和机甲（机械套件），他们去旷野探索潘多拉吗？这种多运行时架构类似于这些Mecha-为类人动物驾驶员赋予超能力的套装。在电影中，您要穿上套装才能获得力量并获得破坏性武器。在这种软件架构中，您具有构成应用程序核心的业务逻辑（称为micrologic微逻辑）和提供强大且拆箱即用的分布式原语的sidecar mecha组件。微逻辑与mecha功能相结合，形成了一个多运行时微服务，该服务将进程外功能用于解决其分布式系统需求。最棒的是，Avatar 2即将面世，以帮助推广这种架构。我们最终可以在所有软件会议上用令人赞叹的机甲图片代替老式的边车摩托车；-)。接下来，让我们看看该软件体系结构的详细信息。\n这是一个类似于客户端-服务器体系结构的两组件模型，其中每个组件都是独立的运行时。它与纯客户端-服务器体系结构的不同之处在于，这两个组件都位于同一主机上，彼此之间没有可靠的网络连接。这两个组件的重要性相同，它们可以在任一方向上启动操作并充当客户端或服务器。其中的一个组件称为为逻辑（Micrologic），它拥有从几乎所有分布式系统问题中剥离出来的非常少的业务逻辑。另一个随附的组件是Mecha，它提供了我们在本文中一直讨论的所有分布式系统功能（生命周期除外，它是一个平台功能）。\n多运行时（进程外）微服务架构\nMicrologic和Mecha可能是一对一的部署（称为sidecar模型），也可以是带有几个Micrologic运行时的共享Mecha。第一种模型最适用于Kubernetes等环境，而第二种模型则适用于边缘部署。\n微逻辑运行时特征 让我们简要地探讨Micrologic运行时的一些特征：\n Micrologic组件本身不是微服务。它包含微服务将具有的业务逻辑，但是该逻辑只能与Mecha组件结合使用。另一方面，微服务是自包含的，没有整体功能的一部分，也没有一部分处理流程扩展到其他运行时中。Micrologic及其与Mecha对应的产品的组合形成了微服务。 这也不是函数或无服务器架构。无服务器最著名的是其提供的快速扩展和从零扩展到零的能力。在无服务器体系结构中，函数实现单个操作，因为这是可伸缩性的单位。在这方面，函数不同于实现多种操作的Micrologic，但实现方式不是端到端的。最重要的是，操作的实现分布在Mecha和Micrologic运行时上。 这是客户端-服务器体系结构的一种特殊形式，针对无需编码即可使用众所周知的分布式原语进行了优化。另外，如果我们假设Mecha扮演服务器角色，那么每个实例都必须经过专门配置以便与单个客户端一起工作。它不是那种旨在与典型的客户端-服务器体系结构同时支持多个客户端的通用服务器实例。（译者：多运行时架构中假如 mecha 作为服务端，那么微逻辑作为客户端。客户端与服务端的关系多为一对一，或者多对一。而传统微服务架构中的客户端服务端一般是多对多） Micrologic中的用户代码不会直接与其他系统交互，也不会实现任何分布式系统原语。它通过事实上的标准（例如HTTP / gRPC，CloudEvents规范）与Mecha进行交互，并且Mecha使用丰富的功能并在配置的步骤和机制的指导下与其他系统进行通信。 尽管Micrologic仅负责实现从分布式系统问题中剥离出来的业务逻辑，但它仍必须至少实现一些API。它必须允许Mecha和平台通过预定义的API和协议与其进行交互（例如，通过遵循Kubernetes部署的云原生设计原则）。（译者：比如微逻辑需要实现健康检查的 API，方便平台-Kubernetes 或者 mecha 进行健康检查）  Mecha运行时特征 以下是一些Mecha运行时特征：\n Mecha是一个通用的、高度可配置的、可重用的组件、提供分布式原语作为现成的功能。 Mecha的每个实例都必须配置为与一个Micrologic组件（边车模型）一起使用，或者配置为与几个组件共享（节点级别）。 Mecha不对Micrologic运行时做任何假设。它与使用开放协议和格式（例如HTTP / gRPC，JSON，Protobuf，CloudEvents）的多语言微服务甚至单片系统一起使用。 Mecha以简单的文本格式（例如YAML，JSON）声明性地配置，该格式表明要启用的功能以及如何将其绑定到Micrologic端点。对于专门的API交互，可以为Mechan附加规范，例如OpenAPI，AsyncAPI，ANSI-SQL等。对于由多个处理步骤组成的有状态工作流，可以使用诸如Amazon State Language的规范。对于无状态集成，可以使用与Camel-K YAML DSL类似的方法来使用企业集成模式（EIP）。这里的关键点是，所有这些都是Mecha无需编码即可实现的简单的、基于文本的、声明性的、多种语言的定义。请注意，这些都是未来派的预测，当前，没有用于状态编排或EIP的Mechas，但是我希望现有的Mechas（Envoy，Dapr，Cloudstate等）很快就会开始添加此类功能。Mecha是应用程序级别的分布式原语抽象层。 与其为了不同目的而依赖于多个代理的（例如网络代理、缓存代理、绑定代理），而应该由一个Mecha提供所有这些功能。一些功能（例如存储、消息持久性、缓存等）的实现将被其他云或本地服务注入并支持。 一些与生命周期管理有关的分布式系统问题可以由管理平台（例如Kubernetes或其他云服务）来处理，而不是使用通用的开放规范（例如Open App Model）提供的Mecha运行时。  这种架构的主要好处是什么？ 好处是业务逻辑和越来越多的分布式系统问题之间的耦合变得松散。软件系统的这两个要素具有完全不同的动力学。业务逻辑始终是内部编写的唯一的自定义代码。它经常更改，具体取决于您的组织优先级和执行能力。另一方面，用于解决本文中列出的问题的分布式原语，并且众所周知。这些由软件供应商开发，并作为库，容器或服务使用。该代码根据供应商的优先级、发布周期、安全补丁、开放源代码管理规则等而更改。这两部分之间几乎不可见并且无法相互控制。\n业务逻辑和分布式系统关注不同架构中的耦合\n微服务原理有助于通过有限的上下文使不同的业务领域脱钩，每个微服务都可以独立发展。但是微服务架构无法解决将业务逻辑与中间件问题耦合在一起带来的困难。对于某些依赖于集成用例的微服务，这可能不是一个大因素。但是，如果您的领域涉及复杂的集成（也是越来越多的人所面临的），那么遵循微服务原则也无法帮助您避免与中间件的耦合。即使中间件是为您包含在微服务中的库，当您开始迁移和更改这些库时，这种耦合便会显现出来。还有您需要的分布的原语越多，您与集成平台的耦合就越强。通过预定义的API（而不是库）来访问作为独立运行时/进程的中间件，有助于解耦并实现每个组件的独立演进。\n这也是为供应商分发和维护复杂的中间件软件的较好的方法。只要与中间件的交互是通过开放API和标准的进程间通信进行的，软件供应商就可以按照自己的节奏自由发布补丁和升级。消费者可以自由使用他们喜欢的语言、库、运行时、部署方法和过程。\n这种架构的主要缺点是什么？ 进程间通信。分布式系统的业务逻辑和中间件机制（您可以看到名称的来源）在不同的运行时中，并且需要HTTP或gRPC调用而不是进程内方法调用。但是请注意，这并不是跨机器或数据中心的网络调用。Micrologic运行时和Mecha应当位于同一主机上，并且延迟时间短，并且出现网络问题的可能性最小。\n复杂。下一个问题是，是否值得为获得某些好处而进行复杂的开发、并维护此类系统。我认为答案将越来越倾向于是。分布式系统的需求和发布周期的步伐正在增加，并且此架构为此进行了优化。我前段时间写道，未来的开发人员必须具备混合开发技能。这种体系结构进一步证实了这一趋势。应用程序的一部分将使用高级编程语言编写，部分功能将由必须进行声明性配置的现成组件提供。这两个部分的互连不是在编译时或在启动时通过进程内依赖注入，而是在部署时通过进程间通信互连。该模型可实现更高的软件重用率和更快的变更速度。\n微服务后无法使用的功能 微服务架构有一个明确的目标。它为变化而优化。通过将应用程序划分到业务域中，此软件架构通过独立的团队分离、管理并以独立的步调发布的服务，为软件演进和可维护性提供了最佳的服务边界。\n如果我们看一下无服务器体系结构的编程模型，它主要基于功能。功能已针对可伸缩性进行了优化。通过功能，我们将每个操作分解为一个独立的组件，以便可以快速，独立和按需扩展。在此模型中，部署粒度是一项功能。之所以选择该函数，是因为它是代码结构：其输入的速率与缩放行为直接相关。这是一种针对极端可伸缩性（而不是复杂系统的长期可维护性）进行了优化的体系结构。\n从AWS Lambda的流行及其完全托管的运营性质来看，Serverless的其他方面又如何呢？在这方面，“AWS无服务器”为配置速度进行了优化，但缺少控制和锁定功能。但是完全托管的方面不是应用程序体系结构，而是一种软件使用模型。它在功能上是正交的，类似于使用基于SaaS的平台，该平台在理想情况下应适用于任何类型的体系结构，无论是整体式、微服务、机甲还是功能。在许多方面，AWS Lambda类似于完全托管的Mecha架构，但有一个很大的区别：Mecha不执行功能模型，而是允许围绕业务域使用更具凝聚力的代码构造，而与所有中间件无关。\n架构优化\n另一方面，Mecha架构为中间件独立性优化了微服务。尽管微服务彼此独立，但它们在很大程度上依赖于嵌入的分布式原语。Mecha架构将这两个问题分为单独的运行时，从而允许独立团队独立发布它们。这种解耦可以改善第二天（day-2）的操作（例如修补和升级），并改善业务逻辑内聚单元的长期可维护性。在这方面，Mecha架构是微服务架构的自然发展，它通过根据引起最大摩擦的边界拆分软件来进行开发。与功能模型相比，该优化以软件重用和演化的形式提供了更多好处，而功能模型则以代码的过度分配为代价进行了优化，以实现极高的可伸缩性。\n 译者：day-2可以理解为前一天的发布带来的问题需要第二天的发布进行修复，而多运行时解耦了业务逻辑和分布式原语，允许其中一个的独立发布，不需要统一安排升级的节点。\n 结论 分布式应用程序有许多要求。创建高效的分布式系统需要多种技术和良好的集成方法。尽管传统的单体中间件提供了分布式系统所需的所有必要的技术功能，但它缺乏业务所需的快速更改、适应和扩展的能力。这就是为什么基于微服务的架构背后的思想为容器和Kubernetes的快速普及做出了贡献。随着云原生领域的最新发展，我们现在通过将所有传统中间件功能转移到平台和现成的辅助运行时中来全面发展。\n应用程序功能的这种产品化主要是使用进程外模型进行功能扩展，而不是运行时库或纯平台功能。这意味着，将来很有可能我们将使用多个运行时来实现分布式系统。多个运行时，不是因为有多个微服务，而是因为每个微服务都将由多个运行时组成。自定义微业务逻辑的运行时，以及拆箱即用的分布式原语运行时。\n关于作者 Bilgin Ibryam是Red Hat的首席架构师、提交者和Apache Software Foundation的成员。他是一位开源的传播者 、博客作者、偶尔的演讲者，并且是Kubernetes Patterns和Camel Design Patterns书籍的作者。在日常工作中，Bilgin乐于指导、编码并带领开发人员成功构建开源解决方案。他当前的工作集中在区块链、分布式系统、微服务、devops和云原生应用程序开发上。\n原文的某些连接：\n 12-Factors：https://12factor.net/zh_cn/ 原文：https://www.infoq.com/articles/multi-runtime-microservice-architecture/  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/translation-multi-runtime-microservices-architecture/","tags":["Kubernetes","Container","云原生"],"title":"翻译：多运行时微服务架构"},{"categories":["笔记"],"contents":"2021.4.30 更新：\n最新的方案，请跳转新篇 Kubernetes 上如何控制容器的启动顺序。\n 背景 众所周知, Kubernetes Pod 内有两种容器: 初始化容器(init container)和应用容器(app container). 其中初始化容器的执行先于应用容器, 并且初始化容器和应用容器的个数分别为 0~n 和 1~n.\n初始化容器会按照顺序执行, 顺序执行的前提是初始化容器始终会运行到完成(completed)状态. 而应用容器恰好相反: 启动顺序随机, 并始终保持运行(running)状态.\n问题 工作中有个架构的方案使用到了 sidecar 容器: 将基础组件功能从容器转移到 sidecar 容器中, 其中有个功能是从远程配置中心获取配置并保持实时更新. 保证实时更新没有问题, 但是配置文件需要在 app 启动之前完成初始化.\n对于同为\u0026quot;应用容器\u0026quot;类型的 sidecar 容器来说, 由于容器启动顺序随机而无法做到这一点.\n当时我们给定的方案是增加一个初始化容器进行配置的初始化, 不可避免的我们需要增加一个额外的容器, 即使是这个容器的生命周期非常短.\n追求极致的我们总是对这个额外增加的容器耿耿于怀: 假如能控制应用容器的启动顺序\u0026hellip;\n新发现 近期在研究 CDF (Continuous Delivery Foundation)下的 Tekton, 其中有个概念是其将流水线(pipeline)中的各个步骤(step)作为应用容器在同一个 Pod 中运行.\n我们都知道流水线中的步骤是按照定义的顺序执行的, 那么 Tekton 是如何保证应用容器的执行顺序的?\n查看 pod 的 manifest 之后发现了下面的容器配置 (这个容器的作用从 git 仓库克隆代码)\nspec: containers: - args: - -wait_file - /tekton/downward/ready - -wait_file_content - -post_file - /tekton/tools/0 - -termination_path - /tekton/termination - -entrypoint - /ko-app/git-init - -- - -url - http://gitlab.nip.io:8088/addozhang/tekton-test - -revision - develop - -path - /workspace/git-source command: - /tekton/tools/entrypoint 克隆代码的命令是:\ngit-init -url http://gitlab.nip.io:8088/addozhang/tekton-test -revision develop -path /workspace/git-source 但是容器的启动命令是/tekton/tools/entrypoint并带上了一坨的参数(此处略过, 后面分析).\n翻看了下文档:\n This binary is used to override the entrypoint of a container by wrapping it. In tektoncd/pipeline this is used to make sure Task\u0026rsquo;s steps are executed in order, or for sidecars.\n这个二进制文件被用于通过包装的方式来覆盖容器的入口点. 在 tektoncd/pipeline 中确保任务中的步骤或者 sidecar 被顺序地执行.\n  -entrypoint: 原始的容器启动命令, 作为 entrypoint 的子进程运行. 即上面的 git-init XXXX -post_file: 子进程运行结束后写的文件路径(即上面的/tekton/tools/0). 如果子进程执行失败, 则写一个{{post_file}}.err文件, 而不是{{post_file}} -wait_file: 启动子进程前监控的文件路径(即上面的/tekton/downward/ready). 通过监控到的{{watch_file}}或者{{watch_file}}.err文件来决定执行子进程, 还是跳过执行然后写入{{post_file}}.err文件并返回错误码(exitCode \u0026gt;= 0) -wait_file_content: 等待wait_file有实际内容写入, 持续监控wait_file直到有内容写入.  回头看上面容器配置:\n 容器的entrypoint启动进程 监控到/tekton/downward/ready文件的创建, 并等待文件内容的写入 执行git-init子进程, 从 git 仓库克隆源码 创建/tekton/tools/0文件  实际应用 这个方案是否能解决我们的问题, 还是有一定的局限性的.\n首先需要应用容器的启动命令进行重新的编排, 这个存在一定的挑战. 需要统一应用的启动命令才能做到规模化+自动化.\n其次引入可用于监控的文件, 需要额外增加Volume用于跨容器的文件访问. 当然通过增加emptyDir的Volume即可.\n同时 sidecar 容器需要在完成启动后创建post_file, 应用容器可以使用这个entrypoint进行包装.\n如果要突破这个局限, CRD 无非是个优秀的方案. 下一篇, 我们通过一个简单的 CRD 来实现.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/control-process-order-of-pod-containers/","tags":["Kubernetes","Container"],"title":"控制 Pod 内容器的启动顺序"},{"categories":["笔记"],"contents":"​[图片来自 https://www.facebook.com/sequenceprocess/]\n问题: 入门到生产级的差距 昨天的文章《为 Go 应用创建 Docker 镜像》, 算是入门级的, 并不适用于生产级. 为什么?\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE addozhang/golang-hello-world latest 4cce1292a87a 4 seconds ago 813MB 整个镜像的大小有 813MB, 这还只有一个简单的 Hello world. 因为其中包含了 Golang 的编译和运行环境. 但是实际生产环境中, 我们并不需要这么多.\n先看结果 精简之后只有 2.07MB, 而且并不影响运行.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE addozhang/golang-hello-world latest 4cce1292a87a 3 minutes ago 813MB addozhang/golang-hello-world2 latest 1da5bb994074 7 minutes ago 2.07MB $ docker run --rm addozhang/golang-hello-world2 Hello world 解决方案 如果做到的? 首先从基础镜像开始, 换成scratch1. 构建时将编译好的文件复制到镜像中\nFROM scratch ADD golang-hello-world / CMD [\u0026quot;/golang-hello-world\u0026quot;] 假如你是使用go build来编译, 在 Macos 上会遇到如下问题:\n$ docker run --rm addozhang/golang-hello-world2 standard_init_linux.go:211: exec user process caused \u0026#34;exec format error\u0026#34; 解决方案是CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build\n从头来看, 构建出一个精简的镜像, 我们需要:\n 运行CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build构建 linux 环境的可执行文件. 该文件并不能在 mac 上运行 使用docker build进行构建  这样的操作步骤太麻烦, 还能不能精简一下?\n进阶: 使用 Docker 的多步构建 #build stage FROM golang as builder ENV GO111MODULE=on ENV GOPROXY=https://goproxy.io WORKDIR /app COPY go.mod . RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build #image stage FROM scratch COPY --from=builder /app/golang-hello-world / CMD [\u0026quot;/golang-hello-world\u0026quot;] 参考 https://dev.to/plutov/docker-and-go-modules-3kkn\n  scratch是一个空的镜像文件 \u0026#x21a9;\u0026#xfe0e;\n   ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/build-minimal-docker-image-for-go-app/","tags":["Docker"],"title":"Go Docker 镜像进阶: 精简镜像"},{"categories":["笔记"],"contents":"嗯嗯, 最近开始用 Golang 了.\n今天需要为 Go 应用创建对象, 看了下官方博客. 拿 hello world 做个测试.\n使用下面的命令创建个新的项目\n$ mkdir -p $GOPATH/src/github.com/addozhang/golang-hello-world \u0026amp;\u0026amp; cd \u0026#34;$_\u0026#34; $ go mod init github.com/addozhang/golang-hello-world go: creating new go.mod: module github.com/addozhang/golang-hello-world $ cat \u0026lt;\u0026lt; EOF \u0026gt; main.go package main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello world\u0026#34;) } EOF # go fmt 运行检查一次\n$ go run main.go Hello world 程序没问题, 下面就是构建镜像了. 创建一个 Dockerfile 文件, 内容如下:\nFROM golang LABEL Author=\u0026quot;addozhang\u0026quot; ADD . /go/src/github.com/addozhang/golang-hello-world RUN go install github.com/addozhang/golang-hello-world ENTRYPOINT [ \u0026quot;/go/bin/golang-hello-world\u0026quot; ] 构建镜像:\n$ docker build -t addozhang/golang-hello-world . 运行镜像:\n$ docker run --rm addozhang/golang-hello-world:latest Hello world 运行没问题, 收工\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/build-docker-image-for-go-app/","tags":["Docker"],"title":"为 Go 应用创建 Docker 镜像"},{"categories":["教程"],"contents":"Trigger的介绍看 这里.\n接上文 Tekton Pipeline 实战 , 我们为某个项目创建了一个Pipeline, 但是执行时通过 PipelineRun 来完成的. 在 PipelineRun 中我们制定了 Pipepline 以及要使用的 PipelineResource. 但是日常的开发中, 我们更多希望在提交了代码之后开始 Pipeline 的执行. 这时我们就要用到 Tekton Trigger 了.\n思路是这样: 代码提交后将Push Event发送给Tekton Trigger EventController(以下简称 Controller), 然后 Controller 基于的TriggerBinding的配置从 payload 中提取信息, 装载在\u0026quot;Params\u0026quot;中作为TriggerTemplate的入参. 最后 Controller 创建PipelineRun.\nTrigger 相关的资源 TriggerTemplate 回看上回用的PipelineRun Yaml, 参数有revision, url, imageUrl和imageTag. (imageUrl 与项目名一直)\n因此定义TriggerTemplate将这 4 个元素作为入参, 然后复用之前的Pipeline.\napiVersion: tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: trigger-test-triggertemplate namespace: tekton-pipelines spec: params: - name: gitrevision description: The git revision default: master - name: gitrepositoryurl description: The git repository url - name: namespace description: The namespace to create the resources default: tekton-pielines - name: projectname description: The project name - name: imagetag description: The image tag default: latest resourcetemplates: - apiVersion: tekton.dev/v1alpha1 kind: PipelineRun metadata: name: tekton-test-pipeline-run-$(uid) namespace: $(params.namespace) spec: serviceAccountName: tekton-test params: - name: imageUrl value: addozhang/$(params.projectname) - name: imageTag value: $(params.imagetag) pipelineRef: name: build-pipeline resources: - name: git-source resourceSpec: type: git params: - name: revision value: $(params.gitrevision) - name: url value: $(params.gitrepositoryurl) 执行kubectl apply -f trigger/trigger-template.yaml\nTriggerBinding TriggerTemplate的入参都可以从PushEvent的 payload中通过 JsonPath 表达式来提取. 更多表达式的使用参考官方的文档.\n注意: 这里的 payload 格式使用的是 Gitlab 的 PushEvent 定义. 为什么要用 Gitlab 后什么会解释\napiVersion: tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: trigger-test-triggerbinding namespace: tekton-pipelines spec: params: - name: gitrevision value: $(body.after) - name: namespace value: tekton-pipelines - name: gitrepositoryurl value: $(body.project.git_http_url) - name: projectname value: $(body.project.name) 执行kubectl apply -f trigger/trigger-binding.yaml\nEventListener EventController将TriggerTemplate和TriggerBinding关联在一起, 资源在创建后会自动创建对应的 pod 和 service.\napiVersion: tekton.dev/v1alpha1 kind: EventListener metadata: name: trigger-test-eventlistener namespace: tekton-pipelines spec: serviceAccountName: tekton-test //需要访问 API, 用上回创建 serviceaccount triggers: - bindings: - name: trigger-test-triggerbinding template: name: trigger-test-triggertemplate 执行kubectl apply -f trigger/event-listener.yaml\n提供 WebHook 的访问入口, 为EventListener创建ingress\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: el-trigger-test-eventlistener namespace: tekton-pipelines spec: rules: - host: trigger-test.el.nip.io http: paths: - backend: serviceName: el-trigger-test-eventlistener servicePort: 8080 做完之后记得添加解析sudo echo \u0026quot;$(minikube ip) trigger-test.el.nip.io\u0026quot; \u0026gt;\u0026gt; /etc/hosts\nGitLab 这里我们没有继续用 github 作为 CVS, minikube 运行在本地, ingress 地址无法在 github 上解析(webhook 无法工作). 因此只能本地搭建个 gitlab.\n使用 Docker Compose 的方式部署, 并制定域名gitlab.nip.io.\nversion: \u0026#39;3.6\u0026#39; services: web: image: \u0026#39;gitlab/gitlab-ce:latest\u0026#39; restart: always hostname: \u0026#39;gitlab.nip.io\u0026#39; environment: GITLAB_OMNIBUS_CONFIG: |external_url \u0026#39;http://gitlab.nip.io:8088\u0026#39; gitlab_rails[\u0026#39;gitlab_shell_ssh_port\u0026#39;] = 8022 # Add any other gitlab.rb configuration here, each on its own line ports: - \u0026#39;8088:8088\u0026#39; - \u0026#39;8022:22\u0026#39; 记得添加解析\nsudo echo \u0026quot;$(ifconfig | grep -Eo 'inet (addr:)?([0-9]*\\.){3}[0-9]*' | grep -Eo '([0-9]*\\.){3}[0-9]*' | grep -v '127.0.0.1' | head -n 1) gitlab.nip.io\u0026quot; \u0026gt;\u0026gt; /etc/hosts GitLab 的默认账号是root, 首次访问http://gitlab.nip.io:8088的时候会提示设置密码.\n创建项目, 提交代码的操作不多说了. 添加 webhook 前, 要先去\u0026quot;Admin Area \u0026raquo; Settings \u0026raquo; Network \u0026raquo; Outbound Requests\u0026quot;勾选Allow requests to the local network from web hooks and services.\nDNS 解析问题 Docker 和 minikube 是在两个独立的虚拟机中运行, 并且我们添加了两个地址解析. 互相访问的时候便会出现 domain 无法解析的问题, 这里是通过部署一个 dnsmasq 作为独立的 dns 解析服务器.\n安装和配置 dnsmasq, dnsmasq 会将 /etc/hosts 中的记录加入到记录中.\n我本地的 minikube 位于192.168.64.0网段, 因次 dnsmasq 添加监听地址192.168.64.1\nbrew install dnsmasq cat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; /usr/local/etc/dnsmasq.conf strict-order listen-address=127.0.0.1,192.168.64.1 EOF sudo brew services start dnsmasq 检查一下, 192.168.1.136是我本机 ip.\n$ dig gitlab.nip.io @192.168.64.1 ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; gitlab.nip.io @192.168.64.1 ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 63393 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;gitlab.nip.io.\tIN\tA ;; ANSWER SECTION: gitlab.nip.io.\t0\tIN\tA\t192.168.1.136 ;; Query time: 0 msec ;; SERVER: 192.168.64.1#53(192.168.64.1) ;; WHEN: Thu Feb 13 18:20:19 CST 2020 ;; MSG SIZE rcvd: 58 测试 推送一个空的提交到代码仓库: git commit -a -m \u0026quot;trigger commit\u0026quot; --allow-empty \u0026amp;\u0026amp; git push origin master\n然后就可以看到新的PipelineRun创建并运行.\n$ tkn pr list NAME STARTED DURATION STATUS tekton-test-pipeline-run-fk2xj 42 seconds ago --- Running 其他文章 Tekton Pipeline 实战: https://atbug.com/tekton-pipeline-practice\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/tekton-trigger-practice/","tags":["DevOps","Tekton"],"title":"云原生CICD: Tekton Trigger 实战"},{"categories":["笔记"],"contents":"背景 Tekton 的介绍请参考Tekton Pipeline 实战.\n通常, CI/CD 事件应该包含如下信息:\n 确定事件的类型(比如 GitHub Push, GitLab Issue, Docker Hub Webhook 等) 可从特定管道访问并映射到特定管道 (从事件负载中获取 SHA 信息, 然后在管道中使用) 准确地触发管道 (基于有效负载值触发管道)  Tekton API 的设计分离了配置(比如 PipelineRun VS Pipeline), 保证了 step 可以被重用. 但是没有提供动态封装配置的机制来生成资源(尤其是 PipelineRun 和 PipelineResource). Triggers 通过下面的 CRDs 在架构上对 Tekton 进行了扩展:\n TriggerTemplate: 创建资源的模板(比如用来创建 PipelineResource 和 PipelineRun) TriggerBinding: 校验事件并提取负载字段 EventListener: 连接TriggerBinding和TriggerTemplate到可寻址的端点(事件接收器). 使用从各个 TriggerBinding中提取的参数来创建TriggerTemplate中指定的 resources. 同样通过 interceptor字段来指定外部服务来对事件负载进行预处理. ClusterTriggerBinding: cluster级别的TriggerBinding  安装 kubectl apply --filename https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml 检查新增的 CRD: kubectl api-resources | grep tekton. Triggers 引入了 3 个 CRD: TriggerTemplate, TriggerBinding,EventListener.\n检查新增的 deployment: tekton-triggers-webhook, tekton-triggers-controller.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/tekton-trigger-glance/","tags":["DevOps","Tekton","Kubernetes"],"title":"Tekton Trigger 介绍"},{"categories":["教程"],"contents":"Tekton 提供了dashboard方便用户管理和查看 Tekton PipelineRun 和 TaskRun 以及创建, 执行和完成过程中涉及的资源. 它还允许按标签过滤 PipelineRun 和 TaskRun.\n安装方法 kubectl apply --filename https://github.com/tektoncd/dashboard/releases/download/v0.4.1/dashboard_latest_release.yaml 检查dashboard的运行情况, STATUS为Running的话则说明运行成功.\nkubectl get pods --namespace tekton-pipelines 访问 访问Tekton的Dashboard有两种方式, 一种是通过port-forward, 另一种是通过ingress来访问.\nport-forward kubectl port-forward svc/tekton-dashboard 9097 ingress 先检查ingress是否开启.\nminikube addon list ... - ingress: enabled ... 如果是disabled的话, 通过命令minikube addons enable ingress.\n注意: 这里拉取quay.io/kubernetes-ingress-controller/nginx-ingress-controller镜像可能比较慢, 建议使用国内的镜像, 比如quay.mirrors.ustc.edu.cn/kubernetes-ingress-controller/nginx-ingress-controller\n修改basic-dashboard-ingress.yaml中的host地址:\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: tekton-dashboard namespace: tekton-pipelines spec: rules: - host: tekton-dashboard.nip.io http: paths: - backend: serviceName: tekton-dashboard servicePort: 9097 执行kubectl apply -f basic-dashboard-ingress.yaml.\n还有最后一步, 在/etc/hosts中添加一条解析x.x.x.x tekton-dashboard.nip.io, ip地址通过minikube ip来获取\n浏览器中打开 tekton-dashboard.nip.io\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/tekton-dashboard-installation/","tags":["DevOps","Tekton","Kubernetes"],"title":"Tekton Dashboard 安装"},{"categories":["云原生"],"contents":"翻译整理自 What’s New in Tekton 0.9\n功能及Bug修复 脚本模式 以前如果要在容器里运行个简单的 bash 脚本, 需要这么写:\n- name: hello image: ubuntu command: [\u0026#39;bash\u0026#39;] args: - -c - |set -ex echo \u0026#34;hello\u0026#34; 在 0.9 之后, 可以更加简单, 不需要再写command 和讨厌的-c.\n- name: hello image: ubuntu script: |#!/bin/bash echo \u0026#34;hello\u0026#34; 性能 通过一系列的工作, 每个 PipelineRun 的运行时间缩短了 5-20 秒\nAPI 变化 为了 beta 版本的发布, API 做了一些调整:\n镜像摘要输出路径的标准化 Tekton 目前提供了一个机制用于存储 task 构建出的镜像的摘要. 这个机制早于 PipelineResource子系统, 并要求 Task 编写者镜像这些摘要写到指定的位置/builder/image-outputs. 从现在开始, 有了输出资源的标准路径/workspace/output/\u0026lt;resource-name\u0026gt;\n简化集群资源 集群PipelineResource使从 Tasks 内部部署和使用 Kubernetes 集群变得简单. 它为用户提供了声明集群的位置以及如何进行身份验证的机制. 然后再执行 Task 过程中, 他们会自动配置.kubeconfig文件, 以便 Kubernetes 工具可以找到该集群.\n这个版本保函了一些更改, 使集群 PipelineResource更易于使用.\n以前用户必须两次指定名字参数: 一次在资源名称中指定, 一次作为资源参数. 现在第二个参数不需要了.\n基础工作 每个Tekton版本中包含的大部分工作都是针对某些功能的, 这些功能要等到以后的版本才能公开.况。\n改进的 PipelineResource 源于Pipeline Resource Redesign\nAPI 的版本控制 源于Create a v1alpha2 apiVersion\n独立的包 Tekton项目发展惊人. 除了这里提到的Pipeline更新, 其他比如Triggers, CLI, Dashboard也有显著的成果.\nTriggers 现在支持开箱即用的 Github 和 Gitlab 校验. CLI加入了交互式创建PipelineResource和启动 task 的支持. Dashboard 接下来也会假如可视化特性.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/tekton-0.9.0-release/","tags":["DevOps","Tekton"],"title":"Tekton 0.9.0 更新"},{"categories":["教程"],"contents":"安装 kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 检查安装的tekton相关的CRD:\nkubectl api-resources | grep tekton clustertasks tekton.dev false ClusterTask conditions tekton.dev true Condition pipelineresources tekton.dev true PipelineResource pipelineruns pr,prs tekton.dev true PipelineRun pipelines tekton.dev true Pipeline taskruns tr,trs tekton.dev true TaskRun tasks tekton.dev true Task tekton的两个pod:\nkubectl get pods --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-556d8f4494-2qthv 1/1 Running 0 11m tekton-pipelines-webhook-849cff5cf-8m5qq 1/1 Running 0 11m 安装CLI cli: https://github.com/tektoncd/cli#installing-tkn\nbrew install tektoncd-cli Tekton: hello world 创建一个简单的Task, 只有一个step就是打印出\u0026quot;hello world\u0026quot;\napiVersion: tekton.dev/v1alpha1 kind: Task metadata: name: echo-hello-world spec: steps: - name: echo image: alpine command: - echo args: - \u0026#34;hello world\u0026#34; 创建一个TaskRun执行上面的Task\napiVersion: tekton.dev/v1alpha1 kind: TaskRun metadata: name: echo-hello-world-task-run spec: taskRef: name: echo-hello-world 运行task:\nkubectl apply -f \u0026lt;name-of-file.yaml\u0026gt; 检查TaskRun的输出, 执行命令:\ntkn taskrun describe echo-hello-world-task-run Name: echo-hello-world-task-run Namespace: tekton-pipelines Task Ref: echo-hello-world Status STARTED DURATION STATUS 21 minutes ago 1 minute Succeeded Input Resources No resources Output Resources No resources Params No params Steps NAME STATUS echo Completed Succeeded状态表示task执行成功.\n查看实际的输出, 执行命令:\ntkn taskrun logs echo-hello-world-task-run 结果:\n[echo] hello world ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/tekton-installation-and-sample/","tags":["Kubernetes","DevOps","Tekton"],"title":"Tekton安装及Hello world"},{"categories":["云原生"],"contents":"准备 注意: istioctl的安装要使用安装里的, 不要是用homebrew里的. github issue\ncurl -L https://istio.io/downloadIstio | sh - cd istio-1.4.2 cp bin/istioctl /usr/local/bin/istioctl 安装前检查 istioctl verify-install 如果检查没问题, 会看到Install Pre-Check passed! The cluster is ready for Istio installation.\n安装 istio有5种内建的安装配置1: remote, sds, default, demo, minimal\nistioctl profile list  minimal: 使用istio的流量管理所需组件的最小化安装 default: 根据IstioControlPlane API的默认设置(建议用于生产部署)启用组件. 您可以通过运行命令istioctl profile dump显示默认设置. demo: 几乎安装所有的特性, 包括logging和tracing的比例为100%. 不适合生产环境, 负载太重      default demo minimal sds remote     Core components        istio-citadel X X  X X   istio-egressgateway  X      istio-galley X X  X    istio-ingressgateway X X  X    istio-nodeagent    X    istio-pilot X X X X    istio-policy X X  X    istio-sidecar-injector X X  X X   istio-telemetry X X  X    Addons        grafana  X      istio-tracing  X      kiali  X      prometheus X X  X     demo profile安装\nistioctl manifest apply --set profile=demo 验证安装结果\nistioctl manifest generate --set profile=demo \u0026gt; /tmp/generated-manifest.yaml istioctl verify-install -f /tmp/generated-manifest.yaml  \u0026hellip;\u0026hellip; Checked 23 crds Checked 9 Istio Deployments Istio is installed successfully\n 卸载 helm template install/kubernetes/helm/istio --namespace istio-system \\  --values install/kubernetes/helm/istio/values-istio-demo.yaml | kubectl delete -f - kubectl delete namespace istio-system #delete all CRDs kubectl delete -f install/kubernetes/helm/istio-init/files   这里可以查看各个配置的详细说明 \u0026#x21a9;\u0026#xfe0e;\n   ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-istio-on-minikube/","tags":["Kubernetes","Istio","Service Mesh"],"title":"Minikube安装istio"},{"categories":["翻译"],"contents":"本文翻译自The Mystery of Eureka Self-Preservation\n根据CAP定理, Eureka是一个AP系统, 这就导致了在网络分区期间多个注册表中的信息不一致. 自我保护功能则是为了尽可能降低这种不一致.\n自我保护的定义 自我保护(self preservation)是Eureka的一项功能, Eureka注册表在未收到实例的心跳情况超过一定阈值时停止驱逐过期的实例.\n从一个健康的系统开始 把下面看成一个健康的系统\n假设所有的微服务都处于健康的状态并成功注册到Eureka注册表中.\n多个注册表间会同步注册表记录, 所有的微服务实例都处于UP状态. 假设实例2从注册中心发现里实例4, 并调用实例4上的服务.\n突发网络分区 假设出现了网络分区, 系统变成下面的状态.\n由于网络分区, 实例4和5丢失了注册中心的连接, 但是实例2仍然可以连接到实例4. Eureka服务端因为没有收到实例4和5的心跳(超过一定时间后), 将他们驱逐. 然后Eureka服务端意识到突然丢失了超过15%(2/5)的心跳, 因此其进入自我保护模式\n从此时开始, Eureka服务端不在驱逐任何实例, 即使实例真正的下线了.\n实例3下线, 但其始终存在注册表中.\n但此时注册表还会接受新实例的注册.\n自我保护的基本原理 自我保护功能在下面两种情况下是合理的:\n Eureka服务端因为弱网分区问题没有收到心跳(这并不意味着客户端下线), 但是这种问题可能会很快被修复. 即使Eureka服务端和客户端的连接断开, 客户端间还可以继续保持连接. (比如上面实例2仍然可以连接到实例4)  配置 (默认) 下面的配置会直接或间接影响到自我保护的行为.\neureka.instance.lease-renewal-interval-in-seconds = 30 客户端发送心跳的频率. 服务端会以此在计算期望收到心跳数, 默认30秒\neureka.instance.lease-expiration-duration-in-seconds = 90 多长时间未收到心跳后, 实例才可以被驱逐, 默认90秒\neureka.server.eviction-interval-timer-in-ms = 60 * 1000 Eureka服务端驱逐操作的执行频率, 默认60秒\neureka.server.renewal-percent-threshold = 0.85 期望心跳数达到该阈值后, 就会进入自我保护模式, 默认0.85\neureka.server.renewal-threshold-update-interval-ms = 15 * 60 * 1000 期望心跳数的计算间隔, 默认15分钟\neureka.server.enable-self-preservation = true 是否允许Eureka服务端进入自我保护模式, 默认开启\n理解配置 Eureka服务端在\u0026quot;上一分钟实际收到的心跳数\u0026quot;小于\u0026quot;每分钟期望的心跳数\u0026quot;时就会进入自我保护模式\n期望的每分钟心跳数 假设renewal-percent-threshold设置为0.85\n计算方式:\n 单个实例每分钟期望的心跳数是: 21   N个实例的每分钟期望的心跳数: 2 * N 期望的上一分钟最小心跳数: 2 * N * 0.85  实际的每分钟心跳数 正如上面所述, 两个定时调度器独立地运行计算实际和期望的心跳数. 此外还有另一个调度任务EvictionTask进行结果比较, 并识别当前系统是否在自我保护状态.\n这个调度任务每个eviction-interval-timer-in-ms时间执行一次, 并决定是否驱逐实例.\n结论  基于使用的经验, 大多数情况下自我保护模式都是错误的, 它错误地认为一些下线的微服务实例是不良的网络分区 自我保护永远不会过期, 除非下线的实例重新上线 如果启用了自我保留, 则无法对实例的心跳间隔进行微调, 因为自我保护在计算期望心跳数是按照30s间隔来计算的 除非环境中经常出现类似的网络分区故障, 否则建议关闭    这个值是固定的, 源于默认的心跳间隔是30s, 故每分钟2次. 见eureka-core-1.7.2的AbstractInstanceRegistryL226 \u0026#x21a9;\u0026#xfe0e;\n   ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/translation-the-mystery-of-eurekas-self-preservation/","tags":["Spring"],"title":"神秘的 Eureka 自我保护"},{"categories":["笔记","云原生"],"contents":"今天来说说日常在Kubernetes开发Java项目遇到的问题.\n当我们新建一个项目的时候, 总是面临需要新建manifest, 平时都是copy+paste+modify. 能否以变成的方式来生成?\n开发时的步骤也比较繁琐: docker build, docker push, kubectl apple, kubectl delete pod. 对于一个Java应用来说还多了一步编译. 操作一次还ok, 但是一天十几次总会有想吐的感觉. 这些步骤能否简化成一个命令, 甚至修改了代码自动就完成上面一系列的操作?\n实现这些我们需要几个工具: dekorate, Jib, Skaffold. 其中Jib也在上一篇文章使用Jib为Java应用构建镜像中介绍过.\ndekorate  Dekorate is a collection of Java compile-time generators and decorators for Kubernetes/OpenShift manifests. Dekorate是Java编译时生成和装饰Kubernetes/OpenShift的manifests的工具\n 快速开始 1. 通过使用Spring Initializer生成一个项目(Spring Boot 2.2.2), 并加入依赖: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.dekorate\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kubernetes-spring-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.10.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2. 加入一个简单的Controller: /** * @author Addo.Zhang * @date 2019/12/22 */ @RestController public class DekorateExampleController { @GetMapping public String hi() { return \u0026#34;Hello World\u0026#34;; } } 3. 执行命令mvn clean install, 然后在target/classes/META-INF/dekorate目录下可以找到kubernetes.json和kubernetes.yml两个文件. kubernetes.yml的内容:\n--- apiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;Service\u0026#34; metadata: labels: app: \u0026#34;dekorate-example\u0026#34; version: \u0026#34;0.0.1-SNAPSHOT\u0026#34; group: \u0026#34;addo\u0026#34; name: \u0026#34;dekorate-example\u0026#34; spec: ports: - name: \u0026#34;http\u0026#34; port: 8081 targetPort: 8081 selector: app: \u0026#34;dekorate-example\u0026#34; version: \u0026#34;0.0.1-SNAPSHOT\u0026#34; group: \u0026#34;addo\u0026#34; type: \u0026#34;ClusterIP\u0026#34; --- apiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; metadata: labels: app: \u0026#34;dekorate-example\u0026#34; version: \u0026#34;0.0.1-SNAPSHOT\u0026#34; group: \u0026#34;addo\u0026#34; name: \u0026#34;dekorate-example\u0026#34; spec: replicas: 1 selector: matchLabels: app: \u0026#34;dekorate-example\u0026#34; version: \u0026#34;0.0.1-SNAPSHOT\u0026#34; group: \u0026#34;addo\u0026#34; template: metadata: labels: app: \u0026#34;dekorate-example\u0026#34; version: \u0026#34;0.0.1-SNAPSHOT\u0026#34; group: \u0026#34;addo\u0026#34; spec: containers: - env: - name: \u0026#34;KUBERNETES_NAMESPACE\u0026#34; valueFrom: fieldRef: fieldPath: \u0026#34;metadata.namespace\u0026#34; image: \u0026#34;addo/dekorate-example:0.0.1-SNAPSHOT\u0026#34; imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; livenessProbe: failureThreshold: 3 httpGet: path: \u0026#34;/actuator/info\u0026#34; port: 8081 scheme: \u0026#34;HTTP\u0026#34; initialDelaySeconds: 0 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 10 name: \u0026#34;dekorate-example\u0026#34; ports: - containerPort: 8081 name: \u0026#34;http\u0026#34; protocol: \u0026#34;TCP\u0026#34; readinessProbe: failureThreshold: 3 httpGet: path: \u0026#34;/actuator/health\u0026#34; port: 8081 scheme: \u0026#34;HTTP\u0026#34; initialDelaySeconds: 0 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 10 yml中包含了Service和Deployment两部分, dekorate完美兼容的Spring:\n app: dekorate-example: 项目名 version: 0.0.1-SNAPSHOT: 项目当前版本 group: addo: 是我系统当前用户名 /actuator/health: Spring Boot 2.2后actuator的health endpoint, 作为readinessProbe /actuator/info: Spring Boot 2.2后actuator的endpoint, 作为livenessProbe  进阶 前面yml的内容都是自动生成的, 假如有些特殊的需求. 比如修改镜像的repository即这里的group, 如何操作?\ndekorate.kubernetes.group = addozhang 结果:\n或者修改Service的类型为NodePort\ndekorate.kubernetes.service-type = NodePort 配置 dekoration提供了丰富的配置来个性化manifest.\n除了上面使用的配置文件(properties/yaml)的方式, 还提供了Annotation注解配置方式.\nimport io.dekorate.kubernetes.annotation.Env; import io.dekorate.kubernetes.annotation.KubernetesApplication; @KubernetesApplication(envVars = @Env(name = \u0026#34;key1\u0026#34;, value = \u0026#34;var1\u0026#34;)) public class Main { public static void main(String[] args) { //Your code goes here  } } Jib Jib的说明请看上一篇文章:使用Jib为Java应用构建镜像\n插件配置 下面是针对该项目添加的配置:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.google.cloud.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jib-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;container\u0026gt; \u0026lt;jvmFlags\u0026gt; \u0026lt;jvmFlag\u0026gt;-Xmx128m\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-Xms64m\u0026lt;/jvmFlag\u0026gt; \u0026lt;/jvmFlags\u0026gt; \u0026lt;labels\u0026gt; \u0026lt;Author\u0026gt;Addo.Zhang\u0026lt;/Author\u0026gt; \u0026lt;/labels\u0026gt; \u0026lt;creationTime\u0026gt;USE_CURRENT_TIMESTAMP\u0026lt;/creationTime\u0026gt; \u0026lt;/container\u0026gt; \u0026lt;from\u0026gt; \u0026lt;image\u0026gt;openjdk:8-jdk-alpine\u0026lt;/image\u0026gt; \u0026lt;/from\u0026gt; \u0026lt;to\u0026gt; \u0026lt;image\u0026gt;addo/dekorate-example\u0026lt;/image\u0026gt; \u0026lt;tags\u0026gt; \u0026lt;tag\u0026gt;latest\u0026lt;/tag\u0026gt; \u0026lt;/tags\u0026gt; \u0026lt;/to\u0026gt; \u0026lt;allowInsecureRegistries\u0026gt;true\u0026lt;/allowInsecureRegistries\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 执行命令mvn compile jib:dockerBuild便可以编译代码, 构建镜像并推送到镜像仓库.\nSkaffold Skaffold也是GoogleContainerTools中的一个工具.\n Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. You can iterate on your application source code locally then deploy to local or remote Kubernetes clusters. Skaffold handles the workflow for building, pushing and deploying your application. It also provides building blocks and describe customizations for a CI/CD pipeline. Skaffold是一个命令行工具, 可促进Kubernetes应用程序的持续开发. 可以在本地迭代应用程序源代码, 然后部署到本地或远程Kubernetes集群. Skaffold处理构建, 推送和部署应用程序的工作流程. 它还提供了构建块并描述了CI/CD管道的自定义.\n 在我们这个例子中, 通过与Jib的联动, 完成编译代码, 构建镜像, 推送镜像, 部署一系列操作.\n![Run](https://raw.githubusercontent.com/addozhang/oss/master/blog/upload/2019-12-23 15.12.24.gif)\n截屏中的操作, 因为没有代码改动而不续构建镜像, Skaffold直接从cache中获取镜像并部署到Kubernetes中.\nSkaffold操作 1. 执行命令skaffold init --XXenableJibInit并在提示出输入y 2. 该命令会生成一个名为skaffold.yaml的文件 由于dekorate同时生成了json和yaml格式的manifest, 被skaffold检测到. 实际操作中只需要其中一个即可.\napiVersion: skaffold/v1 kind: Config metadata: name: dekorate-example build: artifacts: - image: addo/dekorate-example jib: {} deploy: kubectl: manifests: - target/classes/META-INF/dekorate/kubernetes.json - target/classes/META-INF/dekorate/kubernetes.yml 3. 执行skaffold run 4. pod启动完成后, 通过kubectl port-forward PODNAME-HERE 8081 5. 请求http http://localhost:8081 进阶 Skaffold的功能强大, 目前个人使用的有限, 有时间新开一篇来学习一下.\nCLI ➜ ~ skaffold help A tool that facilitates continuous development for Kubernetes applications. Find more information at: https://skaffold.dev/docs/getting-started/ End-to-end pipelines: run Run a pipeline dev Run a pipeline in development mode debug [beta] Run a pipeline in debug mode Pipeline building blocks for CI/CD: build Build the artifacts deploy Deploy pre-built artifacts delete Delete the deployed application render [alpha] Perform all image builds, and output rendered Kubernetes manifests Getting started with a new project: init [alpha] Generate configuration for deploying an application fix Update old configuration to newest schema version Other Commands: completion Output shell completion for the given shell (bash or zsh) config Interact with the Skaffold configuration credits Export third party notices to given path (./skaffold-credits by default) diagnose Run a diagnostic on Skaffold version Print the version information Usage: skaffold [flags] [options] Use \u0026#34;skaffold \u0026lt;command\u0026gt; --help\u0026#34; for more information about a given command. Use \u0026#34;skaffold options\u0026#34; for a list of global command-line options (applies to all commands). Yaml配置 参考skaffold.yaml\n总结 文章的开头我们提到如何做到修改代码后自动完成一些列的操作, 通过skaffold dev就可以实现.\n文章中使用的dekoration-example可在GitHub上找到.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/speed-up-java-development-on-kubernetes/","tags":["Docker","Java","Kubernetes","Tool","云原生"],"title":"加速云原生的 Java 开发"},{"categories":["笔记"],"contents":"Jib是Google Container Tools中的一个工具。\n Jib builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library.\nJib无需Docker守护程序即可为Java应用程序构建优化的Docker和OCI映像-无需深入了解Docker最佳实践. 它可以作为Maven和Gradle的插件以及Java库使用.\n 与Docker构建流程比较 Docker镜像构建流程:\nJib构建流程:\n(pic from Google Cloud Platform Blog)\n快速开始 构建镜像, 并推送到对应的镜像仓库, 比如Docker Hub等, 或者自建仓库.\nmvn compile com.google.cloud.tools:jib-maven-plugin:1.8.0:build -Dimage=\u0026lt;MY IMAGE\u0026gt;\n假如要构建到Docker守护进程的话:\nmvn compile com.google.cloud.tools:jib-maven-plugin:1.8.0:dockerBuild\n插件 设置 pom.xml中使用jib-maven-plugin插件.\n\u0026lt;project\u0026gt; ... \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.google.cloud.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jib-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;to\u0026gt; \u0026lt;image\u0026gt;myimage\u0026lt;/image\u0026gt; \u0026lt;/to\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; ... \u0026lt;/project\u0026gt; 插件配置 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.google.cloud.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jib-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${jib.maven-plugin-version}\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;container\u0026gt; \u0026lt;jvmFlags\u0026gt; \u0026lt;jvmFlag\u0026gt;-Xmx1024m\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-Xms512m\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:NewRatio=1\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:+UseConcMarkSweepGC\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:CMSInitiatingOccupancyFraction=75\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:+UseCMSInitiatingOccupancyOnly\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:ReservedCodeCacheSize=128M\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:ParallelGCThreads=2\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-XX:+ExplicitGCInvokesConcurrent\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-Duser.timezone=Asia/Shanghai\u0026lt;/jvmFlag\u0026gt; \u0026lt;jvmFlag\u0026gt;-Djava.security.egd=file:/dev/./urandom\u0026lt;/jvmFlag\u0026gt; \u0026lt;/jvmFlags\u0026gt; \u0026lt;labels\u0026gt; \u0026lt;Author\u0026gt;Addo.Zhang\u0026lt;/Author\u0026gt; \u0026lt;/labels\u0026gt; \u0026lt;user\u0026gt;apps\u0026lt;/user\u0026gt; \u0026lt;appRoot\u0026gt;/home/apps/local\u0026lt;/appRoot\u0026gt; \u0026lt;workingDirectory\u0026gt;/home/apps/local\u0026lt;/workingDirectory\u0026gt; \u0026lt;creationTime\u0026gt;USE_CURRENT_TIMESTAMP\u0026lt;/creationTime\u0026gt; \u0026lt;/container\u0026gt; \u0026lt;from\u0026gt; \u0026lt;image\u0026gt;PRIVATE_REGISTRY/REPOSITORY/GLOBAL_BASE:1.0.0\u0026lt;/image\u0026gt; \u0026lt;auth\u0026gt; \u0026lt;username\u0026gt;USERNAME\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;PASSWORD\u0026lt;/password\u0026gt; \u0026lt;/auth\u0026gt; \u0026lt;/from\u0026gt; \u0026lt;to\u0026gt; \u0026lt;image\u0026gt;PRIVATE_REGISTRY/REPOSITORY/${project.artifactId}\u0026lt;/image\u0026gt; \u0026lt;auth\u0026gt; \u0026lt;username\u0026gt;USERNAME\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;PASSWORD\u0026lt;/password\u0026gt; \u0026lt;/auth\u0026gt; \u0026lt;/to\u0026gt; \u0026lt;allowInsecureRegistries\u0026gt;true\u0026lt;/allowInsecureRegistries\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 注意: 如果你的私库是insecure的, 需要指定allowInsecureRegistries为true. 同时命令行构建的时候添加-DsendCredentialsOverHttp=true\n更多的配置参见官方文档\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/build-docker-or-oci-image-with-jib-for-java/","tags":["Docekr","Tool"],"title":"使用 Jib 为 Java 应用构建镜像"},{"categories":["资讯"],"contents":"原文\nSpring Cloud Hoxton.RELEASE基于Spring Boot 2.2.1.RELEASE\n文档变化 Hoxton.RELEASE使用了新的首页, 新的样式以及单页面, 多页面和PDF版本.\n新的负载均衡器实现 Hoxton.RELEASE是第一个包含阻塞和非阻塞客户端负载均衡器实现的版本, 替代进入维护状态的Netflix Ribbon.\n搭配BlockingLoadBalancerClient使用RestTemplate, 需要在classpath中引入org.springframework.cloud:spring-cloud-loadbalancer. 这个依赖同样用于使用了@LoadBalanced WebClient.Builder的响应式应用中. 唯一的区别是Spring Cloud会自动配置ReactorLoadBalancerExchangeFilterFunction实例. 更多内容查看文档. 新的ReactorLoadBalancerExchangeFilterFunction可用于自动装配并自动传递给WebClient.Builder(文档).\nSpring Cloud Netflix  增加了新的ReactiveDiscoveryClient, 同时增加了新的Spring Cloud Circuit Breaker API的Hystrix实现. 增加配置项spring.cloud.circuitbreaker.hystrix.enabled来禁用Spring Cloud CircuitBreaker Hystrix的自动配置.  Spring Cloud Cloudfoundry 支持新的ReactiveDiscoveryClient\nSpring Cloud Bus 文档更新\nSpring Cloud Vault  在Pivotal应用程序服务)以前的PCF)中运行的应用程序可以利用容器的身份来使用保险柜的PCF身份验证支持进行身份验证 使用X-Vault-Namespace标头支持Vault名称空间(Vault Enterprise功能)  Spring Cloud Kubernetes 支持新的ReactiveDiscoveryClient\nSpring Cloud Contract  完整的文档重写 主要测试类生成重构 从Groovy到Java的大量重写 添加了对使用Kotlin和Java编写合同的支持 在合同DSL和运行时存根生成中添加了inProgress标志 增加了对生成测试的TestNG支持 许多库版本增量(包括Groovy, WireMock和Pact)  Spring Cloud Consul 支持新的ReactiveDiscoveryClient以及Consul的一致性模型\nSpring Cloud Config  新的环境仓库支持AWS S3 添加了解密纯文本属性的功能  Spring Cloud Gcp  添加BigQuery模块 为Cloud Foundry创建了一个单独的启动器：spring-cloud-gcp-starter-cloudfoundry 可以浏览变更日志文档以获取更多信息  Spring Cloud Stream 从annotation-driven过度到了更加简单的函数式.\n Spring Cloud Stream - demystified and simplified Spring Cloud Stream - functional and reactive Spring Cloud Stream - and Spring Integration Spring Cloud Stream - Event Routing  Spring Cloud Commons 引入阻塞和非阻塞客户端负载均衡器实现, 来替代进入维护状态的Netflix Ribbon.\nSpring Cloud Openfeign  Openfeign升级到10.4.0 支持Spring Cloud LoadBalancer  Spring Cloud Task  支持Micrometer 更新文档 使用Spring Batch分区时启动的任务应用现在加入了external-execution-id  Spring Cloud Sleuth  加入对最新的Brave(包括消息采样)的支持 添加了onLastOperator Reactor跟踪选项，以提高性能 添加了Redis跟踪 将默认采样器设置为限速采样器 添加了对AWS SQS跟踪的支持 增加了对Quartz跟踪的支持 添加了进程内传播机制 默认为Zipkin报告的Micrometer指标  Spring Cloud AWS Bug修复\nSpring Cloud Zookeeper 支持新的ReactiveDiscoveryClient\nSpring Cloud Security Bug修复\nSpring Cloud CurcuitBreaker 引入新的项目Spring Cloud CircuitBreaker, 这个项目包含的抽象的API用于在项目中使用断路器. 支持该API的实现:\n Resilience4j Spring Retry Hystrix (in spring-cloud-netflix) Sentinel (in spring-cloud-alibaba)  更多信息\n 时添加了自动配置, 在使用Resilience4J收集断路器的指标数据 升级到Resilience4J 1.1.0 添加配置项禁用REsilience4J的自动配置  Spring Cloud Function 添加了更多新特性:\n 透明类型转换 函数路由 函数参数  更多详细信息\nSpring Cloud Gateway  支持新的ReactiveDiscoveryClient RSocket模块迁移到了自己维护的位于Spring Cloud Incubator organization项目中 通过增加的使用了新Spring Cloud CircuitBreaker库过滤器为路由提供断路器功能  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-cloud-hoxton-release/","tags":["Spring"],"title":"Spring Cloud Hoxton发布"},{"categories":["折腾"],"contents":"为什么要降级? 既然已经搜到了这里, 相信个中原因也都清楚.\n我的QC35一代, 之前升级到了3.0.3固件. 目前已成功降级到了1.0.6, 下面的操作步骤Mac OSX的, 作者也提供了Windows上的操作步骤.\n操作步骤  确认已经卸载Bose Updater GitHub上下载作者已经改好的6.0.0.4388版本的Bose Updater 解压后复制到/Applications下 运行Bose Updater确认能否正常运行 退出 (右键\u0026gt;Exit) 检查是否还有Bose Updater的进程 打开https://btu.bose.com/ 页面上选择Launch Bose Updater 看到下面界面(借用作者的图)时, 键盘上依次按: a, d, v, 方向上, 方向下  接下来会看到下面界面(借用作者的图)  可以从下拉列表中选择版本进行升/降级 等待升/降级完成 卸载手机app (我是卸载了, 防止再次手贱)  参考  Reddit GitHub  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/bose-qc35-downgrade/","tags":null,"title":"Bose QC35 固件降级"},{"categories":["笔记"],"contents":"根据官方的文档Docker Desktop on Mac vs. Docker Toolbox, Docker Desktop on Mac只提供了UNIX socket/var/run/docker.sock, 并未提供tcp的监听(默认2375端口).\n如果使用linux的配置方式在Docker Desktop中配置host, Docker Desktop将无法启动. 需要去~/.docker/daemon.json中删除hosts配置才能正常启动.\n通过下面的方式暴露出2375的tcp\ndocker run --rm -d -v /var/run/docker.sock:/var/run/docker.sock -p 127.0.0.1:2375:2375 bobrik/socat TCP-LISTEN:2375,fork UNIX-CONNECT:/var/run/docker.sock 然后通过docker version查看当前的docker engine的版本, 比如1.40. 查看官方的Engine API文档: https://docs.docker.com/engine/api/v1.40\n搜索个镜像测试一下:\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/docker-engine-api-on-mac-osx/","tags":null,"title":"Docker Engine API on Mac Osx"},{"categories":["资讯"],"contents":"译自: https://spring.io/blog/2019/10/16/spring-boot-2-2-0\n组件升级  Spring AMQP 2.2 Spring Batch 4.2 Spring Data Moore Spring Framework 5.2 Spring HATEOAS 1.0 Spring Integration 5.2 Spring Kafka 2.3 Spring Security 5.2 Spring Session Corn  第三方库升级  Elasticsearch 6.7 Flyway 6.0 Jackson 2.10 JUnit 5.5 Micrometer 1.3 Reactor Dysprosium Solr 8.0  性能提升 延迟初始化(Lazy initialization) 支持开启全局延迟加载spring.main.lazy-initialization. 代价:\n 初次处理HTTP请求耗时长 本应在启动初始化时出现的问题, 延后出现  更多参考: https://spring.io/blog/2019/03/14/lazy-initialization-in-spring-boot-2-2\nJava 13支持 跟随Spring Framework5.2对Java 13的支持, Spring Boot 2.2现在也支持了Java13. 同时兼容Java 11和8.\n不可变的@ConfigurationProperties绑定 现在加入了基于构造器的绑定, 允许@ConfigurationProperties标注的类不可变(属性不可变).\n可通过@ConfigurationProperties标注类, 或者使用@ConstructorBinding标注构造器来开启.\n额外的注解如@DefaultValue, @DateTimeFormt可对构造参数进行配置.\n更多参考: https://docs.spring.io/spring-boot/docs/2.2.0.RELEASE/reference/html/spring-boot-features.html#boot-features-external-config-constructor-binding\nRScoket支持 使用新的starterspring-boot-starter-rsocket自动配置.\nSpring Security的RScoket集成在classpath中存在spring-security-rsocket时自动完成配置.\n更多参考: https://docs.spring.io/spring-boot/docs/2.2.0.RELEASE/reference/html//spring-boot-features.html#boot-features-rsocket\n健康指示器分组 支持对健康指示器(Health Indicator)进行分组. 比如将应用部署到Kubernetes时, 希望针对\u0026quot;liveness\u0026quot;和\u0026quot;readiness\u0026quot;对指示器进行分组\nmanagement.endpoint.health.group.custom.include=db 检查时使用localhost:8080/actuator/health/custom\n更多参考: https://docs.spring.io/spring-boot/docs/2.2.0.RELEASE/reference/html//production-ready-features.html#health-groups\n其他变化 参考: https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.2-Release-Notes\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-boot-2-2-0-release/","tags":["Spring","Java"],"title":"Spring Boot 2.2.0 发布"},{"categories":["笔记"],"contents":"上回说为了解决吞吐问题, 将zipkin-dependencies的版本升级到了2.3.0.\n好景不长, 从某一天开始作业运行报错:\nIssue communicating with driver in heartbeater org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval ... 19/09/18 08:33:20 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 4) java.lang.OutOfMemoryError: Java heap space ... 解决方案 最新版本(2.3.0)目前不支持额外的spark和elasticsearch-spark的配置, 已经提交了PR\n  超时的解决方案: 为spark指定配置\nspark.executor.heartbeatInterval=600000 spark.network.timeout=600000   OOM解决方案: 根据实际情况通过es.input.max.docs.per.partition配置executor的数量. 调整运行内存及spark.executor.memory\n  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/zipkin-dependencies-bug-two-timeout-and-oom/","tags":null,"title":"Zipkin dependencies的坑之二: 心跳超时和Executor OOM"},{"categories":["笔记"],"contents":"zipkin-dependencies是zipkin调用链的依赖分析工具.\n系统上线时使用了当时的最新版本2.0.1, 运行一年之后随着服务的增多, 分析一天的数据耗时越来越多. 从最初的几分钟, 到最慢的几十小时(数据量18m).\n最终返现是版本的问题, 升级到\u0026gt;=2.3.0的版本之后吞吐迅速上升.\n所以便有了issue: Reminder: do NOT use the version before 2.3.0\n但这也引来了另一个坑: 心跳超时和Executor OOM\nTL;DR 简单浏览了下zipkin-dependencies的源码, 2.0.1和2.3.2的比较大的差距是依赖的elasticsearch-spark的版本. 前者用的是6.3.2, 后者是7.3.0.\n尝试在zipkin-dependencies-2.0.1中使用elasticsearch-spark-7.3.0, 和2.3.2的性能一直.\n通过打开log4j debug日志, 发现到elasticsearch-spark两个版本的运行差异:\n#7.3.0 19/09/05 18:13:14 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[1] at groupBy at ElasticsearchDependenciesJob.java:185) (first 15 tasks are for partitions Vector(0, 1, 2)) #6.3.2 19/09/05 18:09:56 INFO DAGScheduler: Submitting 214 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[1] at groupBy at ElasticsearchDependenciesJob.java:185) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)) RestService#findPartitions()L268的源码: 7.3.0 6.3.2 7.x es.input.max.docs.per.partition为null 计算出partitions=3 (实际分区数为3, 使用方法#findShardPartitions()) 6.x es.input.use.sliced.partitions为true, 计算出partitions=214 (使用方法#findSlicePartitions())\n在7.x中, 可以通过设置es.input.max.docs.per.partition的值来设置切片数量(对单个partition进行切分, 通过增加并行任务数量来提高吞吐)\n**该行代码的commit message: **\n Remove default setting for max documents per partition We added support for sliced scrolls back in 5.0, which allows subdividing scrolls into smaller input splits. there are some cases where the added subdivision of the scroll operations causes high amounts of overhead when reading very large shards. in most cases, shards should be small enough that a regular read operation over them should complete in reasonable time. In order to avoid performance degradation at higher levels, we are removing the default value of 100k from this setting, and instead, checking if it is set. Additionally, the \u0026lsquo;es.input.use.sliced.partitions\u0026rsquo; setting has been removed as it is now redundant.\n 关联的issue#1196\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/zipkin-dependencies-bug-one/","tags":null,"title":"Zipkin dependencies的坑之一: 耗时越来越长"},{"categories":["笔记"],"contents":"在kafka中, topic的分区是并行计算的单元. 在producer端和broker端, 可以同时并发的写数据到不同的分区中. 在consumer端, Kafka总是将某个分区分配个一个consumer线程. 因此同一个消费组内的并行度与分区数息息相关.\nPartition分区数的大小, 更多直接影响到消费端的吞吐(一个分区只能同一消费组的一个消费者消费). 分区数小, 消费端的吞吐就低. 但是太大也会有其他的影响\n原则:\n 更多的分区可提高吞吐量 分区数越多打开的文件句柄越多 分区数越多降低可用性 更多的分区增加端到端的延迟 客户端需要更多的内存  归根结底还是得有个度. 如何找出这个度?\n有个粗略的计算公式: max(t/p, t/c). t就是所预期吞吐量, p是当前生产端单个分区的吞吐, 那c就是消费端单个分区的吞吐.\n比如单个partition的生产端吞吐是200, 消费端是100. 预期的吞吐是500, 那么partition的数量就是5.\n单个分区的吞吐通常通过修改配置来提升, 比如生产端的批处理大小, 压缩算法, acknowledgement类型, 副本数等. 而在消费端则更依赖于消息的处理速度.\n参考  Confluent博客 Linkedin的benchmark  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/how-to-choose-topic-partition-count-number-kafka/","tags":["Kafka","Java"],"title":"如何选择Kafka Topic的分区数"},{"categories":["笔记"],"contents":"上一篇日志更新还是在去年的12月, 至今有差不多10个月没有更新了.\n不是说没有东西可写, 而且想写的东西很多. 工作太忙, 不忙的时候又太懒, 归根结底还是太懒.\n过去一年多都是在做基础架构方面的工作, 围绕技术中台展开的. 有很多技术需要去学习, 也有很多问题要处理. 过程中一直有记笔记的习惯, 所以可以写的东西很多. 不过有些属于公司的部分还是不能写的, 必要的职业道德还是要有的.\n笔记记录一直在用MWeb, 并使用iCloud同步, 最近几个月也在结合幕布整理思路和工作安排. 好用的软件我也比较喜欢分享, 记得最早在Workpress上的博客就分享了很多自己常用的软件. (有点扯远了~~~)\nMWeb没有统计功能, 还有使用的是sqlite. 简单sql查询了下, 从去年这份工作开始有244篇笔记. 今年到现在有109篇. 当然有些笔记的内容比较少, 不得不说这一年多收获甚多.\n为什么今天又写了这么一篇, 源于阮一峰的科技爱好者周刊：第 69 期.\n刊首语是\u0026quot;一件事\u0026quot;做得好\u0026quot;比较好，还是\u0026quot;做得快\u0026quot;比较好？\u0026quot;, 直接copy他的结论.\n 我很赞同一篇文章的结论：做得快更好。\n 做得快不仅可以让你在单位时间内完成更多的工作，而且 因为你工作得很快，所以你会觉得成本低，从而倾向于做更多。\n 写一篇博客，你可能需要两天。这是很高的时间成本，你觉得太贵了，于是你很少写。但是，做好一件事的唯一方法，就是多做这件事。 做得越快，这件事的时间成本就越低，你会愿意做得更多。\n人们总是倾向于，多消费时间成本低的东西。网站很快，就会多访问；搜索很快，就会多搜索；文章很容易读懂，就会多读几篇。做得快的核心，就是要让时间成本降下来，从而多做。\n 之前写博客的时候, 确实投入很多. 笔记记录的时候很随意, 但是发到博客中又要花不少的时间来整理语言. 总想写的大而美, 也正是因为这个原因导致大半年没有更新(其实也还是懒), 每次想写都因为要花时间而萌生退意.\n所以今后还是多写写, 小而美.\n虽然大半年没更新, 访问量居然还有提升.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/no-output-in-past-half-year/","tags":["生活"],"title":"博客最近半年没什么产出"},{"categories":["源码解析"],"contents":"@Configuration注解 @Configuration注解指示一个类声明一个或多个@Bean方法, 并且可以由Spring容器处理, 以在运行时为这些bean生成bean定义和服务请求.\n使用ConfigurationClassParser来对@Configuration标注的类进行解析, 封装成ConfigurationClass实例. 具体的实现通过ConfigurationClassPostProcessor来实现的.\nConfigurationClassPostProcessor 实现了BeanDefinitionRegistryPostProcessor接口, 间接实现了BeanFactorPostProcessor接口.\n #postProcessBeanDefinitionRegistry(): 注册所有ConfigurationClass中的BeanDefinition, 包括@Bean注解的方法, @ImporResource引入的资源中定义的bean, 和@Import注解引入的ImportBeanDefinitionRegistrar中注册的BeanDefinition #postProcessBeanFactory(): 在运行时以通过cglig增强的类来替换ConfigurationClass, 为服务bean请求做准备. 增强的实现是通过ConfigurationClassEnhancer完成的.  插入一点, ConfigurationClassEnhancer实现了直接使用bean注册方法来获取bean的操作, 提供了一个BeanMethodInterceptor的内部类来实行.\n@Configuration public class Config { @Bean public A a() { ... return a; } @Bean public B b() { b.setA(a()); ... return b; } } Full ConfigurationClass VS Lite ConfigurationClass 先说区别: full的ConfigurationClass会使用CGLIB进行增强.\n查看类ConfigurationClassUtils, 其中有两个方法#isFullConfigurationClass()和#isLiteConfigurationClass().\n方法的实现是去检查BeanDefinition中的ConfigurationClassPostProcessor.configurationClass属性, 是full还是lite.\n这个属性的值又来源于#checkConfigurationClassCandidate()方法, 如果BeanDefinition使用的是@Configuration注解, 则为full; 如果是@Component, @ComponentScan, @Import或者@ImportResource中的任何一种, 则为lite. 如果是ConfigurationClass, 则会继续为其添加顺序属性.\npublic static boolean checkConfigurationClassCandidate(BeanDefinition beanDef, MetadataReaderFactory metadataReaderFactory) { ... if (isFullConfigurationCandidate(metadata)) { beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_FULL); } else if (isLiteConfigurationCandidate(metadata)) { beanDef.setAttribute(CONFIGURATION_CLASS_ATTRIBUTE, CONFIGURATION_CLASS_LITE); } else { return false; } // It\u0026#39;s a full or lite configuration candidate... Let\u0026#39;s determine the order value, if any. \tMap\u0026lt;String, Object\u0026gt; orderAttributes = metadata.getAnnotationAttributes(Order.class.getName()); if (orderAttributes != null) { beanDef.setAttribute(ORDER_ATTRIBUTE, orderAttributes.get(AnnotationUtils.VALUE)); } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-boot-configuration-annotation/","tags":["Spring","Java"],"title":"Spring Boot源码分析 - Configuration注解"},{"categories":null,"contents":"Elasticssearch的HTTP基本认证实现有两种方案: x-pack和nginx反向代理. 前者收费, 后者不太适合生产使用. 如果仅仅是开发测试, 第二种完全足够.\n创建密码 htpasswd -bc ./passwd [username] [password] Docker compose version: \u0026#39;3\u0026#39; services: elasticsearch: image: elasticsearch:5.5.2 container_name: elasticsearch restart: unless-stopped volumes: - /tmp/elasticsearch:/usr/share/elasticsearch/data nginx: image: nginx:latest container_name: elasticsearch-proxy ports: - 9200:9200 links: - elasticsearch volumes: - ./passwd:/etc/nginx/.passwd - ./default.conf:/etc/nginx/conf.d/default.conf nginx配置文件 upstream es { server elasticsearch:9200; keepalive 15; } server { listen 9200; server_name localhost; access_log /dev/stdout; error_log /dev/stdout; location / { auth_basic \u0026quot;Administrator’s Area\u0026quot;; auth_basic_user_file /etc/nginx/.passwd; proxy_http_version 1.1; proxy_set_header Connection \u0026quot;Keep-Alive\u0026quot;; proxy_set_header Proxy-Connection \u0026quot;Keep-Alive\u0026quot;; proxy_pass http://es; } location /health { access_log off; return 200 \u0026quot;healthy\\n\u0026quot;; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/elasticsearch-http-basic-authentication-via-nginx/","tags":["DevOps"],"title":"Nginx实现Elasticsearch的HTTP基本认证"},{"categories":["笔记"],"contents":"安装Docker echo \u0026#34;http://dl-2.alpinelinux.org/alpine/edge/main\u0026#34; \u0026gt; /etc/apk/repositories echo \u0026#34;http://dl-2.alpinelinux.org/alpine/edge/community\u0026#34; \u0026gt;\u0026gt; /etc/apk/repositories echo \u0026#34;http://dl-2.alpinelinux.org/alpine/edge/testing\u0026#34; \u0026gt;\u0026gt; /etc/apk/repositories apk -U --no-cache \\ \t--allow-untrusted add \\ \tshadow \\  docker \\  py-pip \\  openrc \\  \u0026amp;\u0026amp; pip install docker-compose rc-update add docker boot 安装OpenShift Client Tools 需要先安装glibc\napk --no-cache add ca-certificates wget wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub wget https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.28-r0/glibc-2.28-r0.apk apk add glibc-2.28-r0.apk curl --retry 7 -Lo /tmp/client-tools.tar.gz \u0026quot;https://mirror.openshift.com/pub/openshift-v3/clients/3.9.1/linux/oc.tar.gz\u0026quot;\ncurl --retry 7 -Lo /tmp/client-tools.tar.gz \u0026#34;https://mirror.openshift.com/pub/openshift-v3/clients/3.9.1/linux/oc.tar.gz\u0026#34; tar zxf /tmp/client-tools.tar.gz -C /usr/local/bin oc \\  \u0026amp;\u0026amp; rm /tmp/client-tools.tar.gz \\  \u0026amp;\u0026amp; apk del .build-deps # ADDED: Resolve issue x509 oc login issue apk add --update ca-certificates 参考: github issue\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-docker-and-openshift-client-tools-in-alpine-container/","tags":["Docker","Openshift"],"title":"Alpine容器安装Docker和OpenShift Client Tools"},{"categories":["笔记"],"contents":"相关配置 #如果路由转发请求发生超时(连接超时或处理超时), 只要超时时间的设置小于Hystrix的命令超时时间,那么它就会自动发起重试. 默认为false. 或者对指定响应状态码进行重试 zuul.retryable = true zuul.routes.\u0026lt;route\u0026gt;.retryable = false #同一实例上的最大重试次数, 默认值为0. 不包括首次调用 ribbon.MaxAutoRetries=0 #重试其他实例的最大重试次数, 不包括第一次选的实例. 默认为1 ribbon.MaxAutoRetriesNextServer=1 #是否所有操作执行重试, 默认值为false, 只重试`GET`请求 ribbon.OkToRetryOnAllOperations=false #连接超时, 默认2000 ribbon.ConnectTimeout=15000 #响应超时, 默认5000 ribbon.ReadTimeout=15000 #每个host的最大连接数 ribbon.MaxHttpConnectionsPerHost=50 #最大连接数 ribbon.MaxTotalHttpConnections=200 #何种响应状态码才进行重试 ribbon.retryableStatusCodes=404,502 实现  SimpleRouteLocator#getRoute返回的route对象中会带上retryable的设置. PreDecorationFilter在对RequestContext进行装饰的时候会将retryable的设置通过keyFilterConstants.RETRYABLE_KEY注入RequestContext中. RibbonRoutingFilter#buildCommandContext会使用RequestContext的retryable设置构造RibbonCommandContext对象. RibbonCommandFactory使用RibbonCommandContext构建出RibbonCommand对象. RibbonCommand#run中, 当retryable为true时, 会调用IClient的execute方法处理请求. 为false时, 会调用IClient的executeWithLoadBalancer方法执行请求.  execute会在失败时进行重试(不超过超时限制) executeWithLoadBalancer方法是先通过LoadBalancer选择出一个Server, 然后构建出请求地址.   IClient#execute执行时, 通过LoadBalancedRetryPolicyFactory创建一个LoadBalancedRetryPolicy对象. LoadBalancedRetryPolicy持有上面ribbon.XXX的设置. 当响应状态码不在ribbon.retryableStatusCodes设置中, 则会直接返回响应. 如果属于可重试的响应状态码, 则会将响应封装为HttpClientStatusCodeException抛出. 异常被RetryTemplate捕获, 然后使用LoadBalancedRetryPolicy对当前状态(MaxAutoRetries, MaxAutoRetriesNextServer)计算出能否进行一次重试. 直至成功, 或者当前状态不满足条件.  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/ribbon-retry-in-zuul/","tags":["Spring","Java"],"title":"Zuul网关Ribbon重试"},{"categories":["笔记"],"contents":"异常处理 Hystrix异常类型  HystrixRuntimeException HystrixBadRequestException HystrixTimeoutException RejectedExecutionException  HystrixRuntimeException HystrixCommand失败时抛出, 不会触发fallback.\nHystrixBadRequestException 用提供的参数或状态表示错误的异常, 而不是执行失败. 与其他HystrixCommand抛出的异常不同, 这个异常不会触发fallback, 也不会记录进failure的指标, 因而也不会触发断路器,\n应该在用户输入引起的错误是抛出, 否则会它与容错和后退行为的目的相悖.\n不会触发fallback, 也不会记录到错误的指标中, 也不会触发断路器.\nRejectedExecutionException 线程池发生reject时抛出\nHystrixTimeoutException 在HystrixCommand.run()或者HystrixObservableCommand.construct()时抛出, 会记录timeout的次数. 如果希望某些类型的失败被记录为timeout, 应该将这些类型的失败包装为HystrixTimeoutException\n异常处理 ignoreExceptions\nfinal Func1\u0026lt;Throwable, Observable\u0026lt;R\u0026gt;\u0026gt; handleFallback = new Func1\u0026lt;Throwable, Observable\u0026lt;R\u0026gt;\u0026gt;() { @Override public Observable\u0026lt;R\u0026gt; call(Throwable t) { circuitBreaker.markNonSuccess(); Exception e = getExceptionFromThrowable(t); executionResult = executionResult.setExecutionException(e); if (e instanceof RejectedExecutionException) { return handleThreadPoolRejectionViaFallback(e); } else if (t instanceof HystrixTimeoutException) { return handleTimeoutViaFallback(); } else if (t instanceof HystrixBadRequestException) { return handleBadRequestByEmittingError(e); } else { /* * Treat HystrixBadRequestException from ExecutionHook like a plain HystrixBadRequestException. */ if (e instanceof HystrixBadRequestException) { eventNotifier.markEvent(HystrixEventType.BAD_REQUEST, commandKey); return Observable.error(e); } return handleFailureViaFallback(e); } } }; Feign中响应状态码处理 Feign使用SynchronousMethodHandler做请求的执行和响应的处理. 响应处理的部分, 对[200, 300)区间的状态, 会将response返回; 如果是404, 根据@FeignClient中decode404(默认为false)和方法返回值判断是否熔断, 如果响应返回404, decode为false, 同时方法返回值不是void, 会包装成FeignException抛出; 其他的状态, 通过包装成FeignException抛出.\nFeignException是RuntimeException的实现, 如果没有ignore的话, 会计入熔断器的计算中.\nfinal class SynchronousMethodHandler implements MethodHandler { Object executeAndDecode(RequestTemplate template) throws Throwable { ... if (response.status() \u0026gt;= 200 \u0026amp;\u0026amp; response.status() \u0026lt; 300) { if (void.class == metadata.returnType()) { return null; } else { return decode(response); } } else if (decode404 \u0026amp;\u0026amp; response.status() == 404 \u0026amp;\u0026amp; void.class != metadata.returnType()) { return decode(response); } else { throw errorDecoder.decode(metadata.configKey(), response); } ... } } Ribbon中响应状态码处理 在Zuul中, 路由使用Ribbon做负载均衡, 同时使用Hystrix做断路器, 使用RibbonCommand接口的实现. RibbonCommand的实现并没有对响应编码封装异常, 因此也不会触发熔断器.\nAbstractRibbonCommand是RibbonCommand的抽象实现, 所有其他实现的父类. 核心run()方法并没有针对响应编码重新封装异常.\npublic abstract class AbstractRibbonCommand\u0026lt;LBC extends AbstractLoadBalancerAwareClient\u0026lt;RQ, RS\u0026gt;, RQ extends ClientRequest, RS extends HttpResponse\u0026gt; extends HystrixCommand\u0026lt;ClientHttpResponse\u0026gt; implements RibbonCommand { ... @Override protected ClientHttpResponse run() throws Exception { final RequestContext context = RequestContext.getCurrentContext(); RQ request = createRequest(); RS response; boolean retryableClient = this.client instanceof AbstractLoadBalancingClient \u0026amp;\u0026amp; ((AbstractLoadBalancingClient)this.client).isClientRetryable((ContextAwareRequest)request); if (retryableClient) { response = this.client.execute(request, config); } else { response = this.client.executeWithLoadBalancer(request, config); } context.set(\u0026#34;ribbonResponse\u0026#34;, response); // Explicitly close the HttpResponse if the Hystrix command timed out to \t// release the underlying HTTP connection held by the response. \t// \tif (this.isResponseTimedOut()) { if (response != null) { response.close(); } } return new RibbonHttpResponse(response); } ... } Observable.error(ex)会捕获run()方法抛出的异常.\npublic abstract class HystrixCommand\u0026lt;R\u0026gt; extends AbstractCommand\u0026lt;R\u0026gt; implements HystrixExecutable\u0026lt;R\u0026gt;, HystrixInvokableInfo\u0026lt;R\u0026gt;, HystrixObservable\u0026lt;R\u0026gt; { ... final protected Observable\u0026lt;R\u0026gt; getExecutionObservable() { return Observable.defer(new Func0\u0026lt;Observable\u0026lt;R\u0026gt;\u0026gt;() { @Override public Observable\u0026lt;R\u0026gt; call() { try { return Observable.just(run()); } catch (Throwable ex) { return Observable.error(ex); } } }).doOnSubscribe(new Action0() { @Override public void call() { // Save thread on which we get subscribed so that we can interrupt it later if needed  executionThread.set(Thread.currentThread()); } }); } ... } Hystrix 超时处理 在Hystrix版本1.4之前, Seamphore策略是不支持超时的. 目前spring-cloud-netflix的1.4.4中使用的是1.5.12\n如果开启了timeout, HystrixCommand会lift一个HystrixObservableTimeoutOperator到Observable中.\nabstract class AbstractCommand\u0026lt;R\u0026gt; implements HystrixInvokableInfo\u0026lt;R\u0026gt;, HystrixObservable\u0026lt;R\u0026gt; { private Observable\u0026lt;R\u0026gt; executeCommandAndObserve(final AbstractCommand\u0026lt;R\u0026gt; _cmd) { ... Observable\u0026lt;R\u0026gt; execution; if (properties.executionTimeoutEnabled().get()) { execution = executeCommandWithSpecifiedIsolation(_cmd) .lift(new HystrixObservableTimeoutOperator\u0026lt;R\u0026gt;(_cmd)); } else { execution = executeCommandWithSpecifiedIsolation(_cmd); } return execution.doOnNext(markEmits) .doOnCompleted(markOnCompleted) .onErrorResumeNext(handleFallback) .doOnEach(setRequestContext); } } 这个HystrixObservableTimeoutOperator会添加注册TimeListener. TimeListener是以tick的方式运行, 即启动一个线程延迟executionTimeoutInMilliseconds运行, 然后每次在executionTimeoutInMilliseconds + n * executionTimeoutInMilliseconds时运行.\n如果判断操作超时? 看tick方法的实现, 线程每次运行时, 尝试修改Command的状态从NOT_EXECUTED到TIMED_OUT. 如果成功, 说明运行超时. 最后抛出HystrixTimeoutException异常, 被handleFallback处理.\n// if we can go from NOT_EXECUTED to TIMED_OUT then we do the timeout codepath // otherwise it means we lost a race and the run() execution completed or did not start if (originalCommand.isCommandTimedOut.compareAndSet(TimedOutStatus.NOT_EXECUTED, TimedOutStatus.TIMED_OUT)) { // report timeout failure  originalCommand.eventNotifier.markEvent(HystrixEventType.TIMEOUT, originalCommand.commandKey); // shut down the original request  s.unsubscribe(); final HystrixContextRunnable timeoutRunnable = new HystrixContextRunnable(originalCommand.concurrencyStrategy, hystrixRequestContext, new Runnable() { @Override public void run() { child.onError(new HystrixTimeoutException()); } }); timeoutRunnable.run(); //if it did not start, then we need to mark a command start for concurrency metrics, and then issue the timeout } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/hystrix-exception-handling/","tags":["Java","Spring"],"title":"Hystrix工作原理三"},{"categories":["笔记"],"contents":"隔离策略 线程和线程池 客户端(库, 网络调用等)在各自的线程上运行. 这种做法将他们与调用线程隔开, 因此调用者可以从一个耗时的依赖调用\u0026quot;离开(walk away)\u0026quot;\nHystrix使用单独的, 每个依赖的线程池作为约束任何给定依赖的一种方式, 因此潜在执行的延迟将仅在该池中使可用线程饱和.\n如果不试用线程池可以保护你免受故障的影响, 但是这需要客户端可信任地快速失败(网络连接/读取超时, 重试的配置)并始终表现良好.\n在Hystrix的设计中, Netflix选择试用线程和线程池来达到隔离的目的, 原因有:\n 很多应用程序调用了由很多不同的团队开发的许多(有时超过1000)不同的后端服务 每个服务都各自提供了其客户端库 客户端库不断地在更新 客户端库可能被添加使用新的网络调用 客户端库的逻辑中可能包含重试, 数据解析, 缓存(内存或者跨网络)和其他类似的行为 客户端库更类似于一个黑盒, 其实现细节, 网络访问模式, 默认配置等是对使用者不透明的 在实际的生产问题中, 根源经常是 \u0026ldquo;有些东西改变了, 配置应该被修改\u0026rdquo; 或者 \u0026ldquo;客户端库修改了逻辑\u0026rdquo; 即使客户端没有改变, 服务端自身发生了变会员. 这种变化会是客户端设置无效而影响性能特性 传递依赖会引入其他客户端, 这些客户端不是可预期的, 也可能没有被正确地配置 大多数网络访问是同步的 失败和延迟也可能发生在客户端, 不只是网络调用  线程池的优势  该应用程序完全免受失控客户端库的保护. 给定依赖库的线程池可以填满而不会影响应用程序的其余部分. 应用程序可以接受风险低得多的新客户端库. 如果发生问题, 它会与其他依赖库隔离, 不会影响其他的依赖库 当发生故障的客户端再次健康时, 线程池将进行清理, 应用程序会立即恢复健康的性能, 而不是整个Tomcat容器不堪重负的长时间恢复. 如果客户端库配置错误, 线程池的运行状况将很快证明这一点(通过增加错误, 延迟, 超时, 拒绝等), 并且你可以在不影响应用程序功能的情况下处理它(通常通过动态属性进行实时修改). 如果客户端服务改变了性能特征(经常发生会以成为一个问题), 从而导致需要调整属性(增加/减少超时, 更改重试等), 这通过线程池指标(错误, 延迟, 超时, 拒绝), 并且可以在不影响其他客户端, 请求或用户的情况下进行处理. 除了隔离优势外, 拥有专用线程池还提供了内置并发性, 可用于在同步客户端库之上构建异步特性(类似于Netflix API在Hystrix命令之上构建反应式, 完全异步的Java API).  简而言之, 由线程池提供的隔离功能可以使客户端库和子系统性能特性的不断变化和动态组合得到适度处理, 而不会造成中断.\n注意: 尽管单独的线程提供了隔离, 但你的底层客户端代码也应该有超时 和/或 响应线程中断, 以便它不会无限制地阻塞并使Hystrix线程池饱和.\n线程池的缺点\n线程池的主要缺点是增加了计算开销, 每个Command的执行设计到队列, 调度和Command单独运行的线程的上下文的切换.\n在设计这个系统时, Netflix决定接受这种开销, 以换取其提供的好处, 并认为它足够小, 不会对成本或性能产生重大影响,.\n线程成本\nHystrix在子线程上执行construct()或run()方法时测量延迟, 以及父线程上的总端到端时间. 通过这种方式, 你可以看到Hystrix开销的成本(线程, 指标, 日志记录, 断路器等).\nNetflix API每天使用线程隔离处理10亿多Hystrix Command执行. 每个API实例都有40多个线程池, 每个线程池中有5-20个线程(大多数设置为10).\n信号量 你可以使用信号量(或计数器)来限制对任何给定依赖项的并发调用数量, 而不是使用线程池/队列大小. 这允许Hystrix在不使用线程池的情况下卸载负载. 如果你信任下客户端, 而你只想要卸载, 你可以使用这种方法.\nHystrixCommand和HystrixObservableCommand支持2个地方的信号量:\n回退: 当Hystrix执行回退时, 它总是在调用Tomcat线程上执行回退 执行: 如果将属性execution.isolation.strategy设置为SEMAPHORE, 则Hystrix将使用信号而不是线程来限制调用该命令的并发父线程的数量.\n你可以通过动态属性来配置这两种信号量的使用, 这些动态属性定义了可以执行多少个并发线程. 在调整线程池大小时, 你应该使用类似的计算来调整它们的大小(内存调用返回的次毫秒时间可以在信号量仅为1或2的情况下执行超过5000rps, 但默认值为10).\n一旦达到限制, 信号量拒绝将开始, 但填充信号量的线程不能离开.\n翻译自How it Works\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/hystrix-isolation/","tags":["Spring","Java"],"title":"Hystrix工作原理二"},{"categories":["笔记"],"contents":"运行时的流程图   构建HystrixCommand或者HystrixObservableCommand对象\n第一步是构建一个HystrixCommand或HystrixObservableCommand对象来代表对依赖服务所做的请求。 将在请求发生时将需要的任何参数传递给构造函数。\n如果依赖的服务预期会返回单一的响应, 构造一个HystrixCommand对象, 例如:\nHystrixCommand command = new HystrixCommand(arg1, arg2); 如果依赖的服务预期会返回一个发出响应的Observable对象, 则构造一个HystrixObservableCommand对象, 例如:\nHystrixObservableCommand command = new HystrixObservableCommand(arg1, arg2);   执行Command\n  响应是否被缓存?\n如果Command的缓存请求被开启, 同时请求的响应在缓存中可用, 缓存的响应被立即以一个Observable的方式返回.\n  断路器是否开启?\n执行Command时, Hystrix会检查断路器(circuti-breaker)是否开始回路(circuit).\n如果回路开启, Hystrix将不会执行Command, 而直接去到流程8: Get the Fallback 如果关闭, 则执行流程5检查是否有足够的容量来运行该命令\n  线程池/队列/限号量是否满?\n假如与Command相关的线程池和队列(或者信号量, 不适用隔离线程的话)满了, Hystrix将不会执行Command, 而是直接去到流程8.\n  HystrixObservableCommand.construct()或HystrixCommand.run()\nHystrix使用下面任一的方式向依赖的服务发出请求:\n HystrixCommand.run() 返回单个响应或抛出异常 HystrixObservableCommand.construct() 返回一个发出响应的Observable对象, 或者发送onError通知  如果run()或者construct()方法执行超过Command的超时设置, 线程会抛出一个TimeoutException(或者独立的timer线程抛出, 如果Command不是运行在它自己的线程上). 这是Hystrix直接去到流程8. 获取Fallback, 如果没有cancel/interrup, 则抛弃run()或construct()的最终返回值.\n请注意, 没有任何方法可以强制任务线程停止工作, 最佳的方式是Hystrix抛出一个InterruptException. 如果Hystrix封装的任务忽略InterruptException, 该任务线程会继续工作, 即使客户端已经收到了一个TimeoutException. 这种行为会是Hystrix的线程池饱和, 尽管负载正确地流出(correctly shed). 大多数Java HTTP客户端库不解释InterruptedExceptions. 因此, 请确保在HTTP客户端上正确配置连接和读/写超时.\n如果Command执行没有超时而返回一个响应, Hystrix在执行某些日志记录和指标报告之后返回这个响应.\n  计算电路健康\nHystrix将成功, 失败, 拒绝服务和超时上报给断路器, 断路器维护着一个计算统计数据的计数器.\n它通过这些统计数据决定断路器何时应该打开, 在哪个点开始短路后续的请求知道恢复期过去, 或者决定在第一次健康检查请求结束后是否要关闭断路器.\n  获取Fallback\n在command执行失败后: 当run()或construct()抛出异常(6), command因为断路器开启而短路(4), command的线程池和队列或者计数器处于满负荷(6), 或者执行超时, Hystrix尝试转向你的Fallback.\n  返回成功的响应\n如果Command处理成功, 它将以Obervable的实行返回response或者responses给调用者. 取决于上面流程2中的Command的执行方式, 该Observable可能在返回给你之前被转换:\n execute() - 返回一个Fature对象, 可以通过调用get()获取Obervable返回的单个值 queue() - 把Observable转换为BlockingObservable, 因此BlockingObservable可以被转换成Future, 并返回 observe() - 理解订阅Observable, 并开始Command的执行流程. 返回一个Observable, 当订阅它时, 重播返回和通知(replay emissions and notifiactions). toObservable() - 不变地返回Observable; 必须订阅它才能真正开始导致执行命令的流程.  更详细的流程图\n  断路器  假设通过断路器的负载达到了阈值 (HystrixCommandProperties.circuitBreakerRequestVolumeThreshold()) 假设错误百分比超过错误的阈值 (HystrixCommandProperties.circuitBreakerErrorThresholdPercentage()) 断路器状态从关闭变为打开 打开后, 断路器会短路所有针对该断路器的请求 过了一段时间后(HystrixCommandProperties.circuitBreakerSleepWindowInMilliseconds()), 下一条请求会被放行(半开状态). 如果请求失败, 断路器重回打开状态(OPEN)并持续一个睡眠窗口(sleep window). 如果成功, 状态变为关闭(CLOSED). 下个请求从逻辑1开始.  翻译自Hystrix Wiki - How it works\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/how-hystrix-works/","tags":["Java"],"title":"Hystrix工作原理一"},{"categories":["笔记"],"contents":"rsyslogd资源占用高问题记录 问题: openshift集群安装在esxi的虚拟机上. 各个节点出现问题, 集群响应很慢.\n kswapd0进程cpu 90%多. rsyslogd进程内存 90%多.  **先上总结: **\nsystem-journal服务监听/dev/logsocket获取日志, 保存在内存中, 并间歇性的写入/var/log/journal目录中.\nrsyslog服务启动后监听/run/systemd/journal/syslogsocket获取syslog类型日志, 并写入/var/log/messages文件中. 获取日志时需要记录日志条目的position到/var/lib/rsyslog/imjournal.state文件中.\n可能是虚拟机系统安装问题, 导致没有创建/var/lib/rsyslog. rsyslog将异常日志写入/dev/logsocket中.\n这样就导致了死循环, rsyslog因为要打开/var/log/messages并写入日志, 消耗cpu, 内存还有磁盘I/O.\n诊断步骤: rsyslog 重启rsyslog服务\n重启之后内存得到释放, 但是rsyslogd进程cpu跑到90%多, 且内存在持续升高.\n检查服务状态发现进程一直在报错:\nfopen() failed: 'Permission denied', path: '/imjournal.state.tmp' [try http://www.rsyslog.com/e/2013 ] fopen() failed: 'Permission denied', path: '/imjournal.state.tmp' [try http://www.rsyslog.com/e/2013 ] ... 检查/etc/rsyslog.conf中的WorkDirectory行是没有被注释的. 检查默认工作目录/var/lib/rsyslog, 发现目录不存在.\n因此创建/var/lib/rsyslog目录, 并赋予600权限.\n再次重启rsyslog服务, 观察一段时间没有错误抛出, /var/lib/rsyslog目录下创建了imjournal.state文件. 检查文件, 内容不断被刷新. 但是占用内存还在升高, /var/log/messages文件中还有错误信息写入. 但是错误日志的时间是比较早的.\n再次检查/etc/rsyslog.conf配置, 有一行配置:\n # Include all config files in /etc/rsyslog.d/ $IncludeConfig /etc/rsyslog.d/*.conf\n 目录中有文件/etc/rsyslog.d/listen.conf, 内容为$SystemLogSocketName /run/systemd/journal/syslog.\n分析:\n /run是linux内存中的数据 journal相关服务:systemd-journald.service.  systemd-journald.service systemd-journald是用来协助rsyslog记录系统启动服务和服务启动失败的情况等等. systemd-journald使用内存保存记录, 系统重启记录会丢失. 所有还要用rsyslog来记录分类信息, 如上面/etc/rsyslog.d/listen.conf中的syslog分类.\n~ systemctl list-sockets LISTEN UNIT ACTIVATES .... /dev/log systemd-journald.socket systemd-journald.service /run/systemd/journal/socket systemd-journald.socket systemd-journald.service /run/systemd/journal/stdout systemd-journald.socket systemd-journald.service .... 查看journal的配置/etc/systemd/jounal.conf, 最终还是会持久化到硬盘上的/var/log/journal目录中. 每个文件的大小是10M, 最多使用8G的空间, 同步间隔1s.\n[Journal] Storage=persistent Compress=True #Seal=yes #SplitMode=uid SyncIntervalSec=1s RateLimitInterval=1s RateLimitBurst=10000 SystemMaxUse=8G SystemMaxFileSize=10M #RuntimeKeepFree= #RuntimeMaxFileSize= MaxRetentionSec=1month ForwardToSyslog=False #ForwardToKMsg=no #ForwardToConsole=no ForwardToWall=False #TTYPath=/dev/console #MaxLevelStore=debug #MaxLevelSyslog=debug #MaxLevelKMsg=notice #MaxLevelConsole=info #MaxLevelWall=emerg 检查/var/log/journal目录, 发现里面文件很多, 每个大小为10m. 清空该目录并重启rsyslog, 观察一段时间后一切正常.\n参考:\n Redhat systemd-journald.service 简介 StackExchange  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/rsyslogd-high-cpu-trouble-shooting/","tags":["Linux","Openshift"],"title":"解决 rsyslogd 资源占用率高问题"},{"categories":["笔记"],"contents":"背景 Nginx运行在kubernets中, 反向代理service提供服务.\nkubernetes版本v1.9.1+a0ce1bc657.\n问题: 配置如下:\nlocation ^~/info { proxy_pass: http://serviceName:port; } 删除并重建Service的时候, nginx会出现下面的问题:\n connect() failed (113: No route to host) \u0026hellip; upstream: \u0026ldquo;xxxxx\u0026rdquo;\n 分析 通过google发现, 是nginx的dns解析方案的问题.\nnginx官方的说明:\n  If the domain name can’t be resolved, NGINX fails to start or reload its configuration. NGINX caches the DNS records until the next restart or configuration reload, ignoring the records’ TTL values. We can’t specify another load‑balancing algorithm, nor can we configure passive health checks or other features defined by parameters to the server directive, which we’ll describe in the next section.   意思是说, nginx在启动的时候就会解析proxy_pass后的域名, 并把ip缓存下来, 而且没有TTL. 只有在restart或者reload的时候才会再次解析.\n解决方案 使用nginx pod的解析服务器作为resolver:\n#nginx conf resolver NAME_SERVER valid=30s ipv6=off; set $service \u0026quot;http://serviceName:port\u0026quot;; location ^~/info { proxy_pass: $service; } 使用shell获取pod中使用的解析服务器\nNAME_SERVER=`cat /etc/resolv.conf | grep \u0026#34;nameserver\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | tr \u0026#39;\\n\u0026#39; \u0026#39; \u0026#39;` 参考: Nginx proxy_pass with $remote_addr\nNginx with dynamic upstreams\n另一个问题  serviceName could not be resolved (3: Host not found)\n service的短名称是解析不了的, 需要使用serviceName.namespace.svc.clusterName.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/nginx-dynamic-domain-parse-in-kubernetes/","tags":["Kubernetes","DevOps"],"title":"Kubernetes 中的 Nginx 动态解析"},{"categories":["源码解析"],"contents":"客户端负载均衡, Ribbon的核心概念是命名的客户端.\n使用 引入Ribbon依赖和配置 加入spring-cloud-starter-netflix-ribbon依赖\n代码中使用RibbonClient注解 @Configuration @RibbonClient(name = \u0026#34;foo\u0026#34;, configuration = FooConfiguration.class) public class TestConfiguration {} @Configuration protected static class FooConfiguration { @Bean public ZonePreferenceServerListFilter serverListFilter() { ZonePreferenceServerListFilter filter = new ZonePreferenceServerListFilter(); filter.setZone(\u0026#34;myTestZone\u0026#34;); return filter; } @Bean public IPing ribbonPing() { return new PingUrl(); } } Ribbon客户端的配置, 如果不指定会使用默认的实现:\n IClientConfig 客户端相关配置 IRule 定义负载均衡策略 IPing 定义如何ping目标服务实例来判断是否存活, ribbon使用单独的线程每隔一段时间(默认10s)对本地缓存的ServerList做一次检查 ServerList 定义如何获取服务实例列表. 两种实现基于配置的ConfigurationBasedServerList和基于Eureka服务发现的DiscoveryEnabledNIWSServerList ServerListFilter 用来使用期望的特征过滤静态配置动态获得的候选服务实例列表. 若未提供, 默认使用ZoneAffinityServerListFilter ILoadBalancer 定义了软负载均衡器的操作的接口. 一个典型的负载均衡器至少需要一组用来做负载均衡的服务实例, 一个标记某个服务实例不在旋转中的方法, 和对应的方法调用从实例列表中选出某一个服务实例. ServerListUpdater DynamicServerListLoadBalancer用来更新实例列表的策略(推EurekaNotificationServerListUpdater/拉PollingServerListUpdater, 默认是拉)  分析 类结构\n实现 实际使用中, 服务调用使用RestTemplate, 请求地址为http://\u0026lt;serviceName\u0026gt;/\u0026lt;path\u0026gt;, 如http://foo/ 通过@RibbonClient注解为服务创建ribbon客户端, 名字为方法名. RestTemplate发送请求的时候, 请求会被LoadBalancerInterceptor拦截到, 使用服务对应的ribbon客户端. Ribbon客户端的LoadBalancer会从ServerList中根据IRule的规则选择某个服务实例作为请求对象. ServerList有动态的实现, 更新列表时会使用ServerListFilter进行过滤.\nRibbonClient注解 从注释上@RibbonClient为一个ribbon客户端声明配置信息. 把这个注解加在任何@Configuration标注的类上, 然后注入SpringClientFactory来访问创建的客户端.\n从代码上看@RibbonClient引入了RibbonClientConfigurationRegistrar. RibbonClientConfigurationRegistrar实现了ImportBeanDefinitionRegistrar接口, 在@Configuration的解析极端调用接口的registerBeanDefinitions方法, 为ribbon客户端创建BeanDefinition 使用name/value和configuration创建一个BeanDefinition. Definition的名为\u0026lt;name\u0026gt;.RibbonClientSpecification, class为RibbonClientSpecification.\nFooConfiguration.class也要使用@Configuration注解, 然后通过RibbonClientConfigurationRegistrar关联到Ribbon客户端的BeanDefinition. 所以不能把FooConfiguration放到@ComponentScan的上下文中, 同样@SpringBootApplication也不行. 必要时使用exclude排除, 否则会变成所有Ribbon客户端共享.\nRibbonAutoConfiguration中在创建SpringClientFactorybean时, 会注入这些RibbonClientSpecification. SpringClientFactory继承了类NamedContextFactory. 从注释看NamedContextFactory可以创建一组子上下文, 每个子上下文中可以使用一组的Specification来定义bean. 对于Ribbon来说, 每个ribbon客户端各自为一个子上下文, @RibbonClient的configuration指定的配置, 就是用来构建该子上下文的配置, 最终被用来构建ribbon客户端. 这些上下文有共同的父上下文, 即ApplicationContext. 这就是为什么上面提到的FooConfiguration不能置于ApplicationContext中, 否则会被所有的Ribbon客户端共享配置.\nLoadBalancerAutoConfiguration配置类 通过RibbonAutoConfiguration引入, 定义了几个重要的bean:\n  LoadBalancerRequestFactory: 1) 将Http请求封装成ServiceRequestWrapper. ServiceRequestWrapper继承并重写了HttpRquestWrapper的getURI方法: 调用LoadBalancerClient的reconstructURI方法,创建实际请求的地址. 2) 如果有提供LoadBalancerRequestTransformer的实例, 则使用这些实例对相求进行响应的转换.   LoadBalancerInterceptor: Http请求拦截器, 将请求的host作为serviceName并使用LoadBalancerRequestFactory封装请求, 调用LoadBalancerClient的execute方法, 发送请求到真实的服务实例地址, 返回响应\n  RestTemplateCustomizer: 提供一个RestTemplateCustomizer的匿名类实现, 为所有的RestTemplate实例添加一个LoadBalancerInterceptor拦截器\n  RibbonAutoConfiguration配置类 通过spring.factories引入, RibbonAutoConfiguration定义了几个重要的bean:\n  SpringClientFactory: 使用@RibbonClient注解引入的ribbon客户端的配置, 构建ribbon客户端的子上下文, 初始化ribbon客户端bean. 四个get方法, 分别返回对应service的IClient, ILoadBalancer, IClientConfig, RibbonLoadBalancerContext实例.   LoadBalancerClient: 使用Spring Cloud提供的实现RibbonLoadBalancerClient. 通过SpringClientFactory创建一个ILoadBalancer实例, 通过ILoadBalancer返回一个Server实例. 使用Server实例.\n  reconstructURI(): 通过SpringClientFactory获取该服务ribbon客户端子上下文RibbonLoadBalancerContext对象, 调用RibbonLoadBalancerContext的reconstructURIWithServer方法构建最终的请求地址\n  choose(): 通过SpringClientFactory获取该服务的服务均衡器, 使用负载均衡器的IRule返回服务实例.\n  execute(): 执行最终的请求, 并记录状态: ServerStats和Stopwatch\n   ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-cloud-ribbon-breakdown-1/","tags":["Spring"],"title":"Spring Cloud Ribbon 详解"},{"categories":["教程","笔记"],"contents":"最近开始客串运维做CI/CD的规划设计, 主要是基于\u0026rsquo;Pipeline as Code in Jenkins'. 整理了下思路和技术点, 慢慢的写.\n这一篇是关于基于角色的授权策略, 用的是Role-Based Authorization Strategy Plugin.\n授权在CI/CD流程中比较常见, 比如我们只让某些特定用户才可以构建Pre-Release的Job. 而更高级的Release发布, 又会需要某些用户的审批才可以进行. 需要授权时, 可能还需要发邮件提醒用户.\nUI上如何使用就不提了, 这里只说Pipeline as Code. 后面的几篇也会是这个背景.\n参考的这篇文章, 文章里的代码运行失败, 做了修复.\n配置 安装完插件, 需要开始基于角色的授权策略. 同时添加角色和为用户分配角色.\n使用Role-Based Strategy作为验证方式 Manage Jenkins / Configure Global Security / Configure Global Security\n添加角色 Manage Jenkins / Manage and Assign Roles / Manage Roles / Global roles \n输入要添加的角色名\n为新加的角色配置权限 为用户指定角色 `Manage Jenkins / Manage and Assign Roles / Assign Roles / Global roles'\n输入已有的用户名\n分配角色\n编码 授权会在很多job里使用, 所以我们使用shared library来定义.\ntimeout(time: 5, unit: \u0026#39;MINUTES\u0026#39;) { notifyAwaitApproval approvers: getApprovers(\u0026#39;pre-release\u0026#39;), emailPrompt: \u0026#39;Build is ready to prepare release\u0026#39; } 给用户发送待审批邮件, 使用input等待用户的交互, 同时使用milestone阻塞后续的构建请求. notifyAwaitApproval.groovy\ndef call(options) { def jobName = env.JOB_NAME if(options.message == null) { options.message = \u0026#34;Action Required For Build $jobName\u0026#34; } def csvApproverUsernames = { switch (options.approvers) { case String: // already csv  return options.approvers case Map: // keys are usernames and values are names  return options.approvers.keySet().join(\u0026#39;,\u0026#39;) case ArrayList: case HashSet: return options.approvers.join(\u0026#39;,\u0026#39;) default: throw new Exception(\u0026#34;Unexpeced approver type ${options.approvers.class}!\u0026#34;) } }() echo \u0026#34;Notify approvers: $csvApproverUsernames for approval\u0026#34; // emailext needs to be inside a node block but don\u0026#39;t want to take up a node while waiting for approval  emailext body: \u0026#34;Action Required For Build \\${jobName} (#\\${env.BUILD_NUMBER})\u0026#34;, to: csvApproverUsernames, subject: \u0026#34;Action Required For Build \\${jobName} (#\\${env.BUILD_NUMBER})\u0026#34; milestone() input id: \u0026#39;Approval\u0026#39;, message: options.message, submitter: csvApproverUsernames, submitterParameter: \u0026#39;submitter\u0026#39; milestone() } 检查并获取Jenkins当前配置的授权策略, 如果是Role-Based Authorization, 返回拥有指定角色的用户列表. getApprovers.groovy\nimport com.cloudbees.groovy.cps.NonCPS import com.michelin.cio.hudson.plugins.rolestrategy.RoleBasedAuthorizationStrategy @NonCPS def call(role) { echo \u0026#34;Retrieving users for $role\u0026#34; def strategy = RoleBasedAuthorizationStrategy.instance; if (strategy != null) { return strategy.getGrantedRoles(RoleBasedAuthorizationStrategy.GLOBAL) .entrySet() .find { entry -\u0026gt; entry.key.getName().equals(role) }.getValue() } else { throw new Exception(\u0026#34;Role Strategy Plugin not in use. Please enable to retrieve users for a role\u0026#34;) } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/using-role-based-authorization-strategy-in-jenkins/","tags":["DevOps","Jenkins"],"title":"Jenkins CI/CD (一) 基于角色的授权策略"},{"categories":["笔记"],"contents":"添加虚拟机流程： 1. 配置网络 2. 配置存储池 3. 上传镜像 4. 安装虚拟机，指定配置  安装KVM虚拟机 1. 关闭防火墙，selinux # service iptables stop # setenforce 0 临时关闭 # chkconfig NetworkManager off 2. 安装kvm虚拟机 # yum install kvm libvirt libvirt-devel python-virtinst python-virtinst qemu-kvm virt-viewer bridge-utils virt-top libguestfs-tools ca-certificates audit-libs-python device-mapper-libs virt-install # 启动服务 # service libvirtd restart 下载virtio-win-1.5.2-1.el6.noarch.rpm 如果不安装window虚拟机或者使用带virtio驱动的镜像可以不用安装 # rpm -ivh virtio-win-1.5.2-1.el6.noarch.rpm 3. Libvirt在管理本地或远程Hypervisor时的表现形式如下。 在libvirt内部管理了五部分：\n 节点：所谓的节点就是我们的物理服务器，一个服务器代表一个节点，上边存放着Hyper和Domain Hypervisor：即VMM，指虚拟机的监控程序，在KVM中是一个加载了kvm.ko的标准Linux系统。 域（Domain）：指虚拟机，一个域代表一个虚拟机（估计思路来源于Xen的Domain0） 存储池（Storage Pool）：存储空间，支持多种协议和网络存储。作为虚拟机磁盘的存储源。 卷组（Volume）：虚拟机磁盘在Host上的表现形式。 上边的五部分，我们必须使用的是前三个，因为很多时候根据业务规则或应用的灵活性并没有使用卷组（其实就是有了编制的虚拟磁盘文件），也就没有必要使用存储池。  配置 1. 修改网络配置 方案一 (推荐)\nbrctl addbr br0 \u0026amp;\u0026amp; brctl addif br0 em1 \u0026amp;\u0026amp; brctl stp br0 on \u0026amp;\u0026amp; ifconfig em1 0.0.0.0 \u0026amp;\u0026amp; ifconfig br0 192.168.1.31 netmask 255.255.255.0 \u0026amp;\u0026amp; route add default gw 192.168.1.1 方案二\n参考\n# vim /etc/sysconfig/network-scripts/ifcfg-br0  DEVICE=br0 TYPE=Bridge BOOTPROTO=static BROADCAST=192.168.1.255 IPADDR=192.168.1.10 NETMASK=255.255.255.0 NETWORK=192.168.1.0 GATEWAY=192.168.1.1 DNS1=119.29.29.29 ONBOOT=yes\n # vim /etc/sysconfig/network-scripts/ifcfg-em1  DEVICE=em1 BOOTPROTO=none ONBOOT=yes BRIDGE=br0\n # vim /etc/sysconfig/network-scripts/ifcfg-bond0   DEVICE=bond0 TYPE=Ethernet NAME=bond0 BONDING_MASTER=yes BOOTPROTO=none BRIDGE=br0 ONBOOT=yes BONDING_OPTS=\u0026ldquo;mode=5 miimon=100\u0026rdquo;\n 方案三\n3. 关闭宿主机的GSO与TSO功能 # ethtool -K em1 gso off # ethtool -K em1 tso off # systemctl restart network 4. 创建基于文件夹的存储池（目录） # mkdir -p /home/vmdisk 定义存储池与其目录\n# virsh pool-define-as vmDiskPool --type dir --target /home/vmdisk Pool vmDiskPool defined 创建已定义的存储池\n# virsh pool-build vmDiskPool Pool vmDiskPool built 查看已定义的存储池，存储池不激活无法使用\n# virsh pool-list --all Name State Autostart ----------------------------------------- vmDiskPool inactive no 查看存储卷信息\n# virsh pool-info vmDiskPool Name: vmDiskPool UUID: 3fc996c4-9bfa-7fdc-2960-445e4c551855 State: inactive Persistent: yes Autostart: no 激活并自动启动已定义的存储池\n# virsh pool-autostart vmDiskPool Pool vmDiskPool marked as autostarted 启动存储卷\n# virsh pool-start vmDiskPool Pool vmDiskPool started 再次查看存储卷信息\n# virsh pool-info vmDiskPool Name: vmDiskPool UUID: 3fc996c4-9bfa-7fdc-2960-445e4c551855 State: running Persistent: yes Autostart: yes Capacity: 39.25 GiB Allocation: 47.89 MiB Available: 39.20 GiB 3. 在存储池中创建虚拟机存储卷（创建卷） # virsh vol-create-as vmDiskPool linux_vm0.qcow2 300G --format qcow2 查看存储卷\nll /home/vmdisk/ 存储池相关管理命令 删除存储池中的存储卷\n# virsh vol-delete --pool vmDiskPool linux_vm1.qcow2 取消激活存储池\n# virsh pool-destroy vmDiskPool 取消定义存储池\n# virsh pool-undefine vmDiskPool 删除存储池\n# virsh pool-delete vmDiskPool 4. 安装虚拟机 bridge网络模式\n# virt-install \\ --virt-type kvm \\ --os-type=linux \\ --os-variant=RHEL7 \\ --name=vm0 \\ --memory 16384 \\ --vcpus 6 \\ --disk path=/home/vmdisk/linux_vm0.qcow2,format=qcow2,bus=virtio \\ --location /root/CentOS-7-x86_64-Minimal-1708.iso \\ --graphics none \\ --network bridge=br0,model=virtio \\ --autostart \\ --boot cdrom,hd,menu=on \\ --console pty,target_type=serial \\ --extra-args \u0026#39;console=ttyS0,115200n8 serial\u0026#39; \\ --debug NAT网络模式\n# virt-install --name=test --ram 512 --vcpus=1 -f /data/kvm/vm/test.qcow2 --cdrom /data/iso/CentOS-6.5-x86_64-bin-DVD1.iso --graphics vnc,listen=0.0.0.0,port=5988, --network network=default,model=virtio --force --accelerate --autostart --boot cdrom,hd,menu=on 安装window主机\n# virt-install --name=window_24 --ram 12288 --vcpus 4 -c /data/iso/windows2008.iso --disk path=/usr/share/virtio-win/virtio-win-1.5.2.iso,device=cdrom --disk path=/data/kvm/vm/window_24.img,format=qcow2,bus=virtio --network bridge=br0,model=virtio --vnc --vncport=5924 --vnclisten=0.0.0.0 --force --autostart --os-type=windows --accelerate --boot cdrom,hd,menu=on 设置虚拟机网络\n# vi /etc/sysconfig/network-scripts/ifcfg-eth0 TYPE=Ethernet BOOTPROTO=static DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no NAME=eth0 DEVICE=eth0 ONBOOT=yes IPADDR=192.168.1.16 PREFIX=24 GATEWAY=192.168.1.1 DNS1=119.29.29.29 5. 启动 使用virsh list \u0026ndash;all查看已安装的kvm\n[root@localhost ~]# virsh list --all Id Name State ---------------------------------------------------- 5 test running 6. 动态调整cpu个数 # virsh setvcpus test --maximum 4 --config #设置test的最大cpu颗数 # virsh setvcpus test 3 #增加到3个CPU 注：使用上面命令修改，虚拟机重启修改的配置会丢失 # virsh edit test #修改CPU个数再保存配置，这样重启之后也会生效 7. 动态调整mem容量 # virsh setmaxmem test 2G --config #设置最大内存 # virsh setmem test 800M --config #重启后生效 # virsh setmem test 800M --config --live #马上生效 8. 查看虚拟化客户机的资源使用情况 # virt-top 9. 一些扩展命令 virsh命令行：\n# virsh list #显示本地活动虚拟机 # virsh list --all #显示本地所有的虚拟机（活动的+不活动的） # virsh define test.xml #通过配置文件定义一个虚拟机（这个虚拟机还不是活动的） # virsh start test #启动名字为test的非活动虚拟机 # virsh create test.xml #创建虚拟机（创建后，虚拟机立即执行，成为活动主机） # virsh suspend test #暂停虚拟机 # virsh resume test #启动暂停的虚拟机 # virsh shutdown test #正常关闭虚拟机 # virsh destroy test #强制关闭虚拟机 # virsh undefine test #清除虚拟机 # virsh dominfo test #显示虚拟机的基本信息 # virsh domname 2 #显示id号为2的虚拟机名 # virsh domid test #显示虚拟机id号 # virsh domuuid test #显示虚拟机的uuid # virsh domstate test #显示虚拟机的当前状态 # virsh dumpxml test #显示虚拟机的当前配置文件（可能和定义虚拟机时的配置不同，因为当虚拟机启动时，需要给虚拟机分配id号、uuid、vnc端口号等等） # virsh setmem test 512000 #给不活动虚拟机设置内存大小 # virsh setmaxmem test 1024000 #设定内存上限 # virsh setvcpus test 4 #给不活动虚拟机设置cpu个数 # virsh edit test #编辑配置文件（一般是在刚定义完虚拟机之后） # virsh vcpuinfo test #显示客户端的虚拟 CPU 信息。 # virsh vcpupin test #控制客户端的虚拟 CPU 亲和性。 # virsh domblkstat test #显示正在运行的客户端的块设备统计。 # virsh domifstat test #显示正在运行的客户端的网络接口统计。 # virsh attach-device test #使用 XML 文件中的设备定义在客户端中添加设备。 # virsh attach-disk test #在客户端中附加新磁盘设备。 # virsh attach-interface test #在客户端中附加新网络接口。 # virsh detach-device test #从客户端中分离设备，使用同样的 XML 描述作为命令attach-device # virsh detach-disk test #从客户端中分离磁盘设备。 # virsh detach-interface #从客户端中分离网络接口。 问题  Could not open \u0026lsquo;/root/CentOS-7-x86_64-Minimal-1708.iso\u0026rsquo;: Permission denied\n 修改/etc/libvirt/qemu.conf\n#取消注释 user = \u0026#34;root\u0026#34; group = \u0026#34;root\u0026#34; 重启libvirtd\nservice libvirtd restart\n error: 操作失败: 这个域有活跃控制台会话\n ps -ef|grep \u0026#39;console VMNAME|grep -v \u0026#39;grep\u0026#39; #然后kill掉相应的进行 ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kvm-installation-note/","tags":["Linux"],"title":"KVM 安装手册"},{"categories":["笔记"],"contents":"Jenkins CLI提供了SSH和Client模式.\nDocker运行Jenkins\nversion: \u0026#39;3\u0026#39; services: jenkins: image: jenkins/jenkins:alpine ports: - 8080:8080 - 50000:50000 - 46059:46059 volumes: - \u0026#34;/Users/addo/DevApps/Docker/data/jenkins:/var/jenkins_home\u0026#34; note: 以为是docker运行, ssh端口设置选用了固定端口.\nClient 从http://JENKINS_URL/cli页面下载client jar\n使用方法:\njava -jar jenkins-cli.jar -s http://localhost:8080/ help 构建:\njava -jar jenkins-cli.jar -s http://localhost:8080/ build JOB [-c] [-f] [-p] [-r N] [-s] [-v] [-w] Starts a build, and optionally waits for a completion. Aside from general scripting use, this command can be used to invoke another job from within a build of one job. With the -s option, this command changes the exit code based on the outcome of the build (exit code 0 indicates a success) and interrupting the command will interrupt the job. With the -f option, this command changes the exit code based on the outcome of the build (exit code 0 indicates a success) however, unlike -s, interrupting the command will not interrupt the job (exit code 125 indicates the command was interrupted). With the -c option, a build will only run if there has been an SCM change. JOB : Name of the job to build -c : Check for SCM changes before starting the build, and if there\u0026#39;s no change, exit without doing a build -f : Follow the build progress. Like -s only interrupts are not passed through to the build. -p : Specify the build parameters in the key=value format. -s : Wait until the completion/abortion of the command. Interrupts are passed through to the build. -v : Prints out the console output of the build. Use with -s -w : Wait until the start of the command SSH 启用SSH, 使用随机端口. 也可以使用固定端口:\n获取SSH端口号:\ncurl -Lv http://localhost:8080/login 2\u0026gt;\u0026amp;1 | grep \u0026#39;X-SSH-Endpoint\u0026#39; #\u0026lt; X-SSH-Endpoint: localhost:46059 生成ssh key:\nssh-keygen -t rsa -b 4096 -C \u0026#34;admin\u0026#34; 添加ssh公钥:\nhttp://localhost:8080/user/admin/configure 修改ssh config, 添加如下配置:\nHost localhost IdentityFile ~/.ssh/id_rsa_jenkins_cli Port 46059 验证:\nssh admin@localhost help add-job-to-view Adds jobs to view. build Builds a job, and optionally waits until its completion. cancel-quiet-down Cancel the effect of the \u0026#34;quiet-down\u0026#34; command. clear-queue Clears the build queue. connect-node Reconnect to a node(s) console Retrieves console output of a build. copy-job Copies a job. ... ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/jenkins-cli-enable/","tags":["Jenkins","DevOps"],"title":"启用Jenkins CLI"},{"categories":["笔记"],"contents":"因为ProcessTreeKiller的存在, 构建过程中使用shell启动的进程在Job完成时都会被kill掉.\n各种搜索以及ProcessTreeKiller提供的解决方式是修改BUILD_ID和添加 -Dhudson.util.ProcessTree.disable=true都无法解决.\n最后参考StackOverflow和Jenkins JIRA, 修改JENKINS_NODE_COOKIE为任何值, 如dontKillMe. 这种方法可以解决, 记录一下. (搜索排名靠前的结果都不对).\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/resolve-process-be-killed-after-jenkins-job-done/","tags":["Jenkins","DevOps"],"title":"Jenkins - 解决execute shell中启动的进程被在Job退出时被杀死问题"},{"categories":["云原生","笔记"],"contents":"MacOS环境安装minishift\n安装minishift cli brew cask install minishift 使用virtualbox安装 安装的时候可以指定HTTP代理, 拉取墙外镜像时需要; 还可以指定insecure的镜像库.\nminishift start --docker-env HTTP_PROXY=\u0026#34;192.168.99.1:1087\u0026#34; --docker-env HTTPS_PROXY=\u0026#34;192.168.99.1:1087\u0026#34; --docker-env NO_PROXY=\u0026#34;192.168.0.0/16,172.30.0.0/16\u0026#34; --insecure-registry=\u0026#34;192.168.1.34\u0026#34; --vm-driver=virtualbox 启动 minishift start --vm-driver=virtualbox 删除 minishift delete 打开Openshift控制面板 minishift dashboard 获取集群ip地址 minishift ip 安装Openshift Cli brew install openshift-cli 可以使用openshift cli进行操作. minishift安装完成后会将配置信息写入到主机的用户目录下, $HOME/.kube目录下除了config信息, 还有openshift的集群信息及支持的api.\noc login -u system:admin oc get pods --all-namespaces ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-minishift-on-mac/","tags":["Kubernetes","Openshift","macOS"],"title":"macOS 安装 minishift"},{"categories":["笔记","源码解析"],"contents":"Spring Cloud对Netflix Zuul做了封装集成, 使得在Spring Cloud环境中使用Zuul更方便. Netflix Zuul相关分析请看上一篇.\n实现 @EnableZuulProxy 与 @EnableZuulServer 二者的区别在于前者使用了服务发现作为路由寻址, 并使用Ribbon做客户端的负载均衡; 后者没有使用. Zuul server的路由都通过ZuulProperties进行配置.\n具体实现:  使用ZuulController(ServletWrappingController的子类)封装ZuulServlet实例, 处理从DispatcherServlet进来的请求. ZuulHandlerMapping负责注册handler mapping, 将Route的fullPath的请求交由ZuulController处理. 同时使用ServletRegistrationBean注册ZuulServlet, 默认使用/zuul作为urlMapping. 所有来自以/zuul开头的path的请求都会直接进入ZuulServlet, 不会进入DispatcherServlet.  使用注解   @EnableZuulProxy引入了ZuulProxyMarkerConfiguration, ZuulProxyMarkerConfiguration只做了一件事, 实例化了内部类Marker.\n@Configuration public class ZuulProxyMarkerConfiguration { @Bean public Marker zuulProxyMarkerBean() { return new Marker(); } class Marker { } }   @EnableZuulServer引入了ZuulServerMarkerConfiguration, ZuulServerMarkerConfiguration也只做了一件事: 实例化了内部类Marker\n@Configuration public class ZuulServerMarkerConfiguration { @Bean public Marker zuulServerMarkerBean() { return new Marker(); } class Marker { } }   EnableAutoConfiguration 项目中使用@EnableAutoConfiguration注解, 开启Spring上下文对象的自动配置功能, 尝试去猜测和实例化你可能需要的bean.\n这个功能是基于classPath来完成的. 比如: 项目中引用了tomcat-embedded.jar, 你可能需要一个TomcatEmbeddedServletContainerFactory实例, 除非定义了自己的EmbeddedServletContainerFactory实例.\n我们来接着看, 在spring-cloud-netflix-core的spring.factories中的org.springframework.boot.autoconfigure.EnableAutoConfiguration实现中我们可以找到org.springframework.cloud.netflix.zuul.ZuulProxyAutoConfiguration和org.springframework.cloud.netflix.zuul.ZuulServerAutoConfiguration\n  ZuulServerAutoConfiguration 它的初始化条件有两个:\n @ConditionalOnClass(ZuulServlet.class)指定classpath中需要有ZuulServlet.class. 这个servlet负责对所有进入Zuul server的请求以及配置应用指定的preRoute, route, postRoute和error. @ConditionalOnBean(ZuulServerMarkerConfiguration.Marker.class) 与@EnableZuulServer注解呼应.  ​java @Configuration @EnableConfigurationProperties({ ZuulProperties.class }) @ConditionalOnClass(ZuulServlet.class) @ConditionalOnBean(ZuulServerMarkerConfiguration.Marker.class) // Make sure to get the ServerProperties from the same place as a normal web app would @Import(ServerPropertiesAutoConfiguration.class) public class ZuulServerAutoConfiguration { ... } ​\n  ZuulProxyAutoConfiguration 它有一个初始化的条件@ConditionalOnBean(ZuulProxyMarkerConfiguration.Marker.class), 就是上下文中需要有ZuulProxyMarkerConfiguration.Marker这个内部类的bean. 与@EnableZuulProxy注解呼应.\n初始化包括内置的filter, 以及Discovery, Ribbon等的初始化.\n@Configuration @Import({ RibbonCommandFactoryConfiguration.RestClientRibbonConfiguration.class, RibbonCommandFactoryConfiguration.OkHttpRibbonConfiguration.class, RibbonCommandFactoryConfiguration.HttpClientRibbonConfiguration.class }) @ConditionalOnBean(ZuulProxyMarkerConfiguration.Marker.class) public class ZuulProxyAutoConfiguration extends ZuulServerAutoConfiguration { ... } ​\n  ZuulServerAutoConfiguration 详解 //声明配置 @Configuration //配置ZuulProperties实例 @EnableConfigurationProperties({ ZuulProperties.class }) //条件1 存在ZuulServlet.class @ConditionalOnClass(ZuulServlet.class) //条件2 存在ZuulServerMarkerConfiguration.Marker.class bean, 即应用使用@EnableZuulServer注解 @ConditionalOnBean(ZuulServerMarkerConfiguration.Marker.class) //配置ServerProperties实例 // Make sure to get the ServerProperties from the same place as a normal web app would @Import(ServerPropertiesAutoConfiguration.class) public class ZuulServerAutoConfiguration { @Autowired protected ZuulProperties zuulProperties; @Autowired protected ServerProperties server; @Autowired(required = false) private ErrorController errorController; @Bean public HasFeatures zuulFeature() { return HasFeatures.namedFeature(\u0026#34;Zuul (Simple)\u0026#34;, ZuulServerAutoConfiguration.class); } //复合结构的RouteLocator \t@Bean @Primary public CompositeRouteLocator primaryRouteLocator( Collection\u0026lt;RouteLocator\u0026gt; routeLocators) { return new CompositeRouteLocator(routeLocators); } //没有SimpleRouteLocator.class的bean时, 使用zuulProperties实例化一个SimpleRouteLocator实例. \t@Bean @ConditionalOnMissingBean(SimpleRouteLocator.class) public SimpleRouteLocator simpleRouteLocator() { return new SimpleRouteLocator(this.server.getServletPrefix(), this.zuulProperties); } //zuulController, 包装了一个ZuulServlet类型的servlet, 实现对ZuulServlet类型的servlet的初始化. \t@Bean public ZuulController zuulController() { return new ZuulController(); } @Bean public ZuulHandlerMapping zuulHandlerMapping(RouteLocator routes) { ZuulHandlerMapping mapping = new ZuulHandlerMapping(routes, zuulController()); mapping.setErrorController(this.errorController); return mapping; } @Bean public ApplicationListener\u0026lt;ApplicationEvent\u0026gt; zuulRefreshRoutesListener() { return new ZuulRefreshListener(); } @Bean @ConditionalOnMissingBean(name = \u0026#34;zuulServlet\u0026#34;) public ServletRegistrationBean zuulServlet() { ServletRegistrationBean servlet = new ServletRegistrationBean(new ZuulServlet(), this.zuulProperties.getServletPattern()); // The whole point of exposing this servlet is to provide a route that doesn\u0026#39;t \t// buffer requests. \tservlet.addInitParameter(\u0026#34;buffer-requests\u0026#34;, \u0026#34;false\u0026#34;); return servlet; } // pre filters  @Bean public ServletDetectionFilter servletDetectionFilter() { return new ServletDetectionFilter(); } @Bean public FormBodyWrapperFilter formBodyWrapperFilter() { return new FormBodyWrapperFilter(); } @Bean public DebugFilter debugFilter() { return new DebugFilter(); } @Bean public Servlet30WrapperFilter servlet30WrapperFilter() { return new Servlet30WrapperFilter(); } // post filters  @Bean public SendResponseFilter sendResponseFilter() { return new SendResponseFilter(); } @Bean public SendErrorFilter sendErrorFilter() { return new SendErrorFilter(); } @Bean public SendForwardFilter sendForwardFilter() { return new SendForwardFilter(); } @Bean @ConditionalOnProperty(value = \u0026#34;zuul.ribbon.eager-load.enabled\u0026#34;, matchIfMissing = false) public ZuulRouteApplicationContextInitializer zuulRoutesApplicationContextInitiazer( SpringClientFactory springClientFactory) { return new ZuulRouteApplicationContextInitializer(springClientFactory, zuulProperties); } @Configuration protected static class ZuulFilterConfiguration { @Autowired private Map\u0026lt;String, ZuulFilter\u0026gt; filters; @Bean public ZuulFilterInitializer zuulFilterInitializer( CounterFactory counterFactory, TracerFactory tracerFactory) { FilterLoader filterLoader = FilterLoader.getInstance(); FilterRegistry filterRegistry = FilterRegistry.instance(); return new ZuulFilterInitializer(this.filters, counterFactory, tracerFactory, filterLoader, filterRegistry); } } @Configuration @ConditionalOnClass(CounterService.class) protected static class ZuulCounterFactoryConfiguration { @Bean @ConditionalOnBean(CounterService.class) public CounterFactory counterFactory(CounterService counterService) { return new DefaultCounterFactory(counterService); } } @Configuration protected static class ZuulMetricsConfiguration { @Bean @ConditionalOnMissingBean(CounterFactory.class) public CounterFactory counterFactory() { return new EmptyCounterFactory(); } @ConditionalOnMissingBean(TracerFactory.class) @Bean public TracerFactory tracerFactory() { return new EmptyTracerFactory(); } } private static class ZuulRefreshListener implements ApplicationListener\u0026lt;ApplicationEvent\u0026gt; { @Autowired private ZuulHandlerMapping zuulHandlerMapping; private HeartbeatMonitor heartbeatMonitor = new HeartbeatMonitor(); @Override public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ContextRefreshedEvent || event instanceof RefreshScopeRefreshedEvent || event instanceof RoutesRefreshedEvent) { this.zuulHandlerMapping.setDirty(true); } else if (event instanceof HeartbeatEvent) { if (this.heartbeatMonitor.update(((HeartbeatEvent) event).getValue())) { this.zuulHandlerMapping.setDirty(true); } } } } } ZuulProxyAutoConfiguration 详解 ​```java //声明配置 @Configuration //引入RibbonCommandFactory配置 @Import({ RibbonCommandFactoryConfiguration.RestClientRibbonConfiguration.class, RibbonCommandFactoryConfiguration.OkHttpRibbonConfiguration.class, RibbonCommandFactoryConfiguration.HttpClientRibbonConfiguration.class, HttpClientConfiguration.class }) //配置生效条件 @ConditionalOnBean(ZuulProxyMarkerConfiguration.Marker.class) public class ZuulProxyAutoConfiguration extends ZuulServerAutoConfiguration { @SuppressWarnings(\u0026quot;rawtypes\u0026quot;) @Autowired(required = false) private List\u0026lt;RibbonRequestCustomizer\u0026gt; requestCustomizers = Collections.emptyList(); //网关服务注册实例信息 @Autowired(required = false) private Registration registration; //服务发现客户端 @Autowired private DiscoveryClient discovery; //serviceId和路由的映射逻辑, 默认为相同 @Autowired private ServiceRouteMapper serviceRouteMapper; @Override public HasFeatures zuulFeature() { return HasFeatures.namedFeature(\u0026quot;Zuul (Discovery)\u0026quot;, ZuulProxyAutoConfiguration.class); } //静态和动态路由寻址: 静态从配置文件获取, 动态通过服务发现客户端完成. 后者优先级更高 @Bean @ConditionalOnMissingBean(DiscoveryClientRouteLocator.class) public DiscoveryClientRouteLocator discoveryRouteLocator() { return new DiscoveryClientRouteLocator(this.server.getServletPrefix(), this.discovery, this.zuulProperties, this.serviceRouteMapper, this.registration); } //装饰过滤器 // pre filters @Bean public PreDecorationFilter preDecorationFilter(RouteLocator routeLocator, ProxyRequestHelper proxyRequestHelper) { return new PreDecorationFilter(routeLocator, this.server.getServletPrefix(), this.zuulProperties, proxyRequestHelper); } //基于Ribbon路由过滤器 // route filters @Bean public RibbonRoutingFilter ribbonRoutingFilter(ProxyRequestHelper helper, RibbonCommandFactory\u0026lt;?\u0026gt; ribbonCommandFactory) { RibbonRoutingFilter filter = new RibbonRoutingFilter(helper, ribbonCommandFactory, this.requestCustomizers); return filter; } //基于host的路由过滤器 @Bean @ConditionalOnMissingBean({SimpleHostRoutingFilter.class, CloseableHttpClient.class}) public SimpleHostRoutingFilter simpleHostRoutingFilter(ProxyRequestHelper helper, ZuulProperties zuulProperties, ApacheHttpClientConnectionManagerFactory connectionManagerFactory, ApacheHttpClientFactory httpClientFactory) { return new SimpleHostRoutingFilter(helper, zuulProperties, connectionManagerFactory, httpClientFactory); } @Bean @ConditionalOnMissingBean({SimpleHostRoutingFilter.class}) public SimpleHostRoutingFilter simpleHostRoutingFilter2(ProxyRequestHelper helper, ZuulProperties zuulProperties, CloseableHttpClient httpClient) { return new SimpleHostRoutingFilter(helper, zuulProperties, httpClient); } //服务发现寻址刷新监听器 @Bean public ApplicationListener\u0026lt;ApplicationEvent\u0026gt; zuulDiscoveryRefreshRoutesListener() { return new ZuulDiscoveryRefreshListener(); } @Bean @ConditionalOnMissingBean(ServiceRouteMapper.class) public ServiceRouteMapper serviceRouteMapper() { return new SimpleServiceRouteMapper(); } @Configuration @ConditionalOnMissingClass(\u0026quot;org.springframework.boot.actuate.endpoint.Endpoint\u0026quot;) protected static class NoActuatorConfiguration { @Bean public ProxyRequestHelper proxyRequestHelper(ZuulProperties zuulProperties) { ProxyRequestHelper helper = new ProxyRequestHelper(); helper.setIgnoredHeaders(zuulProperties.getIgnoredHeaders()); helper.setTraceRequestBody(zuulProperties.isTraceRequestBody()); return helper; } } @Configuration @ConditionalOnClass(Endpoint.class) protected static class EndpointConfiguration { @Autowired(required = false) private TraceRepository traces; @ConditionalOnEnabledEndpoint(\u0026quot;routes\u0026quot;) @Bean public RoutesEndpoint routesEndpoint(RouteLocator routeLocator) { return new RoutesEndpoint(routeLocator); } @ConditionalOnEnabledEndpoint(\u0026quot;routes\u0026quot;) @Bean public RoutesMvcEndpoint routesMvcEndpoint(RouteLocator routeLocator, RoutesEndpoint endpoint) { return new RoutesMvcEndpoint(endpoint, routeLocator); } @ConditionalOnEnabledEndpoint(\u0026quot;filters\u0026quot;) @Bean public FiltersEndpoint filtersEndpoint() { FilterRegistry filterRegistry = FilterRegistry.instance(); return new FiltersEndpoint(filterRegistry); } @Bean public ProxyRequestHelper proxyRequestHelper(ZuulProperties zuulProperties) { TraceProxyRequestHelper helper = new TraceProxyRequestHelper(); if (this.traces != null) { helper.setTraces(this.traces); } helper.setIgnoredHeaders(zuulProperties.getIgnoredHeaders()); helper.setTraceRequestBody(zuulProperties.isTraceRequestBody()); return helper; } } private static class ZuulDiscoveryRefreshListener implements ApplicationListener\u0026lt;ApplicationEvent\u0026gt; { private HeartbeatMonitor monitor = new HeartbeatMonitor(); @Autowired private ZuulHandlerMapping zuulHandlerMapping; @Override public void onApplicationEvent(ApplicationEvent event) { if (event instanceof InstanceRegisteredEvent) { reset(); } else if (event instanceof ParentHeartbeatEvent) { ParentHeartbeatEvent e = (ParentHeartbeatEvent) event; resetIfNeeded(e.getValue()); } else if (event instanceof HeartbeatEvent) { HeartbeatEvent e = (HeartbeatEvent) event; resetIfNeeded(e.getValue()); } } private void resetIfNeeded(Object value) { if (this.monitor.update(value)) { reset(); } } private void reset() { this.zuulHandlerMapping.setDirty(true); } } } ​```  配置项 zuul.servletPath 默认为*/zuul*, 注册ZuulServlet的时候作为urlMapping使用. 即所有来自以*/zuul*开头的path都会由ZuulServlet处理.\nzuul.ignoredPatterns Zuul使用ZuulController封装了ZuulServlet. 所有进入Zuul的请求的入口都是ZuulController. ZuulController的ZuulHandlerMapping默认把zuul.routes.[ITEM].path的请求交给ZuulServlet处理. 如果找不到对应的path的route, 则会走其他的DispatcherServlet\nzuul.ignoredPatterns作用就是进入Zuul的请求, 只要match都会直接交由其他的DispatcherServlet处理, 而不需要先检查是否有对应path的route.\n\u0026hellip;\n过滤器 ZuulServerAutoConfiguration ServletDetectionFilter 检查请求的入口是DispatcherServlet还是ZuulServlet 如果是DispatcherServlet进来的请求, 将RequestContext中的属性isDispatcherServletRequest设置为ture.\n检查的方法是判断RequestContext中的请求类型是否为HttpServletRequestWrapper类型, 因为ZuulServlet进来的请求会使用HttpServletRequestWrapper进行再次封装; 同时检查请求中中是否有DispatcherServlet.CONTEXT属性, 因为DispatcherServlet进来的请求会带有该属性.\nFormBodyWrapperFilter 为下游的服务解析表单数据, 并重新编码. 只针对multipart/form-data和application/x-www-form-urlencoded类型的请求.\nDebugFilter 通过设置zuul.debug.parameter属性控制, 默认启用. 执行时将上下文中的debugRouting和debugRequest设置为true\nServlet30WrapperFilter 使用Servlet30RequestWrapper封装请求, 强制启用.\nSendResponseFilter 后执行的过滤器, 负责将代理请求的响应写入当前的请求的响应中.\nZuulProxyAutoConfiguration PreDecorationFilter Pre类型的过滤器, 通过提供的RouteLocator决定将如何请求路由到哪里和如何路由. 同时为下游请求添加多个与代理相关的头信息. 当RequestContext中不存在FORWARD_TO_KEY和SERVICE_ID_KEY信息时生效.\n将路由判断结果写入routeHost, FORWARD_TO_KEY或者SERVICE_ID_KEY.\nRibbonRoutingFilter Route类型的过滤器, 当RequestContext中routeHost为空, 且有serviceId值时生效.\n使用RequestContext构建RibbonCommandContext, 通过RibbonCommandFactory进而创建RibbonCommand并执行. 最后通过ProxyRequestHelper将响应结果记录到RequestContext中.\nSimpleHostRoutingFilter Route类型的过滤器, 当RequestContext中的routeHost不为空时生效. 使用Apache的HttpClient发送请求\n监听器 ZuulRefreshListener 通过监听应用程序事件(ContextRefreshedEvent, RefreshScopeRefreshedEvent, RoutesRefreshedEvent和RoutesRefreshedEvent)更新handler mapping的注册信息. 前两个事件在ContextRefresh时发出; 第三个是通过JMX重置路由时发出(参考RoutesMvcEndpoint); 最后一个是DiscoveryClient每次拉取服务注册信息后发出.\n收到事件后, 将ZuulHandlerMapping的dirty变量置为true, 当下次请求进来时, 检查到dirty为true, 就会重新注册url mapping.\nZuulDiscoveryRefreshListener 监听应用程序事件(InstanceRegisteredEvent, ParentHeartbeatEvent和HeartbeatEvent)更新handler mapping的注册信息.\nInstanceRegisteredEvent当前路由服务实例完成服务注册后发出的事件. ParentHeartbeatEvent当DiscoveryClient定位到Config Server服务的时候有bootstrapContext发给应用程序上下文的事件. HeartbeatEvent由DiscoveryClient每次拉取服务注册信息后发出.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-cloud-zuul-breakdown/","tags":["Spring"],"title":"Spring Cloud Zuul详解"},{"categories":["源码解析"],"contents":"之前分析过Spring Cloud的Eureka服务发现, 今天分析一下服务注册.\n配置 BootstrapConfiguration EurekaDiscoveryClientConfigServiceBootstrapConfiguration spring-cloud-config环境中使用的配置\n引入EurekaDiscoveryClientConfiguration和EurekaClientAutoConfiguration\nEurekaDiscoveryClientConfiguration  在spring-cloud中(通过是否存在RefreshScopeRefreshedEvent.class判断), 添加RefreshScopeRefreshedEvent的listener. 收到事件后重新注册实例. 在eureka.client.healthcheck.enabled设置为true时, 注册EurekaHealthCheckHandlerbean. EurekaHealthCheckHandler负责将应用状态映射为实例状态InstanceStatus.  EurekaClientAutoConfiguration 支持spring-cloud和非spring-cloud环境, 在spring-cloud环境中, 下面两个bean要使用@RefreshScope标注\n 实例化EurekaClientbean, 在spring-cloud中使用实现类CloudEurekaClient. 使用EurekaInstanceConfig实例, 实例化ApplicationInfoManagerbean  EnableAutoConfiguration EurekaClientConfigServerAutoConfiguration 在spring-cloud-config的环境中, 将configPath加入到实例的metadata map中.\nEurekaDiscoveryClientConfigServiceAutoConfiguration 当config客户端希望通过服务发现寻找config服务的时候使用的引导配置\n在spring.cloud.config.discovery.enabled为true时, 关闭父application context里实例化的EurekaClient实例. 只使用当前上下文里的实例.\nEurekaClientAutoConfiguration 核心配置 支持spring-cloud(支持动态配置)和非spring-cloud环境, EurekaClient和EurekaInstanceConfig两个bean要使用@RefreshScope标注\n 实例化当前服务实例信息EurekaInstanceConfigBean的实例 实例化DiscoveryClient的实现, 在这里是EurekaDiscoveryClient 实例化EurekaServiceRegistry 实例化EurekaRegistration 实例化EurekaAutoServiceRegistration, 这个类实现了StartLifecycle接口. 在ApplicationContext refresh或者shutdown之后注册或者注销当前实例 实例化EurekaClientbean, 在spring-cloud中使用实现类CloudEurekaClient 使用EurekaInstanceConfig实例, 实例化ApplicationInfoManagerbean  RibbonEurekaAutoConfiguration 当启用Eureka client(eureka.client.enable为true和ribbon.eureka.enabled为true时, 默认为true)时配置默认基于eureka的ribbon. 使用EurekaRibbonClientConfiguration提供的配置: RibbonPing, ServerList, ServerIntrospector.\nEurekaDiscoveryClientConfiguration 提供监听RefreshScopeRefreshedEvent的监听器, 当事件发生时注销并重新注册(防止metadata发生改变) 提供一个默认的EurekaHealthCheckHandler实例, 当bean不存在的且eureka.client.healthcheck.enabled为true时.\n主要类 ServiceRegistry spring提供的服务实例注册和注销的接口.\nEurekaServiceRegistry ServiceRegistry的Eureka实现. 注册和注销的实现是通过EurekaRegistration的ApplicationInfoManager修改实例状态实现的.\nEurekaRegistration 通过实现ServiceInstance提供访问实例信息的接口,\nApplicationInfoManager 提供修改实例状态的接口, 并通知状态变化的监听器. 提供内部类StatusChangeListener. 提供注册和注销状态变化监听器的接口, Eureka的DiscoveryClient中通过匿名类的方式实现了该接口, 当实例状态发生变化时, 刷新实例状态.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-cloud-service-registry-via-eureka/","tags":["Spring"],"title":"Spring Cloud - Eureka服务注册"},{"categories":["笔记"],"contents":"嵌入式的zuul代理\n使用了Netfilx OSS的其他组件:\n Hystrix 熔断 Ribbon 负责发送外出请求的客户端, 提供软件负载均衡功能 Trubine 实时地聚合细粒度的metrics数据 Archaius 动态配置  介绍 由于2.0停止开发且会有bug, 故下面的分析基于1.x版本.\n特性  Authentication 认证 Insights 洞察 Stress Testing 压力测试 Canary Testing 金丝雀测试 Dynamic Routing 动态路由 Multi-Region Resiliency 多区域弹性 Load Shedding 负载脱落 Security 安全 Static Response handling 静态响应处理 Multi-Region Resiliency 主动/主动流量管理  Zuul核心架构 过滤器加载器 从文件目录定时的监控文件, 编译成Class并加载到过滤器链中.\n贯穿整个请求的RequestContext 将Servlet的请求和响应初始化成RequestContext, 保存在ThreadLocal中贯穿整个请求.\n以及添加Netfix库的指定概念和数据的扩展对象NFRequestContext, 如Eureka\n四种过滤器:  preRoute route postRoute error  Zuul请求生命周期 Zuul Netflix 使用Netflix的其他组件\nzull在Netfilx的应用 精确路由 创建一个过滤器是特定的用户或者设备的请求重定向到独立的API集群达到调试的目的.\n多区域弹 Zuul是我们称为地峡(Isthmus)的多地区ELB弹性项目的核心. 作为Isthmus的一部分, Zuul被用来将请求从西海岸数据中心传送到东海岸, 以帮助我们在我们的关键领域的ELB中实现多区域冗余.\n压力测试 在Zuul过滤器中使用动态Archaius配置逐步提升进入一部分服务器的流量, 自动实现压力测试.\n原理 如何工作 StartServer初始化 实现了ServletContextListener接口, 如果需要与netflix oss其他组件集成(如Eureka, Archaius)实例化的时候启动一个Karyon服务器.\n在ServletContext初始化完成后调用initGroovyFilterManager和initJavaFilters.\ninitGroovyFilterManager 向过滤器注册表中添加Groovy过滤器.\nprivate void initGroovyFilterManager() { //设置GroovyCompiler  //GroovyCompiler是DynamicCompiler的实现类  FilterLoader.getInstance().setCompiler(new GroovyCompiler()); //从配置中是获取过滤器源文件的根目录  String scriptRoot = System.getProperty(\u0026#34;zuul.filter.root\u0026#34;, \u0026#34;\u0026#34;); if (scriptRoot.length() \u0026gt; 0) scriptRoot = scriptRoot + File.separator; try { //设置文件名过滤器, 这里只过滤`.groovy`类型文件.  FilterFileManager.setFilenameFilter(new GroovyFileFilter()); //初始化过滤器文件管理器  //第一个参数是扫描目录的间隔时间, 单位为秒  //后面跟要扫描的子目录  //1. 初始化的时候会扫描各个子目录, 使用文件名过滤器获取到所有的过滤器源文件.  //2. 遍历这些文件, 使用`FilterLoader.getInstance().putFilter(file)`, compiler编译之后使用FilterFactory进行实例化, 并添加到过滤器注册表中. 是否实例化的逻辑判断是否在上次修改且文件最后修改时间是否相同. 如果是上次修改之后又有改动, 要重建改类型过滤器的列表. 如果没有修改, 对改文件不做任何处理.  //3. 启动线程, 每个5秒执行一个1和2的操作.  FilterFileManager.init(5, scriptRoot + \u0026#34;pre\u0026#34;, scriptRoot + \u0026#34;route\u0026#34;, scriptRoot + \u0026#34;post\u0026#34;); } catch (Exception e) { throw new RuntimeException(e); } } initJavaFilters 向过滤器注册表中添加Java过滤器.\n*官方没有提供从java源代码到classs的编译器.\nZuulServlet 核心zuul servlet, 初始化和卸掉zullFilter的运行. 使用ZuulRunner将Servlet的请求和响应初始化成RequestContext, 并将FilterProcessor的调用包装成preRoute(), route(), postRoute()和error()方法. 初始化时可以选择将请求包装成HttpServletRequestWrapper并缓冲请求消息体.\n初始化后的RequestContext会放在ThreadLocal中, 供后续的filter访问.\nService方法\npublic void service(javax.servlet.ServletRequest servletRequest, javax.servlet.ServletResponse servletResponse) throws ServletException, IOException { try { init((HttpServletRequest) servletRequest, (HttpServletResponse) servletResponse); // Marks this request as having passed through the \u0026#34;Zuul engine\u0026#34;, as opposed to servlets  // explicitly bound in web.xml, for which requests will not have the same data attached  RequestContext context = RequestContext.getCurrentContext(); context.setZuulEngineRan(); try { preRoute(); } catch (ZuulException e) { error(e); postRoute(); return; } try { route(); } catch (ZuulException e) { error(e); postRoute(); return; } try { postRoute(); } catch (ZuulException e) { error(e); return; } } catch (Throwable e) { error(new ZuulException(e, 500, \u0026#34;UNHANDLED_EXCEPTION_\u0026#34; + e.getClass().getName())); } finally { RequestContext.getCurrentContext().unset(); } } 通过FilterProcessor.getInstnace()调用FilterProcessor的preRoute(), route(), postRoute()和error()方法.\n四个方法都是通过FilterLoader.getInstance()获取对应类型的filter列表.\n遍历filter列表, 调用filter的runFilter()方法.\n/** * runFilter checks !isFilterDisabled() and shouldFilter(). The run() method is invoked if both are true. * * @return the return from ZuulFilterResult */ public ZuulFilterResult runFilter() { ZuulFilterResult zr = new ZuulFilterResult(); //动态获取`zuul.filerClassName.filterType.disable`的值  //动态获取使用Archaius的DynamicPropertyFactory获取*, 通过这个可实现动态配置  if (!isFilterDisabled()) { //调用filter类的校验逻辑  if (shouldFilter()) { Tracer t = TracerFactory.instance().startMicroTracer(\u0026#34;ZUUL::\u0026#34; + this.getClass().getSimpleName()); try { //执行filter的逻辑处理  Object res = run(); //执行成功  zr = new ZuulFilterResult(res, ExecutionStatus.SUCCESS); } catch (Throwable e) { t.setName(\u0026#34;ZUUL::\u0026#34; + this.getClass().getSimpleName() + \u0026#34; failed\u0026#34;); //执行失败  zr = new ZuulFilterResult(ExecutionStatus.FAILED); zr.setException(e); } finally { t.stopAndLog(); } } else { //filter不适用, 直接跳过  zr = new ZuulFilterResult(ExecutionStatus.SKIPPED); } } return zr; } ContextLifecycleFilter 清空ThreadLocal中的RequestContext\npublic void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException { try { chain.doFilter(req, res); } finally { RequestContext.getCurrentContext().unset(); } } 调试 调试信息中的名词\n ZUUL_DEBUG 输出zuul的诊断信息 REQUEST_DUBG 输出Http请求的信息. REQUEST -\u0026gt; ZUUL -\u0026gt; ORIGIN_RESPONSE -\u0026gt; OUTBOUND  REQUEST 进入zuul的请求 ZUUL zuul转发给原目标的请求 ORIGIN_RESPONSE 原目标返回的原始响应 OUTBOND zuul返回给客户端的响应    接口和类 接口 DynamicCodeCompiler 从源代码编译成Classes的接口, 目前只有一个GroovyCompiler实现类\nFilterFactory 生成给定的过滤器类实例的接口, 实现类DefaultFilterFactory\nFilterUsageNotifier 注册过滤器使用时的回调的接口\n类 DefaultFilterFactory 使用反射实现\npublic ZuulFilter newInstance(Class clazz) throws InstantiationException, IllegalAccessException { return (ZuulFilter) clazz.newInstance(); } FilterFileManager 从过滤器目录中获取修改和新增的Groovy过滤器文件.\nFilterLoader 持有过滤器注册表, 加载过滤器.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/learn-netflix-zuul/","tags":["Java","Spring"],"title":"初识 Netflix Zuul"},{"categories":["笔记"],"contents":"为什么要讨论这个问题, 工作中一个同事写的类使用了ConfigurationProperties, 只提供了标准的setter方法. 属性的访问, 提供了定制的方法. 可以参考EurekaClientConfigBean.\n他使用的是spring boot 2.0.0.M5版本, 可以正常获取配置文件中的属性值, 但是在1.5.8.RELEASE获取不到.\n看下文档和源码:\n Annotation for externalized configuration. Add this to a class definition or a @Bean method in a @Configuration class if you want to bind and validate some external Properties (e.g. from a .properties file).\n 外置配置的注解. 当需要绑定外置配置(如properties或者yaml配置)的时候, 将其加到使用了@Configuration注解的类声明处或者@Bean标注的方法上.\n值的绑定是通过ConfigurationPropertiesBindingPostProcessor在bean实例创建后, 初始化回调(如InitializingBean的afterPropertiesSet方法)或者init-method之前之前完成的.\n1.5.x 执行绑定的时候如果找不到getter方法, 会抛出RelaxedBindingNotWritablePropertyException异常. debug模式下, 会打印Ignoring benign property binding failure.\n2.x 2.x版本中, 如果找不到getter方法, 会将原值默认为null, 并继续执行绑定.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/configurationproperties-requires-getter-or-not/","tags":["Spring","Java"],"title":"ConfigurationProperties到底需不需要getter"},{"categories":["笔记"],"contents":"并发模式 runner runner展示了如何使用通道来监视程序的执行时间, 如果程序执行时间太长, 也可以用终止程序. 这个程序可用作corn作业执行\npackage runner import ( \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;log\u0026#34; ) type Runner struct { //系统信号通道 \tinterrupt chan os.Signal //任务执行结果通道 \tcomplete chan error //报告任务处理已经超时 \ttimeout \u0026lt;-chan time.Time tasks []func(int) } //超时错误 var ErrTimeout = errors.New(\u0026#34;received timeout\u0026#34;) //系统终端错误 var ErrInterrupt = errors.New(\u0026#34;received interrupt\u0026#34;) //返回一个新的准备使用的Runner func New(d time.Duration) *Runner { return \u0026amp;Runner{ interrupt: make(chan os.Signal, 1), complete: make(chan error), timeout: time.After(d), //After函数会使用goroutine启动一个timer, timer时间到后向channel写入Time \t} } //向Runner中添加task func (r *Runner) AddTask(tasks ... func(int)) { r.tasks = append(r.tasks, tasks...) } func (r *Runner) Start() error { //希望接收所有终端信号 \tsignal.Notify(r.interrupt, os.Interrupt) go func() { //使用goroutine执行任务 \tr.complete \u0026lt;- r.run() }() select { //main线程在select处阻塞, 要么等待任务执行结果结束, 要么等待计时器报告超时 \tcase err := \u0026lt;-r.complete://阻塞等待任务执行结果 \treturn err case \u0026lt;-r.timeout: //阻塞等待超时报告 \treturn ErrTimeout } } func (r *Runner) run() error { for id, task := range r.tasks { //检测是否有来自操作系统的终端信号 \tif r.getInterrupted() { return ErrInterrupt } //执行任务 \ttask(id) } return nil } func (r *Runner) getInterrupted() bool { //使用default将select的阻塞变成非阻塞. 每次方法调用只是检查通道中是否有数据, 不阻塞 \tselect { case \u0026lt;-r.interrupt: return true default: return false } } package main import ( \u0026#34;time\u0026#34; \u0026#34;os\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/addozhang/learning-go-lang/runner\u0026#34; ) func main(){ log.Println(\u0026#34;Starting working.\u0026#34;) const timeout = 3 * time.Second r := runner.New(timeout) r.AddTask(createTask(), createTask(), createTask()) if err := r.Start(); err != nil { switch err { case runner.ErrTimeout: log.Println(\u0026#34;Terminating due to timeout.\u0026#34;) os.Exit(1) case runner.ErrInterrupt: log.Println(\u0026#34;Terminating due to interrupt.\u0026#34;) os.Exit(2) } } log.Println(\u0026#34;Process ended.\u0026#34;) } //创建任务, 返回接受int类型参数的函数 func createTask() func(int){ return func(id int) { log.Printf(\u0026#34;Processor - Task #%d\u0026#34;, id) time.Sleep(time.Duration(id) * time.Second) } } //创建任务, 返回接受int类型参数的函数 func createTask() func(int){ return func(id int) { log.Printf(\u0026#34;Processor - Task #%d\u0026#34;, id) time.Sleep(time.Duration(id) * time.Second) } } //结果输出 //2018/01/01 09:45:57 Starting working. //2018/01/01 09:45:57 Processor - Task #0 //2018/01/01 09:45:57 Processor - Task #1 //2018/01/01 09:45:58 Processor - Task #2 //2018/01/01 09:46:00 Terminating due to timeout. pool 下面的代码展示如何使用有缓冲通道实现资源池, 以1.5版本为基础写的. 1.6之后的版本, 标准库中自带了资源池的实现sycn.Pool\npackage pool import ( \u0026#34;sync\u0026#34; \u0026#34;io\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;log\u0026#34; ) type Pool struct { m sync.Mutex //互斥锁用于安全地方访问资源池 \tresources chan io.Closer //资源池通道, 需要实现io.Closer接口 \tfactory func() (io.Closer, error) //创建资源的工厂方法 \tclosed bool //资源池是否关闭 } //资源池关闭错误 var ErrPoolClosed = errors.New(\u0026#34;Pool has ben closed.\u0026#34;) func New(fn func() (io.Closer, error), size int) (*Pool, error) { if size \u0026lt;= 0 { return nil, errors.New(\u0026#34;Size value too small.\u0026#34;) } return \u0026amp;Pool{ resources: make(chan io.Closer, size), //使用有缓冲资源池 \tfactory: fn, }, nil } //从池中获取资源 func (p *Pool) Acquire() (io.Closer, error) { select { case res, ok := \u0026lt;-p.resources: //从资源池通道获取一个资源, 因为有default, 不阻塞 \tlog.Println(\u0026#34;Acqure: \u0026#34;, \u0026#34;Shared Resources\u0026#34;) if !ok { return nil, ErrPoolClosed } return res, nil default: //资源池通道没有数据时, 新建一个 \tlog.Println(\u0026#34;Acquire: \u0026#34;, \u0026#34;New Resource\u0026#34;) return p.factory() } } //释放资源 func (p *Pool) Release(res io.Closer) { p.m.Lock() //需要使用互斥锁操作资源池 \tdefer p.m.Unlock() if p.closed { // \tres.Close() return } select { case p.resources \u0026lt;- res: //将资源放回通道. 如果通道满不会阻塞, 因为有default \tlog.Println(\u0026#34;Release: \u0026#34;, \u0026#34;In Queue\u0026#34;) default: //如果通道已满, 直接关闭资源 \tlog.Println(\u0026#34;Release: \u0026#34;, \u0026#34;Closing\u0026#34;) res.Close() } } //关闭资源池 func (p *Pool) Close() { p.m.Lock() //加互斥锁 \tdefer p.m.Unlock() if p.closed { return } //将池关闭 \tp.closed = true //在清空通道资源之前关闭通道, 如果不关闭会发声死锁 \tclose(p.resources) for res := range p.resources { res.Close() //关闭通道中的资源 \t} } package main import ( \u0026#34;github.com/addozhang/learning-go-lang/pool\u0026#34; \u0026#34;log\u0026#34; \u0026#34;io\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; \u0026#34;math/rand\u0026#34; ) const ( maxGoRoutines = 25 pooledResources = 2 ) type dbConnection struct { ID int32 } var idCounter int32 func (dbConn *dbConnection) Close() error { log.Println(\u0026#34;Close: Connection, \u0026#34;, dbConn.ID) return nil } func createConnection() (io.Closer, error) { id := atomic.AddInt32(\u0026amp;idCounter, 1) log.Println(\u0026#34;Create: New Connection\u0026#34;, id) return \u0026amp;dbConnection{id}, nil } func main() { var wg sync.WaitGroup wg.Add(maxGoRoutines) p, err := pool.New(createConnection, pooledResources) if err != nil { log.Println(err) } for query := 0; query \u0026lt; maxGoRoutines; query++ { go func(q int) { performQuery(q, p) wg.Done() }(query) } wg.Wait() log.Println(\u0026#34;Shutdown Program.\u0026#34;) p.Close() } func performQuery(query int, pool *pool.Pool) { dbConn, err := pool.Acquire() if err != nil { log.Println(err) } defer dbConn.Close() time.Sleep(time.Duration(rand.Intn(1000)) * time.Millisecond) log.Printf(\u0026#34;QID[%d] CID[%d]\u0026#34;, query, dbConn.(*dbConnection).ID) } work 下面的代码展示了如何使用无缓冲通道来创建一个goroutine池. 这个goroutine执行并控制一组工作, 让其并发执行.\npackage worker import \u0026#34;sync\u0026#34; type Worker interface { Task() } type Pool struct { worker chan Worker wg sync.WaitGroup } func New(maxRoutines int) *Pool { p := Pool{ worker: make(chan Worker), } p.wg.Add(maxRoutines) for i := 0; i \u0026lt; maxRoutines; i++ { go func() { for w := range p.worker { w.Task() } p.wg.Done() }() } return \u0026amp;p; } func (p *Pool) Run(w Worker) { p.worker \u0026lt;- w } func (p *Pool) Shutdown() { close(p.worker) p.wg.Wait() } package main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; worker2 \u0026#34;github.com/addozhang/learning-go-lang/worker\u0026#34; \u0026#34;sync\u0026#34; ) var names = []string{ \u0026#34;bob\u0026#34;, \u0026#34;steve\u0026#34;, \u0026#34;mary\u0026#34;, \u0026#34;therese\u0026#34;, \u0026#34;json\u0026#34;, } type namePrinter struct { name string } func (np *namePrinter) Task() { log.Print(np.name) time.Sleep(time.Second / 10) } func main() { p := worker2.New(2) var wg sync.WaitGroup wg.Add(100 * len(names)) for i := 0; i \u0026lt; 100; i++ { for _, name := range names{ np := namePrinter{name} go func() { p.Run(\u0026amp;np) wg.Done() }() } } wg.Wait() p.Shutdown() } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/go-in-action-four/","tags":["Go"],"title":"Go In Action 读书笔记 四"},{"categories":["学习"],"contents":"并发 Go语言里的并发是指让某个函数可以独立于其他函数运行的能力. 当一个函数创建为goroutine时, Go会将其视为一个独立的工作单元. 这个工作单元会被调度到可用的逻辑处理器上执行.\nGo的运行时调度器可以管理所有创建的goroutine, 并为其分配执行时间. 这个调度器在操作系统之上, 将操作系统的线程与逻辑处理器绑定, 并在逻辑处理器执行goroutine. 调度器可以在任何给定的时间, 全面控制哪个goroutine在哪个逻辑处理器上运行.\nGo的并发同步模型来自一个叫做通信顺序进程(Communicating Sequential Processes, CSP). CSP是一个消息传递模型, 通过在goroutine之前传递数据来传递消息, 不需要通过加锁实现同步访问. 用于在goroutine间传递消息的数据结构叫做通道(channel).\n并发与并行 操作系统的线程(thread)和进程(process).\n进程类似应用程序在运行中需要用到和维护的各种资源的容器. 资源包括但不限于: 内存(来自文件系统的代码和数据), 句柄(文件, 设备, 操作系统), 线程.\n每个进程至少有一个线程, 一个线程是一个执行空间. 这个空间会被操作系统调度来运行函数中所写的代码. 每个线程的初始线程被称为主线程. 主线程终止时, 应用程序也会终止.操作系统将线程调度到某个处理器上运行, 这个处理器不一定是进程所在的处理器.\nGo语言的运行时会在逻辑处理器上调度goroutine运行. 每个逻辑处理器都分别绑定到单个操作系统线程. Go语言的运行时默认会为每个可用的物理处理器分配一个逻辑处理器.\n创建一个gorouine并准备运行, 这个goroutine就会被放到调度器的全局运行队列中. 之后, 调度器就将这些队列中的goroutine分配给一个逻辑处理器, 并放到该逻辑处理器对应的本地运行队列, 然后在队列中等待被逻辑处理器执行.\n如果goroutine执行了阻塞线程的调用, 调度器会将这个操作系统线程与逻辑处理器分离, 并创建一个新的线程与逻辑处理器绑定, 然后. 一旦阻塞的调用完成, 该goroutine会回到本地运行队列.\n如果阻塞调用是网络I/O, goroutine会与逻辑处理器分离, 移到集成了网络轮询器的运行时. 一旦轮询器指示某个网络的读或写操作已经就绪, 对应的goroutine就会重新分配到逻辑处理器上完成操作.\n调度器对可以创建的逻辑处理器的数量没有限制, 但是语言运行时默认限制每个程序最多创建10000个线程. 可以通过调用runtime/debug包的SetMaxThreads方法来更改.\n并发(concurrency)不是并行(parallelism) 并行是让不同的代码同时在不同的物理处理器上执行. 并行的关键是同时做很多事. 并发是指同时管理很多事情, 这些事情可能只做一般就再暂停去做别的事情了.\n使用较少的资源做更多的事情\n多个逻辑处理器时, goroutine会被平均分配到每个逻辑处理器上, 让goroutine在不同的线程上运行.\ngoroutine import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;runtime\u0026#34; ) func main() { //分配一个逻辑处理器给调度器使用 \truntime.GOMAXPROCS(1) var wg sync.WaitGroup wg.Add(2) fmt.Printf(\u0026#34;%s\\n\u0026#34;, \u0026#34;Start\u0026#34;) go func(){ defer wg.Done() for i :=0 ; i \u0026lt; 3; i++ { for ch := \u0026#39;a\u0026#39;; ch \u0026lt; \u0026#39;a\u0026#39; + 26; ch ++ { fmt.Printf(\u0026#34;%c \u0026#34;, ch) } } }() go func(){ defer wg.Done() for i :=0 ; i \u0026lt; 3; i++ { for ch := \u0026#39;A\u0026#39;; ch \u0026lt; \u0026#39;A\u0026#39; + 26; ch ++ { fmt.Printf(\u0026#34;%c \u0026#34;, ch) } } }() fmt.Println(\u0026#34;Wait\u0026#34;) wg.Wait() fmt.Println(\u0026#34;\\nEnd\u0026#34;) } //结果 //Start //Wait //A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z //End  //第一个goroutine完成所有显示需要的时间太短, 以至于在调度器切换到第二个goroutine之前就完成了所有任务. 程序可以使用runtime.GOMAXPROCS来更改调度器可以下使用的逻辑处理器的数量. 如果不想代码里使用, 可以使用跟函数同名的环境变量(GOMAXPROCS)来设置. 使用runtime.NumCPU()可以获取物理处理器的个数.\nWaitGroup是一个计数信号量, 可以用来记录并维护运行的goroutine. 使用defer在goroutine函数调用完成后调用Done方法.\n一个正在运行的goroutine在工作结束前, 可以被停止(回到本地队列)并重新调度. 防止某个goroutine长时间占用逻辑处理器.\n竞争状态 race condition: 多个goroutine在没有互相同步的情况系啊, 访问某个共享的资源, 并试图同时读和写这个资源, 存在竞争的状态.\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;runtime\u0026#34; ) var ( counter int wg sync.WaitGroup ) func main() { wg.Add(2) go incCounter(1) go incCounter(2) wg.Wait() fmt.Println(\u0026#34;Final counter: \u0026#34;, counter) } func incCounter(id int) { defer wg.Done() for i:= 0; i \u0026lt; 2; i++ { val := counter //当前goroutine从线程退出, 并回到队列 \truntime.Gosched() val++ counter = val } } //结果 //Final counter: 2 非原子操作导致最后结果为2\n锁住共享资源 使用atomic和sync包的函数\n原子函数 import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;runtime\u0026#34; ) var ( counter int64 wg sync.WaitGroup ) func main() { wg.Add(2) go incCounter(1) go incCounter(2) wg.Wait() fmt.Println(\u0026#34;Final counter: \u0026#34;, counter) } func incCounter(id int) { defer wg.Done() for i:= 0; i \u0026lt; 2; i++ { //安全地对counter加1 \tatomic.AddInt64(\u0026amp;counter, 1) //当前goroutine从线程退出, 并回到队列 \truntime.Gosched() } } atomic包的AddInt64函数, 会同步整型值的加法, 方法是强制同一时刻只能有一个goroutine运行并完成这个加法操作. 还有LoadInt64和StoreInt64函数, 提供安全的读写整型值的方式.\n互斥锁 使用互斥锁mutex, 名字来自互斥mutual exclusion的概念. 在代码上创建一个链接去, 保证同一时间只有一个goroutine可以执行这个临界区的代码.\n临界区的代码可以使用大括号{}包围, 提升可读性.\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;runtime\u0026#34; ) var ( counter int64 wg sync.WaitGroup mutex sync.Mutex ) func main() { wg.Add(2) go incCounter(1) go incCounter(2) wg.Wait() fmt.Println(\u0026#34;Final counter: \u0026#34;, counter) } func incCounter(id int) { defer wg.Done() for i:= 0; i \u0026lt; 2; i++ { //创建临界区 \tmutex.Lock() { val := counter //当前goroutine从线程退出, 并回到队列 \truntime.Gosched() val++ counter = val } mutex.Unlock() } } 通道 当一个资源需要在goroutine之间共享时, 通道在goroutine之前架起了一个管道, 并提供了确保同步交换数据的机制.\n声明通道时需要指定要共享的数据类型, 包括共享内置类型, 命名类型, 结构类型和引用类型的值或者指针.\n需要使用关键字make创建通道. make的第一个参数需要关键字chan, 之后跟着交换的数据的类型. 如果是创建的有缓冲的通道, 第二个参数要指定通道的缓冲区的大小.\n//无缓冲的整形通道 unbuffered := make(chan int) //有缓冲的字符串通道 buffered := make(chan string, 10) 通道操作\n//写字符串到通道 buffered \u0026lt;- \u0026#34;Gopher\u0026#34; //从通道接收一个字符串 value := \u0026lt;- buffered 无缓冲通道 unbuffered channel是指在接收前没有能力保存任何值的通道. 这种通道要求发送goroutine和接收goroutine同时准备好, 才能完成发送和接收操作. 如果没有同时准备好, 会导致先执行发送或接收操作的goroutine阻塞等待.\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; \u0026#34;math/rand\u0026#34; ) var wg sync.WaitGroup func init(){ rand.Seed(time.Now().UnixNano()) } func main() { wg.Add(2) court := make(chan int) go player(\u0026#34;Lisa\u0026#34;, court) go player(\u0026#34;Bill\u0026#34;, court) court \u0026lt;- 1 wg.Wait() } func player(name string, court chan int) { defer wg.Done() for { ball, ok := \u0026lt;- court if !ok { fmt.Printf(\u0026#34;Player %s Won\\n\u0026#34;, name) return } n := rand.Intn(100) if n%13 == 0 { fmt.Printf(\u0026#34;Player %s missed\\n\u0026#34;, name) close(court) return } fmt.Printf(\u0026#34;Player %s Hit %d\\n\u0026#34;, name, ball) ball++ court \u0026lt;- ball } } 有缓冲通道 buffered channel是一种在被接收前能存储一个或者多个值的通道. 并不要求goroutine之间必须同时完成发送和接收.\n只有在缓冲区里没有数据的时候接收才会阻塞; 同样只有缓冲区满的时候发送才会阻塞.\nimport ( \u0026#34;sync\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;time\u0026#34; \u0026#34;fmt\u0026#34; ) const ( workers = 4 taskLoad = 10 ) var wg sync.WaitGroup func init() { rand.Seed(time.Now().UnixNano()) } func main() { tasks := make(chan string, taskLoad) wg.Add(workers) for i := 0; i \u0026lt; workers; i++ { go worker(tasks, i) } for i := 0; i \u0026lt; taskLoad; i++ { tasks \u0026lt;- fmt.Sprintf(\u0026#34;Task : %d\u0026#34;, i) } close(tasks) wg.Wait() } func worker(tasks chan string, id int) { defer wg.Done() for { task, ok := \u0026lt;-tasks if !ok { fmt.Printf(\u0026#34;Work %d shutting down\\n\u0026#34;, id) return } fmt.Printf(\u0026#34;Worker: %d : Started %s\\n\u0026#34;, id, task) sleep := rand.Int63n(100) time.Sleep(time.Duration(sleep) * time.Millisecond) fmt.Printf(\u0026#34;Worker : %d : Completed %s\\n\u0026#34;, id, task) } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/go-in-action-three/","tags":["Go"],"title":"Go In Action 读书笔记 三"},{"categories":["笔记"],"contents":"Go语言的类型系统 Go语言是静态类型的变成语言. 编译的时候需要确定类型.\n用户定义的类型 type user struct { name string email string ext int privileged bool } 使用 零值和结构字面量初始化\n//引用类型, 各个字段初始化为对应的零值 var bill user #{ 0 false} //创建并初始化, 使用结构字面量 lisa := user{ //{Lisa lisa@email.com 123 true}  name: \u0026#34;Lisa\u0026#34;, email: \u0026#34;lisa@email.com\u0026#34;, ext: 123, privileged: true, } 结构字面量的赋值方式:\n 不同行声明每一个字段和对应的值, 字段名和字段以:分隔, 末尾以,结尾 不适用字段名, 只声明对应的值. 写在一行里, 以,分隔, 结尾不需要,. 要保证顺序  lisa := {\u0026#34;Lisa\u0026#34;, \u0026#34;lisa@email.com\u0026#34;, 123, true} 使用其他类型结构声明字段\ntype admin struct { person user level string } fred := admin{ //{{Fred fred@email.com 123 true} super}  person: user{ name: \u0026#34;Fred\u0026#34;, email: \u0026#34;fred@email.com\u0026#34;, ext: 123, privileged: true, }, level: \u0026#34;super\u0026#34;, } 另一种声明用户定义的类型的方法是, 基于一个已有的类型, 将其作为新类型的类型说明 新的类型是独立的类型, 值互相兼容, 但不能互相赋值.\ntype Duration int64 var d Duration //d = int64(1000) #编译错误cannot use int64(1000) (type int64) as type Duration in assignment d = Duration(1000) 方法 描述用户自定义类型的行为, 实际为函数. 只是在声明的时候在func和方法名之间增加了一个参数(接收者), 将函数和接收者的类型绑定到一起.\ntype user struct { name string email string } func (u user) notify() { fmt.Printf(\u0026#34;Sending User Email To %s\u0026lt;%s\u0026gt;\\n\u0026#34;, u.name, u.email) } func (u *user) changeEmail(email string) { u.email = email } func main() { bill := user{\u0026#34;Bill\u0026#34;, \u0026#34;bill@email.com\u0026#34;} bill.notify() lisa := \u0026amp;user{\u0026#34;Lisa\u0026#34;, \u0026#34;lisa@email.com\u0026#34;} lisa.notify() //实际执行 (*lisa).notify()  bill.changeEmail(\u0026#34;bill@newDomain.com\u0026#34;) //实际执行 (\u0026amp;bill).changeEmail(\u0026#34;bill@newDomain.com\u0026#34;) \tbill.notify() lisa.changeEmail(\u0026#34;lisa@newDomain.com\u0026#34;) lisa.notify() //实际执行 (*lisa).notify() } //Sending User Email To Bill\u0026lt;bill@email.com\u0026gt; //Sending User Email To Lisa\u0026lt;lisa@email.com\u0026gt; //Sending User Email To Bill\u0026lt;bill@newDomain.com\u0026gt; //Sending User Email To Lisa\u0026lt;lisa@newDomain.com\u0026gt; Go语言里有两种类型的接收者: 值接收者和指针接收者.\n 如果使用值接收者, 调用的时候会使用值的副本来执行 如果使用指针接收者, 调用的时候这个方法会共享调用方法时接收者所指向的值  类型的本质 声明类型的方法前要确定该方法是创建一个新值(使用值接收者), 还是修改当前值(使用指针接收者)\n内置类型 由语言提供: 数值类型, 布尔类型, 字符串类型. 本质上是原始类型. 对这些值增加或删除操作的死后, 都会创建新的值.\n如 golang.org/src/strings/strings.go的Trim函数传入字符串值, 返回新的字符串.\nfunc Trim(s string, cutset string) string { if s == \u0026#34;\u0026#34; || cutset == \u0026#34;\u0026#34; { return s } return TrimFunc(s, makeCutsetFunc(cutset)) } 引用类型 Go语言里有几种: 切片, 映射, 通道, 接口和函数类型.\n声明上述类型的变量时, 创建的变量被称作标头(header)值. 每个引用类型创建的标头值是包含一个指向底层数据结构的指针. 标头值里包含一个指针, 通过复制来传递一个引用类型的值得副本, 本质是就是在共享底层数据结构.\ngolang.org/src/net/ip.go\ntype IP []byte 结构类型 描述一组数据值, 这组值可以是原始类型, 也可以是非原始的. 结构类型的本质是非原始的. 对这个类型的值做增加或者删除的操作应该更改值本身. 当需要修改值本身时, 在程序中其他地方, 需要使用指针来共享这个值.\ngolang.org/time/time.go\ntype Time struct{ wall uint64 ext int64 loc *Location } func Now() Time{ ... } //值接收者, 返回新的Time func (time Time) Add(d Duration) Time{ ... } func (time Time) String() string{ ... } //指针接收者 func (t *Time) UnmarshalBinary(data []byte) error { ... } 如果一个创建用的工厂函数返回了一个指针, 就表示这个被返回的值的本质是非原始的. golang.org/src/os/file.go的open函数.\ntype File struct { *file //内嵌类型: 嵌入的指针, 指向一个未公开的类型  //一种保护的方式 } type file struct { pfd poll.FD name string dirinfo *dirInfo // nil unless directory being read \tnonblock bool // whether we set nonblocking mode } func OpenFile(name string, flag int, perm FileMode) (*File, error) { ... } 接口 多态是指代码可以根据类型的具体实现采取不同行为的能力. 如果一个类型实现了某个接口, 所有使用这个接口的地方, 都可以支持这种类型的值.\n标准库 golang.org/src/io/io.go\ntype Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type WriterTo interface { WriteTo(w Writer) (n int64, err error) } func main() { //bytes.Buffer实现了io.Reader, io.WriteTo接口 \tvar b bytes.Buffer b.Write([]byte(\u0026#34;Hello\u0026#34;)) fmt.Fprintf(\u0026amp;b, \u0026#34;World!\u0026#34;) //os.Stdout实现了io.Writer接口\t\tio.Copy(os.Stdout, \u0026amp;b) } 实现 接口是定义行为的类型, 具体的实现由用户定义的类型完成. 用户定义的类型通常称作实体类型. 如果用户定义的类型实现了某个接口类型声明的一组方法, 那么这个用户定义的类型的值就可以赋给这个接口类型的值. 这个赋值会把用户定义的类型的值存入接口类型的值.\n接口的值是一个两个字长度的数据结构:\n 第一个字包含一个指向内部表(iTable)的指针. 内部表包含了所存储的值的类型信息, 还包含了与这个值相关联的一组方法. 第二个字是一个指向所存储的值的指针.  这部分可以参考Laws of Reflecation\n方法集 方法集定义了接口的接受规则. 方法集定义了一组关联到给定类型的值或者指针的方法. 定义方法的时使用的接收者的类型决定了这个方法是关联到值还是关联到指针, 还是两个都关联.\ntype notifier interface { notify() } type user struct { name string email string } //notify是使用指针接收者实现的方法 func (u *user) notify() { fmt.Printf(\u0026#34;Send email to %s\u0026lt;%s\u0026gt;\\n\u0026#34;, u.name, u.email) } func main() { u := user{\u0026#34;Bill\u0026#34;, \u0026#34;bill@email.com\u0026#34;} //sendNotificationTo(u) //用这一行会有编译错误. user没有实现notifier接口, 赋值给notifier会发生错误 \tsendNotificationTo(\u0026amp;u) //上面notify方法的实现的接收者为 user指针, 因此在赋值的时候只能接受user指针 \t//或者上面方法实现的接收者改为user } //接受一个实现了notifier的值作为参数 func sendNotificationTo(n notifier) { n.notify() } Go语言规范里定义的方法集的规则:\n   Values Methods Receiver     T (t T)   *T (t T) and (t *T)    T类型的值的方法集只包含值接收者声明的方法. 而指向T类型的指针的方法集既包括指针接收者声明的方法, 也包含值接收者声明的方法.\n上面的代码稍微做下修改, 更加清晰一些.\ntype notifier interface { notify() } type user struct { name string email string } //notify是使用值接收者实现的方法 func (u user) notify() { fmt.Printf(\u0026#34;Send email to %s\u0026lt;%s\u0026gt;\\n\u0026#34;, u.name, u.email) } func main() { u := user{\u0026#34;Bill\u0026#34;, \u0026#34;bill@email.com\u0026#34;} sendNotificationTo(u) sendNotificationTo(\u0026amp;u) //\u0026amp;u赋值给notifier的变量n时, n的方法集包含了值接收者实现的方法. } func sendNotificationTo(n notifier) { n.notify() } 或者换个角度, 从接收者来看.\n   Method Receiver Value     (t T) T and *T   (t *T) *T    使用指针接收者实现的接口, 那只有指向那个类型的指针才能实现对应的接口. 使用值接收者实现的接口, 那么那个类型的值和指针都能够实现对应的接口.\n多态 上面的函数sendNotificationTo其实就是一个多态函数.\n嵌入类型 type embedding, Go语言允许用户扩展或者修改已有类型的行为. 可用于代码复用, 或修改已有类型以符合新类型. 嵌入类型是将已有类型直接声明在新的结构类型里. 被嵌入的类型称为新的外部类型的内部类型.\n通过嵌入类型, 与内部类型相关的标识符会提升到外部类型上, 也成为外部类型的一部分. 外部类型也可以通过声明相同名称的标识符来覆盖内部类型的标识符的字段或者方法, 这就是修改内部类型的属性或者行为实现. 也可以添加新的字段和方法.\ntype notifier interface { notify() } type user struct { name string email string } type admin struct { //外部类型 \tuser //内部类型 \tlevel string } func (u *user) notify() { fmt.Printf(\u0026#34;Send email to %s\u0026lt;%s\u0026gt;\\n\u0026#34;, u.name, u.email) } func main() { ad := admin { user: user{ name: \u0026#34;John\u0026#34;, email: \u0026#34;john@email.com\u0026#34;, }, level: \u0026#34;super\u0026#34;, } ad.user.notify() //可以直接访问内部类型的方法 \tad.notify() //内部类型的方法也被提升到外部类型 \tendNotificationTo(\u0026amp;ad) //由于内部类型的提升, 内部类型实现的接口也被提升到外部类型. 外部类型也可以提供同名的方法实现, 以达到覆盖的效果. } func sendNotificationTo(n notifier) { n.notify() } 如果外部类型做了方法覆盖, 对内部类型方法的访问也还是会继续执行内部类型的方法\nfunc (ad *admin) notify() { ... } ad.user.notify() //执行内部类型的方法 ad.notify() //执行外部类型的方法 公开或未公开的标识符 使用规则来控制声明后的标识符的可见性. Go语言支持从包里公开或者隐藏表示. 这里的标识符包括类型, 变量, 方法. 当一个标识符的名字是小写开头的时候, 这个标识符就是未公开的. 如果是大写字母开头就是公开的, 包外的代码可见.\npackage user type User struct{ Name string //公开字段  email string //未公开字段 } //构造器 func New(name, email string) User { return user{name, email} } --------------- package another //在另一个包里使用User类型 u := user{ Name: \u0026#34;Bill\u0026#34;, email: \u0026#34;bill@email.com\u0026#34;, //编译器报错, 找不到email字段. 因为email字段未公开 } //使用构造器 ur : user.New(\u0026#34;Bill\u0026#34;, \u0026#34;bill@email.com\u0026#34;)  公开或者未公开的标识符, 不是一个值 短变量声明操作符(:=), 有能力捕获引用的类型, 并创建一个未公开的类型的变量.  package counter type alertCounter int //未公开类型 --------------- package another import ( \u0026#34;fmt\u0026#34; ) func main() { //c = counter.alertCounter 编译会报错, 无法访问未公开标识符  c := counter.alertCounter(20) fmt.Println(counter) // 20 } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/go-in-action-two/","tags":["Go"],"title":"Go In Action 读书笔记 二"},{"categories":["笔记"],"contents":"关键字 var 变量使用var声明, 如果变量不是定义在任何一个函数作用域内, 这个变量就是包级变量.\n Go语言中, 所有变量都被初始化为其零值. 对于数值类型, 其零值是0; 对于字符串类型, 其零值是空字符串\u0026quot;\u0026quot;; 对于布尔类型, 其零值是false. 对于引用类型来说, 底层数据结构会被初始化对应的零值. 但是被生命被起零值的引用类型的变量, 会返回nil作为其值.\n const 定义常量\ninterface 声明接口\nfunc 声明函数\ndefer 安排后面的函数调用在当前函数返回时才执行.\nfile, err = os.open(\u0026#34;filePath\u0026#34;) if err != nil return defer file.close() # more file operation go 启动后面的函数作为goroutine, 如下面启动匿名函数作为goroutine\ngo func(){}() import 导入包, 让使用者可以访问其中的标识符, 如类型, 函数, 常量和接口. 编译器查找包时会从GOROOT和GOPATH环境变量引用的位置去查找. 如果引用的包名前使用下划线_, 表明不直接使用包里的标识符, 只是调用其init函数执行初始化操作\nimport ( \u0026#34;log\u0026#34; \u0026#34;fmt\u0026#34; _ package/hasNoPublicIdentifier package/hasPublicIdentifier ) range 用于迭代数组, 字符串, 切片, 映射和通道\n迭代通道时, 如果通道中没有数据时会阻塞; 有数据写入时会触发执行后面的代码. 如果通道关闭, 迭代退出.\ntype 声明结构类型\nstruct 结构类型\n语法 标识符 小写字母开头的标识符不会暴露, 只会暴露大写字母开头的标识符\nmain包 程序的入口可以在main.go文件里找到. 每个可执行的Go程序有2个特征: 有main函数, 程序的第01行包名为main\npackage main import () func init() {} func main() {} init函数 init函数总是在main函数调用之前被调用. 常见import中使用下划线_引入没有暴露任何标识符的包, 调用其init函数\n包 所有处于同一个文件夹下的代码文件, 必须使用同样的包名\n一个函数多个返回值 value, err := RetriveValue() 简化声明变量运算符 (:=) 声明变量同时赋值, 根据后面的类型确定变量的类型.\nval := make(map[string]string) 函数间的变量传递 都是值传递\n数组, 切片和映射 数据 长度固定的数据类型. 在内存中的占用是连续的.\n声明和初始化 var array [5]int array :=[5]int{1,2,3,4,5} array :=[...]int{1,2,3,4,5} array :=[5]int{1: 1, 2: 2} #array[0]为0 使用 使用索引访问\narray[2] #2 array[2]=3 #修改 指针数组 array := [5]*int{0: new(int), 1: new(int)} //复制 *array[0]=1 *array[1]=2 多维数组 array :=[4][2]int{0: {1, 2}, 1: {3,4}} 函数间传递数组 值传递\n//声明一个需要8MB的数组 var array [1e6]int //传递数组 foo(array) //接受一个100w个整形值的数组 func foo(array [1e6]int){} 使用指针传递\n//声明一个需要8MB的数组 var array [1e6]int //传递数组 foo(\u0026amp;array) func foo(array *[1e6]int){} 切片 一种数据结构, 便于使用和管理数据集合. 是围绕动态数组的概念创建的, 就是可变(增长或缩小)数组. 切面的底层内存也是在连续快中分配的, 也能获得索引,迭代. 切片的动态增长是通过内置函数append实现的.\n切片是一个很小的对象, 对底层数组进行了抽象, 并提供操作方法. 包含三个字段: 指向底层数组的指针, 元素个数(长度)和容量.\n创建和初始化 //指定长度, 长度等于容量 slice := make([]string, 5) //指定长度和容量, 只能访问3个, 其余2个通过后期操作合并 slice := make([]string, 3, 5) slice := []int{1,2,3,4,5} nil和空切片 nil切片\nvar slice []int 空切片, 长度为0, 容量为0\nslice := make([]int, 0) slice := []int{} 使用切片 使用一个索引访问数组元素 slice[1]=10 slice[1] #10 使用切片创建切片, 新旧切片共享底层数组\n使用两个索引创建新的索引(共用底层数组) //底层数组长度为5 //长度为2 = 3 - 1 //容量为4 = 5 - 1 newSlice := slice[1:3] 增长 使用内置的append方法追加, 返回一个新的切片(新的底层数组, 数组指针改变, 长度改变, 容器可能改变)\nnewSlice := appen(slice[1,3], 6) 三个索引 使用三个索引, 第一个索引表示起始位置, 第二个元素表示起始索引加上希望包括的元素个数 2 + 1 = 3. 第三个索引是起始索引加上容量 2 + 2 = 4.\n//从第3个元素开始截取 //长度为1 = 3 - 2 //容量为2 = 4 - 2 newSlice := slice[2,3,4] 迭代切片 使用关键字range\nslice := []int{1,2,3,4,5} for idx, val := range slice { fmt.Printf(\u0026#34;Index: %d Values: %d\\n\u0026#34;, idx, val) } //返回长度 len(slice) //返回容量 cap(slice) 多维切片 函数间的切片传递 映射 存储一系列无序键值对的数据结构, 可以基于键快速检索.\n是一个集合, 可以使用类似数组或切片的方式迭代数组. 但是是无序的, 无法预测键值对被返回的顺序.\n无序的原因是映射使用了散列表.\n实现方式 桶的数据结构: 两个数组. 一个存储散列键的高八位值, 用来做桶定位. 另一个是字节数组, 用于存储键值对. 先一次存储所有的键, 再一次存储所有的值.\n将键通过散列函数计算出散列值, 然后通过散列值的高八位定位出桶, 然后在桶的数组里进行存储, 删除或者查找.\n键可以是任何类型, 只要这个值可以使用==运算符做比较. 切片, 函数以及包含切片的机构类型由于具有引用语义, 不能作为映射的键.\n创建和初始化 dict := make(map[string]int) dict := map[string]string{\u0026#34;red\u0026#34;:\u0026#34;#da1337\u0026#34;, \u0026#34;orange\u0026#34;:\u0026#34;#e95a22\u0026#34;} //空映射 dict := map[string]int{} //使用切片作为键 dict := map[[]string]int{} #编译错误 invalid map key type []string 使用 空映射\n#声明一个空映射 colors := map[string]string[]{} colors[\u0026#34;red\u0026#34;] = \u0026#34;#da1337\u0026#34; nil映射\n//声明为nil映射 var colors map[string]string colors[\u0026#34;red\u0026#34;] = \u0026#34;#da1337\u0026#34; //运行时出错 assignment to entry in nil map 判断是否存在键. 如果不存在exist为false, value为零值. 如果存在exist为true, value为对应的值.\nvalue, exists = colors[\u0026#34;blue\u0026#34;] if exists { ... } 遍历映射\nfor key, value range colors { fmt.printf(\u0026#34;Key: %s, Value %s\\n\u0026#34;, key, value) } 删除键值对\ndelete(colors, \u0026#34;red\u0026#34;) ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/go-in-action-one/","tags":["Go"],"title":"Go In Action 读书笔记 一"},{"categories":["笔记"],"contents":"我的环境变量是这样的:\nexport GOROOT=/usr/local/go export GOPATH=/Users/addo/Workspaces/go_w export GOBIN=$GOROOT/bin export PATH=$PATH:$GOBIN 使用下面的命令安装报错:\ngo get -v github.com/tools/godep\n github.com/tools/godep (download) github.com/tools/godep/vendor/github.com/pmezard/go-difflib/difflib github.com/tools/godep/vendor/github.com/kr/fs github.com/tools/godep/vendor/github.com/kr/text github.com/tools/godep/vendor/golang.org/x/tools/go/vcs github.com/tools/godep/vendor/github.com/kr/pretty github.com/tools/godep go install github.com/tools/godep: open /usr/local/go/bin/godep: permission denied\n 默认是安装到$GOBIN目录下, 权限不够.\n使用:\nsudo go get -v github.com/tools/godep\n sudo go get -v github.com/tools/godep github.com/tools/godep (download) created GOPATH=/Users/addo/go; see \u0026lsquo;go help gopath\u0026rsquo; github.com/tools/godep/vendor/github.com/kr/fs github.com/tools/godep/vendor/github.com/kr/text github.com/tools/godep/vendor/github.com/pmezard/go-difflib/difflib github.com/tools/godep/vendor/golang.org/x/tools/go/vcs github.com/tools/godep/vendor/github.com/kr/pretty github.com/tools/godep\n $GOBIN并没有找到godef. 输出提示created GOPATH=/Users/addo/go; . 因为sudo的时候找不到GOPATH变量, 便重新创建了目录.\n解决方案一:\n 临时修改GOBIN: export GOBIN=$GOPATH/bin 运行go get github.com/tools/godep 将生成的godef复制到GOROOT/bin下 回滚修改export GOBIN=$GOROOT/bin; export PATH=$PATH:$GOBIN  解决方案二:\n修改GOROOT/bin的属组属主, 安全性问题, 不推荐.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-godep-issue-in-custom-gopath/","tags":["Go"],"title":"自定义GOPATH下安装godep失败"},{"categories":["笔记"],"contents":"SpringBoot Application启动部分的源码阅读.\nSpringApplication 常用的SpringApplication.run(Class, Args)启动Spring应用, 创建或者更新ApplicationContext\n静态方法run 使用source类实例化一个SpringApplication实例, 并调用实例方法run.\npublic static ConfigurableApplicationContext run(Object[] sources, String[] args) { return new SpringApplication(sources).run(args); } 初始化initialize   实例化的时候首先通过尝试加载javax.servlet.Servlet和org.springframework.web.context.ConfigurableWebApplicationContext推断当前是否是web环境.\n  然后从spring.factories获取ApplicationContextInitializer的实现类.\n  从spring.factories获取ApplicationListener的实现类\n  推断出应用的启动类(包含main方法的类): 检查线程栈中元素的方法名是否是main\n  private Class\u0026lt;?\u0026gt; deduceMainApplicationClass() { try { //获取线程栈数据  StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) { if (\u0026#34;main\u0026#34;.equals(stackTraceElement.getMethodName())) { return Class.forName(stackTraceElement.getClassName()); } } } catch (ClassNotFoundException ex) { // Swallow and continue  } return null; } 到此实例化就完成了.\n实例方法run public ConfigurableApplicationContext run(String... args) { StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; //默认设置java.awt.headless为true  configureHeadlessProperty(); //从spring.factories中获取org.springframework.boot.SpringApplicationRunListener的实现类  SpringApplicationRunListeners listeners = getRunListeners(args); //通过EventPublishingRunListener发布started事件  listeners.started(); try { ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); //重点: 创建更新上下文对象  context = createAndRefreshContext(listeners, applicationArguments); //上下文对象更新完调用  afterRefresh(context, applicationArguments); //通过EventPublishingRunListener发布finished事件  listeners.finished(context, null); stopWatch.stop(); if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); } return context; } catch (Throwable ex) { handleRunFailure(context, listeners, ex); throw new IllegalStateException(ex); } } SpringApplicationRunListener 监听SpringApplication的run方法. 通过SpringFactoriesLoader加载, 实现时需要提供public的构造方法接受SpringApplication和String[]为参数. 事件的发生顺序为started -\u0026gt; environmentPrepared -\u0026gt; contextPrepared -\u0026gt; contextLoaded -\u0026gt; finished.\nSpringBoot默认使用EventPublishingRunListener这个实现类, 将各个事件封装并发布出去, 最终被ApplicationListener捕获.\npublic interface SpringApplicationRunListener { void started(); void environmentPrepared(ConfigurableEnvironment environment); void contextPrepared(ConfigurableApplicationContext context); void contextLoaded(ConfigurableApplicationContext context); void finished(ConfigurableApplicationContext context, Throwable exception); } 创建并更新上下文对象createAndRefreshContext private ConfigurableApplicationContext createAndRefreshContext(SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) { ConfigurableApplicationContext context; // Create and configure the environment  //获取或创建环境实例, web环境使用StandardServletEnvironment, 非web环境使用StandardEnvironment  ConfigurableEnvironment environment = getOrCreateEnvironment(); //配置环境数据  //1. **commandLineArgs**属性从启动参数中解析, 格式\u0026#34;--name=value\u0026#34;  //2. 配置profiles. 有效的profile(通过**spring.profiles.active**配置) 和 通过SpringApplication.profiles()指定的额外profile  configureEnvironment(environment, applicationArguments.getSourceArgs()); //通过EventPublishingRunListener发布environmentPrepared事件  listeners.environmentPrepared(environment); //如果是web环境, 将非web环境实例转换成web环境实例:  //使用有效的profile配置和jndiProperties, servletConfigInitParams, servletContextInitParams的配置.  if (isWebEnvironment(environment) \u0026amp;\u0026amp; !this.webEnvironment) { environment = convertToStandardEnvironment(environment); } //输出banner  if (this.bannerMode != Banner.Mode.OFF) { printBanner(environment); } //创建上下文对象, 没有指定实现类的话(使用SpringApplicationBuilder.contextClass), 使用默认context类. 然后通过反射实例化上下文对象.  //1. web环境使用org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext  //2. org.springframework.context.annotation.AnnotationConfigApplicationContext  //初始化实例的时候会做很多事,  //1. 创建AnnotatedBeanDefinitionReader. 注册相关的Annotation Post Processor, 包括: ConfigurationClassPostProcessor(处理@Configuration标注的类), AutowiredAnnotationBeanPostProcessor, RequiredAnnotationBeanPostProcessor, CommonAnnotationBeanPostProcessor, PersistenceAnnotationBeanPostProcessor, EventListenerMethodProcessor, DefaultEventListenerFactory  //2. 创建ClassPathBeanDefinitionScanner. 扫描器, 扫描默认的过滤器@Service, @Component, @Registry, @Controller. 同时支持J2EE6的@ManagedBean和@Named  // Create, load, refresh and run the ApplicationContext  context = createApplicationContext(); //设置环境  context.setEnvironment(environment); //后续的处理  postProcessApplicationContext(context); //应用初始化器(ApplicationContextInitializer的实现类), 对上下文对象做更多初始化的操作, 比如:  //1. 添加BeanFactoryPostProcessor  //2 .设置上下文对象id  //3 .代理配置中context.initializer.classes指定的初始化类  //4. 添加listener, 在web容器启动后更新环境变量中的端口号(server.ports中的local.server.port)  applyInitializers(context); //通过EventPublishingRunListener发布contextPrepared事件  listeners.contextPrepared(context); //打印启动信息和有效的profile信息  if (this.logStartupInfo) { logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); } //将ApplicationArguments实例注册到BeanFactory中, 名字为springApplicationArguments  // Add boot specific singleton beans  context.getBeanFactory().registerSingleton(\u0026#34;springApplicationArguments\u0026#34;, applicationArguments); //从source(可以是Resource, Package, CharSequence或者Class. 从run方法进来的为Class)类加载Bean到上下文对象中  // Load the sources  Set\u0026lt;Object\u0026gt; sources = getSources(); Assert.notEmpty(sources, \u0026#34;Sources must not be empty\u0026#34;); load(context, sources.toArray(new Object[sources.size()])); //通过EventPublishingRunListener发布contextLoaded事件  listeners.contextLoaded(context); //更新上下文对象, 调用ApplicationContext.refresh()方法  // Refresh the context  refresh(context); if (this.registerShutdownHook) { try { context.registerShutdownHook(); } catch (AccessControlException ex) { // Not allowed in some environments.  } } return context; } 更新上下文 ApplicationContext.refresh()  prepareRefresh 记录启动时间, 初始化上下文环境信息中的占位符, 检查必须的属性 obtainFreshBeanFactory 重建内置的BeanFactory, 并加载bean定义 prepareBeanFactory 初始化BeanFactory的标准上下文属性, 如BeanClassLoader, ExpressionResolver, PropertyEditorRegistrar, BeanPostProcessor, LoadTimeWeaverAwarePostProcessor等等. postProcessBeanFactory 标准初始化后修改上下文内置的BeanFactory invokeBeanFactoryPostProcessors 实例化并调用注册的BeanFactoryPostProcessor, 基于精确的顺序如果指定了顺序的话. 有些processor是操作Bean定义注册表的(如@Configuration标注的类bean包含其他的bean定义), 会在常规的BeanFactoryPostProcessor的检查发生之前. 在上下文对象的bean定义注册器进行了标准初始化之后进, 所有的常规bean定义都已经被加载了, 但是还没有bean被实例化. 在post-processiong之前可以添加更多的bean定义. @Configuration标注的类中的bean定义会在此时假如到注册器中. registerBeanPostProcessors 实例化并调用注册的BeanPostProcessor, 如果有顺序的话, 按照顺序来调用. initMessageSource 初始化名为messageSource的MessageSource实例. initApplicationEventMulticaster 初始化名为applicationEventMulticaster的ApplicationEventMulticaster实例, 应用可以用来注册应用事件的监听. onRefresh 供子类实现添加更多的更新操作. registerListeners 通过applicationEventMulticaster注册ApplicationListener实现类的监听器. finishBeanFactoryInitialization 进行上下文的BeanFactory初始化的收尾. 如提前初始化LoadTimeWeaverAware的bean, 冻结配置禁止修改bean定义, 实例化non-lazy-init的bean. finishRefresh 完成更新, 调用LifecycleProcessor.onRefresh(), 发布ContextRefreshedEvent事件, 将上下文实例暴露在MBean中.  ConfigurationClassPostProcessor BeanFactoryPostProcessor的实现类, 用于引导@Configuration类. 默认情况下通过使用\u0026lt;context:annotation-config/\u0026gt;或者\u0026lt;context:component-scan/\u0026gt;注册.\n注解 @SpringBootApplication 集合了@Configuration, @EnableAutoConfiguration和@ComponentScan 属性: exclude, excludeName, scanBasePackage , scanBasePackageClass\n@Configuration 类似旧版配置中的xml配置文件, 提供Bean的定义和引入其他xml配置. 分别通过@Bean和@Import实现. 在ApplicationContext.refresh()时是用ConfigurationClassPostProcessor进行bean的实例化.\n可以与@PropertySource, @Autowired,  @Value, @Profile搭配使用.\n@Configuration @PropertySource(\u0026#34;classpath:/com/acme/app.properties\u0026#34;) public class AppConfig { @Value(\u0026#34;${bean.name}\u0026#34;) String beanName; @Autowired DataSource dataSource; @Bean public MyBean myBean() { return new MyBean(beanName); } @Configuration @Profile(\u0026#34;test\u0026#34;) static class DatabaseConfigTest { @Bean DataSource dataSource() { return new EmbeddedDatabaseBuilder().build(); } } @Configuration @Profile(\u0026#34;production\u0026#34;) static class DatabaseConfigProduction { @Bean DataSource dataSource() { return new EmbeddedDatabaseBuilder().build(); } } } @EnableAutoConfiguration 开启Spring上下文对象的自动配置功能, 尝试去猜测和实例化你可能需要的bean. 这个功能是基于classPath来完成的. 比如: 项目中引用了tomcat-embedded.jar, 你可能需要一个TomcatEmbeddedServletContainerFactory实例, 除非定义了自己的EmbeddedServletContainerFactory实例.\n@ComponentScan 扫描使用@Configuration标注的类, 类似于Spring XML的\u0026lt;context:component-scan\u0026gt;元素. 使用basePackages和basePackageClasses属性来指定要扫描的包, 如果没有指定, 则默认从使用了该注解的类的包开始扫描.\n@Import 提示@Configuration有更多的类需要引入, 类似xml中的\u0026lt;import\u0026gt;标签. 可以引入@Configuration类, ImportSelector的实现类和ImportBeanDefinitionRegistrar的实现类, 还有常规的Component类.\n三者的处理方式不一样:\n @Configuration常规方式 ImportSelector会根据泛型类型从spring.factories找到对应的配置类. ImportBeanDefinitionRegistrar 可以实现在bean definition级别的处理 (@Bean实例级别)  在引入@Configuration类中使用@Bean标注的实例, 可以通过@Autowired注入. Bean和声明Bean的Configuration类本身都可以通过@Autowired注入.\n引入XML或者非Configuration, 使用@ImportResource.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/glance-over-spring-boot-source/","tags":["Spring","Java"],"title":"SpringBoot源码 - 启动"},{"categories":["笔记"],"contents":"最近在调整系统的性能, 系统中正使用Jackson作为序列化工具. 做了下与fastJson, Avro, ProtoStuff的序列化吞吐对比.\n由于只是做横向对比, 没有优化系统或者JVM任何参数. 服务器一般都用Linux, 在Docker里做了Linux系统的测试.\nMac:\nBenchmark Mode Cnt Score Error Units JMHTest.avroSerializer thrpt 2 3124799.325 ops/s JMHTest.fastJsonSerializer thrpt 2 3122720.917 ops/s JMHTest.jacksonSerializer thrpt 2 2373347.208 ops/s JMHTest.protostuffSerializer thrpt 2 4196009.673 ops/s Docker:\nBenchmark Mode Cnt Score Error Units JMHTest.avroSerializer thrpt 2 3293260.676 ops/s JMHTest.fastJsonSerializer thrpt 2 2996908.084 ops/s JMHTest.jacksonSerializer thrpt 2 2189518.443 ops/s JMHTest.protostuffSerializer thrpt 2 3998265.173 ops/s ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/java-serval-serializer-benchmark/","tags":["Java"],"title":"Java序列化工具性能对比"},{"categories":["笔记"],"contents":"Kafka提供的基础保障可以用来构建可靠的系统, 却无法保证完全可靠. 需要在可靠性和吞吐之间做取舍.\n Kafka在分区上提供了消息的顺序保证. 生产的消息在写入到所有的同步分区上后被认为是已提交 (不需要刷到硬盘). 生产者可以选择在消息提交完成后接收broker的确认, 是写入leader之后, 或者所有的副本 只要有一个副本存在, 提交的消息就不会丢失 消费者只能读取到已提交的消息  复制 Kafka的复制机制保证每个分区有多个副本, 每个副本可以作为leader或者follower的角色存在. 为了保证副本的同步, 需要做到:\n 保持到zk的连接会话: 每隔6s向zk发送心跳, 时间可配置 每隔10s向leader拉取消息, 时间可配置 从leader拉取最近10s的写入的消息. 保持不间断的从leader获取消息是不够的, 必须保证几乎没有延迟  Broker配置 复制因子 default.replication.factor broker级别的副本数设置, 通过这个配置来控制自动创建的topic的副本数. 为N的时候, 可以容忍失去N-1个副本, 保证topic的可读写.\n脏副本的leader选举 unclean.leader.election.enable 0.11.0.0之前的版本, 默认为true; 之后的版本默认为false. 这个设置控制不同步的副本能否参与leader的选举. 如果设置为true, 当没有同步副本可用的时候, 不同步的副本会成为leader, 意味着有数据丢失. 如果设置为false, 则意味着系统会处于不可用的状态, 该部分没有leader提供服务. 需要在可用性和一致性之间做取舍.\n最小同步副本数 min.insync.replicas 这个设置可以作用于broker和topic级别. 假如broker数为3, 最小同步副本数为2. 当2个同步副本中的一个出现问题, 集群便不会再接受生产者的发送消息请求. 同事客户端会收到NotEnoughReplicasException. 此时, 消费者还可以继续读取存在的数据. 唯一的同步副本变成只读.\n可靠系统中使用生产者 发送确认 acks 可选0, 1或者all. 设置影响吞吐和一致性.\n acks=0 意味着消息发送出去后就认为是成功写入topic. acks=1 发送后等待leader写入后确认 acks=all 发送后等待所有副本写入后确认  重试 retries 消息发送后会收到成功或者错误码. 错误有两种, 可重试的和不可重试的. 对于可重试的错误, 生产者会重复发送, 而reties控制重试的次数. 比如borker返回LEADER_NOT_AVAILABLE错误, 生产者会自动进行重试(retries不等于0), 因为broker之后会选择新的leader. 如果返回INVALID_CONFIG, 重试也不会解决问题. 同时retries有可能导致消息重复, 这就是Kafka消息的at least once保证. 在0.11.0.0之后, 提供了幂等的特性, 保证消息的exactly one. 对于跨数据中心的复制(比如MirrorMaker), 默认设置为Integer.MAX_VALUE\n额外的错误处理 使用生产者内置的重试是一个正确处理多种错误而不丢失消息的简单途径. 但是开发者还需要处理其他的错误, 比如:\n 不可重试错误 发送之前的错误 场试完所有的重试次数后还是未成功发送.  可靠系统中使用消费者 已提交消息和已提交偏移量 完全不同的两个概念, 前者是对生产者有效, 后者是对消费者有效.\n重要设置  group.id 两个有相同group.id并且订阅同一个topic的消费者, 会分配到topic下分区的一个子集, 并且是独立的子集. auto.offset.reset 这个参数控制当broker端没有发现任何提交的偏移量的时候, 消费者应该从什么位置开始读取消息. 接受earliest和latest两种设置. earliest意思是会从0开始读取, 而latest意思是从最末尾开始. enable.auto.commit 按照时间计划提交偏移量或者代码中手动提交. 对consumer来说这是一个重大的决定. 自动提交会保证只提交循环中已经处理的数据, 但是有可能会在下次提交始前系统崩溃. 这就导致已经被处理的消息的偏移量没有提交到broker. 下次拉取的时候(consumer重新上线或者rebalance时候由其他消费者处理该分区)会重新拉取已经处理过的消息, 重复消费. 假如你是将拉取的消息交由其他的线程处理, 那自动提交可能会到时消息被拉取, 却没有被处理. 自动提交的好处是吞吐量大. auto.commit.interval.ms 当enable.auto.commit设置为true的时候, 通过这个配置控制自动提交的时间间隔. 越大吞吐就越大, 一致性就越低. 越小, 则会增加提交的次数, 影响吞吐, 但是会提高一致性.  准确提交偏移量 总是提交已经处理过得消息 假如你是在循环中处理所有的消息, 并且不需要维护跨多次轮询的状态, 会比较容易实现. 可以使用自动提交, 或者在轮询循环的末尾进行偏移量提交.\n提交频率是性能和系统崩溃时重复的消息数量间的取舍 一次轮询循环中可以进行多次偏移量提交, 甚至每处理一条提交一次. 或者几个轮询提交一次. 提交会有性能上的开销, 类似生产者的acks=all\n保证你清楚的了解将要提交什么偏移量 常见的一个陷阱就是一次轮询循环中的偏移量提交了读到的最大偏移量, 而不是已经处理过得最大偏移量. 会导致消息丢失.\n再平衡 准确处理consumer的再平衡(consumer上线或者下线). 再平衡会引起先从消费者上摘取某些分区, 然后在分配某些分区. 通过实现RebalanceListener接口来实现控制.\n消费者可能需要重试 某些场景下, 暂时不提交偏移量, 下次轮询的时候会重复拉取消息. 比如数据库连接暂时不可用的情况下.\n消费者可能需要维护状态 某些场景下, 需要在多个轮询间存在聚合运算.\n处理长时间的处理 有些时候, 消息的处理耗时较长, 比如与其他系统交互或者进行比较复杂的运算. 某些Kafka版本的消费者, 两次轮询的间隔不能太长 (0.10.0.0之前版本的消费者没有单独的心跳进程, 是通过轮询同时达到心跳目的). 太长, 消费者则会被认为是下线, 会发生再平衡.\n有且只有一次的消息投递 有些场景需要至少一次的语义(没有消息丢失); 而某些场景则需要有些只有一次的语义. 但是当前Kafka没有提供完美的有且只有一次的支持. 需要与其他系统结合一起实现, 比如使用唯一的key写入数据库或者redis等存储中.\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-reliable-data-delivery/","tags":["Kafka"],"title":"Kafka的消息可靠传递"},{"categories":["源码解析"],"contents":"准备做个Spring Cloud源码分析系列, 作为Spring Cloud的源码分析笔记.\n这一篇是Eureka的客户端.\n客户端 两种方式, 最终的实现基本一样.\n显示指定服务发现的实现类型 使用@EnableEurekaClient注解显示的指定使用Eureka作为服务发现的实现, 并实例化EurekaClient实例. 实际上使用的是@EnableDiscoveryClient注解.\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @EnableDiscoveryClient public @interface EnableEurekaClient { } 动态配置实现 使用@EnableDiscoveryClient注解来配置服务发现的实现.\n源码分析 EnableDiscoveryClient\n@Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @Import(EnableDiscoveryClientImportSelector.class) public @interface EnableDiscoveryClient { } EnableDiscoveryClient注解的作用主要是用来引入EnableDiscoveryClientImportSelector\nEnableDiscoveryClientImportSelector\n@Order(Ordered.LOWEST_PRECEDENCE - 100) public class EnableDiscoveryClientImportSelector extends SpringFactoryImportSelector\u0026lt;EnableDiscoveryClient\u0026gt; { @Override protected boolean isEnabled() { return new RelaxedPropertyResolver(getEnvironment()).getProperty( \u0026#34;spring.cloud.discovery.enabled\u0026#34;, Boolean.class, Boolean.TRUE); } @Override protected boolean hasDefaultFactory() { return true; } } EnableDiscoveryClientImportSelector继承了SpringFactoryImportSelector并指定了泛型EnableDiscoveryClient. 这里的泛型是重点.\nSpringFactoryImportSelector\npublic abstract class SpringFactoryImportSelector\u0026lt;T\u0026gt; implements DeferredImportSelector, BeanClassLoaderAware, EnvironmentAware { private ClassLoader beanClassLoader; private Class\u0026lt;T\u0026gt; annotationClass; protected SpringFactoryImportSelector() { this.annotationClass = (Class\u0026lt;T\u0026gt;) GenericTypeResolver .resolveTypeArgument(this.getClass(), SpringFactoryImportSelector.class); } public String[] selectImports(AnnotationMetadata metadata) { ... } } 这里只截取了部分变量和方法 SpringFactoryImportSelector是spring cloud common包中的一个抽象类, 主要作用是检查泛型T是否有指定的factory实现, 即spring.factories中有对应类的配置.\nspring.factories\n在spring-cloud-netflix-eureka-client.jar!/META-INF/spring.factories中EnableDiscoveryClient的指定factory实现是\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\ org.springframework.cloud.netflix.eureka.config.EurekaClientConfigServerAutoConfiguration,\\ org.springframework.cloud.netflix.eureka.config.EurekaDiscoveryClientConfigServiceAutoConfiguration,\\ org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration,\\ org.springframework.cloud.netflix.ribbon.eureka.RibbonEurekaAutoConfiguration org.springframework.cloud.bootstrap.BootstrapConfiguration=\\ org.springframework.cloud.netflix.eureka.config.EurekaDiscoveryClientConfigServiceBootstrapConfiguration org.springframework.cloud.client.discovery.EnableDiscoveryClient=\\ org.springframework.cloud.netflix.eureka.EurekaDiscoveryClientConfiguration 同时EnableAutoConfiguration中包含了org.springframework.cloud.netflix.eureka.EurekaClientAutoConfiguration, EurekaClientAutoConfiguration会为EurekaDiscoveryClientConfiguration的实例依赖进行初始化, 如EurekaClient. EurekaClient在构造时会启动一个HeartBeat线程, 线程在运行的时候会做renew的操作, 将Application的信息注册更新到Eureka的服务端.\nEurekaDiscoveryClientConfiguration\n@Configuration @EnableConfigurationProperties @ConditionalOnClass(EurekaClientConfig.class) @ConditionalOnProperty(value = \u0026#34;eureka.client.enabled\u0026#34;, matchIfMissing = true) @CommonsLog public class EurekaDiscoveryClientConfiguration implements SmartLifecycle, Ordered { ... } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/spring-cloud-eureka-client-source-code-analysis/","tags":["Java","Spring"],"title":"Spring Cloud - Eureka Client源码分析"},{"categories":["笔记"],"contents":"Raft 强一致性算法\n名词 复制状态机 复制状态机是通过复制日志来实现的, 按照日志中的命令的顺序来执行这些命令. 相同的状态机执行相同的日志命令, 获得相同的执行结果.\n任期号 (currentTerm) 每个成员都会保存一个任期号, 称为服务器最后知道的任期号.\n投票的候选人id (votedFor) 当前任期内, 投票的候选人id, 即响应投票请求(见下文)返回true时的候选人id.\n已被提交的最大日志条目的索引值 (commitIndex) 每个成员都会持有已被提交的最大日志条目的索引值\n被状态机执行的最⼤日志条⽬的索引值 (lastApplied) 每个成员都会持有被状态机执行的最⼤日志条⽬的索引值\n请求 日志复制请求 (AppendEntries RPC) 由领导人发送给其他服务器, 也用作heartbeat\n请求内容\n term 领导人的任期号 leaderId 领导人的id prevLogIndex 已经被状态机执行的最大索引值, 即最新日志之前的日志的索引值. preLogTerm 最新日志之前的日志的领导人的任期号 entries[] 需要被复制的日志条目 leaderCommit 领导人提交的日志条目索引值  响应内容\n term 当前的任期号, 用于领导人更新自己的任期号 success 目标服务器是否能够匹配prevLogIndex和preLogTerm  接受者的处理\n 如果term \u0026lt; currentTerm返回false, 即发送请求的领导人任期号小于服务器最后知道的任期号, 意味着领导人发生了变更. 如果prevLogIndex和preLogTerm不匹配, 返回false. 即发送请求的领导人的日志不是最新的, 如果有一条已经存在的⽇志与新的冲突（index 相同但是任期号 term不同），则删除已经存在的⽇志和它之后所有的日志 添加任何在已有的日志中不存在的条目 如果leaderCommit \u0026gt; commitIndex, 则更新commitIndex为leaderCommit和最新日志条目索引值中较小的一个  发起投票请求 (RequestVote RPC) 由候选人发起的, 发给集群中已知的其他成员\n请求内容\n term 候选人的任期号 (在变更为领导人之前的保存的任期号的基础上加1) candidatedId 请求投票的候选人id lastLogIndex 候选人最新日志条目的索引值 lastLogTerm 候选人最新日志条目对应的任期号  响应内容\n term 目前的任期号, 用于候选人更新自己的任期号 voteGranted 收到选票为true  接受者的处理\n 如果term \u0026lt; currentTerm 返回voteGranted为false 如果votedFor为空, 并且lastLogIndex和lastLogTerm匹配成功, 则为该候选人投票, 返回voteGranted为true. 并更新为候选人id  安装快照请求 (InstalSnapshotRPC) 在领导人发送快照给跟随者时使用, 按顺序发送.\n请求内容\n term 领导人的任期号 leaderId 领导人id lastIncludedIndex 快照中包含的最后日志条目的索引值 lastIncludedTerm 快照中包含的最后日志条目的任期号 offset 分块在快照块中的偏移量 data[] 快照的原始数据 done 如果是最后一块数据则为true  响应内容\n term 目标服务器的currentTerm, 用于领导人更新自己  接受者的处理\n 如果term \u0026lt; currentTerm 立刻回复 如果是第一个分块(offset为0)则创建新的快照 在指定的偏移量写入数据 如果done未false, 则回复并继续等待之后的数据 保存快照文件, 丢弃所有存在的或者部分有着更新索引号的快照 如果现存的日志拥有相同的最后任期号和索引值, 则后面的数据继续保留并且回复 丢弃全部日志 能够使用快照来恢复状态机 (并且装载快照中的集群配置)  约束/原则  选举安全原则 Election Safety: 一个任期内最多只有一个领导人当选 领导人只增加原则 Leader Append-Only: 领导人永远不会覆盖或者删除自己的日志, 它只会增加条目 日志匹配原则 Log Matching: 如果两个日志在相同的索引位置上的日志条目的任期号相同, 那么我们就认为日志从头到这个索引位置之间的条目完全相同 领导人完全原则 Leader Completeness: 如果一个日志条目在一个给定任期内被提交, 那么这个条目一定会出现在所有任期号更大的领导人中 状态机安全原则 State Machine Safety: 如果一个服务器已经将给定索引位置的日志条目应用到状态机中, 则所有的其他服务器不会在该索引位置应用不同的条目  领导人选举 (Leader election) 集群成员的状态  领导人 候选人 追随者.  在同一时间, 成员只会属于其中的一种状态. 并且集群中只会存在一个领导人.\n有领导人时: 一个领导人, n-1个追随者 无领导人时: x个候选人, n-x个追随者\n约束  集群中最多存在一个领导人 追随者不会发送请求, 只会接受来自领导人的AppendEntries RPC请求, 和候选人的RequestVote RPC请求. AppendEntries RPC请求同时提供heartbeat机制 领导人只接受来自客户端的请求  任期 时间被划分为一个个的任期, 每一个任期的开始都是领导人的选举.\n随机的选举超时时间 例如150~300毫秒, 防止无限选举失败.\n日志复制 约束 日志的流向只会是从领导人到追随者. 领导人不会覆盖自己的日志.\n流程 领导人接受来自客户端的请求, 把请求中的命令作为日志条目加入到自己的日志中, 然后向追随者发送AppendEnties RPC请求, 要求追随者复制这条日志条目. 追随者复制完成后会响应领导人. 所有的请求都会响应后, 领导人会将该条目应用到状态机中, 并响应客户端. 假如有追随者没有响应, 领导人会无限地重试AppendEnties RPC请求直到所有的追随者都复制了该条目.\n安全性 没有包含全部日志的服务器不会赢得选举, 即某些投票请求的响应返回false.\n日志压缩 把当前的系统状态写入快照(snapshot)中, 并持久化到存储中, 然后丢弃之前的全部日志.\n快照中包含了最后的索引值和任期号.\n增量压缩(incremental approaches)\n领导人必须偶尔地发送快照给一些落后的跟随者. 运行非常缓慢或者新加入的跟随者不能与领导人保持同步, 可以通过发送快照的方式让跟随者更新到最新的状态.\n参考 Raft 一致性算法论文译文\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/learning-raft/","tags":null,"title":"Raft算法学习"},{"categories":["笔记"],"contents":"背景 Kafka的性能众所周知，Producer支持acknowledge模式。即Kafka会想Producer返回消息发送的结果。但是在Java Client中，acknowledge的确认有两种：同步和异步。 同步是通过调用future.get()实现的；异步则是通过提供callback方法来实现。写了个简单的程序测试一下单线程中吞吐差异能有多大。注意这里只考虑横向对比。\n 发送端单线程 Kafka为单集群节点 topic的分区数为1 key长度1 payload长度100  测试工具  JMeter Kafka Meter  future.get() + batch size =1 future.get() + batch size = 16K callback + batch size = 16k callback + batch size = 1 ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-producer-acknowledge-benchmark/","tags":["Java","Kafka"],"title":"Kafka发送不同确认方式的性能差异"},{"categories":["笔记"],"contents":"Kafka消费端的offset主要由consumer来控制, Kafka降每个consumer所监听的tocpic的partition的offset保存在__consumer_offsets主题中. consumer需要将处理完成的消息的offset提交到服务端, 主要有ConsumerCoordinator完成的.\n每次从kafka拉取数据之前, 假如是异步提交offset, 会先调用已经完成的offset commit的callBack, 然后检查ConsumerCoordinator的连接状态. 如果设置了自动提交offset, 会继续上次从服务端获取的数据的offset异步提交到服务端. 这里需要注意的是会有几种情况出现:\n 消息处理耗时较多, 假如处理单条消息的耗时为t, 拉取的消息个数为n. t * n \u0026gt; auto_commit_interval_ms, 会导致没有处理完的消息的offset被commit到服务端. 假如此时消费端挂掉, 没有处理完的数据将会丢失. 假如消息处理完成, offset还未commit到服务端的时候消费端挂掉, 已经处理完的消息会被再次消费.  下面配置影响着数据一致性和性能, 因此需要结合业务场景合理配置一下参数, 进行取舍.\n  enable.auto.commit 默认为true\n  auto.commit.interval.ms 默认为5000 ms (5s)\n  max.poll.records 默认为500\n  fetch.max.bytes 默认为52428800 bytes (50Mib).\n  一致性 这里我们针对前面出现的两个问题给出解决方案.\nKafka Java Client 把enable.auto.commit设置为false, 并在每处理完一条数据后手动提交offset.\n这里需要主意的时, 提交的offset是对当前消息的offset基础上进行加1.\npublic class ConsumerTest { public static void main(String[] args) throws InterruptedException { Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;192.168.31.186:9092\u0026#34;); props.put(ConsumerConfig.GROUP_ID_CONFIG, \u0026#34;test\u0026#34;); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OffsetResetStrategy.NONE.toString().toLowerCase(Locale.ROOT)); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \u0026#34;false\u0026#34;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \u0026#34;org.apache.kafka.common.serialization.StringDeserializer\u0026#34;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \u0026#34;org.apache.kafka.common.serialization.StringDeserializer\u0026#34;); KafkaConsumer\u0026lt;String, String\u0026gt; consumer = new KafkaConsumer\u0026lt;\u0026gt;(props); consumer.subscribe(Arrays.asList(\u0026#34;my-topic\u0026#34;)); while (true) { ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(100); if (!records.isEmpty()) { for (ConsumerRecord\u0026lt;String, String\u0026gt; record : records) { System.out.printf(\u0026#34;offset = %d, key = %s, value = %s%n\u0026#34;, record.offset(), record.key(), record.value()); //Manually commit each record  consumer.commitSync(Collections.singletonMap(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1))); } } } } } Spring Kafka   把enable.auto.commit设置为false\n  设置ContainerProperties的ackMode为MANUAL_IMMEDIATE\n  使用AcknowledgingMessageListener作为listener, 并在消息处理完成后调用acknowledgment.acknowledge().\n  public class SpringConsumerTest { public static void main(String[] args) { String bootstrapServer = \u0026#34;192.168.31.186:9092\u0026#34;; String groupId = \u0026#34;spring-consumer-group\u0026#34;; ContainerProperties containerProperties = new ContainerProperties(\u0026#34;my-topic\u0026#34;); containerProperties.setAckMode(AbstractMessageListenerContainer.AckMode.MANUAL_IMMEDIATE); containerProperties.setMessageListener( (AcknowledgingMessageListener\u0026lt;String, String\u0026gt;) (consumerRecord, acknowledgment) -\u0026gt; { System.out.println(consumerRecord); acknowledgment.acknowledge(); }); Map\u0026lt;String, Object\u0026gt; consumerConfigs = new HashMap\u0026lt;\u0026gt;(); consumerConfigs.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer); consumerConfigs.put(ConsumerConfig.GROUP_ID_CONFIG, groupId); consumerConfigs.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \u0026#34;false\u0026#34;); consumerConfigs.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10 * 1000); consumerConfigs.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \u0026#34;earliest\u0026#34;); consumerConfigs.put(ConsumerConfig.METADATA_MAX_AGE_CONFIG, 10 * 1000); consumerConfigs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); consumerConfigs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); KafkaMessageListenerContainer\u0026lt;String, String\u0026gt; listenerContainer = new KafkaMessageListenerContainer\u0026lt;String, String\u0026gt;( new DefaultKafkaConsumerFactory(consumerConfigs), containerProperties); listenerContainer.start(); } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-consumer-consistency/","tags":["Kafka"],"title":"Kafka消息消费一致性"},{"categories":["笔记"],"contents":"核心思想  生产端一致性: 开启幂等和事务, 包含重试, 发送确认, 同一个连接的最大未确认请求数. 消费端一致性: 通过设置读已提交的数据和同时处理完成每一条消息之后手动提交offset.  生产端 public class ProducerTest { public static void main(String[] args) throws InterruptedException, ExecutionException { Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;192.168.31.186:9092\u0026#34;); props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \u0026#34;my-transactional-id\u0026#34;); props.put(ProducerConfig.ACKS_CONFIG, \u0026#34;all\u0026#34;); props.put(ProducerConfig.RETRIES_CONFIG, \u0026#34;3\u0026#34;); props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, \u0026#34;1\u0026#34;); Producer\u0026lt;String, String\u0026gt; producer = new KafkaProducer\u0026lt;\u0026gt;(props, new StringSerializer(), new StringSerializer()); producer.initTransactions(); try { producer.beginTransaction(); for (int i = 0; i \u0026lt; 5; i++) { Future\u0026lt;RecordMetadata\u0026gt; send = producer .send(new ProducerRecord\u0026lt;\u0026gt;(\u0026#34;my-topic\u0026#34;, Integer.toString(i), Integer.toString(i))); System.out.println(send.get().offset()); TimeUnit.MILLISECONDS.sleep(1000L); } producer.commitTransaction(); } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) { // We can\u0026#39;t recover from these exceptions, so our only option is to close the producer and exit.  producer.close(); } catch (KafkaException e) { // For all other exceptions, just abort the transaction and try again.  producer.abortTransaction(); } producer.close(); } } 消费端 public class ConsumerTest { public static void main(String[] args) throws InterruptedException { Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;192.168.31.186:9092\u0026#34;); props.put(ConsumerConfig.GROUP_ID_CONFIG, \u0026#34;test\u0026#34;); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OffsetResetStrategy.NONE.toString().toLowerCase(Locale.ROOT)); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \u0026#34;false\u0026#34;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \u0026#34;org.apache.kafka.common.serialization.StringDeserializer\u0026#34;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \u0026#34;org.apache.kafka.common.serialization.StringDeserializer\u0026#34;); props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT)); KafkaConsumer\u0026lt;String, String\u0026gt; consumer = new KafkaConsumer\u0026lt;\u0026gt;(props); consumer.subscribe(Arrays.asList(\u0026#34;my-topic\u0026#34;)); while (true) { ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(100); if (!records.isEmpty()) { for (ConsumerRecord\u0026lt;String, String\u0026gt; record : records) { System.out.printf(\u0026#34;offset = %d, key = %s, value = %s%n\u0026#34;, record.offset(), record.key(), record.value()); //Manually commit each record  consumer.commitSync(Collections.singletonMap(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1))); } } } } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging-example/","tags":["Kafka","Java"],"title":"Kafka 恰好一次发送和事务消费示例"},{"categories":["笔记"],"contents":"Kafka提供“至少一次”交付语义, 这意味着发送的消息可以传送一次或多次. 人们真正想要的是“一次”语义,因为重复的消息没有被传递。\n普遍地发声重复消息的情况有两种:\n 如果客户端尝试向集群发送消息并获取网络错误, 则重试可能会导致重复. 如果在发送消息之前发生网络错误, 则不会发生重复. 但是, 如果在将消息附加到日志之后发生网络错误, 但在将响应发送给发件人之前, 发件人将不知道发生了什么. 唯一的选择是重试和冒险重复或放弃并声明消息丢失。 如果客户端尝试向集群发送消息并获取网络错误, 则重试可能会导致重复. 如果在发送消息之前发生网络错误, 则不会发生重复. 但是, 如果在将消息附加到日志之后发生网络错误, 但在将响应发送给发件人之前, 发件人将不知道发生了什么. 唯一的选择是重试和冒险重复或放弃并声明消息丢失。  第二种情况可以通过使用Kafka提供的偏移量由消费者处理. 他们可以将偏移量与其输出进行存储, 然后确保新消费者始终从最后存储的偏移量中提取. 或者, 他们可以使用偏移量作为一种关键字, 并使用它来对其输出的任何最终目标系统进行重复数据删除。\nProducer API改动 KafkaProducer.java\npublic interface Producer\u0026lt;K,V\u0026gt; extends Closeable { /** * Needs to be called before any of the other transaction methods. Assumes that * the transactional.id is specified in the producer configuration. * * This method does the following: * 1. Ensures any transactions initiated by previous instances of the producer * are completed. If the previous instance had failed with a transaction in * progress, it will be aborted. If the last transaction had begun completion, * but not yet finished, this method awaits its completion. * 2. Gets the internal producer id and epoch, used in all future transactional * messages issued by the producer. * * @throws IllegalStateException if the TransactionalId for the producer is not set * in the configuration. */ void initTransactions() throws IllegalStateException; /** * Should be called before the start of each new transaction. * * @throws ProducerFencedException if another producer is with the same * transactional.id is active. */ void beginTransaction() throws ProducerFencedException; /** * Sends a list of consumed offsets to the consumer group coordinator, and also marks * those offsets as part of the current transaction. These offsets will be considered * consumed only if the transaction is committed successfully. * * This method should be used when you need to batch consumed and produced messages * together, typically in a consume-transform-produce pattern. * * @throws ProducerFencedException if another producer is with the same * transactional.id is active. */ void sendOffsetsToTransaction(Map\u0026lt;TopicPartition, OffsetAndMetadata\u0026gt; offsets, String consumerGroupId) throws ProducerFencedException; /** * Commits the ongoing transaction. * * @throws ProducerFencedException if another producer is with the same * transactional.id is active. */ void commitTransaction() throws ProducerFencedException; /** * Aborts the ongoing transaction. * * @throws ProducerFencedException if another producer is with the same * transactional.id is active. */ void abortTransaction() throws ProducerFencedException; /** * Send the given record asynchronously and return a future which will eventually contain the response information. * * @param record The record to send * @return A future which will eventually contain the response information * */ public Future\u0026lt;RecordMetadata\u0026gt; send(ProducerRecord\u0026lt;K, V\u0026gt; record); /** * Send a record and invoke the given callback when the record has been acknowledged by the server */ public Future\u0026lt;RecordMetadata\u0026gt; send(ProducerRecord\u0026lt;K, V\u0026gt; record, Callback callback); } OutOfSequenceException 如果broker检测到数据丢失，生产者将抛出OutOfOrderSequenceException。 换句话说，如果它接收到大于其预期的序列的序列号。 未来将返回此异常，并传递给回调（如果有）。 这是一个致命的异常，新的Producer方法如send，beginTransaction，commitTransaction等将会抛出IlegalStateException。\n应用示例 public class KafkaTransactionsExample { public static void main(String args[]) { KafkaConsumer\u0026lt;String, String\u0026gt; consumer = new KafkaConsumer\u0026lt;\u0026gt;(consumerConfig); // Note that the ‘transactional.id’ configuration _must_ be specified in the  // producer config in order to use transactions.  KafkaProducer\u0026lt;String, String\u0026gt; producer = new KafkaProducer\u0026lt;\u0026gt;(producerConfig); // We need to initialize transactions once per producer instance. To use transactions,  // it is assumed that the application id is specified in the config with the key  // transactional.id.  //  // This method will recover or abort transactions initiated by previous instances of a  // producer with the same app id. Any other transactional messages will report an error  // if initialization was not performed.  //  // The response indicates success or failure. Some failures are irrecoverable and will  // require a new producer instance. See the documentation for TransactionMetadata for a  // list of error codes.  producer.initTransactions(); while(true) { ConsumerRecords\u0026lt;String, String\u0026gt; records = consumer.poll(CONSUMER_POLL_TIMEOUT); if (!records.isEmpty()) { // Start a new transaction. This will begin the process of batching the consumed  // records as well  // as an records produced as a result of processing the input records.  //  // We need to check the response to make sure that this producer is able to initiate  // a new transaction.  producer.beginTransaction(); // Process the input records and send them to the output topic(s).  List\u0026lt;ProducerRecord\u0026lt;String, String\u0026gt;\u0026gt; outputRecords = processRecords(records); for (ProducerRecord\u0026lt;String, String\u0026gt; outputRecord : outputRecords) { producer.send(outputRecord); } // To ensure that the consumed and produced messages are batched, we need to commit  // the offsets through  // the producer and not the consumer.  //  // If this returns an error, we should abort the transaction.  sendOffsetsResult = producer.sendOffsetsToTransaction(getUncommittedOffsets()); // Now that we have consumed, processed, and produced a batch of messages, let\u0026#39;s  // commit the results.  // If this does not report success, then the transaction will be rolled back.  producer.endTransaction(); } } } } 新增配置 Broker配置    配置 描述     transactional.id.timeout.ms 事务协调器在主动过期生成器TransactionalId之前等待的最大时间（以ms为单位），而不从中接收任何事务状态更新。默认为604800000（7天）。 这允许定期的每周生产者工作来维护其ID。   max.transaction.timeout.ms 允许的最大的事务超时时间. 如果一个客户端的事务请求超出这个设置, broker会在InitPidRequest的时候返回一个InvalidTransactionTimeout. 这样可以防止客户端太大的超时，这可能会延迟消费者从包含在事务中的主题中读取消息. 默认值为900000（15分钟）。 这是在消息的交易需要发送的时间段内的保守上限。   transaction.state.log.replication.factor 事务状态主题(__transaction_state)的副本数, 默认为3   transaction.state.log.num.partitions 事务状态主题(__transaction_state)的的分区数, 默认为50   transaction.state.log.min.isr 事务状态主题的每个分区的最小数量的异步副本需要被视为联机的。 默认为2   transaction.state.log.segment.bytes 事务状态主题的段大小。默认值：104857600字节。100m    生产者配置    配置 描述     enable.idempotence 是否启用幂等（默认情况下为false）。 如果禁用，生产者将不会在生成请求中设置PID字段，并且当前的生产者传递语义将生效。 请注意，必须启用幂等才能使用事务。当启用幂等时，我们强制执行acks = all，retries\u0026gt; 1和max.inflight.requests.per.connection = 1。 没有这些配置的这些值，我们不能保证幂等。 如果这些设置未被应用程序显式覆盖，则在启用幂等时，生产者将设置acks = all，retries = Integer.MAX_VALUE和max.inflight.requests.per.connection = 1。   transaction.timeout.ms 在主动中止正在进行的事务之前，事务协调器将等待生产者的事务状态更新的最长时间（以ms为单位）。   transactional.id 用于事务传递的TransactionalId。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的TransactionalId的事务已经完成。 如果没有提供TransactionalId，则生产者被限制为幂等传递。请注意，如果配置了TransactionalId，则必须启用enable.idempotence。默认值为空，这意味着无法使用事务。    消费者配置    配置 描述     isolation.level 以下是可能的值（默认为read_uncommitted）：read_uncommitted：在偏移顺序中消费已提交和未提交的消息; read_committed：仅以偏移顺序消耗非事务性消息或已提交事务消息。 为了保持偏移顺序，该设置意味着我们必须缓冲消费者中的消息，直到我们看到给定事务中的所有消息。    2 Idempotent Producer 幂等生产者保障 为了实现幂等生产者语义, 引入了producer id的概念, 下面称PID. 每个producer在初始化的时候会被分配一个唯一PID. PID的分配对用户来说是完全透明的, 且没有被客户端暴露.\nPID是从0开始单调递增的, 还有一个将要将要接受消息的主题分区的序号. 序号会随着producer向broker发送消息增长. broker在内存中维护着从每个PID中发过来的序号. 如果序号不是比上次提交PID/TopicParition组中的的序号大一, broker会拒绝producer的请求. 带有较小序号的消息会引发重复错误, producer可以忽略该错误. 带有较大的序号的消息会导致超出序号的错误, 意味着存在消息丢失, 这是致命的错误.\n为了保证每条消息都被恰好一次地持久化在log中, producer需要在失败的时候重试请求. 每个生产者实例都会得到一个新的唯一的PID, 因此我们只能在单一的生产者会话中保证幂等.\n这些幂等生成器语义对于无状态应用程序（如指标跟踪和审计）是非常有用的。\n事务保障 在核心上, 事务保证使应用程序能够以原子方式生成多个主题分区, 对这些主题分区的所有写入将成功或失败作为一个单元。\n此外, 由于消费者进度被记录为对偏移主题的写入, 所以利用上述能力来使得应用能够将消费和产生的消息批量化成单个原子单元. 只有整个“消费变换产品”全部执行, 才能将消息集合视为消费。\n为了跨多个生产者会话实现幂等, 需要提供一个在应用层面可以稳定的跨多个会话的transactionalId. transactionalId由用户提供.\n有transactionalId后, Kafka可以保证:\n 一个给定的transactionalId只有一个活跃的producer. 如果有新的使用同一个transactionalId的producer实例上线, 旧的实例会被隔离. 跨应用会话的事务恢复, 当一个应用实例死掉后, broker会结束(取消或者提交)未完成的事务以保护新上线的实例, 在恢复工作之前将新实例置于干净的状态.  注意这里提到的事务保障是从producer的角度. 在consumer端, 保障就会弱一些. 特别是, 我们不能保证承诺事务的所有消息都将一起被消费。原因如下:\n 对于压缩主题, 事务的一些消息可能被较新版本覆盖。 事务可能跨越日志段. 因此, 当旧段删除时, 我们可能会在事务的第一部分丢失一些消息。 消费者可能会在事务中寻求任意的offset, 因此缺少一些初始消息。 消费者可能不会从参与事务的所有分区中消费. 因此, 他们永远无法读取包含该事务的所有消息。  关键概念 实现事务, 即确保一组消息以原子方式产生和消费, 我们介绍几个新概念：\n 我们引进一个称为事务协调器(Transaction Coordinator)的新实体。与消费者组协调器类似, 每个生产者都被分配一个事务协调器, 所有分配PID和管理事务的逻辑都由事务协调器完成。 我们引入一个名为事务日志(Transaction Log)的新的内部kafka主题(__transaction_state)。与Consumer Offsets主题(__consumer_offsets)类似, 事务日志是每个事务的持久和复制记录。事务日志是事务协调器的状态存储, 最新版本的日志的快照封装了每个活动事务的当前状态。 我们引入控制消息(Control Messages)的概念。这些是写入用户主题的特殊消息, 由客户端处理, 但不会暴露给用户。例如, 它们被用于让broker向消费者表明先前提取的消息是否已经原子性地提交。以前在这里提出控制消息。 我们引入了TransactionalId的概念, 使用户能够以持续的方式唯一地识别生产者。具有相同TransactionalId的生产者的不同实例将能够恢复（或中止）由上一个实例实例化的任何事务。 我们引入生产者代(producer epoch)的概念, 这使我们能够确保只有一个具有给定的TransactionalId的生产者的合法活动实例, 从而使我们能够在发生故障的情况下维护事务保证。  除了上述新概念之外, 我们还引入了新的请求类型, 新版本的现有请求以及新版本的核心消息格式, 以支持事务。所有这些的细节将推迟到其他文档。\n数据流 在上图中, 尖锐的边框表示不同的机器. 底部的圆形盒子表示Kafka TopicPartitions, 而对角圆形的框代表在broker内部运行的逻辑实体。\n每个箭头表示RPC或写入Kafka主题. 这些操作按照每个箭头旁边的数字表示的顺序进行. 下面的部分编号为与上图中的操作相匹配, 并描述相关操作。\n1. 查找一个事务协调器 — FindCoordinatorRequest 事务协调器是分配PIDs和管理事务的核心组件, producer的第一件事是发送一个FindCoordinatorRequest请求(之前被称为GroupCoordinatorRequest, 但是现在更名为更一般的用法)到broker去获取其coordinator的位置. 译者补充比如ip, port.\n2. 获取一个Producer Id — InitPidRequest 获取到coordinator位置之后, 下一步是获取producer的PID. 这个通过发送InitPidRequest请求到事务协调器完成.\n2.1当有指定TransactionlId时 如果有配置transactionl.id, TransactionalId会随着InitPidRequest请求发出, 同时在2a中将PID和TransactionalId的对应关系保存在事务日志中. 这使我们能够将TransactionalId返回相同的PID给生产者的未来实例, 因此可以恢复或中止以前不完整的事务。\n除了返回PID之外, InitPidRequest还执行以下任务：\n 1. 提升PID的代, 使生产者的任何之前的僵尸实例被隔离起来, 不能处理事务. 2. 恢复(向前滚动或回滚)由生产者的上一个实例没有完成的任务事务.  InitPIDRequest的处理是同步完成的. 一旦返回, producer可以发送数据和开始新的事务.\n2.2当没有指定TransactionalId 如果没有配置TransactionalId, 会分配一个新的PID. 这是producer只在单一的session中实现了幂等语义和事务语义.\n3. 启动事务 — beginTransaction() API 新的KafkaProducer有一个beginTransaction()方法用来发出开始事务的信号. 生产者记录指示交易已经开始的本地状态, 但是在发送第一条记录之前, 在协调器看来事务还没有开始.\n4. 消费-转换-生产循环 在这个阶段, producer开始执行组成事务消费-转换-生产消息的流程. 这是一个很长的阶段, 可能包含多个请求\n4.1 AddPartitionsToTxnRequest 作为事务的一部分，生产者首次将新的TopicPartition作为事务的一部分发送给事务协调器。 协调器在步骤4.1a中记录了将此TopicPartition添加到事务中。 我们需要这些信息，以便我们可以将提交或中止标记写入每个TopicPartition（有关详细信息，请参阅第5.2节）。 如果这是添加到事务的第一个分区，协调器也将启动事务计时器。\n4.2 ProduceRequest 生产者通过一个或多个ProduceRequests（从生产者的发送方法触发）向用户的主题分区写入一堆消息。 这些请求包括如4.2a所示的PID，代和序号。\n4.3 AddOffsetCommitsToTxnRequest 生产者有一个新的KafkaProducer.sendOffsetsToTransaction API方法，它可以批量消费和生成的消息。 此方法接受Map \u0026lt;TopicPartitions，OffsetAndMetadata\u0026gt;和groupId参数。\nsendOffsetsToTransaction方法向事务协调器发送一个带有groupId的AddOffsetCommitsToTxnRequests，从而可以在内部__consumer-offsets主题中推导出该消费者组的TopicPartition。 事务协调器将在步骤4.3a中将该主题分区添加到事务日志中。\n4.4 TxnOffsetCommitRequest 另外作为sendOffset的一部分，生产者将向消费者协调器发送一个TxnOffsetCommitRequest，以在__consumer-offsets主题中保留偏移量（步骤4.4a）。 消费者协调员通过使用作为该请求的一部分发送的PID和生产者代来验证生产者是否允许发出请求（而不是僵尸）。\n消费的offsets在事务提交之前不可见，这是我们现在将讨论的过程。\n5. 提交或者终结事务 一旦写入数据，用户必须调用KafkaProducer的新的commitTransaction或abortTransaction API方法。 这些方法将分别开始提交或中止事务。\n5.1 EndTxnRquest 当生产者完成事务时，必须调用新引入的KafkaProducer.endTransaction或KafkaProducer.abortTransaction API方法。 前者使得步骤4中生产的数据可用于下游消费者。 后者有效地从日志中擦除生成的数据: 用户永远不可访问。 下游消费者将读取并丢弃已中止的消息。\n无论调用哪个生产者方法，生产者向事务协调器发出一个EndTxnRequest请求，附加数据指示事务是提交还是中止。 在收到此请求后，协调器：\n 将PREPARE_COMMIT或PREPARE_ABORT消息写入事务日志。 (步骤5.1a) 通过WriteTxnMarkerRequest开始向用户日志写入称为COMMIT(或ABORT)标记的命令消息的过程。 (见下文第5.2节)。 最后将COMMITTED（或ABORTED）消息写入事务日志。 (见下文5.3)。  5.2 WriteTxnMarkerRequest 该请求由事务协调器发送给作为事务一部分的每个主题分配的leader. 在收到此请求后, 每个代理将向日志写入COMMIT(PID)或ABORT(PID)控制消息。 (步骤5.2a)\n该消息向消费者指示具有给定PID的消息是否必须传递给用户或丢弃。 因此，消费者将缓冲具有PID的消息，直到它读取相应的COMMIT或ABORT消息，此时它将分别递送或丢弃消息。\n请注意，如果__consumer-offsets主题是事务中的TopicPartition之一，则提交（或中止）标记也将写入日志，并且通知消费者协调器，以便在以下情况下实现这些偏移量 在中止情况下提交或忽略它们（左侧的步骤5.2a）。\n5.3 Writing the final Commit or Abort Message 在所有提交或中止标记写入数据日志之后，事务协调器将最后的COMMITTED或ABORTED消息写入事务日志，指示事务完成（图中的步骤5.3）。 此时，可以删除与事务日志中的事务有关的大多数消息。\n我们只需要保留完成的事务的PID以及时间戳，所以我们最终可以删除生产者的TransactionalId-\u0026gt; PID映射。 请参阅下面的过期PID部分。\n简单的实现代码 这里\n参考资料  Exactly Once Delivery and Transactional Messaging  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-exactly-once-delivery-and-transactional-messaging/","tags":["Kafka"],"title":"恰好一次发送和事务消息(译)"},{"categories":["笔记"],"contents":"按照重要性分类, 基于版本0.11.0.0\n高 bootstrap.servers 一组host和port用于初始化连接. 不管这里配置了多少台server, 都只是用作发现整个集群全部server信息. 这个配置不需要包含集群所有的机器信息. 但是最好多于一个, 以防服务器挂掉.\nkey.serializer 用来序列化key的Serializer接口的实现类.\nvalue.serializer 用来序列化value的Serializer接口的实现类\nacks producer希望leader返回的用于确认请求完成的确认数量. 可选值 all, -1, 0 1. 默认值为1\n acks=0 不需要等待服务器的确认. 这是retries设置无效. 响应里来自服务端的offset总是-1. producer只管发不管发送成功与否。延迟低，容易丢失数据。 acks=1 表示leader写入成功（但是并没有刷新到磁盘）后即向producer响应。延迟中等，一旦leader副本挂了，就会丢失数据。 acks=all等待数据完成副本的复制, 等同于-1. 假如需要保证消息不丢失, 需要使用该设置. 同时需要设置unclean.leader.election.enable为true, 保证当ISR列表为空时, 选择其他存活的副本作为新的leader.  buffer.memory producer可以使用的最大内存来缓存等待发送到server端的消息. 如果消息速度大于producer交付到server端的阻塞时间max.block.ms, 将会抛出异常. 默认值33554432 byte (32m). 这个设置不是一个严格的边界, 因为producer除了用来缓存消息, 还要用来进行压缩.\ncompression.type producer压缩数据的类型, 默认为none, 就是不压缩. 可选none, gzip, snappy 和lz4. 压缩整个batch的数据, 因此batch的效果对压缩率也有影响. 更多的批处理意味着更好的压缩\nretries 设置大于零的值将导致客户端重新发送其发送失败并发生潜在的瞬时错误的记录. 相当于client在发送失败的时候会重新发行. 如果设置了retries而没有将max.in.flight.request.per.connection设置为1, 在两个batch发送到同一个partition时有可能打乱消息的发送顺序(第一个发送失败, 而第二个发送成功)\n中 batch.size producer会尝试批量发送属于同一个partition的消息以减少请求的数量. 这样可以提升客户端和服务端的性能. 默认大小是16348 byte (16k).\n发送到broker的请求可以包含多个batch, 每个batch的数据属于同一个partition.\n太小的batch会降低吞吐. 太大会浪费内存.\nclient.id 发送请求时传递给服务端的id字符. 用来追溯请求源, 除了使用ip/port. 服务端的请求日志中会包含一个合理的应用名. 默认为空\nlinger.ms 在正常负载的情况下, 要想减少请求的数量. 加上一个认为的延迟: 不是立即发送消息, 而是延迟等待更多的消息一起批量发送. 类似TCP中的Nagle算法. 当获得了batch.size的同一partition的消息会立即发送, 不管linger.ms的设置. 假如要发送的消息比较少, 会等待指定的时间以获取更多的消息.\n默认设置为0 ms(没有延迟).\nmax.block.ms 控制KafkaProducer.send()和KafkaProducer.partitionsFor()的阻塞时间. 这些方法会因为buffer满了或者metadata不可用而阻塞. 用户设置在serializers或者partitioner中的阻塞不会计算在内.\nmax.request.size 请求的最大大小（以字节为单位）。 此设置将限制生产者在单个请求中发送的记录批次数，以避免发送巨大的请求。 这也是最大记录批量大小的上限。 请注意，服务器拥有自己的记录批量大小，可能与此不同。\npartitioner.class Partitioner接口的实现类, 默认是org.apache.kafka.clients.producer.internals.DefaultPartitioner. 需要处理数据倾斜等原因调整分区逻辑的时候使用.\nrequest.timeout.ms 配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于replica.lag.time.max.ms(broker配置)，以减少由于不必要的生产者重试引起的消息重复的可能性。\n低 enable.idempotence 设置为\u0026rsquo;true', 将开启exactly-once模式. 设置为\u0026rsquo;false'(默认值), producer会因为borker失败等原因重试发送, 可能会导致消息重复.\n设置为\u0026rsquo;true\u0026rsquo;时需要结合max.in.flight.requests.per.connection设为'1\u0026rsquo;和retires不能为'0', 同时acks需要设置为\u0026rsquo;all\u0026rsquo;或者''-1'.\ninterceptor.classes 一组ProducerInterceptor接口的实现类, 默认为null. 可以通过该接口的实现类去拦截(可能需要修改)producer要发送的消息在发送到服务端之前.\nmax.in.flight.requests.per.connection 没有被确认unacknowledge的batch数, 如果设置大于1在retries设置了的情况下会出现消息发送顺序错误.\nretry.backoff.ms 失败请求重试的间隔时间. 默认是100毫秒\ntransaction.timeout.ms 事务协调器等待producer更新事务状态的最大毫秒数, 超过的话事务协调器会终止进行中的事务. 如果设置的时间大于broker的max.transaction.timeout.ms会收到InvalidTransactionTimeout错误.\ntransactional.id 用于事务传递的TransactionalId。 这使得可以跨越多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的TransactionalId的事务已经完成。 如果没有提供TransactionalId，则生产者被限制为幂等传递。 请注意，如果配置了TransactionalId，则必须启用enable.idempotence。 默认值为空，这意味着无法使用事务。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-producer-config/","tags":["Kafka","Java"],"title":"Kafka Producer配置解读"},{"categories":["笔记"],"contents":"JSON Path是在使用Kubernetes API的过程中首次使用的. 使用API做扩缩容的时候, 发送整个Deployment的全文不是个明智的做法, 虽然可行. 因此便使用了JSON Patch.\nJsonObject item = new JsonObject(); item.add(\u0026#34;op\u0026#34;, new JsonPrimitive(\u0026#34;replace\u0026#34;)); item.add(\u0026#34;path\u0026#34;, new JsonPrimitive(\u0026#34;/spec/replicas\u0026#34;)); item.add(\u0026#34;value\u0026#34;, new JsonPrimitive(instances)); JsonArray body = new JsonArray(); body.add(item); appsV1beta1Api.patchNamespacedScaleScale(id, namespace, body, null); fabric8s提供的kubernetes-client中使用的zjsonpatch则封装了JSON Patch操作. 例如在做扩缩容的时候或者当前的deployment, 修改replicas的值. 然后比较对象的不同(JsonDiff.asJson(sourceJsonNode, targetJsonNode)).\n下面的内容部分翻译自JSON PATH, 有兴趣的可以跳转看原文.\n什么是JSON Patch JSON Path是一直描述JSON文档变化的格式. 使用它可以避免在只需要修改某一部分的时候发送整个文档内容. 当与HTTP PATCH方法混合使用的时候, 它允许在标准规范的基础上使用HTTP APIs进行部分更新.\n补丁(Patch)内容的格式也是JSON.\nJSON Patch由IETF在RFC 6902中规范.\n简单的例子 原始文档 { \u0026#34;baz\u0026#34;: \u0026#34;qux\u0026#34;, \u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34; } 补丁 [ { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/baz\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;boo\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;world\u0026#34;] }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/foo\u0026#34;} ] 结果 { \u0026#34;baz\u0026#34;: \u0026#34;boo\u0026#34;, \u0026#34;hello\u0026#34;: [\u0026#34;world\u0026#34;] } 如何实现 一个JSON Path文档是一个包含了一组patch操作的JSON文件. 支持的patch操作包括\u0026quot;add\u0026quot;, \u0026ldquo;remove\u0026rdquo;, \u0026ldquo;replace\u0026rdquo;, \u0026ldquo;move\u0026rdquo;, \u0026ldquo;copy\u0026quot;和\u0026quot;test\u0026rdquo;. 这些patch操作是按照顺序应用的: 如果有任何一个操作失败, 整个patch都会被终止.\nJSON Pointer(指针) JSON指针IETF RFC 6901定义了一个如何在JSON文档中定位指定值的字符格式. 用来在所有的JSON Patch操作中指定要修改的文档部分.\nJSON指针是使用/分隔的token字符串, 这些token指定了对象的key或者是数组的索引. 例如, 给定JSON\n{ \u0026#34;biscuits\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Digestive\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Choco Leibniz\u0026#34; } ] } /biscuits将指向数组biscuits, 同时/biscuits/1/name指向Choco Leibniz.\n要指向JSON文档的根要使用一个空的字符串''. 指针/并不是指向根, 而是指向根上key为\u0026quot;\u0026quot;的位置(在JSON中是非法的).\n操作 Add { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/biscuits/1\u0026#34;, \u0026#34;value\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Ginger Nut\u0026#34; } } 在对象上增加一个值, 或者数组中插入数据. 如果是数组, 值将被插入到给定位置的前面. -用来表示插入到数组的尾部.\nRemove { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/biscuits\u0026#34; } 删除对象或者数组中的值.\nReplace { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/biscuits/0/name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Chocolate Digestive\u0026#34; } 替换一个值. 等同于先删除再增加.\nCopy { \u0026#34;op\u0026#34;: \u0026#34;copy\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/biscuits/0\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/best_biscuit\u0026#34; } 从一个位置(from)复制数据到指定的位置(path)上. from和to都是JSON指针.\nMove { \u0026#34;op\u0026#34;: \u0026#34;move\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/biscuits\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/cookies\u0026#34; } 从一个位置(from)移动数据到指定的位置(path)上. from和to都是JSON指针.\nTest { \u0026#34;op\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/best_biscuit/name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Choco Leibniz\u0026#34; } 检查某个位置的值是否是指定的值.如果失败, 整个Patch操作就会终止.\n库 JavaScript  jsonpatch.js jsonpatch-js jiff Fast-JSON-Patch JSON8 Patch JSON Patch Utils  Python  python-json-patch  PHP  json-patch-php php-jsonpatch/php-jsonpatch xp-forge/json-patch JSONPatch  Ruby  json_tools json_patch hana  Perl  perl-json-patch  C  cJSON (JSON library in C, includes JSON Patch support in cJSON_Utils)  Java  zjsonpatch json-patch  Scala  diffson  C++  JSON  C####  Ramone (a framework for consuming REST services, includes a JSON Patch implementation) JsonPatch (Adds JSON Patch support to ASP.NET Web API) Starcounter (In-memory Application Engine, uses JSON Patch with OT for client-server sync) Nancy.JsonPatch (Adds JSON Patch support to NancyFX)  Go  json-patch jsonpatch  Haskell  Haskell-JSON-Patch  Erlang  json-patch.erl  Elm  norpan/elm-json-patch  测试套件 github上维护的一组一致性测试 github.com/json-patch/json-patch-tests\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/json-patch/","tags":["Java"],"title":"JSON Patch"},{"categories":["笔记"],"contents":"使用openshift搭建的k8s的api创建Deployment，在启动的时候报下面的错误：\n Invalid value: \u0026ldquo;hostPath\u0026rdquo;: hostPath volumes are not allowed to be used]\n 解决方案：\n一个方案是将user加入privileged scc中，另一个方案就是：\noc edit scc restricted #添加下面这行 allowHostDirVolumePlugin: true ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/how-to-use-hostpath-in-openshift/","tags":["Kubernetes","Openshift"],"title":"如何在Openshift中使用hostPath"},{"categories":["笔记","云原生"],"contents":"Persistent Volume 译自Persistent Volumes\n介绍 管理存储是管理计算的独特问题。 PersistentVolume子系统为用户和管理员提供了一个API，其中提供了如何从如何使用存储提供存储的详细信息。为此，我们介绍两种新的API资源：PersistentVolume和PersistentVolumeClaim。\nPersistentVolume（PV）是由管理员配置的集群中的一段存储。它是集群中的一种资源就像一个节点是一个集群的资源。 PV是类似Volumes的卷插件，但是具有独立于使用PV的任何单个pod的生命周期。该API对象捕获存储的实现细节，即NFS，iSCSI或云提供商特定的存储系统。\nPersistentVolumeClaim（PVC）是用户存储的请求。它类似于pod。 Pod消耗节点资源，PVC消耗PV资源。Pods可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，一次读写或者多次只读）。\n虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是常见的是，用户需要具有不同属性（如性能）的PersistentVolumes，用于不同的问题。 集群管理员需要能够提供多种彼此不同的PersistentVolumes，而不仅仅是大小和访问模式，而不会使用户了解这些卷的实现细节。 对于这些需求，有一个StorageClass资源。\nStorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”。\n请参阅详细演练与工作示例。\n存储和声明的生命周期 PVs是集群中的资源；PVCs是对这种资源的声明，同时也扮演者对资源声明的检查。PVs和PVCs之前的交互遵循生命周期：供应、绑定、使用中、重新申请。\n集群管理员创建多个PV。它们携带可供集群用户使用的真实存储的详细信息。它们存在于Kubernetes API中，可用于消费。\n供应(Provisioning) PVs会以两种方式供应：静态和动态。\n静态 集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可被使用。\n动态 当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC指定动态配置卷。 此配置基于StorageClasses：PVC必须指定一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置。\n绑定(Binding) 当用户创建、或已经创建了一个PersistenVolumenClaim并指定大小和访问类型。Master中的控制循环会检测新的PVC，找到一个匹配的PV（如果可能的话），并将它们绑定在一起。如果一个PV被动态地供应某个PVC，循环将总是把这个PV和该PVC绑定。否则，用户总是至少得到他们要求的内容，但是卷可能超出了要求。一旦绑定，PersistentVolumeClaim绑定是排他的，不管用于绑定它们的模式。\n如果匹配的卷不存在，请求将无限期地保持。 随着匹配卷变得可用，请求将被绑定。 例如，提供许多50Gi PV的集群将不匹配要求100Gi的PVC。 当集群中添加100Gi PV时，可以绑定PVC。\n使用(使用) PODs把PVC当做volume使用。集群检查声明以找到绑定的卷并为POD挂载该卷。 对于支持多种访问模式的卷，用户在将其声明用作pod中的卷时指定所需的模式。\n一旦用户有声明并且该声明被绑定，绑定的PV属于用户，只要他们需要它。 用户通过在其Pod的卷块中包含persistentVolumeClaim来安排Pods并访问其声明的PV。 请参阅下面的语法详细信息：\nkind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \u0026#34;/var/www/html\u0026#34; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim 回收(Reclaiming) 当用户使用完volume，可以通过请求允许回收资源的API来删除该PVC对象。PersistentVolume的回收策略告诉集群如何处理当声明释放PV后。目前，卷可以被保留，回收或删除。\n保留(Retaining) 保留回收策略允许手动回收资源。 当PersistentVolumeClaim被删除时，PersistentVolume仍然存在，并且该卷被认为是“释放的”。 但是，由于上一个声明者的数据仍保留在卷上，因此尚不可用于其他声明。 管理员可以通过以下步骤手动回收卷。\n 删除PersistentVolume。 删除PV后，外部基础设施（如AWS EBS，GCE PD，Azure Disk或Cinder卷）中的关联存储资产仍然存在。 相应地手动清理相关存储资产上的数据。 手动删除关联的存储资产，或者如果要重用相同的存储资产，请使用存储资产定义创建一个新的PersistentVolume。  回收(Recycling) 如果受相应的卷插件支持，回收将对卷执行基本的擦除（rm -rf / thevolume / *），并使其再次可用于新的声明。 但是，管理员可以使用Kubernetes控制器管理器命令行参数来配置自定义的回收器pod模板，如这里所述。 定制回收站模板必须包含卷规范，如下例所示：\napiVersion: v1 kind: Pod metadata: name: pv-recycler- namespace: default spec: restartPolicy: Never volumes: - name: vol hostPath: path: /any/path/it/will/be/replaced containers: - name: pv-recycler image: \u0026#34;gcr.io/google_containers/busybox\u0026#34; command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;test -e /scrub \u0026amp;\u0026amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/* \u0026amp;\u0026amp; test -z \\\u0026#34;$(ls -A /scrub)\\\u0026#34; || exit 1\u0026#34;] volumeMounts: - name: vol mountPath: /scrub 但是，卷部分中的自定义回收器pod模板中指定的特定路径将替换为正在回收的卷的特定路径。\n删除(Deleting) 对于支持“删除回收”策略的卷插件，删除将从Kubernetes中删除PersistentVolume对象，并删除外部基础架构（如AWS EBS，GCE PD，Azure Disk或Cinder卷）中关联的存储资产。 动态配置的卷始终被删除。 如果不希望这样做，目前唯一的选择是在创建PV之后编辑或修补PV。 请参阅更改PersistentVolume的回收策略。\nPersistent Volume的类型  GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) VMware Photon Portworx Volumes ScaleIO Volumes StorageOS  Persistent Volumes 每个PV都包含规格和状态，这是规格和状态。\napiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /tmp server: 172.17.0.2 容量 通常，PV将具有特定的存储容量。这是使用PV的capacity属性设置的。看到Kubernetes的资源模型，以了解容量使用的单位。\n目前，存储大小是唯一可以设置或请求的资源。未来的属性可能包括IOPS，吞吐量等。\n访问模式 PersistentVolume可以以资源提供者支持的任何方式安装在主机上。 如下表所示，提供者将具有不同的特性，每个PV的访问模式都被设置为该特定卷支持的特定模式。 例如，NFS可以支持多个读/写客户端，但是特定的NFS PV可能会以只读方式在服务器上导出。 每个PV都有自己的一组访问模式来描述具体的PV功能。\n访问模式：\n ReadWriteOnce - 卷可以由单个节点作为读写装载 ReadOnlyMany - 许多节点可以只读容量 ReadWriteMany - 卷可以通过许多节点的读写装载  在CLI中，访问模式缩写为：\n RWO - ReadWriteOnce ROX - ReadOnlyMany RWX - ReadWriteMany  重要！一个卷只能一次使用一种访问模式进行挂载，即使它支持很多。例如，GCEPersistentDisk可以由单个节点挂载为ReadWriteOnce或多个节点挂载为ReadOnlyMany，但不能同时使用两种。\n   Volume插件 单节点读写 多个节点只读 多个节点读写     AWSElasticBlockStore ✓ - -   AzureFile ✓ ✓ ✓   AzureDisk ✓ - -   CephFS ✓ ✓ ✓   Cinder ✓ - -   FC ✓ ✓ -   FlexVolume ✓ ✓ -   Flocker ✓ - -   GCEPersistentDisk ✓ ✓ -   Glusterfs ✓ ✓ ✓   HostPath ✓ - -   iSCSI ✓ ✓ -   PhotonPersistentDisk ✓ - -   Quobyte ✓ ✓ ✓   NFS ✓ ✓ ✓   RBD ✓ ✓ -   VsphereVolume ✓ - -   PortworxVolume ✓ - ✓   ScaleIO ✓ ✓ -   StorageOS ✓ - -    类型 PV可以有一个类型，通过将storageClassName属性设置为StorageClass的名称来指定。 特定类型的PV只能绑定到请求该类型的PVC。 没有storageClassName的PV没有类型，只能绑定到不需要特定类型的PVC。 在过去，使用了注释volume.beta.kubernetes.io/storage-class而不是storageClassName属性。 该注释仍然可以工作，但将来Kubernetes版本将不再适用。\n回收策略 目前的回收策略是：\n Retain - 手动回收 Recycle - 基本擦洗（“rm -rf / thevolume / *”） Delete - 相关联的存储资产，如AWS EBS，GCE PD，Azure Disk或OpenStack Cinder卷被删除  目前，只有NFS和HostPath支持回收。 AWS EBS，GCE PD，Azure Disk和Cinder卷支持删除。\n阶段 卷将处于以下阶段之一：\n Available 可用 - 一个尚未绑定到索赔的免费资源 Bound 绑定 - 音量必须是声明 Released 释放 - 声明已被删除，但资源尚未被集群回收 Failed 失败 - 卷自动回收失败  CLI将显示绑定到PV的PVC的名称。\n挂载选项 Kubernetes管理员可以指定在一个节点上挂载一个持久卷时的其他挂载选项。\n您可以通过使用持久卷上的注释volume.beta.kubernetes.io/mount-options来指定安装选项。\n例如：\napiVersion: \u0026#34;v1\u0026#34; kind: \u0026#34;PersistentVolume\u0026#34; metadata: name: gce-disk-1 annotations: volume.beta.kubernetes.io/mount-options: \u0026#34;discard\u0026#34; spec: capacity: storage: \u0026#34;10Gi\u0026#34; accessModes: - \u0026#34;ReadWriteOnce\u0026#34; gcePersistentDisk: fsType: \u0026#34;ext4\u0026#34; pdName: \u0026#34;gce-disk-1\u0026#34; 安装选项是一个字符串，在将卷安装到磁盘时将被累积地连接和使用。\n请注意，并非所有Persistent卷类型都支持挂载选项。 在Kubernetes 1.6版中，以下卷类型支持挂载选项。\n GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes VMware Photon  PersistentVolumeClaims 每个PVC包含规格和状态，这是声明的规范和状态。\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: \u0026#34;stable\u0026#34; matchExpressions: - {key: environment, operator: In, values: [dev]} 访问模式 当请求具有特定访问模式的存储时，声明使用与卷相同的约定。\n资源 声明（就像pods）可以请求特定数量的资源。 在这种情况下，请求用于存储。 相同的资源模型适用于卷和声明。\n选择器 声明可以指定标签选择器以进一步过滤Volumes集。 只有标签与选择器匹配的卷才能绑定到声明。 选择器可以由两个字段组成：\n matchLabels - 卷必须具有带此值的标签 matchExpressions - 通过指定关键字和值的关键字，值列表和运算符所做的要求列表。 有效运算符包括In，NotIn，Exists和DoesNotExist。  所有来自matchLabels和matchExpressions的要求是组合在一起的，所有这些要求都必须满足才能匹配。\n类型 声明可以通过使用属性storageClassName指定StorageClass的名称来请求特定的类型。只有所请求的类型的PV，与PVC相同的storageClassName的PV可以绑定到PVC。\nPVC不一定要求一个类型。 它的storageClassName设置为等于“”的PVC总是被解释为请求没有类型的PV，因此它只能绑定到没有类型的PV（没有注释或一个等于“”）。 没有storageClassName的PVC不完全相同，并且根据是否启用了DefaultStorageClass admission插件，集群的处理方式不同。\n 如果admission插件已打开，则管理员可以指定默认的StorageClass。 没有storageClassName的所有PVC只能绑定到该默认的PV。 通过将StorageClass对象中的注解storageclass.kubernetes.io/is-default-class设置为“true”来指定默认的StorageClass。 如果管理员没有指定默认值，则集群会对PVC创建做出响应，就像admission插件被关闭一样。 如果指定了多个默认值，则admission插件禁止创建所有PVC。 如果admission插件已关闭，则不存在默认StorageClass的概念。 没有storageClassName的所有PVC只能绑定到没有类的PV。 在这种情况下，没有storageClassName的PVC的处理方式与将其storageClassName设置为“”的PVC相同。  根据挂载方法，挂载过程中可以通过addon manager在Kubernetes群集中部署默认的StorageClass。\n当PVC指定一个选择器，除了请求一个StorageClass之外，这些要求被AND组合在一起：只有所请求的类和所请求的标签的PV可能被绑定到PVC。 请注意，当前，具有非空选择器的PVC不能为其动态配置PV。\n在过去，使用了注解volume.beta.kubernetes.io/storage-class，而不是storageClassName属性。 该注解仍然可以工作，但在未来的Kubernetes版本中它将不被支持。\nClaims VS Volumes Pods通过将声明用作卷来访问存储。 声明必须存在于与使用声明的pod相同的命名空间中。 集群在pod的命名空间中查找声明，并使用它来获取支持声明的PersistentVolume。 然后将卷挂载到主机并进入pod。\nkind: Pod apiVersion: v1 metadata: name: mypod spec: containers: - name: myfrontend image: dockerfile/nginx volumeMounts: - mountPath: \u0026#34;/var/www/html\u0026#34; name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: myclaim 关于命名空间的注意 PersistentVolumes绑定是独占的，并且由于PersistentVolumeClaims是命名空间对象，因此只能在一个命名空间内挂载“many”模式（ROX，RWX）的声明。\nStorageClasses 每个StorageClass包含字段provisioninger和parameter，当属于类型的PersistentVolume需要动态配置时使用。\nStorageClass对象的名称很重要，用户可以如何请求特定的类。 管理员在首次创建StorageClass对象时设置类的名称和其他参数，并且在创建对象后无法更新对象。\n管理员可以仅为不要求任何特定类绑定的PVC指定默认的StorageClass：有关详细信息，请参阅PersistentVolumeClaim部分。\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: standard provisioner: kubernetes.io/aws-ebs parameters: type: gp2 供应者(Provisioner) 存储类有一个供应者，它确定用于配置PV的卷插件。必须指定此字段。\n   Volume Plugin Internal Provisioner Config Example     AWSElasticBlockStore ✓ AWS   AzureFile ✓ Azure File   AzureDisk ✓ Azure Disk   CephFS - -   Cinder ✓ OpenStack Cinder   FC - -   FlexVolume - -   Flocker ✓ -   GCEPersistentDisk ✓ GCE   Glusterfs ✓ Glusterfs   iSCSI - -   PhotonPersistentDisk ✓ -   Quobyte ✓ Quobyte   NFS - -   RBD ✓ Ceph RBD   VsphereVolume ✓ vSphere   PortworxVolume ✓ Portworx Volume   ScaleIO ✓ ScaleIO    你不限于指定此处列出的“internal”供应者（其名称前缀为“kubernetes.io”并与Kubernetes一起发送）。 你还可以运行和指定外部提供程序，它们是遵循Kubernetes定义的规范的独立程序。 外部提供者的作者对代码的生命周期，供应商的运输状况，运行状况以及使用的卷插件（包括Flex）等都有充分的自主权。存储库kubernetes-incubator /外部存储库包含一个库 用于编写实施大部分规范的外部提供者以及各种社区维护的外部提供者。\n例如，NFS不提供内部提供程序，但可以使用外部提供程序。 一些外部提供者列在存储库kubernetes-incubator/external-storage中。 还有第三方存储供应商提供自己的外部供应商的情况。\n回收策略 由存储类动态创建的持久卷将具有delete的回收策略。 如果不希望这样做，唯一的当前选项是在创建PV之后编辑PV。\n通过存储类手动创建和管理的持久卷将具有在创建时分配的任何回收策略。\n参数 存储类型具有描述属于存储类型的卷的参数。 取决于供应者，可以接受不同的参数。 例如，参数类型的值io1和参数iopsPerGB特定于EBS。 当省略参数时，使用一些默认值。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kubernetes-persistent-volumes/","tags":["Kubernetes","云原生"],"title":"Kubernetes — 持久卷"},{"categories":["云原生"],"contents":"Kubernetes 安装 macos 检查环境 sysctl -a | grep machdep.cpu.features | grep VMX 安装VirtualBox http://download.virtualbox.org/virtualbox/5.1.26/Oracle_VM_VirtualBox_Extension_Pack-5.1.26-117224.vbox-extpack 安装minikube curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.21.0/minikube-darwin-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 创建集群 默认使用virtualbox。\n主机的ip是192.168.31.186， 1087是proxy的端口。需要将ss的http代理监听地址从127.0.0.1改为主机的ip。\n#启动 minikube start #使用私有库 minikube start --insecure-registry=\u0026#34;192.168.31.34\u0026#34; #使用proxy，用于获取镜像 minikube start --docker-env HTTP_PROXY=\u0026#34;192.168.31.186:1087\u0026#34; --docker-env HTTPS_PROXY=\u0026#34;192.168.31.186:1087\u0026#34; --docker-env NO_PROXY=192.168.99.0/24 安装kubectl curl -Lo kubectl http://storage.googleapis.com/kubernetes-release/release/v1.7.3/bin/darwin/amd64/kubectl \u0026amp;\u0026amp; chmod +x kubectl \u0026amp;\u0026amp; sudo mv kubectl /usr/local/bin/ oh-my-zsh tab completion vi ~/.zshrc  添加到plugin部分\nplugins=(git zsh-completions kubectl)\n 使用 minikube 检查版本 minikube version #minikube version: v0.21.0 kubectl version #Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;3\u0026#34;, GitVersion:\u0026#34;v1.3.0\u0026#34;, GitCommit:\u0026#34;283137936a498aed572ee22af6774b6fb6e9fd94\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2016-07-01T19:26:38Z\u0026#34;, GoVersion:\u0026#34;go1.6.2\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} #Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;7\u0026#34;, GitVersion:\u0026#34;v1.7.0\u0026#34;, GitCommit:\u0026#34;d3ada0119e776222f11ec7945e6d860061339aad\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2017-07-26T00:12:31Z\u0026#34;, GoVersion:\u0026#34;go1.8.3\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 获取集群地址 minikube ip 192.168.99.100 获取服务列表 minikube service list 打开dashboard minikube dashboard kubectl 部署Dashboard UI 默认minikube会自动部署dashboard\nkubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml 启动proxy kubectl proxy #Starting to serve on 127.0.0.1:8001 获取pod信息 kubectl get pods --namespace kube-system NAME READY STATUS RESTARTS AGE kube-addon-manager-minikube 0/1 Running 0 1h kubernetes-dashboard-3313488171-90s64 0/1 Running 0 20m 如果STATUS一直处于ContainerCreating状态，应该是pull image失败。默认是去gcr.io拉镜像，被墙了。需要在启动minikube的时候设置docker使用的代理。\n获取pod详细信息 kubectl describe pod kubernetes-dashboard-3313488171-90s64 --namespace kube-system 查看log kubectl logs -f kubernetes-dashboard-3313488171-90s64 ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-kubernetes-on-macos/","tags":["Kubernetes","macOS"],"title":"Kubernetes学习 — Macos安装Kubernetes"},{"categories":["笔记"],"contents":"停止，stop，这里说的是真的停止。如何优雅的结束，这里就不提了。\n这里要用Thread.stop()。众所周知，stop()方法在JDK中是废弃的。\n 该方法天生是不安全的。使用thread.stop()停止一个线程，导致释放（解锁）所有该线程已经锁定的监视器（因沿堆栈向上传播的未检查异常ThreadDeath而解锁）。如果之前受这些监视器保护的任何对象处于不一致状态，则不一致状态的对象（受损对象）将对其他线程可见，这可能导致任意的行为。\n 有时候我们会有这种需求，不需要考虑线程执行到哪一步。一般这种情况是外部执行stop，比如执行业务的线程因为各种原因假死或者耗时较长，由于设计问题又无法响应优雅的停止指令。\n现在大家在项目中都很少直接使用线程，而是通过concurrent包中的类来实现多线程，例如ExecutorService的各种实现类。\n一个简单的停止线程的例子：\npublic class ExecutorServiceTest { public static void main(String[] args) throws InterruptedException { ExecutorService executor = Executors.newSingleThreadExecutor(); final AtomicReference\u0026lt;Thread\u0026gt; t = new AtomicReference\u0026lt;\u0026gt;(); Future\u0026lt;?\u0026gt; firstFuture = executor.submit(new Runnable() { public void run() { Thread currentThread = Thread.currentThread(); t.set(currentThread); while (true) { } } }); try { firstFuture.get(500, TimeUnit.MILLISECONDS); } catch (Exception e) { while (t.get().isAlive()) { t.get().stop(); TimeUnit.MILLISECONDS.sleep(50); } } executor.submit(new Runnable() { @Override public void run() { System.out.println(\u0026#34;submit again\u0026#34;); } }); executor.shutdown(); } } 如果你运行了上面的代码就会发现程序假死了，通过stack dump看是发生了死锁：\n\u0026quot;pool-1-thread-2\u0026quot; #11 prio=5 os_prio=31 tid=0x00007fa91006e800 nid=0x5903 waiting on condition [0x00007000060f8000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ab76ea0\u0026gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836) at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 死锁发生在第二次submit后，在LinkedBlockingQueue.take()时，LinkedBlockingQueue在ThreadPoolExecutor中用来暂存task的。真正执行任务线程的时候再从队列中取出。我们都知道LinkedBlockingQueue是线程的安全的，其高并发和线程安全是通过一个ReentrantLock代替内置锁来实现的（减小了锁的粒度）。submit第二个task时，再次执行take会再次获取锁。但是由于stop直接杀死了线程，没有释放当次执行take方法时获取ReentrantLock锁，导致了死锁。\nstop直接停止了线程，抛出了ThreadDeath。ThreadDeath是Error，不是Exception。\npublic class ThreadDeath extends Error { private static final long serialVersionUID = -4417128565033088268L; } 这种情况下，原有的ExecutorService实例就不能再使用了，因为我们无法通过程序来释放未释放的锁（由虚拟机的GC来解决）。如此，便需要重建ExecutorService实例。\n对上面的代码做了修改：\npublic class ExecutorServiceTest { public static void main(String[] args) throws InterruptedException { ExecutorService executor = Executors.newSingleThreadExecutor(); final AtomicReference\u0026lt;ExecutorService\u0026gt; es = new AtomicReference\u0026lt;\u0026gt;(); es.set(executor); final AtomicReference\u0026lt;Thread\u0026gt; t = new AtomicReference\u0026lt;\u0026gt;(); Future\u0026lt;?\u0026gt; future = es.get().submit(new Runnable() { public void run() { Thread currentThread = Thread.currentThread(); t.set(currentThread); currentThread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() { public void uncaughtException(Thread t, Throwable e) { if (e instanceof ThreadDeath || e instanceof IllegalMonitorStateException) { e.printStackTrace(); es.get().shutdownNow(); es.set(Executors.newSingleThreadExecutor()); } } }); while (true) { } } }); try { future.get(500, TimeUnit.MILLISECONDS); } catch (Exception e) { while (t.get().isAlive()) { t.get().stop(); TimeUnit.MILLISECONDS.sleep(50); } } es.get().submit(new Runnable() { @Override public void run() { System.out.println(\u0026#34;submit again\u0026#34;); } }); es.get().shutdown(); } } 注：这个例子只考虑了ExecutorService实例在单线程环境中的使用，如果需要在多线程环境中需要考虑重建实例时的排他性。\n修改后的核心是UncaughtExceptionHandler：\n 当线程由于未捕获的异常突然终止而调用处理程序的接口。 当线程由于未捕获的异常即将终止时，Java虚拟机将使用Thread.getUncaughtExceptionHandler（）向线程查询其UncaughtExceptionHandler，并将调用处理程序的uncaughtException方法，将线程和异常作为参数传递。 如果一个线程没有显示它的UncaughtExceptionHandler，那么它的ThreadGroup对象充当它的UncaughtExceptionHandler。 如果ThreadGroup对象没有处理异常的特殊要求，它可以将调用转发到默认的未捕获的异常处理程序。\n ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/stop-a-thread-of-executor-service/","tags":["Java"],"title":"暴力停止ExecutorService的线程"},{"categories":["笔记"],"contents":"《Java并发编程实践》的注解中有提到这一概念。\n The private constructor exists to avoid the race condition that would occur if the copy constructor were implemented as this (p.x, p.y); this is an example of the private constructor capture idiom (Bloch and Gafter, 2005).\n 结合原文代码：\n@ThreadSafe public class SafePoint{ @GuardedBy(\u0026#34;this\u0026#34;) private int x,y; private SafePoint (int [] a) { this (a[0], a[1]); } public SafePoint(SafePoint p) { this (p.get()); } public SafePoint(int x, int y){ this.x = x; this.y = y; } public synchronized int[] get(){ return new int[] {x,y}; } public synchronized void set(int x, int y){ this.x = x; this.y = y; } } 这里的构造器public SafePoint(SafePoint p) { this (p.get()); }是为了捕获另一个实例的状态。get()方法是一个同步方法，为了避免竞态没有分别提供x、y的公有getter方法。\n为了保证SafePoint的多线程安全性，在使用另一个实例构造新的实例时，使用了一个私有的构造器。\n首先为什么不用下面这种，还是为了避免竞态（p.x和p.y调用不是原子操作）。\npublic SafePoint(SafePoint p) { this(p.x, p.y) } 同理，这种也不行，两次调用get()方法不是原子操作。\npublic SafePoint(SafePoint p) { this(p.get()[0], p.get()[1]) } 为什么不用直接用数组，编译不通过：Call to \u0026quot;this()\u0026quot; must be first statement in constructor body\npublic SafePoint(SafePoint p) { int[] a = p.get(); this(a[0], a[1]); } 为什么接受数组为参数的构造器不能公开，数组a是有外部传入的，并不能保证数组内容不会其他线程修改。\npublic SafePoint (int [] a) { this (a[0], a[1]); } 当然我们可以使用下面这种代替私有的构造器，这种方法是安全的，但是会产生重复的初始化代码。\npublic SafePoint(SafePoint p) { int[] a = p.get(); this.x = a[0]; this.y = a[1]; } 再回头看SafePoint的线程安全性，SafePoint有两个状态变量x、y。为了保证线程安全性，没有为其分别提供getter和setter方法，而是将其封装后发布并使用内置锁保护。\n可以参考stackoverflow上的示例代码。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/private-constructor-capture-idiom/","tags":["Java"],"title":"私有构造函数捕获模式"},{"categories":["笔记"],"contents":"搭建Cassandra 使用docker创建Cassandra，方便快捷\ndocker pull cassandra:latest docker run -d --name cassandra -p 9042:9042 cassandra docker exec -it cassandra bash 创建keyspace、table #cqlsh\u0026gt; #create keyspace CREATE KEYSPACE contacts WITH REPLICATION = { \u0026#39;class\u0026#39; : \u0026#39;SimpleStrategy\u0026#39;, \u0026#39;replication_factor\u0026#39; : 1 }; #use USE contacts; #create table CREATE TABLE contact ( id UUID, email TEXT PRIMARY KEY ); 查看表数据 cqlsh:contacts\u0026gt; SELECT * FROM contact; email | id -------+---- (0 rows) Java客户端 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.datastax.cassandra\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cassandra-driver-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 连接到Cassandra并插入数据 Cluster cluster = Cluster.builder().addContactPoint(\u0026#34;127.0.0.1\u0026#34;).build(); Session session = cluster.connect(\u0026#34;contacts\u0026#34;); String insert = \u0026#34;INSERT INTO contact (id, email) \u0026#34; + \u0026#34;VALUES (\u0026#34; + \u0026#34;bd297650-2885-11e4-8c21-0800200c9a66,\u0026#34; + \u0026#34;\u0026#39;contact@example.com\u0026#39; \u0026#34; + \u0026#34;);\u0026#34;; session.execute(insert); 查看表数据 cqlsh:contacts\u0026gt; SELECT * FROM contact; email | id ---------------------+-------------------------------------- contact@example.com | bd297650-2885-11e4-8c21-0800200c9a66 (1 rows) ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/java-operate-cassandra-deployed-in-docker/","tags":["Java","Docker"],"title":"Docker 快速构建 Cassandra 和 Java 操作"},{"categories":["笔记","教程"],"contents":"假设已经安装好Docker\nSpringboot应用 pom添加依赖和构建插件 \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.3.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 应用代码 package com.atbug.spring.boot.test; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.context.annotation.Bean; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; /** * Created by addo on 2017/5/15. */ @SpringBootApplication @RestController public class Application { @RequestMapping(\u0026#34;/\u0026#34;) public String home(){ return \u0026#34;Hello world!\u0026#34;; } public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 应用构建 mvn clean package Centos 7 with Java8 获取Centos7 镜像 docker pull centos:7 准备centos-java8的dockerfile FROM centos:7 MAINTAINER Addo Zhang \u0026#34;duwasai@gmail.com\u0026#34; # Set correct environment variables. ENV\tHOME /root ENV\tLANG en_US.UTF-8 ENV\tLC_ALL en_US.UTF-8 RUN yum install -y curl; yum upgrade -y; yum update -y; yum clean all ENV JDK_VERSION 8u11 ENV JDK_BUILD_VERSION b12 RUN curl -LO \u0026#34;http://download.oracle.com/otn-pub/java/jdk/$JDK_VERSION-$JDK_BUILD_VERSION/jdk-$JDK_VERSION-linux-x64.rpm\u0026#34; -H \u0026#39;Cookie: oraclelicense=accept-securebackup-cookie\u0026#39; \u0026amp;\u0026amp; rpm -i jdk-$JDK_VERSION-linux-x64.rpm; rm -f jdk-$JDK_VERSION-linux-x64.rpm; yum clean all ENV JAVA_HOME /usr/java/default RUN yum remove curl; yum clean all 创建centos-java8镜像 docker build -t addo/centos-java8 . Docker中运行应用 准备应用镜像的dockerfile FROM addo/centos-java8 ADD target/boot-test-1.0-SNAPSHOT.jar /opt/app.jar EXPOSE 8080 CMD java -jar /opt/app.jar 构建应用镜像 docker build -t temp/spring-boot-app . 运行 docker run --name spring-boot-app -p 8080:8080 temp/spring-boot-app ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/run-spring-boot-app-in-docker/","tags":["Spring","Docker"],"title":"从零开始用 docker 运行 spring boot 应用"},{"categories":["笔记"],"contents":"最近因为需求在看CAS相关的只是，由于需要后端调用，用到proxy（代理）模式。整理了下web flow和proxy web flow的流程。\nWeb Flow Proxy Web Flow ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/jasig-cas-web-and-proxy-flow/","tags":null,"title":"Jasig CAS Web and Proxy flow"},{"categories":["笔记"],"contents":"这几天生产上有台机器的Metaspace一直在告警，Metaspace使用达到了97%。使用-XX:MetaspaceSize=512m，告警也还在在持续，查看MC只有81536.0，显然这个参数没起作用。\n也有人遇到类似的问题，并在openjdk上提过类似的bug，其实是一个注释的bug，最终在JDK-8151845中修复了。\n Class metadata is deallocated when the corresponding Java class is unloaded. Java classes are unloaded as a result of garbage collection, and garbage collections may be induced in order to unload classes and deallocate class metadata. When the space committed for class metadata reaches a certain level (a high-water mark), a garbage collection is induced. After the garbage collection, the high-water mark may be raised or lowered depending on the amount of space freed from class metadata. The high-water mark would be raised so as not to induce another garbage collection too soon. The high-water mark is initially set to the value of the command-line option MetaspaceSize. It is raised or lowered based on the options MaxMetaspaceFreeRatio and MinMetaspaceFreeRatio. If the committed space available for class metadata as a percentage of the total committed space for class metadata is greater than MaxMetaspaceFreeRatio, then the high-water mark will be lowered. If it is less than MinMetaspaceFreeRatio, then the high-water mark will be raised.\n 查看了Oracle的手册，Metaspace的GC会在committed size达到high-water mark之后发生。并且GC之后high-water mark会变化：变大或者变小，变大的话会防止下次GC发生得太早。high-water mark的默认初始大小20.8M，通过MetaspaceSize来设置，可见MetaspaceSize是控制Metaspace发生GC的阈值。GC后high-water mark的变化，通过MaxMetaspaceFreeRatio和MinMetaspaceFreeRatio控制。\nMaxMetaspaceSize默认为-1，无限大。不过如果没有限制的话，一直增大会被系统干掉进程。最好还是设置一下，比如1G。\n下面是我测试了分别设置MetaspaceSize、MaxMetaspaceSize、InitialBootClassLoaderMetaspaceSize为1G，Metaspace的变化。\n-XX:MetaspaceSize=1024m\n committed: 29360128 init: 0 max: -1 used: 28440648\n -XX:MaxMetaspaceSize=1024m\n committed: 29360128 init: 0 max: 1073741824 used: 28503552\n -XX:InitialBootClassLoaderMetaspaceSize=1024m\n committed: 1087635456 init: 0 max: -1 used: 28500344\n 三个参数都是没有改变init的大小，但是InitialBootClassLoaderMetaspaceSize改变了committed的大小，其实也是最终我们要的设置。\n关于这个参数，可以看你假笨的关于Metaspace的源码解读，发现的有点晚了。\n最后的解决方案是使用这个配置：-XX:MaxMetaspaceSize=1024m -XX:InitialBootClassLoaderMetaspaceSize=256m。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/java8-metaspace-size-issue/","tags":["Java"],"title":"MetaspaceSize的坑"},{"categories":["笔记"],"contents":"背景 一个Tomcat实例中运行了三个应用，其中一个对接了Apereo的CAS系统。现在要求另外两个系统也对接CAS系统，问题就出现了：\n 应用启动后打开其中两个应用的任何一个，登录完成后系统都没有问题。唯独首选打开第三个，其他两个报错ClassNotFoundException: org.apache.xerces.parsers.SAXParser。\n 发现这个类来自xerces:xercesImpl:jar:2.6.2，使用mvn dependency:tree发现是被xom:xom:1.1简洁引用。\n分析 CAS client jar中使用XMLReaderFactory创建XMLReader，首次创建会从classpath中查找META-INF/services/org.xml.sax.driver文件，这个文件里的内容是一个类的全名。比如xercesImpl中该文件的内容是org.apache.xerces.parsers.SAXParser。\n找到之后会将类名保存在XMLReaderFactory的静态变量_clsFromJar，并标记不会再查找org.xml.sax.driver文件。找不到的话则使用com.sun.org.apache.xerces.internal.parsers.SAXParser类。\n然后再使用当前线程的ContextClassLoader对类进行加载，这里的的ContextClassLoader是一个WebAppClassLoader的实例。\n同时XMLReaderFactory类是被BootStrapClassLoader加载的，为三个应用共享。\nTomcat类记载机制 Tomcat中有四个位置可以存放Java类库：/commons、/server、/shared和各Web应用的WEB-INF/lib目录。\n /commons目录中的类库可以被Tomcat和所有Web应用使用 /server目录中的类库只能被Tomcat使用 /shared目录中的可以被所有Web应用的使用，但是对Tomcat不可见 各Web应用的WEB-INF/lib目录中的类库则只能被该的应用使用\n Tomcat的使用CommonClassLoader、CatalinaClassLoader、SharedClassLoader、WebAPPClassLoader加载对应目录中的类库。\nBootstrap、Extension、Application是虚拟机使用的系统类加载器。\n类的加载使用双亲委派机制(Parent-Delegation)。\n Bootstrap | Extension | Application | System | Common / \\ Catalina Shared / \\ WebApp1 ... WebApp2 | | Jasper Jasper 解决方案 在另外两个应用中添加xerces:xercesImpl:jar:2.6.2依赖。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/one-tomcat-class-load-issue/","tags":["Java"],"title":"一个Tomcat类加载问题"},{"categories":["笔记"],"contents":"这里会用到几个概念高阶函数、函数字面量、参数组\n高阶函数 high-order function 函数的一种，简单来说它包含了一个函数类型的参数或者返回值。\n所谓的高阶是跟一阶函数相比，深入一下：\n  一个或多个参数是函数，并返回一个值。 返回一个函数，但没有参数是函数。 上述两者叠加：一个或多个参数是函数，并返回一个函数。   示例：\ndef stringSafeOp(s: String, f: String =\u0026gt; String) = { if ( s != null) f(s) else s } //stringSafeOp: (s: String, f: String =\u0026gt; String)String def reverse(s: String) = s.reverse //reverse: (s: String)String stringSafeOp(\u0026#34;Ready\u0026#34;, reverse) //res86: String = ydaeR 函数字面量 function literal，其他名字：匿名函数、Lambda表达式等。 函数字面量可以存储在函数值和变量中，或者也可以定义为高阶函数调用的一部分。在任何接受函数类型的地方都可以使用函数字面量。\nreverse的函数字面量定义：\nval reverse = (s:String) =\u0026gt; s.reverse (s:String) =\u0026gt; s.reverse定义了一个有类型的输入参数（s:String）和函数体（s.reverse）。\n定义为高阶函数调用的一部分：\nstringSafeOp(\u0026#34;Ready\u0026#34;, (s:String) =\u0026gt; s.reverse) 由于已经定义了参数f的类型String =\u0026gt; String，可以从函数字面量中删除显示类型，交由编译器自动推导其类型。\nstringSafeOp(\u0026#34;Ready\u0026#34;, s =\u0026gt; s.reverse) 占位符语法 Placeholder syntax是函数字面量的一种缩写形式，将命名参数替换为通配符（_）。\n使用条件：\n 函数的显示类型在字面量之外指定\n参数最多只使用一次\n 前面的例子正好符合条件：\n stringSafeOp定义中指定了字面量的类型（字面量之外） String =\u0026gt; String\n参数s只使用一次 s.reverse\n stringSafeOp(\u0026#34;Ready\u0026#34;, _.reverse) 参数组 parameter groups 函数的参数列表的另一种形式：分解并使用小括号分隔。\nstringSafeOp的参数组表示：\ndef stringSafeOp(s: String)(f: String =\u0026gt; String) = { if ( s != null) f(s) else s } //stringSafeOp: (s: String)(f: String =\u0026gt; String)String 用函数字面量块调用高阶函数 组合上面的特性，就有了用函数字面量块调用高阶函数。\nval newStringSafeOp = stringSafeOp(\u0026#34;Ready\u0026#34;){_.reverse} 进阶 def timer[A](f: =\u0026gt; A) = { def now = System.currentTimeMillis val start = now; val a = f; val end = now println(s\u0026#34;Executed int ${end - start}ms\u0026#34;) a } val veryRandomAmount = timer { util.Random.setSeed(System.currentTimeMillis) for (i \u0026lt;- 1 to 100000) util.Random.nextDouble util.Random.nextDouble } timer方法对方法调用进行时间记录，对目标代码没有侵入性。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/call-high-order-function-in-function-literal/","tags":["Java"],"title":"Scala笔记：用函数字面量块调用高阶函数"},{"categories":["笔记"],"contents":"在网上搜索GreenPlum（GPDB）的数据源配置的时候，发现搜索结果都是用postgresql的配置。\nimport com.mchange.v2.c3p0.DataSources; import javax.sql.DataSource; import java.sql.*; import java.util.Properties; /** * Created by addo on 2017/4/10. */ public class JDBCTest { private static String POSTGRESQL_URL = \u0026#34;jdbc:postgresql://192.168.56.101:5432/example\u0026#34;; private static String POSTGRESQL_USERNAME = \u0026#34;dbuser\u0026#34;; private static String POSTGRESQL_PASSWORD = \u0026#34;password\u0026#34;; private static String GPDB_URL = \u0026#34;jdbc:pivotal:greenplum://192.168.56.101:5432;DatabaseName=test\u0026#34;; private static String GPDB_USERNAME = \u0026#34;dbuser\u0026#34;; private static String GPDB_PASSWORD = \u0026#34;password\u0026#34;; /** * Postgresql Connection * * @return * @throws ClassNotFoundException * @throws SQLException */ public static Connection postgresqlConnection() throws ClassNotFoundException, SQLException { Class.forName(\u0026#34;org.postgresql.Driver\u0026#34;); return DriverManager.getConnection(POSTGRESQL_URL, POSTGRESQL_USERNAME, POSTGRESQL_PASSWORD); } /** * GreenPlum Connection * * @return * @throws ClassNotFoundException * @throws SQLException */ public static Connection gpdbConnection() throws ClassNotFoundException, SQLException { Class.forName(\u0026#34;com.pivotal.jdbc.GreenplumDriver\u0026#34;); return DriverManager.getConnection(GPDB_URL, GPDB_USERNAME, GPDB_PASSWORD); } /** * GreenPlud C3P0 Datasource Connection * * @return * @throws SQLException */ public static Connection gpdbC3P0Connection() throws SQLException { Properties c3p0Props = new Properties(); c3p0Props.setProperty(\u0026#34;driverClass\u0026#34;, \u0026#34;com.pivotal.jdbc.GreenplumDriver\u0026#34;); c3p0Props.setProperty(\u0026#34;jdbcUrl\u0026#34;, GPDB_URL); c3p0Props.setProperty(\u0026#34;user\u0026#34;, GPDB_USERNAME); c3p0Props.setProperty(\u0026#34;password\u0026#34;, GPDB_PASSWORD); c3p0Props.setProperty(\u0026#34;acquireIncrement\u0026#34;, \u0026#34;5\u0026#34;); c3p0Props.setProperty(\u0026#34;initialPoolSize1\u0026#34;, \u0026#34;1\u0026#34;); c3p0Props.setProperty(\u0026#34;maxIdleTime\u0026#34;, \u0026#34;60\u0026#34;); c3p0Props.setProperty(\u0026#34;maxPoolSize\u0026#34;, \u0026#34;50\u0026#34;); c3p0Props.setProperty(\u0026#34;minPoolSize\u0026#34;, \u0026#34;1\u0026#34;); c3p0Props.setProperty(\u0026#34;idleConnectionTestPeriod\u0026#34;, \u0026#34;60\u0026#34;); return DataSources.unpooledDataSource(GPDB_URL, c3p0Props).getConnection(); } public static void main(String[] args) throws ClassNotFoundException, SQLException { Connection[] connections = new Connection[]{postgresqlConnection(), gpdbConnection(), gpdbC3P0Connection()}; for (Connection connection : connections) { CallableStatement callableStatement = connection.prepareCall(\u0026#34;select * from user\u0026#34;); boolean execute = callableStatement.execute(); ResultSet resultSet = callableStatement.getResultSet(); while (resultSet.next()) { System.out.println(resultSet.getString(\u0026#34;current_user\u0026#34;)); } callableStatement.close(); connection.close(); } } } 源代码\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/greenplum-jdbc-and-c3p0-datasource/","tags":["Java"],"title":"GreenPlum JDBC和C3P0数据源"},{"categories":["笔记"],"contents":"先说原理：  val修饰的在定义的时候执行\ndef修饰的在调用的时候执行\n 直观的例子： //注释的行为REPL输出 def test: () =\u0026gt; Int = { println(\u0026#34;def called\u0026#34;) val r = util.Random.nextInt () =\u0026gt; r } //test: () =\u0026gt; Int test() //def called //res82: Int = -950077410  test() //def called //res83: Int = 1027028032  val test: () =\u0026gt; Int = { println(\u0026#34;def called\u0026#34;) val r = util.Random.nextInt () =\u0026gt; r } //def called //test: () =\u0026gt; Int = $$Lambda$1382/338526071@42f2515d  test() //res84: Int = 300588352 test() //res84: Int = 300588352  def在方法定义的时候除了新的方法没有任何输出；之后每次调用的时候都会执行一次，而且是每次调用都获得一个新的方法（random值不同）\nval在方法定义的时候除了新的方法，还会执行并获得一个方法；之后每次调用都只是执行了定义的时候获得的方法（() =\u0026gt; r，r值固定）\n 进阶 def timer[A](f: =\u0026gt; A) = { def now = System.currentTimeMillis val start = now; val a = f; val end = now println(s\u0026#34;Executed int ${end - start}ms\u0026#34;) a } val veryRandomAmount = timer { util.Random.setSeed(System.currentTimeMillis) for (i \u0026lt;- 1 to 100000) util.Random.nextDouble util.Random.nextDouble } 看过上面的例子就不难理解了，重新定义now是为了使用简洁优雅的方式获取当前毫秒数。\n val start = now; 用val修饰，记录方法执行前的时间到start中。 val a = f 用val修饰，执行f方法，并保存数据到a中。 val end = now 用vla修饰，记录方法执行结束时间到end中。 最后返回a，  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/def-vs-val-in-scala/","tags":["Java"],"title":"Scala笔记：def VS val"},{"categories":["笔记"],"contents":"版本  Centos7\nRedis3.2.8\n 编译安装 wget http://download.redis.io/releases/redis-3.2.8.tar.gz tar -zxvf redis-3.2.8.tar.gz cd redis-3.2.8 sudo make test sudo make install 启动 redis-server 问题   /bin/sh: cc: command not found\n**原因：**Centos安装时选择的类型是Infrastructure，没有c++的编译工具。\n解决：sudo yum -y install gcc gcc-c++ libstdc++-devel\n  malloc.h:50:31: fatal error: jemalloc/jemalloc.h: No such file or directory\n**原因：**Redis使用的默认的memory allocator是libc，而Linux系统中默认的是jemalloc，需要制动MALLOC变量。\n解决：sudo make MALLOC=libc install\n  ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-redis-on-centos/","tags":["DevOps","Linux"],"title":"Centos 编译安装 Redis"},{"categories":["笔记"],"contents":"版本 Centos7\nPostgresql9.2\nEnable ssh  service sshd start\n Open firewall for 22  firewall-cmd —state\nfirewall-cmd —list-all\nfirewall-cmd —permanent —zone=public —add-port=22/tcp\nfirewall-cmd —reload\n Install Postgresql  yum install postgres\nsu postgres\npostgres —version\n 默认会创建postgres:postgres用户和组\n切换用户  su - postgres\n 初始化数据库 通过指定数据文件目录初始化db\n initdb -D /var/lib/pgsql/data\n 修改端口防火墙 默认端口是5432，需要在防火墙中打开端口\n firewall-cmd \u0026ndash;permanent \u0026ndash;zone=public \u0026ndash;add-port=5432/tcp\n 修改监听的ip 需要外部访问的话，需要修改postgresql.conf中的监听ip，\u0026lsquo;0.0.0.0\u0026rsquo;允许所有ipv4的ip访问，''::\u0026lsquo;\u0026lsquo;允许所有ipv6的ip访问\n listen_addresses = \u0026ldquo;0.0.0.0\u0026rdquo;\n 修改需要重启postgresql\n启动  postgres -D /var/lib/pgsql/data \u0026gt;logfile 2\u0026gt;\u0026amp;1 \u0026amp;\n 登录控制台 会使用当前系统用户postgres访问，系统提示符会变成\u0026rsquo;postgres=#\u0026rsquo;\n psql\n 修改密码\n \\password postgres\n 创建用户\n CREATE USER dbuser WITH PASSWORD \u0026lsquo;password\u0026rsquo;;\nCREATE DATABASE example OWNER dbuser;\nGRANT ALL PRIVILEGES ON DATABASE example to dbuser;\n 客户端权限配置文件 默认只允许本地客户端连接，需要修改pg_hba.conf文件，\n host all all 127.0.0.1/32 trust\n改为\nhost all all 0.0.0.0 /0 trust\n 客户端连接  psql -U dbuser -d example -h 127.0.0.1 -p 5432\n ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/install-postgresql-on-centos/","tags":["DevOps","Linux"],"title":"Centos 上安装 Postgresql"},{"categories":["笔记"],"contents":"最近Redis的使用中用的到key可能比较长，但是Redis的官方文档没提到key长度对性能的影响，故简单做了个测试。\n环境 Redis和测试程序都是运行在本地，不看单次的性能，只看不同的长度堆读写性能的影响。\n测试方法 使用长度分别为10, 100, 500, 1000, 2500, 5000, 7500, 10,000, and 20,000的key，value长度1000，读写1000次。\n结果 从结果来看随着长度的增加，读写的耗时都随之增加。\n 长度为10：写平均耗时0.053ms，读0.040ms 长度为20000：写平均耗时0.352ms，读0.084ms  测试代码 源码\n/** * Created by addo on 2017/3/16. */ public class RedisTest { private static String[] keys = new String[1000]; private static String randomString(int length) { Random random = new Random(); char[] chars = \u0026#34;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#34;.toCharArray(); StringBuilder key = new StringBuilder(length); int i = length; while (i \u0026gt; 0) { key.append(chars[random.nextInt(chars.length)]); i--; } return key.toString(); } private static void write(Jedis jedis, int length) { long start = System.currentTimeMillis(); String key = null; String value = null; for (int i = 0; i \u0026lt; 1000; i++) { key = randomString(length); keys[i] = key; value = randomString(1000); jedis.set(key, value); } System.out.println(String.format(\u0026#34;put key length %d with value length 1000 in 1000 tims costs: %d ms\u0026#34;, length, System.currentTimeMillis() - start)); } private static void read(Jedis jedis, int length) { long start = System.currentTimeMillis(); for (int i = 0; i \u0026lt; keys.length; i++) { jedis.get(keys[i]); } System.out.println(String.format(\u0026#34;get key length %d with value length 1000 in 1000 tims costs: %d ms\u0026#34;, length, System.currentTimeMillis() - start)); } public static void main(String[] args) throws InterruptedException { Jedis jedis = new Jedis(\u0026#34;localhost\u0026#34;, 6379); int[] lengths = {10, 100, 500, 1000, 2500, 5000, 7500, 10000, 20000}; for (int i = 0; i \u0026lt; lengths.length; i++) { write(jedis, lengths[i]); read(jedis, lengths[i]); keys = new String[1000]; jedis.flushAll(); } System.out.println(); } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/redis-performance-key-length/","tags":["Java"],"title":"Key长度对Redis性能影响"},{"categories":["笔记"],"contents":"其实标题我想用《为什么foreach边循环边移除元素要用Iterator？》可是太长。\n不用Iterator，用Collection.remove()，会报ConcurrentModificationException错误。\nfor(Integer i : list) { list.remove(i); //Throw ConcurrentModificationException } 其实使用foreach的时候，会自动生成一个Iterator来遍历list。不只是remove，使用add、clear等方法一样会出错。\n拿ArrayList来说，它有一个私有的Iterator接口的内部类Itr：\nprivate class Itr implements Iterator\u0026lt;E\u0026gt; { int cursor; // index of next element to return  int lastRet = -1; // index of last element returned; -1 if no such  int expectedModCount = modCount; //sevrval methods } 使用Iterator来遍历ArrayList实际上是通过两个指针来遍历ArrayList底层的数组：cursor是下一个返回的元素在数组中的下标；lastRet是上一个元素的下标。还有一个重要的expectedModCount使用的是ArrayList的modCount的（modCount具体是什么意思下文会提到）。\n从Itr的实现来看，有三种情况会抛出ConcurrentModificationException：\n cursor超出了数组的最大下标 expectedModCount不等于modCount 删除元素最终还是调用ArrayList的remove方法，此方法可能会抛出IndexOutOfBoundsException  expectedModCount不等于modCount 开头所说的问题正是是第二种情况下出现的。modCount简单说记录了Collection被修改的次数：添加或者删除元素。\n假如在foreach循环中删除元素，且此时modCount等2：\n 循环开始创建新Itr实例，expectedModCount=modCount=2 使用ArrayList.remove()删除元素，modCount加1 继续调用next()方法指向下一个元素，此时检查expectedModCount是否等于modCount，不等则抛ConcurrentModificationException  public E next() { checkForComodification(); int i = cursor; if (i \u0026gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i \u0026gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } 上面提到Iterator的实现中删除元素实际调用的还是ArrayList.remove()方法，为什么不会抛错？\nItr的remove方法在调用ArrayList.remove()之后，会更新expectedModCount。\npublic void remove() { if (lastRet \u0026lt; 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/remove-element-while-looping-collection/","tags":["Java"],"title":"遍历 Collection 时删除元素"},{"categories":["笔记"],"contents":"volatile通过保证对变量的读或写都是直接从内存中读取或直接写入内存中，保证了可见性；但是volatile并不足以保证线程安全，因为无法保证原子性，如count++操作：\n 将值从内存读入寄存器中 进行加1操作，内存保存到寄存器中 结果从寄存器flush到内存中  借用一张图来看：\n不是volatile的变量的指令执行顺序是1-\u0026gt;2-\u0026gt;3；而声明为volatile的变量，顺序是1-\u0026gt;23。从这里看，volatile保证了一个线程修改了volatile修饰的变量，变化会马上体现在内存中。线程间看到的值是一样的。\n上面说了无法保证原子性是指：多核cpu，线程A执行了指令1，线程B也执行了指令1。A进行了加1操作，结果写入寄存器同时flush到内存；随后B也执行了同样的操作。count本来应该的结果是加2，但是却只加了1。原因就是我们通常所指的读和写不是原子操作。我们最希望看到的是123同时执行，手段就是sychronized或者java.util.concurrent包中的原子数据类型。\n简单拿AtomicInteger来看，其中的一个int类型的value字段声明为volatile，保证了123同时执行。\n参考：Java Volatile\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/deep-in-java-volatile-keywork/","tags":["Java"],"title":"Java Volatile关键字"},{"categories":["笔记"],"contents":"Haproxy为多个域名配置SSL\n生成自签名证书 sudo mkdir /etc/ssl/atbug.com sudo openssl genrsa -out /etc/ssl/atbug.com/atbug.com.key 1024 sudo openssl req -new -key /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.csr sudo openssl x509 -req -days 365 -in /etc/ssl/atbug.com/atbug.com.csr -singkey /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.crt sudo openssl x509 -req -days 365 -in /etc/ssl/atbug.com/atbug.com.csr -signkey /etc/ssl/atbug.com/atbug.com.key -out /etc/ssl/atbug.com/atbug.com.crt sudo cat /etc/ssl/atbug.com/atbug.com.crt /etc/ssl/atbug.com/atbug.com.key | sudo tee /etc/ssl/atbug.com/atbug.com.pem Haproxy配置 frontend https bind *:443 ssl crt /etc/ssl/atbug.com/atbug.com.pem option tcplog mode http #option forwardfor ###atbug-https acl atbug-https hdr_beg(host) -i test.atbug.com use_backend rome-atbug-https-backend if atbug-https backend rome-atbug-https-backend balance roundrobin mode http option ssl-hello-chk server node-1 ip:port cookie dw2-vm-test-apps003 check inter 2000 rise 3 fall 3 weight 50 ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/haproxy-multi-host-with-ssl/","tags":["DevOps"],"title":"Haproxy虚拟主机SSL"},{"categories":["笔记"],"contents":"这是工作中遇到的一个问题：测试环境部署出错，报了下面的问题。\nCaused by: java.lang.IllegalArgumentException: Result Maps collection already contains value for xxx.xxx.xxxRepository.BaseResultMap at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:802) at org.apache.ibatis.session.Configuration$StrictMap.put(Configuration.java:774) at org.apache.ibatis.session.Configuration.addResultMap(Configuration.java:556) at org.apache.ibatis.builder.MapperBuilderAssistant.addResultMap(MapperBuilderAssistant.java:217) at org.apache.ibatis.builder.ResultMapResolver.resolve(ResultMapResolver.java:47) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:285) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElement(XMLMapperBuilder.java:252) at org.apache.ibatis.builder.xml.XMLMapperBuilder.resultMapElements(XMLMapperBuilder.java:244) at org.apache.ibatis.builder.xml.XMLMapperBuilder.configurationElement(XMLMapperBuilder.java:116) 检查了对应的mapper文件和java文件，已经8个多月没有修改过了。也检查了内容，没有发现重复的BaseResultMap；select中也resultMap的引用也都正确。\n其实到最后发现跟代码一丁点关系都没有，是部署的时候没有删除旧版本的代码导致两个不同版本的jar同时存在，相应的mapper文件也有两个。\n看了下源码，mybatis在创建SessionFactoryBean解析xml时候，会把xml中的resultMap放入到一个HashMap的子类StrictMap中，key是mapper的namespace与resultmap的id拼接成的。\nStrictMap在put元素的时候，会检查map中是否已存在key。\npublic void addResultMap(ResultMap rm) { resultMaps.put(rm.getId(), rm); checkLocallyForDiscriminatedNestedResultMaps(rm); checkGloballyForDiscriminatedNestedResultMaps(rm); } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/duplicate-resultmap-in-mybatis-mapper/","tags":["Java"],"title":"mybatis报错“Result Maps collection already contains value for ***”"},{"categories":["笔记"],"contents":"这是实际使用时遇到的问题：kafka api的版本是0.10，发现有重复消费问题；检查log后发现在commit offset的时候发生超时。\nAuto offset commit failed for group test: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records. 15:12:12.364 [main] WARN o.a.k.c.c.i.ConsumerCoordinator - Auto offset commit failed for group test: Commit offsets failed with retriable exception. You should retry committing offsets. 看了Kafka的API文档，发现0.10中提供了新的配置max.poll.records：\nThe maximum number of records returned in a single call to poll(). type: int default: 2147483647 如果生产端写入很快，消费端处理耗时。一个batch的处理时间大于session.timeout.ms，会导致session time out，引起offset commit失败。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/offset-be-reset-when-consuming/","tags":["Kafka","Java"],"title":"消费时offset被重置导致重复消费"},{"categories":["笔记"],"contents":"TheadPoolExecutor源码分析 ThreadPoolExecutor是多线程中经常用到的类，其使用一个线程池执行提交的任务。\n实现 没有特殊需求的情况下，通常都是用Executors类的静态方法如newCachedThreadPoll来初始化ThreadPoolExecutor实例：\npublic static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } 从Executors的方法实现中看出，BlockingQueue使用的SynchronousQueue，底层使用了栈的实现。值得注意的是，这个SynchronousQueue是没有容量限制的，Executors也将maximumPoolSize设为Integer.MAX_VALUE。\nThreadPoolExecutor的构造方法：\n按照javadoc的解释：\n corePoolSize是池中闲置的最小线程数 maximumPoolSize是池中允许的最大线程数 keepAliveTime是线程数大于最小线程数时，过量闲置线程的最大存活时间 unit是上面存活时间的单位 workQueue是用来暂时保存运行前的任务  public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue) public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command); } 除去第一个做任务非空检查的if。\n第二个if，检查当前使用的线程数是否超过corePoolSize。未超过，调用addWorker并指定第二个参数为true。addWorker会再次检查线程数是否超过corePoolSize，如果还未超过，则创建一个新的线程执行任务。\n第三个if，当目前使用的线程数大于等于corePoolSize，将任务保存到workQueue中。保存成功，再次检查是否需要再创建一个线程。\n最后一个else，调用addWorker并指定第二个参数为false。在创建线程前，检查当时线程数是否超过maximumPoolSize，为超过则创建一个新的线程。\nprivate boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary.  if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; ! (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl  if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop  } } ... } 问题 一般场景下，不能使用Integer.MAX_VALUE如此大的线程数。所以需要使用构造器自己进行实例化。\n如指定corePoolSize=5、maximumPoolSize=20\n、keepAliveTime=60L、unit=TimeUnit.SECONDS、workQueue=new SynchronousQueue()。\n但是实际执行的时候，线程数一直是5。\n回头看ThreadPoolExecutor的实现，如果想要达到我们想要的效果需要程序进入最后的那个else。那重点就在第三个if里的workQueue.offer(command)。\n看BlockingQueue接口中该方法的描述：将元素插入到队列中，没有超过容量限制则插入并返回true。\n而使用的SynchronousQueue底层实现使用的栈没有容量限制，这就是为什么线程池中的线程数一直是corePoolSize。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/threadpoolexecutor-sourcecode-analysis/","tags":["Java"],"title":"TheadPoolExecutor源码分析"},{"categories":["笔记"],"contents":"Producer初始化 初始化KafkaProducer实例，同时通过Config数据初始化MetaData、NetWorkClient、Accumulator和Sender线程。启动Sender线程。\nMetaData信息 记录Cluster的相关信息，第一次链接使用Config设置，之后会从远端poll信息回来，比如host.name等信息。\nAccumulator实例 Accumulator持有一个Map实例，key为TopicPartition（封装了topic和partition信息）对象，Value为RecordBatch的Deque集合。\nNetworkClient实例 通过MetaData信息初始化NetworkClient实例，NetworkClient使用NIO模型。\nSender线程 sender持有NetworkClient和Accumulator实例，在Producer实例初始化完成之后，持续地将Accumulator中的Batch数据drain到一个List中，调用NetworkClient进行发送。\n发送 调用Producer实例进行消息发送，首先将消息序列化之后追加到Accumulator的Deque的最后一个batch中，之后唤醒sender-\u0026gt;client-\u0026gt;Selector进行消息发送。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/kafka-java-producer-model/","tags":["Java","Kafka"],"title":"Kafka Java生产者模型"},{"categories":["笔记"],"contents":"最近有个需求需要主动的去清理部分缓存，考虑的原子性的问题，用Lua脚本进行实现。\nLua脚本\nlocal count = 0 for _,k in ipairs(redis.call(\u0026#39;KEYS\u0026#39;, ARGV[1])) do redis.call(\u0026#39;DEL\u0026#39;, k) count = count + 1 end return count shell运行\nredis-cli --eval file.lua ,[KEY PATTERN] #sample: 清理所有key以Test开头的记录 redis-cli --eval clear.lua , Test* Java\nJedis jedis = new Jedis(\u0026#34;127.0.0.1\u0026#34;, 6379); URL resource = Resources.getResource(\u0026#34;META-INF/scripts/clear.lua\u0026#34;); String lua = Resources.toString(resource, Charsets.UTF_8); Object eval = jedis.eval(lua, 0, \u0026#34;Name*\u0026#34;); System.out.println(eval); ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/clean-speicified-keys-in-redis/","tags":["Java"],"title":"Redis清理缓存"},{"categories":["笔记"],"contents":"概述 当使用Flume的时候，每个流程都包含了输入源、通道和输出。一个典型的例子是一个web服务器将事件通过RPC（搬入AvroSource）写入输入源中，输入源将其写入MemoryChannel，最后HDFS Sink消费事件将其写入HDFS中。\nMemeoryChannel提供了高吞吐量但是在系统崩溃或者断电时会丢失数据。因此需要开发一个可持久话通道。FileChannel是在FLUME-1085里实现的。目标是提供一个高可用高吞吐量的通道。FileChannle保证了在失误提交之后，在崩溃或者断电后不丢失数据。\n需要注意的是FileChannel自己不做任何的数据复制，因此它只是和基本的磁盘一样高可用。使用FileChannle的用户需要购买配置更多的硬盘。硬盘最好是RAID、SAN或者类似的。\n很多需要通过损失少量的数据（每隔几秒将内存数据fsync到硬盘）换取高吞吐量。Flume团队决定使用另一种方式实现FileChannel。Flue是一个事务型的系统，在一次存或取的事务中可以操作多个事件。通过改变批量大小来控制吞吐量。使用大的批量，Flume可以以比较高的吞吐量传送数据，同时不丢失数据。批量的大小完全由客户端控制。使用RDBMS的用户对这种方式会比较熟悉。\n一个Flume事务由存或取组成，但不能同时做两种操作，同样提交和回滚也是一样。每个事务实现了存和取的方法。数据源将事件存入通道，输出从通道中将事件取出。\n设计 FileChannel在WAL（预写式日志）的基础上添加了一个内存队列。每个事务都被写成一个基于事务类型（存或取）的WAL，内存队列也相应的被更新。每次是事务提交，正确的文件被fsync保证数据被真正地保存到磁盘上，同时该事件的指针也被保存到了内存队列中。这个队列提供的功能跟其他队列没有区别：管理那些还没有被输出消费的事件。在取的过程中，指针被从队列中删除。事件直接从WAL中读取。得益于当前大容量的RAM，从操作系统的文件缓存中读取很常见。\n在系统崩溃之后，WAL可以被重现到队列中保持原来的状态，没有被提交的事务会丢失。重现WAL是耗时的，因此队列也被周期性地写到磁盘上。写队列到磁盘被称作checkpoint。崩溃后，从磁盘读取队列。只有队列保存到磁盘之后提交的事务被重现，这样可以显著的减少需要读取的WAL的数量。\n例如，如下有两个事件的通道：\nWAL包含了三个重要的元素：事务id、序列号和事件数据。每个事务都有一个唯一的事务id，每个事件都有一个唯一的序列号。事务id只被用来标识事务中的一组事件，序列号在重演日志的时候被用到。上面的例子中，事务id是1，序列号是1、2、3。\n当队列被保存到硬盘后 \u0026ndash; 一次checkpoint \u0026ndash; 序列号自动增加并同样被保存。在重启时，队列最先被从硬盘上加载，所有序列号大于队列的WAL项被重现。在checkpoint操作时，channle被锁住以保证没有存取操作改变它的状态。如果允许修改，会导致保存到硬盘上的队列快照不一致。\n上面例子中的队列，checkpoint发送在事务1提交之后，因此事件a、b的指针和序列号4被保存到硬盘。\n之后，事件a在事务2中被从队列中取出：\n如果这时系统崩溃，队列的checkpoint从硬盘中加载。注意这个checkpoint发生在事务2之前，事件a、b的指针存在队列中。因此WAL中序列号大于4的已提交的事务被重现，事件a指针被从队列中删除。\n上面的设计有两点没提到。checkpoint时发生的存和取操作会丢失。假设checkpoint在取事件a之后发生：\n如果这时系统崩溃，根据上面的设计，事件b指针保存在队列中，所有序列号大于5的WAL项被重现：事务2的回滚被重现。但是事务2的取操作不会被重现。因此事件a指针不会被放回队列因而导致数据丢失。存的场景也类似。因此在队列checkpoint的时候，进行中的事务操作也会被重现，这样这种情况能被正确处理。\n实现 FileChannel被保存在flume项目的flume-file-channel模块中，他的java包名是org.apache.flume.channel.file。上面提到队列被叫做FlumeEventQueue，WAL被叫做 Log。队列是一个环形数组，使用Memory Mapped File。WAL是一组以LogFile或其子类序列化的文件。\n总结 FileChannle在硬件、软件和系统故障下的持久化并同时保证高吞吐量。如果这亮点都看中的话，FileChannel是推荐使用的通道。\n原文\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/flume-filechannel-overview/","tags":["DevOps"],"title":"Flume - FileChannel （一）"},{"categories":["笔记"],"contents":"AMQPConnection 实例初始化 创建Connection时会通过FrameHandlerFacotry创建一个SocketFrameHandler，SocketFrameHandler对Socket进行了封装。\npublic AMQConnection(ConnectionParams params, FrameHandler frameHandler) { checkPreconditions(); this.username = params.getUsername(); this.password = params.getPassword(); this._frameHandler = frameHandler; this._virtualHost = params.getVirtualHost(); this._exceptionHandler = params.getExceptionHandler(); this._clientProperties = new HashMap\u0026lt;String, Object\u0026gt;(params.getClientProperties()); this.requestedFrameMax = params.getRequestedFrameMax(); this.requestedChannelMax = params.getRequestedChannelMax(); this.requestedHeartbeat = params.getRequestedHeartbeat(); this.shutdownTimeout = params.getShutdownTimeout(); this.saslConfig = params.getSaslConfig(); this.executor = params.getExecutor(); this.threadFactory = params.getThreadFactory(); this._channelManager = null; this._brokerInitiatedShutdown = false; this._inConnectionNegotiation = true; // we start out waiting for the first protocol response  } 启动连接 初始化WorkService和HeartBeatSender。\n创建一个channel0的AMQChannel，这个channel不会被ChannelManager管理。\n首先channel0会将一个BlockingRpcContinuation作为当前未完成的Rpc请求，用于接收handshake的响应。\n然后channel0会向socket中写入一条只有header的消息作为handshake，header中包含了客户端的版本号。\n紧接着会启动主循环线程，主循环会通过SocketFrameHandler从socket接收字节流。此时接收到的第一条数据是服务端响应handshake返回的Connection.Start信息（包含服务端版本、机制、基础信息）。\n主循环线程启动后，主线程会阻塞地等待服务端的handshake响应。拿到响应之后会对服务器回传的信息进行比对，然后发送Connection.StartOK的信息去服务端（这个请求也还是阻塞式的），等待服务端回传Connection.Tune（包含最大channel数、最大frame长度和heartbeat间隔）。将这些信息与实例初始化是的设置信息进行对比，初始化ChannelManager\n紧接着发送Connection.TuneOk和Connection.Open消息去服务端，完成connection的建立。\n Connection \u0026gt; MainLoop \u0026gt; readFrame\n 消息体 Frame是对AMQP消息的封装：包含frame的type、channel号、消息内容\n type|channelNumber|payloadSize|payload|frameEndMarker\n Payload包含了消息类型、消息头和消息主题\n method|header|body\n 消息发送和接收 消息的发送和接收都要channel来完成。\n创建Channel 通过Connection的ChannelManager来创建Channel，通过指定的ChannelNumber或者由分配器分配。创建好的Channel实例会放入ChannelManager的Map中，key为ChannelNumber。由此可见Channel是Connection唯一的。\npublic ChannelN createChannel(AMQConnection connection); public ChannelN createChannel(AMQConnection connection, int channelNumber); private ChannelN addNewChannel(AMQConnection connection, int channelNumber); protected ChannelN instantiateChannel(AMQConnection connection, int channelNumber, ConsumerWorkService workService); Channel实例化之后会调用Channel.open方法，发送Channel.Open去服务端（阻塞式），等待服务端响应Channel.OpenOk。\n消息发送 Channel.transmit 发送消息，调用AMQCommand.transmit完成发送。\nAMQCommand.transmit将消息封装成Frame，通过connection的SocketFrameHandler写入OutpuStream。\n消息接收 主循环线程在链接创建完成后会监听socket，从InputStream中读取二进制流封装成Frame。通过Frame中的ChannelNumber从ChannelManager中获取对应的Channel实例处理Frame。\nwhile (_running) { Frame frame = _frameHandler.readFrame(); if (frame != null) { _missedHeartbeats = 0; if (frame.type == AMQP.FRAME_HEARTBEAT) { // Ignore it: we\u0026#39;ve already just reset the heartbeat counter.  } else { if (frame.channel == 0) { // the special channel  _channel0.handleFrame(frame); } else { if (isOpen()) { // If we\u0026#39;re still _running, but not isOpen(), then we  // must be quiescing, which means any inbound frames  // for non-zero channels (and any inbound commands on  // channel zero that aren\u0026#39;t Connection.CloseOk) must  // be discarded.  ChannelManager cm = _channelManager; if (cm != null) { cm.getChannel(frame.channel).handleFrame(frame); } } } } } else { // Socket timeout waiting for a frame.  // Maybe missed heartbeat.  handleSocketTimeout(); } } Channel会使用已经准备好的AMQCommand处理Frame，并未下一个Frame准备新的AMQCommand。\npublic void handleFrame(Frame frame) throws IOException { AMQCommand command = _command; if (command.handleFrame(frame)) { // a complete command has rolled off the assembly line  _command = new AMQCommand(); // prepare for the next one  handleCompleteInboundCommand(command); } } AMQCommad会使用CommandAssembler依次从Frame的payload中检出对应的Method、Header和Body。如果检出了Body，整个Frame会被检出完成。如过未完成，会进入主循环再次处理直至完成。\npublic synchronized boolean handleFrame(Frame f) throws IOException { switch (this.state) { case EXPECTING_METHOD: consumeMethodFrame(f); break; case EXPECTING_CONTENT_HEADER: consumeHeaderFrame(f); break; case EXPECTING_CONTENT_BODY: consumeBodyFrame(f); break; default: throw new AssertionError(\u0026#34;Bad Command State \u0026#34; + this.state); } return isComplete(); } Frame被检出完后，会根据Method的类型进入不同的异步处理流程。\nMethod在channel打开和关闭的情况下会以下的可能：\n Channel打开：Basic.Deliver, Basic.Return, Basic.Flow, Basic.Ack, Basic.Nack, Basic.RecoveryOk, Basic.Cancel\nChannel关闭：Channel.CloseOk\n 生产和消费 生产 调用Channel.basicPublish()方法，指定exchange、routingKey等信息，消息属性、消息体。封装成Baisc.Publish，放入AMQCommand，最后调用transmit方法完成发送。参考消息发送\npublic void basicPublish(String exchange, String routingKey, boolean mandatory, boolean immediate, BasicProperties props, byte[] body) throws IOException { if (nextPublishSeqNo \u0026gt; 0) { unconfirmedSet.add(getNextPublishSeqNo()); nextPublishSeqNo++; } BasicProperties useProps = props; if (props == null) { useProps = MessageProperties.MINIMAL_BASIC; } transmit(new AMQCommand(new Basic.Publish.Builder() .exchange(exchange) .routingKey(routingKey) .mandatory(mandatory) .immediate(immediate) .build(), useProps, body)); } 消费 创建QueueingConsumer实例，然后调用Channel.basicConsume方法。\nqueueingConsumer = new QueueingConsumer(channel); channel.basicConsume(\u0026#34;queue_name\u0026#34;, queueingConsumer); new Thread() { @Override public void run() { while (true) { try { final QueueingConsumer.Delivery delivery = queueingConsumer.nextDelivery(); new Thread() { @Override public void run() { try{ delivery.getEnvelope();//消息头  delivery.getProperties();//消息属性  delivery.getBody()；//消息体  }finally{ //channel.basicAck();  //channel.basicNack()  } } }.start(); } catch (InterruptedException e) { e.printStackTrace(); } } } }.start(); QueueingConsumer实现了Consumer接口。\nChannel.basicConsume方法会封装Channel.Consume消息发送到服务端（阻塞式），等待服务端的Channel.ConsumeOk响应（包含了服务端为Consumer分配的ConsumerTag）。然后将QueueingConsumer放入Map中，key为ConsumerTag。consumer是Channel唯一。\n当客户端接收到消息，参考消息接收。Basic.Deliver类型的消息（consumerTag、deliveryTag、redelivered、exchange、routingKey）会进入消费处理流程。Channel根据ConsumerTag从Map中获取对应的QueueConsumer实例，由Channel的ConsumerDispatcher通过Connection初始化的WorkService创建新的处理线程，调用QueueConsumer实例的handleDelivery方法。QueueConsumer将消息封装成Delivery对象，放入BlockingQueue中。\n消费线程等待新的Delivery（阻塞式），之后创建新的线程完成消息的处理。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/deep-in-rabbitmq-java-client/","tags":["Java"],"title":"探索Rabbitmq的Java客户端"},{"categories":["笔记"],"contents":"最近又个项目，checkout之后，没做任何改动前git status发现已经有modified了，通过git diff发现有两种改动：\n - warning: CRLF will be replaced by LF in **\n- 删除并添加的同样的行\n 使用git diff -w却没有改动；使用git diff –ws-error-highlight=new,old发现行尾有**^M**\n我本人用的是Linux，其他同事有用Windows，问题就出在平台上。\n Windows用CR LF来定义换行，Linux用LF。CR全称是Carriage Return ,或者表示为\\r, 意思是回车。 LF全称是Line Feed，它才是真正意义上的换行表示符。\n git config中关于CRLF有两个设定：core.autocrlf和core.safecrlf。\n 一、AutoCRLF\n#提交时转换为LF，检出时转换为CRLF\ngit config –global core.autocrlf true\n#提交时转换为LF，检出时不转换\ngit config –global core.autocrlf input\n#提交检出均不转换\ngit config –global core.autocrlf false\n二、SafeCRLF\n#拒绝提交包含混合换行符的文件\ngit config –global core.safecrlf true\n#允许提交包含混合换行符的文件\ngit config –global core.safecrlf false\n#提交包含混合换行符的文件时给出警告\ngit config –global core.safecrlf warn\n 这种情况，把autocrlf置为false，safecrlf也置为false，可以忽略不同平台上回车换行的差异。\n设置完成后，发现第二个问题还是存在。这时要查看代码库中是否存在.gitattributes文件，如果存在打开.gitattributes。\n在该项目的.gitattributes中有几种设定：\n  * text=auto !eol 所有带有text属性的文件使用auto的EOL，但是不指定EOL方式（CRLF or LF） [path]/file -text 取消文件的text属性   如果不指定EOL，git会使用config中的core.eol。如果未设置core.eol，git会使用平台默认的回车换行。\n优先级 core.autocrlf \u0026gt; text=auto + core.eol。\n以下设置的结果相同：\n core.autocrlf=true\ncore.eol=CRLF 同时 * text=auto !eol\n* text=auto CRLF\n .gitattributes设置会影响checkout和checkin\n最后的解决方案是直接清空了.gitattributes内容，这个问题应该是在项目从svn迁移到git时迁移工具自动添加的结果。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/crlf-in-git/","tags":["DevOps"],"title":"Git回车换行"},{"categories":["笔记"],"contents":"HashSet是一个包含非重复元素的集合，如何实现的，要从底层实现代码看起。\n背景 首先非重复元素如何定义，看Set的描述：\n More formally, sets contain no pair of elements e1 and e2 such that e1.equals(e2), and at most one null element.\nSet不会找到两个元素，并且两个元素满足e1.equals(e2)为true；并且最多只有一个null元素。\n 如果没有重写equals方法，查看Object类中equal方法的实现，==比较的其实是两个对象在内存中的地址。\npublic boolean equals(Object obj) { return (this == obj); } 说起equals方法，就不得不说hashCode方法了。Java中对于hashCode有个常规协定\n The general contract of hashCode is:\n  Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application.\n  If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result.\n  It is not required that if two objects are unequal according to the equals(java.lang.Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results. However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables.\n  程序执行期间，在同一个对象上执行多次hashCode方法，都返回相同的整数，前提是equals比较中所使用的字段没有被修改。跨应用中的hashCode方法调用返回的整数不要求相同。\n  如果两个对象根据equals方法比较相同，那hashCode返回的整数也必须相同。\n  如果两个对象equals方法比较不相同，调用hashCode返回的整数不需要不同。但是程序员应该知道为不相等的对象生成不同的整数可以提高哈希表的性能。\n   HashSet的底层实现 HashSet的底层是通过HashMap实现的，将元素作为map的key以达到去重的目的，value使用的是同一个虚拟的Object实例。\nprivate transient HashMap\u0026lt;E,Object\u0026gt; map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); public boolean add(E e) { return map.put(e, PRESENT)==null; } HashMap的底层实现 {% asset_img hashmap-structure.jpg %}\n到最后我们要看HashMap的实现了，简单说就是一个数组+链表的结合。\n 默认初始容量16 默认负荷系数0.75 Entry数组 大小 阈值：初始值等于初始容量 负荷系数  static final int DEFAULT_INITIAL_CAPACITY = 1 \u0026lt;\u0026lt; 4; static final float DEFAULT_LOAD_FACTOR = 0.75f; static final Entry\u0026lt;?,?\u0026gt;[] EMPTY_TABLE = {}; transient Entry\u0026lt;K,V\u0026gt;[] table = (Entry\u0026lt;K,V\u0026gt;[]) EMPTY_TABLE; transient int size; int threshold; final float loadFactor; Entry元素 Entry是链表的结果，key为Map中的key，value为Map中的value，hash为key的hash结果，next为下一个元素。\nstatic class Entry\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final K key; V value; Entry\u0026lt;K,V\u0026gt; next; int hash; } 添加元素  如果数组为空（即map初始化后第一次添加元素）扩充table 如果key为null，则调用putForNullKey方法，null位于table的下标0处 算出key的hash值 通过hash值算出元素在table中的下标值  如果该位置元素不为空，然后需要比较元素的hash值和上面算出的hash值是否相等，同时元素的key对象和要出入的key是否为同一对象（相同的地址 ==比较为true）或者equals方法是否为true。如果满足条件，则更新该entry的value值；若不满足则遍历整个链表。 如果为空直接添加新的entry。    JDK8此处有更新，见末尾\npublic V put(K key, V value) { if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; addEntry(hash, key, value, i); return null; } 扩充table 对toSzie算出最小的2的幂值，用了Integer.highestOneBit((toSize -1) \u0026laquo; 1)。减一之后左移一位，然后取最高位值，其余为补0。\n为什么数组长度必须为2的幂值，请继续看。\n/** * 扩充table **/ private void inflateTable(int toSize) { // Find a power of 2 \u0026gt;= toSize  int capacity = roundUpToPowerOf2(toSize); threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; initHashSeedAsNeeded(capacity); } 计算hash值 hashSeed值为0，将key的hashCode值做多次位移和异或运算\nfinal int hash(Object k) { int h = hashSeed; if (0 != h \u0026amp;\u0026amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by  // constant multiples at each bit position have a bounded  // number of collisions (approximately 8 at default load factor).  h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } 计算元素位置 这里的逻辑很简单：将hash值跟数组长度-1做了按位与。\n在进行查找的时候是通过key的hash值，如果我们将元素的位置分布得尽量均匀一些，尽量做到每个位置上只有一个元素，达到O(1)的查找。这种查找通过取余就可以做到，在Java中如何做到比较快的取余呢，答案是位与运算。\n上面扩充数组的时候我们保证长度为2的幂值，那减一之后就是每位都是1。做位与运算就能保证低位不同的hash值会落在不同的位置上，降低冲突（碰撞），最大程度做到均匀分布，减少链表的出现（查找变成O(n)）。\nstatic int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \u0026#34;length must be a non-zero power of 2\u0026#34;;  return h \u0026amp; (length-1); } 添加entry 添加新的元素时要检查元素个数是否达到阈值，否则要做扩容处理，新table的容量为当前table长度的两倍。\nvoid addEntry(int hash, K key, V value, int bucketIndex) { if ((size \u0026gt;= threshold) \u0026amp;\u0026amp; (null != table[bucketIndex])) { resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } resize 新table的容量为当前table长度的两倍（table.length \u0026gt;= size），将旧数据中的数据迁移到新的数组中，迁移的过程中要重新计算元素在新数组中的位置。网上很多地方提到这个操作rehash，但我觉得reindex反而更恰当一些。JDK中对rehash有额外的定义，就是initHashSeedAsNeeded。当新的容量\u0026gt;=jdk.map.althashing.threshold的配置时，会重新计算key的hash值，即hash(e.key)。\nvoid resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); } reindex void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; for (Entry\u0026lt;K,V\u0026gt; e : table) { while(null != e) { Entry\u0026lt;K,V\u0026gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } } }  JDK8 update 添加元素的时候，如果发生哈希冲突，会遍历链表。加入链表的长度大于TREEIFY_THRESHOLD（默认为8），会将链表转成红黑树。\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node\u0026lt;K,V\u0026gt; e; K k; if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st  treeifyBin(tab, hash); break; } if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key  V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size \u0026gt; threshold) resize(); afterNodeInsertion(evict); return null; } \u0026lt;!----\u0026gt; final void treeifyBin(Node\u0026lt;K,V\u0026gt;[] tab, int hash) { int n, index; Node\u0026lt;K,V\u0026gt; e; if (tab == null || (n = tab.length) \u0026lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) \u0026amp; hash]) != null) { TreeNode\u0026lt;K,V\u0026gt; hd = null, tl = null; do { TreeNode\u0026lt;K,V\u0026gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } 同样，get(key)的时候也会相应的从树中查找元素。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/deep-in-implementation-of-hashset/","tags":["Java"],"title":"深入剖析 HashSet 和 HashMap 实现"},{"categories":["笔记"],"contents":"多线程下的单例模式的实现，顺便做了反汇编。\npublic class MySingleton { private static MySingleton INSTANCE; private MySingleton() { } public static MySingleton getInstance() { if (INSTANCE == null) { synchronized (MySingleton.class) { INSTANCE = new MySingleton(); } } return INSTANCE; } } Compiled from \u0026#34;MySingleton.java\u0026#34; public class MySingleton { public static MySingleton getInstance(); Code: 0: getstatic #2 // Field INSTANCE:LMySingleton; //+获得类的指定域，并压入栈顶  3: ifnonnull 32\t//+不为null时跳转到行号32  6: ldc_w #3 // class MySingleton //+常量值从常量池中推送至栈顶（宽索引），推送的为地址  9: dup //+复制栈顶数值，并且复制值进栈  10: astore_0 //+将栈顶数值（objectref）存入当前 frame的局部变量数组中指定下标(index）处的变量中，栈顶数值出栈。这里存的是MySingleton类定义的地址  11: monitorenter //+获得对象锁即MySingleton地址  12: new #3 // class MySingleton //+创建一个对象，并且其引用进栈  15: dup //+复制栈顶数值，并且复制值进栈  16: invokespecial #4 // Method \u0026#34;\u0026lt;init\u0026gt;\u0026#34;:()V //+调用超类构造方法、实例初始化方法、私有方法  19: putstatic #2 // Field INSTANCE:LMySingleton; //+为指定的类的静态域赋值  22: aload_0 //+当前frame的局部变量数组中下标为 index的引用型局部变量进栈，这里是MySingleton类定义的地址  23: monitorexit //+释放对象锁  24: goto 32\t//+跳转到行号32  27: astore_1 //+将栈顶数值（objectref）存入当前 frame的局部变量数组中指定下标(index）处的变量中，栈顶数值出栈。  28: aload_0 //+当前frame的局部变量数组中下标为 0的引用型局部变量进栈  29: monitorexit //+//+释放对象锁  30: aload_1 //+当前frame的局部变量数组中下标为 1的引用型局部变量进栈  31: athrow //+将栈顶的数值作为异常或错误抛出  32: getstatic #2 // Field INSTANCE:LMySingleton; //+获得类的指定域，并压入栈顶  35: areturn //+从方法中返回一个对象的引用  Exception table: from to target type 12 24 27 any 27 30 27 any } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/singleton-in-multi-threads-programming/","tags":["Java"],"title":"多线程下的单例模式+反汇编"},{"categories":["笔记"],"contents":"spring amqp的原生并没有对Kryo加以支持，Kryo的优点就不多说了。\ngit地址：https://github.com/addozhang/spring-kryo-messaeg-converter\npublic class KryoMessageConverter extends AbstractMessageConverter { public static final String CONTENT_TYPE = \u0026#34;application/x-kryo\u0026#34;; public static final String DEFAULT_CHARSET = \u0026#34;UTF-8\u0026#34;; private String defaultCharset = DEFAULT_CHARSET; private KryoFactory kryoFactory = new DefaultKryoFactory(); /** * Crate a message from the payload object and message properties provided. The message id will be added to the * properties if necessary later. * * @param object the payload * @param messageProperties the message properties (headers) * @return a message */ @Override protected Message createMessage(Object object, MessageProperties messageProperties) { byte[] bytes = null; Kryo kryo = kryoFactory.create(); Output output = new ByteBufferOutput(4096, 1024 * 1024); try { kryo.writeClassAndObject(output, object); bytes = output.toBytes(); } finally { output.close(); } messageProperties.setContentType(CONTENT_TYPE); if (messageProperties.getContentEncoding() == null) { messageProperties.setContentEncoding(defaultCharset); } return new Message(bytes, messageProperties); } @Override public Object fromMessage(Message message) throws MessageConversionException { Object content = null; MessageProperties properties = message.getMessageProperties(); if (properties != null) { if (properties.getContentType() != null \u0026amp;amp;\u0026amp;amp; properties.getContentType().contains(\u0026#34;x-kryo\u0026#34;)) { Kryo kryo = kryoFactory.create(); content = kryo.readClassAndObject(new ByteBufferInput(message.getBody())); } else { throw new MessageConversionException(\u0026#34;Converter not applicable to this message\u0026#34;); } } return content; } private class DefaultKryoFactory implements KryoFactory { @Override public Kryo create() { Kryo kryo = new Kryo(); return kryo; } } } ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/use-kryo-in-spring-amqp-serialization/","tags":["Java","Spring"],"title":"使用Kryo替换spring amqp的Java序列化"},{"categories":["笔记"],"contents":"工作中很多场景需要用到定时任务、延迟任务，常用的方法用crontab job、Spring的Quartz，然后扫描整张数据库表，判断哪些数据需要处理。控制的粒度没办法做到特定数据上。 后来就想到了Rabbitmq，Rabbitmq本来不没有延迟队列的功能，但是有个[Dead Letter Exchange](https://www.rabbitmq.com/dlx.html)功能。  DLX是指队列中的消息在下面几种情况下会变为死信（dead letter），然后会被发布到另一个exchange中。  在requeue=false的情况系，消息被client reject 消息过期 队列长度超过限制  有了DLX，就可以将需要延迟的操作设置下次执行时间（如消息的TTL时间）放入一个存储队列中，消息过期后会经由DLX进入监听的队列中。有消费方进行相关的操作，结束或者再次进入存储队列中。  Spring AMQP实现  Configuration:  \u0026lt;rabbit:connection-factory id=\"rabbitMQConnectionFactory\" requested-heartbeat=\"\" host=\"${rabbit.host}\" port=\"${rabbit.port}\" username=\"${rabbit.username}\" password=\"${rabbit.password}\" publisher-confirms=\"true\" channel-cache-size=\"10\"/\u0026gt; \u0026lt;rabbit:admin connection-factory=\"rabbitMQConnectionFactory\"/\u0026gt;    \u0026lt;!--声明延时队列--\u0026gt; \u0026lt;rabbit:queue id=\"delayQueue\" name=\"${rabbit.tracking.no.pre.track.delay.queue}\"\u0026gt; \u0026lt;rabbit:queue-arguments\u0026gt; \u0026lt;entry key=\"x-dead-letter-exchange\" value=\"tracking_dead_exchange\"/\u0026gt; \u0026lt;/rabbit:queue-arguments\u0026gt; \u0026lt;/rabbit:queue\u0026gt; \u0026lt;!--声明监听队列--\u0026gt; \u0026lt;rabbit:queue id=\"preTrackingQueue\" name=\"${rabbit.tracking.no.pre.track.queue}\"/\u0026gt;    \u0026lt;!--声明DLX--\u0026gt; \u0026lt;rabbit:topic-exchange name=\"tracking_dead_exchange\"\u0026gt; \u0026lt;rabbit:bindings\u0026gt; \u0026lt;rabbit:binding pattern=\"#\" queue=\"${rabbit.tracking.no.pre.track.queue}\"/\u0026gt; \u0026lt;/rabbit:bindings\u0026gt; \u0026lt;/rabbit:topic-exchange\u0026gt;     \u0026lt;rabbit:listener-container connection-factory=\"rabbitMQConnectionFactory\" concurrency=\"1\" prefetch=\"1\" acknowledge=\"auto\" message-converter=\"jackson2JsonMessageConverter\"\u0026gt; \u0026lt;rabbit:listener ref=\"trackingListener\" method=\"handleMessage\" queues=\"preTrackingQueue\"/\u0026gt; \u0026lt;/rabbit:listener-container\u0026gt;   Code:  @Component public class TrackingListener{ private Logger LOGGER = LoggerFactory.getLogger(TrackingListener.class);  @Autowired  private MessageConverter jackson2JsonMessageConverter;  @Autowired  private RabbitTemplate rabbitTemplate;  @Autowired  private Queue delayQueue; public void handleMessage(TrackingMessage trackingMessage){ LOGGER.info(\"In Function: TrackingListener.handleMessage(trackingMessage={})\", new Object[]{trackingMessage});  String expiration = 60*60*1000 + \"\"; if(trackingMessage.getRecordCount() == 2 \u0026amp;\u0026amp; trackingMessage.getStatus() == 0){ //更新运单及订单状态  trackingMessage.setStatus(1);  } MessageProperties messageProperties = new MessageProperties();  messageProperties.setExpiration(expiration); //one hour  messageProperties.setTimestamp(new Date());  rabbitTemplate.send(delayQueue.getName(), jackson2JsonMessageConverter.toMessage(trackingMessage, messageProperties));  LOGGER.info(\"End Function: TrackingListener.handleMessage()\"); ","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/rabbitmq-delay-queue-implementation/","tags":["Java"],"title":"Rabbitmq延迟队列实现"},{"categories":["笔记"],"contents":"Spring的功能越来越强大，同时也越来越臃肿。比如想快速搭建一个基于Spring的项目，解决依赖问题非常耗时。Spring的项目模板的出现就解决了这个问题，通过这个描述文件，可以快速的找到你所需要的模板。\n第一次认识SLF4J就是在这些项目模板里，它的全称是Simple Logging Facade for Java。从字面上可以看出它只是一个Facade，不提供具体的日志解决方案，只服务于各个日志系统。简单说有了它，我们就可以随意的更换日志系统（如java.util.logging、logback、log4j）。比如在开发的时候使用logback，部署的时候可以切换到log4j；如果关闭所有的log，切换到NOP就可以了。只需要更改依赖，提供日志配置文件，免去了修改代码的麻烦。\n首先看如何使用：\n[java] import org.slf4j.Logger; import org.slf4j.LoggerFactory;\npublic class HelloWorld { public static void main(String[] args) { Logger logger = LoggerFactory.getLogger(HelloWorld.class); logger.info(\u0026quot;Hello World\u0026quot;); } } [/java]\nSLF4J封装了使用起来和其他日志系统一样简单。上面提到过SLF4J不提供具体的日志解决方案，所以使用的时候除了要引用SLF4J包，还要引用具体的日志解决方案包（log4j、logging\u0026ndash;JDK提供、logback），还有所对应的binding包（slf4j-log4j_、slf4j-jdk14、logback-classic_）。\n以log4j为例，我们看SLF4J的实现方式。\nSLF4J类在初始化的时候会尝试从ClassLoader中org/slf4j/impl/StaticLoggerBinder.class。这个类比较特殊，每个binding包里都有。不同binding包里的StaticLoggerBinder类会去初始化一个相应的实例，如slf4j-log4j里：\n[java] /**\n 截取的部分代码 */ private StaticLoggerBinder() { loggerFactory = new Log4jLoggerFactory(); } [/java]  而Log4jLoggerAdapter实现了SLF4J的Logger接口，使用了Adapter模式对Log4j的Logger进行了封装并暴露了Logger的接口，Log4jLoggerFactory持有了Log4jLoggerAdapter的实例。\n[java] /**\n 截取的部分代码 */ public class Log4jLoggerFactory implements ILoggerFactory { public Logger getLogger(String name) { Logger slf4jLogger = null; // protect against concurrent access of loggerMap synchronized (this) { slf4jLogger = (Logger) loggerMap.get(name); if (slf4jLogger == null) { org.apache.log4j.Logger log4jLogger; if(name.equalsIgnoreCase(Logger.ROOT_LOGGER_NAME)) { log4jLogger = LogManager.getRootLogger(); } else { log4jLogger = LogManager.getLogger(name); } slf4jLogger = new Log4jLoggerAdapter(log4jLogger); loggerMap.put(name, slf4jLogger); } } return slf4jLogger; } } [/java]  具体的Log解决方案就不做剖析了。\n","image":"https://atbug.oss-cn-hangzhou.aliyuncs.com/2022/12/28/mocha.png","permalink":"https://atbug.com/about-slf4j/","tags":["Java"],"title":"关于SLF4J"}]